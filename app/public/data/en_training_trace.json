{
  "format_version": 1,
  "num_steps": 1000,
  "step_options": [
    50,
    100,
    500,
    1000
  ],
  "optimizer": {
    "name": "Adam",
    "beta1": 0.85,
    "beta2": 0.99,
    "eps": 1e-08,
    "base_learning_rate": 0.003,
    "schedule": "linear_decay(lr_t = lr * (1 - step / num_steps))"
  },
  "parameter_options": [
    {
      "id": "token_letter_a",
      "label": "Letter a token embedding",
      "matrix": "wte",
      "row_index": 0,
      "token_char_nfd": "a",
      "token_char_display": "a"
    },
    {
      "id": "lm_head_letter_e",
      "label": "Letter e LM Head parameter",
      "matrix": "lm_head",
      "row_index": 4,
      "token_char_nfd": "e",
      "token_char_display": "e"
    },
    {
      "id": "position_0",
      "label": "POS 0 position embedding",
      "matrix": "wpe",
      "row_index": 0
    },
    {
      "id": "attn_wq_row_0",
      "label": "W_Q row 0",
      "matrix": "attn_wq",
      "row_index": 0
    }
  ],
  "steps": [
    {
      "step": 0,
      "word": "yuheng",
      "loss": null,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0427,
            0.077,
            0.1084,
            0.0374,
            -0.0031,
            -0.0044,
            0.0974,
            -0.1123,
            -0.0332,
            -0.0499,
            0.0111,
            -0.0289,
            0.047,
            -0.0179,
            -0.0005,
            -0.0125
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0095,
            0.0649,
            0.0239,
            -0.1267,
            0.0429,
            -0.0061,
            0.0201,
            0.2039,
            -0.0525,
            -0.0731,
            0.0855,
            -0.0498,
            0.0639,
            0.0328,
            -0.0321,
            0.1262
          ]
        },
        "position_0": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0222,
            -0.0696,
            -0.1805,
            0.0569,
            0.0474,
            -0.0272,
            -0.0623,
            0.0715,
            0.1182,
            -0.1022,
            0.0458,
            0.1045,
            0.0399,
            0.0241,
            -0.1115,
            0.0766
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0452,
            0.0622,
            -0.0391,
            0.0445,
            0.0019,
            0.1169,
            0.0122,
            -0.0947,
            -0.016,
            0.0776,
            -0.0192,
            -0.03,
            0.1029,
            -0.0695,
            -0.0121,
            0.0205
          ]
        }
      }
    },
    {
      "step": 1,
      "word": "yuheng",
      "loss": 3.366,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0427,
            0.077,
            0.1084,
            0.0374,
            -0.0031,
            -0.0044,
            0.0974,
            -0.1123,
            -0.0332,
            -0.0499,
            0.0111,
            -0.0289,
            0.047,
            -0.0179,
            -0.0005,
            -0.0125
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1844,
            -0.0273,
            0.0245,
            -0.2916,
            -0.0707,
            -0.2509,
            0.0663,
            0.0859,
            -0.0466,
            0.0506,
            0.1003,
            -0.1104,
            0.1461,
            -0.2146,
            0.2424,
            -0.1908
          ],
          "after": [
            0.0125,
            0.0679,
            0.0209,
            -0.1237,
            0.0459,
            -0.0031,
            0.0171,
            0.2009,
            -0.0495,
            -0.0761,
            0.0825,
            -0.0468,
            0.0609,
            0.0358,
            -0.0351,
            0.1292
          ]
        },
        "position_0": {
          "grad": [
            0.0695,
            -0.0025,
            0.0618,
            -0.0431,
            -0.0386,
            0.1209,
            -0.0169,
            -0.0348,
            -0.0169,
            0.0416,
            -0.1464,
            0.0703,
            0.061,
            0.0985,
            0.1054,
            0.0802
          ],
          "after": [
            -0.0252,
            -0.0666,
            -0.1835,
            0.0599,
            0.0504,
            -0.0302,
            -0.0593,
            0.0745,
            0.1212,
            -0.1052,
            0.0488,
            0.1015,
            0.0369,
            0.0211,
            -0.1145,
            0.0736
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0001,
            -0.0001,
            -0.0005,
            0.0005,
            -0.0001,
            -0.0003,
            -0.0003,
            0.0004,
            -0.0002,
            -0.0003,
            0.0,
            -0.0,
            0.0003,
            -0.0005,
            0.0001
          ],
          "after": [
            0.0422,
            0.0592,
            -0.0361,
            0.0475,
            -0.0011,
            0.1199,
            0.0152,
            -0.0917,
            -0.019,
            0.0806,
            -0.0162,
            -0.033,
            0.1059,
            -0.0725,
            -0.0091,
            0.0175
          ]
        }
      }
    },
    {
      "step": 2,
      "word": "diondre",
      "loss": 3.4254,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0427,
            0.077,
            0.1084,
            0.0374,
            -0.0031,
            -0.0044,
            0.0974,
            -0.1123,
            -0.0332,
            -0.0499,
            0.0111,
            -0.0289,
            0.047,
            -0.0179,
            -0.0005,
            -0.0125
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0375,
            -0.1152,
            -0.1061,
            -0.0421,
            0.2436,
            -0.0002,
            -0.1576,
            0.0673,
            -0.0527,
            -0.1297,
            0.0955,
            -0.1169,
            -0.2402,
            0.0454,
            0.0111,
            0.0099
          ],
          "after": [
            0.0139,
            0.0706,
            0.0227,
            -0.1214,
            0.0443,
            -0.0012,
            0.0185,
            0.198,
            -0.0465,
            -0.0747,
            0.0795,
            -0.0438,
            0.0618,
            0.0372,
            -0.0371,
            0.131
          ]
        },
        "position_0": {
          "grad": [
            0.0185,
            -0.021,
            -0.0357,
            -0.1237,
            0.0818,
            -0.0985,
            0.0313,
            -0.0863,
            0.0734,
            0.0485,
            -0.1161,
            -0.0567,
            0.0054,
            0.1293,
            0.1044,
            0.0583
          ],
          "after": [
            -0.0277,
            -0.0641,
            -0.184,
            0.0627,
            0.0492,
            -0.0303,
            -0.0604,
            0.0774,
            0.1194,
            -0.1083,
            0.0518,
            0.1014,
            0.0348,
            0.0181,
            -0.1175,
            0.0706
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0001,
            -0.0,
            0.0003,
            0.0004,
            0.0,
            0.0004,
            0.0002,
            -0.0001,
            -0.0001,
            -0.0,
            -0.0001,
            0.0001,
            -0.0002,
            -0.0,
            -0.0002
          ],
          "after": [
            0.0392,
            0.0562,
            -0.0336,
            0.048,
            -0.0041,
            0.1204,
            0.0149,
            -0.0909,
            -0.0201,
            0.0835,
            -0.014,
            -0.0311,
            0.1041,
            -0.0732,
            -0.0071,
            0.0188
          ]
        }
      }
    },
    {
      "step": 3,
      "word": "xavien",
      "loss": 3.1791,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0642,
            -0.1189,
            -0.0163,
            0.0203,
            0.087,
            0.0671,
            -0.1607,
            -0.0346,
            0.0137,
            -0.0124,
            -0.0909,
            -0.0697,
            0.1602,
            -0.0653,
            -0.0517,
            0.1408
          ],
          "after": [
            -0.0407,
            0.079,
            0.1104,
            0.0354,
            -0.0051,
            -0.0064,
            0.0994,
            -0.1103,
            -0.0352,
            -0.0479,
            0.0132,
            -0.0269,
            0.045,
            -0.0159,
            0.0015,
            -0.0145
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1036,
            -0.0443,
            0.0373,
            0.232,
            -0.2038,
            -0.2075,
            -0.1997,
            0.2277,
            -0.0566,
            -0.1241,
            -0.0518,
            0.1012,
            0.1058,
            0.0018,
            -0.0078,
            0.1657
          ],
          "after": [
            0.0139,
            0.0732,
            0.0233,
            -0.1213,
            0.0446,
            0.0012,
            0.0207,
            0.1952,
            -0.0435,
            -0.0725,
            0.0781,
            -0.043,
            0.0618,
            0.0383,
            -0.0386,
            0.1307
          ]
        },
        "position_0": {
          "grad": [
            -0.0666,
            0.0555,
            0.0305,
            0.1304,
            0.0066,
            -0.0754,
            -0.0972,
            -0.0406,
            -0.0171,
            -0.1222,
            0.0925,
            -0.1157,
            -0.0043,
            -0.1445,
            -0.2421,
            0.0611
          ],
          "after": [
            -0.0277,
            -0.0653,
            -0.1852,
            0.0627,
            0.0481,
            -0.0295,
            -0.0588,
            0.0801,
            0.1186,
            -0.1075,
            0.0528,
            0.1029,
            0.0333,
            0.0177,
            -0.1169,
            0.0677
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0001,
            0.0001,
            0.0003,
            -0.0004,
            -0.0002,
            -0.0,
            0.0004,
            0.0,
            -0.0,
            -0.0,
            0.0004,
            0.0001,
            -0.0,
            0.0,
            0.0004
          ],
          "after": [
            0.0373,
            0.0551,
            -0.0337,
            0.0476,
            -0.005,
            0.1224,
            0.0147,
            -0.092,
            -0.0212,
            0.0857,
            -0.0123,
            -0.0328,
            0.102,
            -0.0736,
            -0.0056,
            0.0177
          ]
        }
      }
    },
    {
      "step": 4,
      "word": "jori",
      "loss": 3.0879,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0391,
            0.0806,
            0.112,
            0.0338,
            -0.0067,
            -0.008,
            0.101,
            -0.1087,
            -0.0368,
            -0.0463,
            0.0147,
            -0.0253,
            0.0434,
            -0.0143,
            0.003,
            -0.0161
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0065,
            0.0067,
            -0.0152,
            -0.0039,
            0.0112,
            -0.006,
            0.0453,
            0.0111,
            0.0132,
            -0.0219,
            -0.0016,
            0.0106,
            0.0322,
            -0.006,
            -0.0133,
            0.0026
          ],
          "after": [
            0.014,
            0.0751,
            0.024,
            -0.1213,
            0.0448,
            0.0032,
            0.0221,
            0.1929,
            -0.0414,
            -0.0706,
            0.077,
            -0.0424,
            0.0615,
            0.0392,
            -0.0397,
            0.1305
          ]
        },
        "position_0": {
          "grad": [
            -0.1034,
            0.0622,
            0.1923,
            0.0765,
            -0.0002,
            0.0065,
            0.0416,
            0.1505,
            -0.0045,
            -0.1547,
            0.1365,
            0.1476,
            0.1696,
            -0.0986,
            -0.1021,
            -0.008
          ],
          "after": [
            -0.0263,
            -0.0673,
            -0.1873,
            0.0621,
            0.0472,
            -0.0289,
            -0.0583,
            0.0798,
            0.118,
            -0.1057,
            0.0525,
            0.1025,
            0.0311,
            0.0183,
            -0.1159,
            0.0655
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0,
            -0.0001,
            -0.0001,
            0.0001,
            0.0001,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0001,
            0.0001,
            -0.0001,
            -0.0002,
            0.0,
            0.0001,
            -0.0
          ],
          "after": [
            0.036,
            0.0544,
            -0.0327,
            0.0474,
            -0.0061,
            0.123,
            0.0148,
            -0.0926,
            -0.0217,
            0.0869,
            -0.0116,
            -0.0339,
            0.1025,
            -0.0739,
            -0.0048,
            0.0169
          ]
        }
      }
    },
    {
      "step": 5,
      "word": "juanluis",
      "loss": 3.2829,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0083,
            0.0069,
            0.0336,
            -0.0152,
            -0.0951,
            0.0223,
            -0.075,
            -0.0944,
            0.0574,
            -0.0939,
            0.0272,
            -0.0333,
            -0.0099,
            0.0632,
            0.0124,
            -0.0014
          ],
          "after": [
            -0.0381,
            0.0818,
            0.111,
            0.0339,
            -0.0063,
            -0.0098,
            0.1029,
            -0.1066,
            -0.0388,
            -0.0443,
            0.0155,
            -0.0233,
            0.0422,
            -0.0146,
            0.0039,
            -0.0174
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0044,
            0.0439,
            -0.0006,
            -0.0094,
            0.0393,
            -0.0363,
            0.038,
            0.0183,
            0.0013,
            -0.0165,
            0.0065,
            0.036,
            0.0215,
            -0.0094,
            -0.0036,
            0.0195
          ],
          "after": [
            0.014,
            0.076,
            0.0246,
            -0.1211,
            0.0447,
            0.005,
            0.0229,
            0.1909,
            -0.0397,
            -0.0689,
            0.0761,
            -0.0423,
            0.0612,
            0.04,
            -0.0405,
            0.1301
          ]
        },
        "position_0": {
          "grad": [
            -0.072,
            0.0348,
            0.1084,
            0.05,
            0.0248,
            0.0077,
            0.0206,
            0.0883,
            0.0026,
            -0.0849,
            0.0717,
            0.1011,
            0.1061,
            -0.0594,
            -0.0761,
            0.0029
          ],
          "after": [
            -0.0245,
            -0.0695,
            -0.1896,
            0.0611,
            0.0461,
            -0.0285,
            -0.0583,
            0.0788,
            0.1175,
            -0.1037,
            0.0518,
            0.1014,
            0.0287,
            0.0191,
            -0.1146,
            0.0637
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0001,
            -0.0001,
            0.0,
            0.0001,
            -0.0001,
            0.0,
            -0.0001,
            -0.0002,
            0.0001,
            0.0002,
            -0.0002,
            0.0,
            0.0001,
            -0.0001,
            -0.0
          ],
          "after": [
            0.0343,
            0.0529,
            -0.031,
            0.0472,
            -0.007,
            0.1241,
            0.0148,
            -0.0928,
            -0.0212,
            0.0871,
            -0.0122,
            -0.034,
            0.1029,
            -0.0745,
            -0.0038,
            0.0163
          ]
        }
      }
    },
    {
      "step": 6,
      "word": "erandi",
      "loss": 3.1188,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0221,
            0.0092,
            0.0503,
            -0.0192,
            -0.1431,
            0.0208,
            -0.0797,
            -0.1345,
            0.0728,
            -0.1153,
            0.0351,
            -0.0555,
            -0.0221,
            0.0891,
            0.0247,
            -0.0135
          ],
          "after": [
            -0.0378,
            0.0826,
            0.1091,
            0.0349,
            -0.0047,
            -0.0118,
            0.1051,
            -0.1041,
            -0.0412,
            -0.042,
            0.0154,
            -0.021,
            0.0415,
            -0.016,
            0.0038,
            -0.0183
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0486,
            0.1561,
            0.371,
            -0.0315,
            -0.0399,
            -0.031,
            0.0018,
            -0.0686,
            0.0015,
            0.1876,
            -0.1829,
            -0.1126,
            0.041,
            -0.0224,
            -0.0797,
            -0.2173
          ],
          "after": [
            0.0145,
            0.0752,
            0.0231,
            -0.1209,
            0.0448,
            0.0066,
            0.0236,
            0.1898,
            -0.0383,
            -0.0692,
            0.0769,
            -0.0413,
            0.0607,
            0.0408,
            -0.0406,
            0.131
          ]
        },
        "position_0": {
          "grad": [
            0.0183,
            -0.1371,
            -0.0746,
            0.1223,
            -0.0756,
            0.051,
            -0.0569,
            -0.2074,
            0.0668,
            0.0705,
            -0.0444,
            0.0236,
            -0.0527,
            -0.0362,
            0.0763,
            -0.1202
          ],
          "after": [
            -0.0232,
            -0.0691,
            -0.1909,
            0.0595,
            0.0465,
            -0.0286,
            -0.0574,
            0.0794,
            0.116,
            -0.1026,
            0.0515,
            0.1002,
            0.0272,
            0.0201,
            -0.114,
            0.0639
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0003,
            0.0002,
            0.0002,
            -0.0,
            0.0002,
            0.0002,
            0.0003,
            -0.0,
            -0.0001,
            0.0,
            -0.0,
            0.0001,
            -0.0002,
            -0.0,
            0.0
          ],
          "after": [
            0.0335,
            0.0508,
            -0.0311,
            0.0465,
            -0.0078,
            0.1238,
            0.0142,
            -0.0936,
            -0.0207,
            0.0879,
            -0.0128,
            -0.034,
            0.1025,
            -0.0741,
            -0.0029,
            0.0157
          ]
        }
      }
    },
    {
      "step": 7,
      "word": "phia",
      "loss": 3.3565,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.286,
            0.0378,
            -0.058,
            -0.0755,
            -0.1531,
            0.2323,
            0.0529,
            0.104,
            -0.1899,
            0.0254,
            -0.0032,
            -0.1825,
            0.2348,
            0.0121,
            0.1888,
            -0.1309
          ],
          "after": [
            -0.0394,
            0.0828,
            0.1091,
            0.0369,
            -0.0027,
            -0.0139,
            0.1064,
            -0.1033,
            -0.0406,
            -0.0403,
            0.0155,
            -0.0186,
            0.0397,
            -0.0174,
            0.0021,
            -0.0177
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0091,
            0.0217,
            -0.0388,
            0.0044,
            0.016,
            -0.0114,
            0.067,
            0.0031,
            -0.0118,
            -0.0399,
            0.0036,
            0.0012,
            0.0012,
            -0.0042,
            -0.0023,
            0.0259
          ],
          "after": [
            0.0148,
            0.0743,
            0.022,
            -0.1207,
            0.0449,
            0.0081,
            0.0238,
            0.1888,
            -0.0369,
            -0.0691,
            0.0776,
            -0.0405,
            0.0603,
            0.0416,
            -0.0407,
            0.1317
          ]
        },
        "position_0": {
          "grad": [
            0.1066,
            0.284,
            -0.0509,
            -0.1914,
            0.0755,
            -0.09,
            0.3496,
            0.0919,
            -0.2423,
            0.0628,
            0.0003,
            -0.0079,
            -0.303,
            0.2,
            -0.022,
            0.1754
          ],
          "after": [
            -0.0233,
            -0.0704,
            -0.1917,
            0.0596,
            0.0458,
            -0.028,
            -0.0588,
            0.0794,
            0.1171,
            -0.1022,
            0.0512,
            0.0994,
            0.0279,
            0.0196,
            -0.1134,
            0.0628
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0002,
            -0.0002,
            0.0002,
            -0.0004,
            -0.0004,
            0.001,
            0.0002,
            -0.0003,
            -0.0002,
            0.0001,
            0.0002,
            0.0001,
            -0.0006,
            0.0003,
            -0.0002
          ],
          "after": [
            0.0332,
            0.0483,
            -0.0302,
            0.0454,
            -0.0077,
            0.1251,
            0.0125,
            -0.0948,
            -0.0196,
            0.0893,
            -0.0136,
            -0.0348,
            0.1016,
            -0.0725,
            -0.0031,
            0.0158
          ]
        }
      }
    },
    {
      "step": 8,
      "word": "samatha",
      "loss": 3.2794,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0321,
            -0.3016,
            -0.0324,
            0.1898,
            -0.0535,
            0.2452,
            0.2559,
            0.2305,
            -0.2461,
            -0.1688,
            -0.0633,
            -0.3518,
            0.1772,
            -0.0278,
            0.194,
            0.197
          ],
          "after": [
            -0.041,
            0.0844,
            0.1097,
            0.036,
            -0.0006,
            -0.0164,
            0.1058,
            -0.1041,
            -0.0389,
            -0.0381,
            0.0164,
            -0.0161,
            0.0375,
            -0.0181,
            -0.0001,
            -0.0185
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0092,
            0.026,
            0.002,
            0.018,
            0.0279,
            -0.0069,
            0.0594,
            -0.0005,
            -0.0249,
            -0.0273,
            0.004,
            0.0234,
            0.02,
            -0.0097,
            -0.0185,
            0.024
          ],
          "after": [
            0.0151,
            0.0733,
            0.021,
            -0.1206,
            0.0448,
            0.0093,
            0.0236,
            0.188,
            -0.0353,
            -0.0689,
            0.0782,
            -0.04,
            0.0598,
            0.0423,
            -0.0406,
            0.1321
          ]
        },
        "position_0": {
          "grad": [
            0.0214,
            0.1116,
            -0.0679,
            0.0179,
            -0.0094,
            0.0337,
            0.0148,
            0.1096,
            -0.0514,
            0.0992,
            -0.063,
            0.0929,
            0.1264,
            0.0262,
            0.0136,
            -0.0191
          ],
          "after": [
            -0.0235,
            -0.072,
            -0.1918,
            0.0595,
            0.0454,
            -0.0277,
            -0.06,
            0.0788,
            0.1183,
            -0.1025,
            0.0514,
            0.098,
            0.0278,
            0.0191,
            -0.1129,
            0.062
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0003,
            0.0004,
            0.0003,
            0.0002,
            -0.0001,
            0.0007,
            -0.0001,
            -0.0002,
            -0.0004,
            -0.0002,
            0.0002,
            0.0005,
            -0.0002,
            -0.0002,
            0.0001
          ],
          "after": [
            0.0334,
            0.0456,
            -0.031,
            0.0439,
            -0.008,
            0.1266,
            0.0103,
            -0.0956,
            -0.0181,
            0.0914,
            -0.0133,
            -0.0359,
            0.0997,
            -0.0708,
            -0.0027,
            0.0156
          ]
        }
      }
    },
    {
      "step": 9,
      "word": "phoenix",
      "loss": 3.1436,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0423,
            0.0858,
            0.1102,
            0.0352,
            0.0012,
            -0.0185,
            0.1052,
            -0.1048,
            -0.0375,
            -0.0362,
            0.0172,
            -0.0139,
            0.0356,
            -0.0188,
            -0.002,
            -0.0193
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1502,
            0.0282,
            -0.1406,
            -0.0703,
            0.0404,
            -0.0863,
            0.0244,
            -0.0129,
            -0.0164,
            -0.1404,
            -0.0386,
            -0.1077,
            -0.1279,
            -0.0638,
            0.3483,
            0.0753
          ],
          "after": [
            0.0163,
            0.0723,
            0.0209,
            -0.1203,
            0.0444,
            0.0108,
            0.0232,
            0.1874,
            -0.0337,
            -0.0679,
            0.0789,
            -0.0389,
            0.0601,
            0.0434,
            -0.042,
            0.1321
          ]
        },
        "position_0": {
          "grad": [
            0.07,
            0.1634,
            -0.0425,
            -0.1234,
            0.0474,
            -0.0583,
            0.2114,
            0.0504,
            -0.1485,
            0.0496,
            0.001,
            -0.0084,
            -0.1829,
            0.1184,
            -0.0129,
            0.1144
          ],
          "after": [
            -0.0243,
            -0.0739,
            -0.1916,
            0.0601,
            0.0444,
            -0.0271,
            -0.0618,
            0.0781,
            0.1201,
            -0.1031,
            0.0516,
            0.0969,
            0.0286,
            0.0181,
            -0.1125,
            0.0606
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0004,
            -0.0006,
            0.0004,
            -0.0007,
            -0.0005,
            -0.0005,
            -0.0001,
            -0.0004,
            -0.0,
            -0.0001,
            -0.0001,
            -0.0004,
            0.0003,
            0.0008,
            0.0
          ],
          "after": [
            0.033,
            0.0451,
            -0.0302,
            0.042,
            -0.0072,
            0.1287,
            0.0091,
            -0.0959,
            -0.0162,
            0.0932,
            -0.0126,
            -0.0364,
            0.0994,
            -0.0701,
            -0.0039,
            0.0154
          ]
        }
      }
    },
    {
      "step": 10,
      "word": "emmelynn",
      "loss": 3.3018,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0435,
            0.087,
            0.1106,
            0.0345,
            0.0028,
            -0.0203,
            0.1047,
            -0.1054,
            -0.0363,
            -0.0345,
            0.0179,
            -0.0121,
            0.034,
            -0.0193,
            -0.0036,
            -0.0199
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0922,
            0.035,
            0.3013,
            -0.0966,
            -0.2522,
            -0.0739,
            0.0146,
            -0.0162,
            0.0793,
            0.0094,
            -0.1612,
            -0.1434,
            -0.0277,
            -0.1832,
            0.2584,
            -0.2355
          ],
          "after": [
            0.0179,
            0.0711,
            0.0197,
            -0.1195,
            0.0453,
            0.0124,
            0.0228,
            0.187,
            -0.0338,
            -0.0671,
            0.0805,
            -0.0372,
            0.0605,
            0.0452,
            -0.0439,
            0.1331
          ]
        },
        "position_0": {
          "grad": [
            0.0033,
            -0.1043,
            -0.0548,
            0.0998,
            -0.0546,
            0.042,
            -0.0494,
            -0.1492,
            0.0551,
            0.053,
            -0.0428,
            0.0217,
            -0.0335,
            -0.0267,
            0.0526,
            -0.0985
          ],
          "after": [
            -0.025,
            -0.0751,
            -0.1912,
            0.0601,
            0.0443,
            -0.0268,
            -0.0631,
            0.0782,
            0.1213,
            -0.1039,
            0.052,
            0.0959,
            0.0293,
            0.0173,
            -0.1124,
            0.0601
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0001,
            0.0001,
            0.0001,
            0.0002,
            -0.0002,
            0.0001,
            -0.0003,
            -0.0001,
            -0.0001,
            0.0,
            0.0002,
            0.0003,
            0.0003,
            -0.0002,
            0.0002
          ],
          "after": [
            0.0326,
            0.0443,
            -0.0298,
            0.0403,
            -0.0068,
            0.1309,
            0.008,
            -0.0955,
            -0.0144,
            0.0951,
            -0.012,
            -0.0373,
            0.0985,
            -0.0701,
            -0.0045,
            0.0144
          ]
        }
      }
    },
    {
      "step": 11,
      "word": "hollan",
      "loss": 3.0443,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0361,
            -0.0071,
            0.0284,
            -0.0327,
            -0.1036,
            -0.0017,
            -0.0568,
            -0.1069,
            0.0531,
            -0.1299,
            0.0092,
            -0.0612,
            -0.0062,
            0.0635,
            0.0509,
            -0.0452
          ],
          "after": [
            -0.0446,
            0.0881,
            0.1105,
            0.0342,
            0.0047,
            -0.0219,
            0.1046,
            -0.1053,
            -0.0356,
            -0.0324,
            0.0183,
            -0.0102,
            0.0327,
            -0.0205,
            -0.0053,
            -0.0202
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0063,
            0.0194,
            -0.0139,
            0.0138,
            0.0053,
            -0.0238,
            0.0615,
            0.0099,
            -0.0151,
            -0.0238,
            0.0217,
            0.0294,
            0.0174,
            -0.0266,
            -0.0126,
            0.0248
          ],
          "after": [
            0.0193,
            0.0699,
            0.0188,
            -0.1189,
            0.046,
            0.014,
            0.0221,
            0.1866,
            -0.0336,
            -0.0663,
            0.0816,
            -0.0359,
            0.0607,
            0.0469,
            -0.0455,
            0.1338
          ]
        },
        "position_0": {
          "grad": [
            -0.0501,
            -0.0966,
            0.0933,
            -0.0131,
            -0.0119,
            0.0591,
            0.0456,
            0.0805,
            0.1022,
            -0.0115,
            -0.0088,
            -0.0414,
            0.1074,
            0.024,
            0.0294,
            0.0288
          ],
          "after": [
            -0.0252,
            -0.0757,
            -0.1913,
            0.0601,
            0.0443,
            -0.0271,
            -0.0644,
            0.0779,
            0.1217,
            -0.1046,
            0.0524,
            0.0952,
            0.0295,
            0.0166,
            -0.1124,
            0.0595
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0002,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0001,
            -0.0002,
            0.0002,
            0.0001,
            0.0001,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0001,
            0.0001,
            0.0
          ],
          "after": [
            0.0326,
            0.0441,
            -0.0292,
            0.039,
            -0.0063,
            0.1326,
            0.0072,
            -0.0957,
            -0.0131,
            0.0964,
            -0.0111,
            -0.038,
            0.098,
            -0.0701,
            -0.0053,
            0.0136
          ]
        }
      }
    },
    {
      "step": 12,
      "word": "hollis",
      "loss": 3.0733,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0457,
            0.089,
            0.1103,
            0.0339,
            0.0063,
            -0.0232,
            0.1045,
            -0.1053,
            -0.0349,
            -0.0306,
            0.0187,
            -0.0087,
            0.0315,
            -0.0216,
            -0.0067,
            -0.0204
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0033,
            0.0019,
            -0.0135,
            0.006,
            0.0078,
            -0.0091,
            0.0439,
            0.0036,
            -0.01,
            -0.018,
            0.0158,
            0.0241,
            0.0071,
            -0.0209,
            -0.0163,
            0.0192
          ],
          "after": [
            0.0205,
            0.0689,
            0.0181,
            -0.1184,
            0.0466,
            0.0153,
            0.0213,
            0.1862,
            -0.0333,
            -0.0655,
            0.0825,
            -0.035,
            0.0609,
            0.0485,
            -0.0468,
            0.1343
          ]
        },
        "position_0": {
          "grad": [
            -0.0529,
            -0.0949,
            0.0922,
            -0.0149,
            -0.0073,
            0.0595,
            0.0419,
            0.0793,
            0.106,
            -0.0057,
            -0.0065,
            -0.0444,
            0.1065,
            0.0271,
            0.0253,
            0.0333
          ],
          "after": [
            -0.0249,
            -0.0757,
            -0.192,
            0.0602,
            0.0444,
            -0.0278,
            -0.0657,
            0.0773,
            0.1215,
            -0.1051,
            0.0528,
            0.095,
            0.0293,
            0.0158,
            -0.1126,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0001,
            0.0002,
            -0.0003,
            0.0002,
            0.0002,
            -0.0001,
            0.0001,
            0.0,
            -0.0001,
            -0.0003,
            -0.0001,
            -0.0001,
            0.0001,
            -0.0002,
            -0.0003
          ],
          "after": [
            0.0324,
            0.0443,
            -0.0293,
            0.0386,
            -0.0063,
            0.1337,
            0.0066,
            -0.0961,
            -0.012,
            0.0978,
            -0.0096,
            -0.0383,
            0.0978,
            -0.0704,
            -0.0056,
            0.0137
          ]
        }
      }
    },
    {
      "step": 13,
      "word": "callalily",
      "loss": 3.1942,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0434,
            0.2688,
            -0.0921,
            -0.1087,
            0.0783,
            -0.1046,
            -0.1527,
            0.2418,
            -0.1051,
            0.0325,
            -0.0736,
            0.1429,
            -0.054,
            0.0189,
            0.1851,
            -0.2904
          ],
          "after": [
            -0.0463,
            0.0885,
            0.1115,
            0.0346,
            0.0072,
            -0.0238,
            0.1052,
            -0.1063,
            -0.0338,
            -0.0293,
            0.02,
            -0.008,
            0.0308,
            -0.0227,
            -0.0088,
            -0.0192
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.013,
            0.0197,
            0.0007,
            0.0182,
            0.0065,
            -0.0157,
            0.0443,
            -0.0269,
            -0.012,
            -0.0116,
            0.0353,
            0.0201,
            0.0029,
            -0.0231,
            0.0045,
            0.0293
          ],
          "after": [
            0.0216,
            0.0679,
            0.0174,
            -0.1181,
            0.0471,
            0.0166,
            0.0202,
            0.186,
            -0.0329,
            -0.0647,
            0.0831,
            -0.0343,
            0.0611,
            0.05,
            -0.0479,
            0.1347
          ]
        },
        "position_0": {
          "grad": [
            -0.0324,
            -0.0036,
            0.0358,
            -0.0277,
            -0.0408,
            -0.002,
            0.0534,
            0.056,
            0.0541,
            0.0138,
            0.1198,
            0.0333,
            -0.0642,
            -0.0077,
            -0.0412,
            -0.0229
          ],
          "after": [
            -0.0244,
            -0.0758,
            -0.1929,
            0.0605,
            0.045,
            -0.0284,
            -0.0671,
            0.0766,
            0.1211,
            -0.1056,
            0.0523,
            0.0946,
            0.0293,
            0.0151,
            -0.1126,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            0.0,
            0.0001,
            0.0,
            0.0,
            -0.0,
            0.0,
            0.0001
          ],
          "after": [
            0.0324,
            0.0445,
            -0.0293,
            0.0382,
            -0.0062,
            0.1348,
            0.0061,
            -0.0964,
            -0.011,
            0.0989,
            -0.0085,
            -0.0387,
            0.0975,
            -0.0706,
            -0.0059,
            0.0136
          ]
        }
      }
    },
    {
      "step": 14,
      "word": "adelayde",
      "loss": 3.2254,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0853,
            0.0094,
            -0.0528,
            -0.1058,
            -0.0097,
            0.0312,
            0.1165,
            -0.1256,
            0.0144,
            0.0222,
            -0.1774,
            0.0215,
            0.1359,
            0.1841,
            0.1897,
            0.1135
          ],
          "after": [
            -0.0473,
            0.088,
            0.1131,
            0.0358,
            0.008,
            -0.0245,
            0.1052,
            -0.1066,
            -0.033,
            -0.0283,
            0.022,
            -0.0074,
            0.0296,
            -0.0247,
            -0.0112,
            -0.0187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3084,
            -0.0506,
            -0.0114,
            -0.3645,
            -0.1843,
            0.12,
            -0.264,
            0.0238,
            0.2258,
            0.1382,
            0.2654,
            0.0706,
            -0.2359,
            -0.107,
            0.0006,
            0.0731
          ],
          "after": [
            0.0236,
            0.0674,
            0.0169,
            -0.1167,
            0.0482,
            0.017,
            0.0208,
            0.1857,
            -0.0343,
            -0.0649,
            0.0822,
            -0.0341,
            0.0622,
            0.0519,
            -0.0489,
            0.1346
          ]
        },
        "position_0": {
          "grad": [
            0.04,
            0.0639,
            0.0551,
            -0.027,
            -0.0013,
            0.0049,
            -0.0407,
            -0.0417,
            0.0385,
            -0.0766,
            0.1255,
            0.0168,
            -0.0834,
            0.057,
            0.0035,
            -0.0457
          ],
          "after": [
            -0.0242,
            -0.0761,
            -0.1939,
            0.0609,
            0.0454,
            -0.0289,
            -0.068,
            0.0761,
            0.1205,
            -0.1056,
            0.0513,
            0.0941,
            0.0297,
            0.0142,
            -0.1125,
            0.0582
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0,
            0.0001,
            -0.0002,
            0.0,
            -0.0001,
            0.0001,
            0.0,
            -0.0,
            0.0,
            0.0,
            0.0,
            0.0001,
            -0.0001,
            0.0,
            -0.0
          ],
          "after": [
            0.0325,
            0.0446,
            -0.0295,
            0.0381,
            -0.0062,
            0.1358,
            0.0055,
            -0.0967,
            -0.0101,
            0.0997,
            -0.0077,
            -0.0391,
            0.0972,
            -0.0706,
            -0.0062,
            0.0135
          ]
        }
      }
    },
    {
      "step": 15,
      "word": "josephyne",
      "loss": 3.1877,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0482,
            0.0875,
            0.1145,
            0.0369,
            0.0087,
            -0.025,
            0.1052,
            -0.1069,
            -0.0322,
            -0.0275,
            0.0238,
            -0.007,
            0.0285,
            -0.0264,
            -0.0133,
            -0.0183
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1528,
            0.1366,
            -0.1135,
            -0.0101,
            -0.065,
            -0.0419,
            -0.1567,
            -0.0814,
            -0.2274,
            -0.2112,
            0.04,
            -0.313,
            0.1258,
            0.0412,
            0.1954,
            -0.1862
          ],
          "after": [
            0.0246,
            0.0661,
            0.0168,
            -0.1154,
            0.0494,
            0.0176,
            0.0219,
            0.186,
            -0.034,
            -0.064,
            0.0813,
            -0.0327,
            0.0626,
            0.0532,
            -0.0504,
            0.1354
          ]
        },
        "position_0": {
          "grad": [
            -0.058,
            0.0141,
            0.079,
            0.0322,
            0.0068,
            -0.0048,
            0.0338,
            0.0669,
            0.008,
            -0.0755,
            0.0719,
            0.0627,
            0.1029,
            -0.0687,
            -0.0534,
            -0.0072
          ],
          "after": [
            -0.0237,
            -0.0765,
            -0.1952,
            0.061,
            0.0458,
            -0.0293,
            -0.069,
            0.0753,
            0.1199,
            -0.1051,
            0.05,
            0.0933,
            0.0296,
            0.0139,
            -0.1122,
            0.0581
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0002,
            -0.0002,
            -0.0003,
            0.0001,
            -0.0001,
            -0.0,
            0.0,
            -0.0,
            0.0001,
            -0.0,
            -0.0003,
            0.0001,
            -0.0,
            -0.0,
            -0.0004
          ],
          "after": [
            0.032,
            0.0442,
            -0.0293,
            0.0386,
            -0.0062,
            0.1369,
            0.0051,
            -0.097,
            -0.0093,
            0.1002,
            -0.007,
            -0.0385,
            0.0965,
            -0.0706,
            -0.0064,
            0.0145
          ]
        }
      }
    },
    {
      "step": 16,
      "word": "weldon",
      "loss": 3.1688,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0489,
            0.0871,
            0.1157,
            0.0378,
            0.0093,
            -0.0255,
            0.1052,
            -0.1071,
            -0.0316,
            -0.0267,
            0.0254,
            -0.0066,
            0.0276,
            -0.0279,
            -0.0151,
            -0.018
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0637,
            -0.0797,
            -0.1212,
            -0.3097,
            0.168,
            0.0029,
            -0.3431,
            -0.0056,
            0.0717,
            0.0557,
            -0.0545,
            0.0096,
            0.2008,
            0.0918,
            -0.0977,
            -0.0366
          ],
          "after": [
            0.0252,
            0.0656,
            0.0172,
            -0.1135,
            0.0497,
            0.018,
            0.0239,
            0.1863,
            -0.0341,
            -0.0636,
            0.0807,
            -0.0315,
            0.0622,
            0.0538,
            -0.0513,
            0.1362
          ]
        },
        "position_0": {
          "grad": [
            0.0315,
            0.1647,
            -0.1364,
            -0.0441,
            0.01,
            -0.0784,
            0.1926,
            0.0392,
            -0.0065,
            0.0052,
            -0.0114,
            -0.0572,
            0.036,
            -0.0797,
            -0.1533,
            -0.019
          ],
          "after": [
            -0.0235,
            -0.0774,
            -0.1955,
            0.0614,
            0.0459,
            -0.0291,
            -0.0706,
            0.0745,
            0.1195,
            -0.1046,
            0.0489,
            0.0929,
            0.0293,
            0.014,
            -0.1111,
            0.0582
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0003,
            -0.0002,
            0.0004,
            -0.0003,
            -0.0001,
            0.0002,
            -0.0005,
            0.0001,
            0.0004,
            0.0003,
            0.0005,
            -0.0005,
            0.0001,
            0.0003,
            0.0008
          ],
          "after": [
            0.0327,
            0.0431,
            -0.0288,
            0.0383,
            -0.0059,
            0.1381,
            0.0044,
            -0.0962,
            -0.0089,
            0.0996,
            -0.0075,
            -0.0392,
            0.097,
            -0.0708,
            -0.0071,
            0.0137
          ]
        }
      }
    },
    {
      "step": 17,
      "word": "kayle",
      "loss": 3.2335,
      "learning_rate": 0.003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1398,
            -0.027,
            0.0199,
            -0.0684,
            -0.0733,
            0.1419,
            0.0776,
            -0.0136,
            -0.0379,
            0.0413,
            -0.1958,
            0.0902,
            0.1031,
            0.1058,
            0.1171,
            0.0983
          ],
          "after": [
            -0.0503,
            0.0869,
            0.1165,
            0.0391,
            0.0103,
            -0.0267,
            0.1048,
            -0.1072,
            -0.0308,
            -0.0264,
            0.0276,
            -0.0067,
            0.0263,
            -0.0299,
            -0.0172,
            -0.0181
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2226,
            -0.0533,
            0.1034,
            -0.0659,
            -0.0,
            0.1382,
            -0.2782,
            0.1519,
            0.0221,
            -0.1413,
            -0.2545,
            -0.1964,
            0.1389,
            0.3351,
            -0.1883,
            -0.1267
          ],
          "after": [
            0.0248,
            0.0655,
            0.0172,
            -0.1116,
            0.05,
            0.0177,
            0.0262,
            0.1856,
            -0.0343,
            -0.0625,
            0.0813,
            -0.0298,
            0.0612,
            0.0528,
            -0.0515,
            0.1373
          ]
        },
        "position_0": {
          "grad": [
            -0.1359,
            0.0352,
            -0.1974,
            -0.0461,
            -0.0206,
            -0.108,
            0.1873,
            0.0062,
            -0.0524,
            0.0113,
            0.0118,
            -0.1103,
            0.0126,
            -0.0242,
            -0.033,
            -0.1608
          ],
          "after": [
            -0.0223,
            -0.0784,
            -0.1948,
            0.0619,
            0.0463,
            -0.0281,
            -0.0725,
            0.0738,
            0.1194,
            -0.1043,
            0.0479,
            0.0934,
            0.0291,
            0.0142,
            -0.1101,
            0.0592
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0004,
            0.0002,
            0.0,
            0.0004,
            -0.0005,
            0.0006,
            -0.0001,
            -0.0002,
            -0.0003,
            0.0,
            -0.0001,
            0.0004,
            -0.0005,
            0.0,
            -0.0
          ],
          "after": [
            0.0336,
            0.0414,
            -0.0288,
            0.0381,
            -0.0062,
            0.14,
            0.0031,
            -0.0953,
            -0.0081,
            0.0997,
            -0.008,
            -0.0394,
            0.0966,
            -0.0701,
            -0.0078,
            0.0131
          ]
        }
      }
    },
    {
      "step": 18,
      "word": "ragnar",
      "loss": 3.1266,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0029,
            -0.074,
            -0.0611,
            -0.0048,
            -0.0015,
            0.0141,
            0.0614,
            -0.2218,
            0.1122,
            0.0526,
            0.1667,
            0.0279,
            -0.1172,
            -0.1445,
            -0.0011,
            0.2358
          ],
          "after": [
            -0.0516,
            0.0871,
            0.1178,
            0.0403,
            0.0111,
            -0.0277,
            0.1042,
            -0.1064,
            -0.0308,
            -0.0265,
            0.0284,
            -0.0069,
            0.0258,
            -0.0305,
            -0.019,
            -0.0191
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0132,
            0.0304,
            -0.0144,
            0.0045,
            0.0182,
            -0.0284,
            0.0778,
            -0.0166,
            -0.0194,
            0.0097,
            0.0135,
            0.0199,
            0.0273,
            -0.0278,
            -0.0343,
            0.0258
          ],
          "after": [
            0.0245,
            0.0652,
            0.0172,
            -0.11,
            0.0502,
            0.0176,
            0.028,
            0.1851,
            -0.0344,
            -0.0617,
            0.0818,
            -0.0283,
            0.0603,
            0.052,
            -0.0515,
            0.1382
          ]
        },
        "position_0": {
          "grad": [
            0.1025,
            -0.0182,
            0.0873,
            -0.0769,
            0.0729,
            0.0881,
            0.0465,
            -0.179,
            0.0467,
            0.0097,
            -0.0184,
            -0.0392,
            -0.1061,
            -0.0862,
            0.0949,
            0.1169
          ],
          "after": [
            -0.0221,
            -0.0792,
            -0.1946,
            0.0628,
            0.0458,
            -0.0279,
            -0.0744,
            0.074,
            0.119,
            -0.1041,
            0.0472,
            0.0941,
            0.0293,
            0.0149,
            -0.1096,
            0.0593
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0004,
            0.0004,
            -0.0,
            0.0004,
            -0.0003,
            0.0006,
            -0.0004,
            -0.0002,
            -0.0002,
            -0.0,
            -0.0,
            0.0004,
            -0.0002,
            -0.0003,
            -0.0003
          ],
          "after": [
            0.0341,
            0.0392,
            -0.0297,
            0.0379,
            -0.007,
            0.1422,
            0.0014,
            -0.0938,
            -0.0068,
            0.1004,
            -0.0084,
            -0.0396,
            0.0956,
            -0.0692,
            -0.0078,
            0.0131
          ]
        }
      }
    },
    {
      "step": 19,
      "word": "colbie",
      "loss": 3.0524,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0526,
            0.0872,
            0.119,
            0.0413,
            0.0119,
            -0.0286,
            0.1036,
            -0.1057,
            -0.0308,
            -0.0266,
            0.0291,
            -0.007,
            0.0253,
            -0.0309,
            -0.0205,
            -0.0201
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1224,
            -0.0817,
            -0.0399,
            0.1736,
            -0.32,
            -0.0548,
            -0.1097,
            0.2103,
            0.058,
            0.0773,
            -0.0047,
            0.155,
            0.049,
            -0.0905,
            0.2428,
            0.1781
          ],
          "after": [
            0.0247,
            0.0655,
            0.0174,
            -0.1092,
            0.0514,
            0.0178,
            0.0299,
            0.1836,
            -0.0348,
            -0.0613,
            0.0823,
            -0.0278,
            0.0592,
            0.0518,
            -0.0522,
            0.1383
          ]
        },
        "position_0": {
          "grad": [
            -0.0521,
            -0.0056,
            0.0342,
            -0.0478,
            -0.039,
            -0.0198,
            0.0879,
            0.0646,
            0.0554,
            0.0257,
            0.1806,
            0.0355,
            -0.0764,
            -0.028,
            -0.0761,
            -0.0122
          ],
          "after": [
            -0.0215,
            -0.0798,
            -0.1946,
            0.0639,
            0.0458,
            -0.0276,
            -0.0763,
            0.0739,
            0.1184,
            -0.1042,
            0.0457,
            0.0944,
            0.0298,
            0.0156,
            -0.1089,
            0.0595
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0002,
            -0.0002,
            0.0006,
            -0.0004,
            -0.0001,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0003,
            0.0004,
            0.0003,
            -0.0003,
            -0.0,
            0.0002,
            0.0006
          ],
          "after": [
            0.0348,
            0.0369,
            -0.03,
            0.0368,
            -0.007,
            0.1442,
            -0.0001,
            -0.0924,
            -0.0055,
            0.1003,
            -0.0098,
            -0.0404,
            0.0952,
            -0.0683,
            -0.008,
            0.0121
          ]
        }
      }
    },
    {
      "step": 20,
      "word": "taveon",
      "loss": 3.0383,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1205,
            -0.0865,
            -0.0249,
            0.0251,
            0.1116,
            0.0543,
            -0.1358,
            -0.0278,
            0.0157,
            -0.0118,
            -0.045,
            -0.0535,
            0.1454,
            -0.0854,
            -0.1138,
            0.1695
          ],
          "after": [
            -0.0528,
            0.0878,
            0.1204,
            0.0419,
            0.0118,
            -0.0296,
            0.1038,
            -0.105,
            -0.0309,
            -0.0266,
            0.0299,
            -0.0069,
            0.0243,
            -0.0308,
            -0.0212,
            -0.0215
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2394,
            0.0951,
            -0.1639,
            -0.1499,
            -0.0468,
            -0.0283,
            -0.1018,
            0.1021,
            0.0662,
            -0.1134,
            0.0668,
            -0.0696,
            -0.1088,
            -0.2296,
            0.3477,
            -0.0702
          ],
          "after": [
            0.0257,
            0.0652,
            0.0181,
            -0.1081,
            0.0526,
            0.0181,
            0.0318,
            0.1819,
            -0.0355,
            -0.0605,
            0.0824,
            -0.027,
            0.0588,
            0.0524,
            -0.0538,
            0.1386
          ]
        },
        "position_0": {
          "grad": [
            -0.0042,
            -0.2374,
            0.0709,
            -0.01,
            -0.0089,
            0.1077,
            0.1161,
            -0.0084,
            -0.1436,
            -0.0246,
            -0.1209,
            -0.2575,
            0.1069,
            0.0512,
            -0.0085,
            0.1291
          ],
          "after": [
            -0.021,
            -0.0794,
            -0.1949,
            0.0648,
            0.046,
            -0.028,
            -0.0783,
            0.0739,
            0.1187,
            -0.104,
            0.045,
            0.096,
            0.0298,
            0.016,
            -0.1082,
            0.059
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0,
            -0.0,
            0.0001,
            -0.0,
            0.0001,
            -0.0,
            -0.0,
            0.0,
            0.0001,
            -0.0,
            -0.0,
            -0.0001,
            0.0001,
            0.0001,
            0.0001
          ],
          "after": [
            0.0355,
            0.035,
            -0.0301,
            0.0357,
            -0.0069,
            0.1459,
            -0.0013,
            -0.0911,
            -0.0044,
            0.0998,
            -0.0108,
            -0.0411,
            0.0951,
            -0.0677,
            -0.0084,
            0.011
          ]
        }
      }
    },
    {
      "step": 21,
      "word": "aki",
      "loss": 3.3772,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2082,
            0.0413,
            -0.347,
            -0.1886,
            0.066,
            -0.1891,
            0.1569,
            0.0211,
            -0.0738,
            0.1063,
            -0.0148,
            -0.1048,
            0.0023,
            -0.0463,
            -0.1789,
            -0.2445
          ],
          "after": [
            -0.0519,
            0.088,
            0.1227,
            0.0436,
            0.0112,
            -0.0296,
            0.1033,
            -0.1045,
            -0.0305,
            -0.0273,
            0.0307,
            -0.0064,
            0.0234,
            -0.0303,
            -0.021,
            -0.0217
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0202,
            0.0241,
            -0.002,
            0.0033,
            0.022,
            -0.0325,
            0.0689,
            -0.0192,
            -0.009,
            -0.0143,
            0.0051,
            -0.0197,
            0.0024,
            0.0027,
            -0.0223,
            0.0425
          ],
          "after": [
            0.0267,
            0.0647,
            0.0188,
            -0.1071,
            0.0536,
            0.0185,
            0.0333,
            0.1805,
            -0.0361,
            -0.0597,
            0.0824,
            -0.0263,
            0.0585,
            0.053,
            -0.0551,
            0.1387
          ]
        },
        "position_0": {
          "grad": [
            0.1336,
            0.116,
            0.1244,
            -0.088,
            0.0362,
            0.045,
            -0.17,
            -0.1082,
            0.1141,
            -0.176,
            0.2483,
            0.0256,
            -0.1518,
            0.1842,
            -0.0248,
            -0.105
          ],
          "after": [
            -0.0215,
            -0.0794,
            -0.1958,
            0.0661,
            0.0456,
            -0.0287,
            -0.0794,
            0.0744,
            0.1183,
            -0.1028,
            0.0435,
            0.0972,
            0.0304,
            0.0153,
            -0.1074,
            0.0591
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0002,
            0.0002,
            -0.0,
            0.0002,
            0.0001,
            0.0003,
            -0.0003,
            0.0001,
            0.0,
            -0.0,
            -0.0001,
            -0.0,
            0.0002,
            -0.0003,
            -0.0
          ],
          "after": [
            0.0363,
            0.0329,
            -0.0307,
            0.0347,
            -0.0072,
            0.147,
            -0.0026,
            -0.0894,
            -0.0037,
            0.0993,
            -0.0117,
            -0.0414,
            0.095,
            -0.0677,
            -0.0081,
            0.01
          ]
        }
      }
    },
    {
      "step": 22,
      "word": "peyten",
      "loss": 2.9671,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0511,
            0.0883,
            0.1246,
            0.045,
            0.0108,
            -0.0295,
            0.1028,
            -0.104,
            -0.0302,
            -0.028,
            0.0314,
            -0.0059,
            0.0226,
            -0.0299,
            -0.0208,
            -0.0219
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2012,
            -0.3033,
            0.1784,
            -0.267,
            0.0677,
            0.2824,
            -0.7116,
            0.0203,
            0.1786,
            -0.0444,
            -0.047,
            -0.1215,
            -0.0404,
            0.3892,
            0.0816,
            0.038
          ],
          "after": [
            0.0267,
            0.0659,
            0.0187,
            -0.1056,
            0.0542,
            0.0176,
            0.0356,
            0.1791,
            -0.0374,
            -0.0588,
            0.0827,
            -0.0252,
            0.0583,
            0.0521,
            -0.0564,
            0.1386
          ]
        },
        "position_0": {
          "grad": [
            0.1009,
            0.145,
            -0.0749,
            -0.1216,
            0.0122,
            -0.0879,
            0.2246,
            0.0242,
            -0.1409,
            0.0437,
            0.0011,
            -0.0098,
            -0.1735,
            0.154,
            -0.0049,
            0.121
          ],
          "after": [
            -0.0225,
            -0.0801,
            -0.1962,
            0.0679,
            0.0452,
            -0.0288,
            -0.0811,
            0.0747,
            0.1187,
            -0.1021,
            0.0421,
            0.0982,
            0.0315,
            0.014,
            -0.1068,
            0.0586
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0002,
            0.0002,
            0.0001,
            0.0002,
            -0.0004,
            0.0004,
            0.0004,
            0.0001,
            0.0,
            -0.0001,
            0.0002,
            0.0005,
            -0.0004,
            -0.0003,
            0.0005
          ],
          "after": [
            0.0376,
            0.0306,
            -0.0317,
            0.0338,
            -0.0078,
            0.1486,
            -0.0043,
            -0.089,
            -0.0033,
            0.0988,
            -0.0123,
            -0.0421,
            0.0942,
            -0.0669,
            -0.0074,
            0.0086
          ]
        }
      }
    },
    {
      "step": 23,
      "word": "kevari",
      "loss": 3.0722,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1052,
            -0.0875,
            -0.0055,
            -0.05,
            0.1217,
            0.1373,
            0.0303,
            -0.14,
            0.1164,
            -0.0427,
            0.0451,
            0.0311,
            -0.0948,
            -0.0816,
            0.0637,
            0.2628
          ],
          "after": [
            -0.051,
            0.0889,
            0.1264,
            0.0465,
            0.0096,
            -0.0301,
            0.1022,
            -0.103,
            -0.0306,
            -0.0282,
            0.0317,
            -0.0056,
            0.0224,
            -0.029,
            -0.0209,
            -0.0229
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2485,
            -0.1251,
            -0.1557,
            -0.0834,
            0.1858,
            0.0984,
            -0.233,
            -0.1408,
            0.0326,
            0.0175,
            0.1009,
            0.0861,
            0.0962,
            0.0269,
            -0.2584,
            -0.1071
          ],
          "after": [
            0.026,
            0.0674,
            0.0191,
            -0.104,
            0.0541,
            0.0164,
            0.0381,
            0.1788,
            -0.0388,
            -0.0581,
            0.0825,
            -0.0246,
            0.0578,
            0.0513,
            -0.0568,
            0.139
          ]
        },
        "position_0": {
          "grad": [
            -0.1126,
            0.0396,
            -0.1819,
            -0.0556,
            0.0013,
            -0.0873,
            0.1556,
            -0.0006,
            -0.0563,
            0.0115,
            0.0114,
            -0.0809,
            0.0215,
            -0.0293,
            -0.0424,
            -0.1253
          ],
          "after": [
            -0.0227,
            -0.0808,
            -0.1957,
            0.0696,
            0.0449,
            -0.0282,
            -0.083,
            0.0749,
            0.1194,
            -0.1015,
            0.0409,
            0.0996,
            0.0324,
            0.0131,
            -0.106,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0,
            0.0002,
            -0.0001,
            0.0,
            0.0003,
            0.0005,
            -0.0004,
            -0.0,
            0.0003,
            0.0003,
            0.0004,
            -0.0004,
            -0.0002,
            0.0001,
            -0.0001
          ],
          "after": [
            0.0395,
            0.0286,
            -0.0329,
            0.0331,
            -0.0083,
            0.1494,
            -0.0063,
            -0.0879,
            -0.0029,
            0.0978,
            -0.0137,
            -0.0434,
            0.094,
            -0.0659,
            -0.007,
            0.0074
          ]
        }
      }
    },
    {
      "step": 24,
      "word": "joella",
      "loss": 2.7671,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1919,
            0.0892,
            0.0491,
            0.0274,
            -0.1294,
            0.0566,
            0.0602,
            0.0092,
            -0.0875,
            -0.0319,
            -0.0162,
            -0.0777,
            0.1266,
            0.0485,
            0.0426,
            -0.0713
          ],
          "after": [
            -0.0518,
            0.089,
            0.1276,
            0.0477,
            0.0095,
            -0.0308,
            0.1015,
            -0.1022,
            -0.0304,
            -0.0282,
            0.0321,
            -0.005,
            0.0216,
            -0.0286,
            -0.0213,
            -0.0235
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.069,
            0.149,
            -0.0963,
            0.1089,
            0.076,
            0.1765,
            -0.1572,
            -0.042,
            -0.0188,
            0.0549,
            0.0573,
            0.1476,
            -0.3329,
            0.0567,
            0.2847,
            0.0818
          ],
          "after": [
            0.0256,
            0.068,
            0.0198,
            -0.103,
            0.0537,
            0.0147,
            0.0405,
            0.1787,
            -0.0398,
            -0.0578,
            0.082,
            -0.0247,
            0.0586,
            0.0505,
            -0.0578,
            0.1389
          ]
        },
        "position_0": {
          "grad": [
            -0.085,
            0.0268,
            0.0979,
            0.0451,
            0.0234,
            -0.0021,
            0.0378,
            0.0962,
            -0.0158,
            -0.0968,
            0.0952,
            0.1179,
            0.1765,
            -0.1334,
            -0.0925,
            -0.0068
          ],
          "after": [
            -0.0223,
            -0.0815,
            -0.1957,
            0.0709,
            0.0443,
            -0.0277,
            -0.0847,
            0.0747,
            0.12,
            -0.1004,
            0.0394,
            0.1001,
            0.0325,
            0.0129,
            -0.1048,
            0.059
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            -0.0001,
            -0.0002,
            -0.0001,
            -0.0,
            0.0002,
            -0.0003,
            -0.0,
            -0.0,
            0.0,
            -0.0001,
            -0.0001,
            -0.0004,
            0.0003,
            0.0004,
            -0.0001
          ],
          "after": [
            0.0411,
            0.027,
            -0.0334,
            0.0327,
            -0.0087,
            0.1496,
            -0.0077,
            -0.087,
            -0.0025,
            0.0968,
            -0.0145,
            -0.0445,
            0.0945,
            -0.0656,
            -0.0074,
            0.0066
          ]
        }
      }
    },
    {
      "step": 25,
      "word": "mecca",
      "loss": 3.2073,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3485,
            -0.009,
            -0.0943,
            0.0046,
            -0.1341,
            0.0813,
            -0.0635,
            0.0441,
            -0.1011,
            -0.0251,
            -0.0224,
            -0.1963,
            0.1116,
            0.1327,
            0.0884,
            -0.1828
          ],
          "after": [
            -0.0536,
            0.0891,
            0.1292,
            0.0487,
            0.0101,
            -0.0319,
            0.1011,
            -0.1017,
            -0.0298,
            -0.028,
            0.0326,
            -0.0036,
            0.0204,
            -0.0291,
            -0.0219,
            -0.0235
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2988,
            -0.0968,
            0.0882,
            -0.2787,
            -0.0143,
            -0.0474,
            -0.2068,
            -0.1116,
            0.3131,
            0.0428,
            0.0015,
            0.0602,
            -0.2229,
            0.0378,
            0.079,
            -0.1678
          ],
          "after": [
            0.0244,
            0.069,
            0.0201,
            -0.1013,
            0.0534,
            0.0134,
            0.043,
            0.1792,
            -0.0418,
            -0.0577,
            0.0816,
            -0.025,
            0.06,
            0.0496,
            -0.0589,
            0.1396
          ]
        },
        "position_0": {
          "grad": [
            -0.0928,
            0.0878,
            0.0073,
            0.1532,
            0.0801,
            -0.0778,
            0.2573,
            0.1967,
            0.0156,
            -0.0699,
            -0.0127,
            -0.06,
            -0.1089,
            -0.1028,
            0.0389,
            0.0726
          ],
          "after": [
            -0.0214,
            -0.0824,
            -0.1958,
            0.0711,
            0.043,
            -0.0268,
            -0.087,
            0.0736,
            0.1204,
            -0.0991,
            0.0382,
            0.1009,
            0.033,
            0.0133,
            -0.1039,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0003,
            -0.0003,
            0.0003,
            -0.0007,
            0.0003,
            -0.0006,
            0.0002,
            -0.0002,
            -0.0,
            -0.0002,
            0.0,
            -0.0001,
            0.0007,
            0.0003,
            0.0002
          ],
          "after": [
            0.0431,
            0.0262,
            -0.0332,
            0.0318,
            -0.0081,
            0.1493,
            -0.0082,
            -0.0865,
            -0.0016,
            0.0959,
            -0.0146,
            -0.0455,
            0.0952,
            -0.0665,
            -0.0081,
            0.0056
          ]
        }
      }
    },
    {
      "step": 26,
      "word": "egan",
      "loss": 2.8011,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0226,
            -0.0048,
            0.0496,
            0.0176,
            -0.206,
            -0.0306,
            -0.0229,
            -0.2092,
            0.1358,
            -0.1418,
            0.0219,
            -0.08,
            -0.0797,
            0.137,
            -0.0127,
            0.0117
          ],
          "after": [
            -0.0553,
            0.0892,
            0.1302,
            0.0494,
            0.0116,
            -0.0326,
            0.1009,
            -0.1004,
            -0.0299,
            -0.0269,
            0.0328,
            -0.0021,
            0.0197,
            -0.0303,
            -0.0225,
            -0.0234
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0991,
            0.2891,
            0.5309,
            -0.1068,
            -0.0077,
            -0.0815,
            0.1404,
            -0.0545,
            -0.015,
            0.2262,
            -0.1523,
            -0.1074,
            -0.0299,
            -0.033,
            -0.1438,
            -0.2553
          ],
          "after": [
            0.0236,
            0.0685,
            0.0189,
            -0.0996,
            0.0532,
            0.0127,
            0.0449,
            0.1799,
            -0.0435,
            -0.0587,
            0.082,
            -0.0249,
            0.0613,
            0.0489,
            -0.0595,
            0.141
          ]
        },
        "position_0": {
          "grad": [
            0.0003,
            -0.2285,
            -0.068,
            0.0761,
            -0.0976,
            0.0964,
            -0.1696,
            -0.29,
            0.1581,
            0.1032,
            -0.0951,
            -0.0472,
            -0.0972,
            -0.0641,
            0.1409,
            -0.2221
          ],
          "after": [
            -0.0207,
            -0.0823,
            -0.1955,
            0.0709,
            0.0429,
            -0.0266,
            -0.0883,
            0.0739,
            0.12,
            -0.0986,
            0.0376,
            0.1018,
            0.0338,
            0.0139,
            -0.104,
            0.0596
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0,
            -0.0,
            0.0,
            0.0001,
            -0.0002,
            0.0006,
            0.0002,
            0.0,
            0.0002,
            0.0002,
            0.0003,
            0.0,
            -0.0003,
            -0.0,
            0.0001
          ],
          "after": [
            0.0454,
            0.0256,
            -0.0329,
            0.0309,
            -0.0077,
            0.1494,
            -0.0092,
            -0.0865,
            -0.0009,
            0.0948,
            -0.0153,
            -0.047,
            0.0958,
            -0.0667,
            -0.0086,
            0.0044
          ]
        }
      }
    },
    {
      "step": 27,
      "word": "justyce",
      "loss": 3.1073,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0567,
            0.0893,
            0.1311,
            0.05,
            0.013,
            -0.0333,
            0.1008,
            -0.0993,
            -0.0301,
            -0.0259,
            0.0331,
            -0.0008,
            0.0192,
            -0.0314,
            -0.0229,
            -0.0234
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0971,
            -0.1415,
            -0.2055,
            -0.0504,
            -0.014,
            -0.0601,
            -0.174,
            -0.0191,
            -0.1035,
            -0.0247,
            -0.0362,
            -0.1405,
            -0.0158,
            0.2395,
            0.1206,
            -0.2126
          ],
          "after": [
            0.0227,
            0.0687,
            0.0184,
            -0.098,
            0.053,
            0.0123,
            0.0468,
            0.1806,
            -0.0445,
            -0.0594,
            0.0824,
            -0.0242,
            0.0625,
            0.0476,
            -0.0603,
            0.1429
          ]
        },
        "position_0": {
          "grad": [
            -0.0889,
            0.0503,
            0.0947,
            0.0799,
            0.0386,
            -0.021,
            0.0695,
            0.0907,
            -0.0382,
            -0.0962,
            0.0878,
            0.1148,
            0.1837,
            -0.1171,
            -0.0994,
            0.0317
          ],
          "after": [
            -0.0195,
            -0.0824,
            -0.1957,
            0.0704,
            0.0425,
            -0.0263,
            -0.0897,
            0.0739,
            0.1199,
            -0.0976,
            0.0367,
            0.1019,
            0.0337,
            0.0149,
            -0.1036,
            0.0602
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0001,
            -0.0002,
            -0.0002,
            -0.0005,
            0.0002,
            -0.0003,
            -0.0002,
            0.0002,
            -0.0001,
            0.0,
            -0.0004,
            -0.0001,
            0.0002,
            0.0004,
            -0.0001
          ],
          "after": [
            0.0469,
            0.025,
            -0.0323,
            0.0306,
            -0.0068,
            0.1491,
            -0.0098,
            -0.0862,
            -0.0009,
            0.094,
            -0.0159,
            -0.0473,
            0.0965,
            -0.0671,
            -0.0098,
            0.0037
          ]
        }
      }
    },
    {
      "step": 28,
      "word": "taliyah",
      "loss": 2.9876,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1061,
            0.0396,
            0.0159,
            -0.1164,
            0.028,
            -0.0247,
            -0.0299,
            0.1759,
            0.0527,
            0.0035,
            -0.078,
            -0.0209,
            0.0278,
            0.0402,
            0.1348,
            -0.1712
          ],
          "after": [
            -0.0575,
            0.0892,
            0.1318,
            0.0513,
            0.014,
            -0.0337,
            0.1008,
            -0.0991,
            -0.0305,
            -0.0251,
            0.0338,
            0.0004,
            0.0186,
            -0.0325,
            -0.0239,
            -0.0228
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0403,
            0.0519,
            0.0146,
            0.0409,
            0.0673,
            -0.0591,
            0.1207,
            -0.0329,
            -0.05,
            -0.0052,
            0.0276,
            0.0346,
            0.0177,
            -0.0523,
            -0.0713,
            0.0635
          ],
          "after": [
            0.022,
            0.0687,
            0.018,
            -0.0968,
            0.0526,
            0.0122,
            0.0482,
            0.1814,
            -0.0452,
            -0.06,
            0.0827,
            -0.0237,
            0.0634,
            0.0467,
            -0.0608,
            0.1444
          ]
        },
        "position_0": {
          "grad": [
            0.0065,
            -0.22,
            0.056,
            -0.0033,
            -0.0228,
            0.0874,
            0.0903,
            0.0014,
            -0.1019,
            -0.0298,
            -0.1177,
            -0.2425,
            0.096,
            0.0458,
            0.0235,
            0.0931
          ],
          "after": [
            -0.0186,
            -0.0817,
            -0.1961,
            0.0699,
            0.0423,
            -0.0267,
            -0.0912,
            0.0738,
            0.1202,
            -0.0966,
            0.0365,
            0.1031,
            0.0333,
            0.0156,
            -0.1033,
            0.0603
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0008,
            0.0003,
            -0.0008,
            -0.0005,
            -0.0004,
            0.0003,
            -0.0002,
            0.0008,
            0.0008,
            0.0003,
            -0.0006,
            0.0003,
            -0.0002,
            -0.0,
            -0.0008
          ],
          "after": [
            0.0483,
            0.0232,
            -0.0325,
            0.0315,
            -0.0053,
            0.1495,
            -0.0105,
            -0.0856,
            -0.0026,
            0.0921,
            -0.0172,
            -0.0465,
            0.0966,
            -0.0672,
            -0.0107,
            0.0044
          ]
        }
      }
    },
    {
      "step": 29,
      "word": "hayley",
      "loss": 2.6819,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1024,
            -0.0285,
            0.0226,
            -0.0462,
            -0.0689,
            0.094,
            0.0817,
            -0.0175,
            -0.0374,
            0.0337,
            -0.1447,
            0.0595,
            0.0809,
            0.0865,
            0.0979,
            0.084
          ],
          "after": [
            -0.0586,
            0.0893,
            0.1323,
            0.0527,
            0.0152,
            -0.0346,
            0.1004,
            -0.0989,
            -0.0306,
            -0.0246,
            0.0352,
            0.0011,
            0.0177,
            -0.034,
            -0.0253,
            -0.0226
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2224,
            -0.0548,
            0.0902,
            -0.0715,
            -0.0058,
            0.1512,
            -0.269,
            0.1195,
            0.0313,
            -0.1251,
            -0.1784,
            -0.13,
            0.0938,
            0.3149,
            -0.0982,
            -0.1334
          ],
          "after": [
            0.0207,
            0.0689,
            0.0173,
            -0.0955,
            0.0523,
            0.0115,
            0.05,
            0.1814,
            -0.0459,
            -0.0599,
            0.0837,
            -0.0228,
            0.0639,
            0.045,
            -0.061,
            0.146
          ]
        },
        "position_0": {
          "grad": [
            -0.0331,
            -0.1403,
            0.0893,
            -0.039,
            -0.0443,
            0.0946,
            -0.0136,
            0.0556,
            0.1393,
            0.015,
            -0.0166,
            -0.0861,
            0.0804,
            0.0486,
            0.0758,
            0.0095
          ],
          "after": [
            -0.0176,
            -0.0806,
            -0.1969,
            0.0697,
            0.0426,
            -0.0275,
            -0.0924,
            0.0735,
            0.1199,
            -0.0958,
            0.0364,
            0.1045,
            0.0327,
            0.0159,
            -0.1035,
            0.0603
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0001,
            0.0002,
            -0.0,
            0.0002,
            -0.0002,
            0.0002,
            0.0,
            -0.0001,
            0.0002,
            0.0001,
            0.0002,
            0.0002,
            -0.0003,
            -0.0001,
            0.0
          ],
          "after": [
            0.0498,
            0.0214,
            -0.033,
            0.0324,
            -0.0042,
            0.1502,
            -0.0114,
            -0.0852,
            -0.0038,
            0.0901,
            -0.0186,
            -0.046,
            0.0964,
            -0.0669,
            -0.0114,
            0.0048
          ]
        }
      }
    },
    {
      "step": 30,
      "word": "alleah",
      "loss": 2.7052,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1389,
            0.1119,
            0.0769,
            -0.0631,
            -0.035,
            0.0652,
            0.008,
            0.2111,
            0.027,
            0.0005,
            -0.0599,
            0.0286,
            0.0339,
            0.0173,
            0.1533,
            -0.1902
          ],
          "after": [
            -0.059,
            0.0888,
            0.1323,
            0.0543,
            0.0164,
            -0.0356,
            0.1,
            -0.0995,
            -0.0309,
            -0.0242,
            0.0368,
            0.0017,
            0.0167,
            -0.0354,
            -0.0271,
            -0.0218
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.006,
            -0.1352,
            -0.0305,
            -0.2105,
            -0.0268,
            0.1182,
            -0.1671,
            0.2286,
            0.0402,
            -0.1561,
            -0.2184,
            -0.1773,
            0.0752,
            0.0639,
            0.1848,
            -0.2536
          ],
          "after": [
            0.0197,
            0.0695,
            0.0169,
            -0.0938,
            0.0522,
            0.0105,
            0.0518,
            0.1804,
            -0.0466,
            -0.0592,
            0.0854,
            -0.0214,
            0.0641,
            0.0433,
            -0.0616,
            0.1482
          ]
        },
        "position_0": {
          "grad": [
            0.117,
            0.0073,
            0.0633,
            -0.0969,
            -0.0108,
            0.0469,
            -0.174,
            -0.065,
            0.1259,
            -0.0843,
            0.1194,
            -0.0054,
            -0.1353,
            0.1393,
            0.0269,
            -0.1141
          ],
          "after": [
            -0.0174,
            -0.0797,
            -0.1979,
            0.07,
            0.043,
            -0.0286,
            -0.0929,
            0.0735,
            0.119,
            -0.0946,
            0.0358,
            0.1058,
            0.0327,
            0.0156,
            -0.1038,
            0.0609
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0003,
            0.0002,
            0.0002,
            -0.0001,
            -0.0003,
            0.0005,
            -0.0004,
            -0.0002,
            -0.0001,
            0.0003,
            -0.0001,
            0.0002,
            -0.0001,
            0.0001,
            -0.0
          ],
          "after": [
            0.0514,
            0.0195,
            -0.0337,
            0.0328,
            -0.0031,
            0.1514,
            -0.0126,
            -0.084,
            -0.0044,
            0.0885,
            -0.0205,
            -0.0455,
            0.096,
            -0.0663,
            -0.0122,
            0.0053
          ]
        }
      }
    },
    {
      "step": 31,
      "word": "kymberlynn",
      "loss": 3.0901,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0594,
            0.0883,
            0.1322,
            0.0557,
            0.0175,
            -0.0365,
            0.0996,
            -0.1001,
            -0.0311,
            -0.0238,
            0.0382,
            0.0021,
            0.0159,
            -0.0365,
            -0.0286,
            -0.0211
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0112,
            -0.0118,
            -0.0835,
            -0.0268,
            0.0117,
            0.0113,
            -0.0413,
            0.1216,
            0.0251,
            -0.1133,
            0.0286,
            -0.2054,
            -0.0691,
            0.0254,
            -0.0957,
            -0.0575
          ],
          "after": [
            0.0187,
            0.0702,
            0.0167,
            -0.0922,
            0.052,
            0.0095,
            0.0535,
            0.1789,
            -0.0474,
            -0.0581,
            0.0867,
            -0.0195,
            0.0645,
            0.0418,
            -0.0619,
            0.1503
          ]
        },
        "position_0": {
          "grad": [
            -0.0632,
            0.0261,
            -0.1126,
            -0.0219,
            0.0081,
            -0.0513,
            0.1052,
            0.0044,
            -0.0383,
            0.0121,
            0.0091,
            -0.0367,
            0.0202,
            -0.0151,
            -0.0405,
            -0.0597
          ],
          "after": [
            -0.0169,
            -0.079,
            -0.1982,
            0.0704,
            0.0432,
            -0.0292,
            -0.0936,
            0.0735,
            0.1184,
            -0.0937,
            0.0352,
            0.1071,
            0.0326,
            0.0154,
            -0.1039,
            0.0616
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            0.0001,
            0.0001,
            0.0001,
            -0.0002,
            0.0001,
            -0.0002,
            -0.0001,
            -0.0,
            -0.0003,
            -0.0002,
            0.0002,
            -0.0003,
            -0.0002,
            0.0005,
            -0.0004
          ],
          "after": [
            0.0527,
            0.0177,
            -0.0345,
            0.033,
            -0.0019,
            0.1522,
            -0.0135,
            -0.0829,
            -0.0049,
            0.0878,
            -0.0216,
            -0.0454,
            0.0961,
            -0.0655,
            -0.0137,
            0.0062
          ]
        }
      }
    },
    {
      "step": 32,
      "word": "parrish",
      "loss": 3.0129,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0424,
            -0.0165,
            0.012,
            -0.0209,
            0.1213,
            0.1195,
            0.0793,
            -0.0982,
            0.0806,
            -0.0001,
            0.0585,
            0.0463,
            -0.059,
            -0.109,
            0.0132,
            0.263
          ],
          "after": [
            -0.0598,
            0.088,
            0.1321,
            0.057,
            0.0177,
            -0.0379,
            0.0989,
            -0.1002,
            -0.0318,
            -0.0235,
            0.039,
            0.0023,
            0.0155,
            -0.0369,
            -0.03,
            -0.0213
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0373,
            0.0343,
            -0.0128,
            0.0344,
            0.0535,
            -0.0816,
            0.1356,
            -0.0171,
            -0.0447,
            -0.002,
            0.002,
            0.0395,
            0.0106,
            -0.0564,
            -0.0867,
            0.0691
          ],
          "after": [
            0.018,
            0.0706,
            0.0166,
            -0.091,
            0.0516,
            0.009,
            0.0547,
            0.1778,
            -0.0479,
            -0.0571,
            0.0878,
            -0.018,
            0.0648,
            0.0407,
            -0.0619,
            0.1518
          ]
        },
        "position_0": {
          "grad": [
            0.0794,
            0.1722,
            -0.073,
            -0.0735,
            0.011,
            -0.0987,
            0.2389,
            0.0407,
            -0.1592,
            0.0226,
            -0.0109,
            0.0198,
            -0.1185,
            0.1252,
            -0.0105,
            0.1432
          ],
          "after": [
            -0.017,
            -0.079,
            -0.1981,
            0.0711,
            0.0433,
            -0.029,
            -0.095,
            0.0734,
            0.1186,
            -0.093,
            0.0347,
            0.108,
            0.033,
            0.0146,
            -0.1039,
            0.0616
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0006,
            0.0001,
            -0.0014,
            -0.0002,
            -0.0,
            -0.0001,
            0.0001,
            0.0006,
            0.0016,
            0.0006,
            -0.001,
            0.0003,
            0.0,
            -0.0,
            -0.0018
          ],
          "after": [
            0.0525,
            0.0154,
            -0.0355,
            0.0347,
            -0.0005,
            0.153,
            -0.0141,
            -0.082,
            -0.0064,
            0.0856,
            -0.0238,
            -0.044,
            0.0959,
            -0.0648,
            -0.015,
            0.0084
          ]
        }
      }
    },
    {
      "step": 33,
      "word": "houstyn",
      "loss": 2.8573,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0602,
            0.0877,
            0.132,
            0.0582,
            0.0179,
            -0.039,
            0.0983,
            -0.1002,
            -0.0323,
            -0.0233,
            0.0397,
            0.0025,
            0.0151,
            -0.0372,
            -0.0312,
            -0.0216
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0504,
            0.0375,
            -0.0048,
            0.0492,
            0.0901,
            -0.0888,
            0.1413,
            0.0153,
            -0.0535,
            0.0096,
            0.0127,
            0.0596,
            0.0336,
            -0.0819,
            -0.1132,
            0.0922
          ],
          "after": [
            0.0176,
            0.0708,
            0.0165,
            -0.0901,
            0.0509,
            0.009,
            0.0554,
            0.1767,
            -0.0481,
            -0.0563,
            0.0887,
            -0.0169,
            0.0649,
            0.04,
            -0.0616,
            0.1528
          ]
        },
        "position_0": {
          "grad": [
            -0.0433,
            -0.1015,
            0.0787,
            -0.0195,
            -0.0121,
            0.082,
            -0.0026,
            0.0439,
            0.109,
            0.0181,
            -0.0015,
            -0.07,
            0.0926,
            0.0463,
            0.0482,
            0.0369
          ],
          "after": [
            -0.0167,
            -0.0787,
            -0.1983,
            0.0719,
            0.0436,
            -0.0294,
            -0.0961,
            0.073,
            0.1184,
            -0.0925,
            0.0343,
            0.1092,
            0.0329,
            0.0137,
            -0.1041,
            0.0614
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0001,
            0.0003,
            -0.0003,
            0.0008,
            -0.0005,
            0.0009,
            0.0002,
            0.0001,
            0.0001,
            0.0,
            0.0001,
            0.0011,
            -0.0006,
            -0.0012,
            -0.0005
          ],
          "after": [
            0.0516,
            0.0136,
            -0.0369,
            0.0366,
            -0.0005,
            0.1545,
            -0.0156,
            -0.0817,
            -0.0078,
            0.0835,
            -0.0257,
            -0.0428,
            0.0943,
            -0.0633,
            -0.0142,
            0.0108
          ]
        }
      }
    },
    {
      "step": 34,
      "word": "jamaya",
      "loss": 2.7501,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0538,
            0.1061,
            0.0403,
            0.1781,
            -0.0228,
            0.1707,
            0.3405,
            0.2468,
            -0.0479,
            0.0262,
            0.0075,
            0.1612,
            -0.0229,
            -0.1332,
            0.0312,
            0.2205
          ],
          "after": [
            -0.0603,
            0.0869,
            0.1317,
            0.0579,
            0.0182,
            -0.0408,
            0.0964,
            -0.1012,
            -0.0325,
            -0.0233,
            0.0402,
            0.0018,
            0.0149,
            -0.0366,
            -0.0324,
            -0.0225
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0915,
            0.0866,
            0.0188,
            0.0507,
            0.1395,
            -0.1278,
            0.1901,
            -0.0187,
            -0.0896,
            0.0136,
            0.0297,
            0.0561,
            0.0324,
            -0.0839,
            -0.1541,
            0.1032
          ],
          "after": [
            0.0175,
            0.0706,
            0.0164,
            -0.0894,
            0.0498,
            0.0095,
            0.0555,
            0.1759,
            -0.0478,
            -0.0557,
            0.0894,
            -0.0162,
            0.0649,
            0.0397,
            -0.0609,
            0.1533
          ]
        },
        "position_0": {
          "grad": [
            -0.1147,
            0.1024,
            0.0941,
            0.0988,
            0.0585,
            -0.0358,
            0.123,
            0.1111,
            -0.0971,
            -0.1108,
            0.0951,
            0.1712,
            0.2226,
            -0.1823,
            -0.1286,
            0.0663
          ],
          "after": [
            -0.0159,
            -0.0788,
            -0.199,
            0.072,
            0.0431,
            -0.0295,
            -0.0975,
            0.0723,
            0.1186,
            -0.0915,
            0.0335,
            0.1094,
            0.0321,
            0.0138,
            -0.1036,
            0.0609
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0001,
            -0.0001,
            0.0001,
            -0.0001,
            0.0005,
            -0.0004,
            0.0003,
            0.0001,
            0.0002,
            0.0001,
            0.0003,
            -0.0003,
            0.0,
            -0.0,
            0.0001
          ],
          "after": [
            0.0508,
            0.0122,
            -0.0379,
            0.0381,
            -0.0003,
            0.1549,
            -0.0164,
            -0.0819,
            -0.0092,
            0.0816,
            -0.0275,
            -0.0423,
            0.0933,
            -0.062,
            -0.0135,
            0.0128
          ]
        }
      }
    },
    {
      "step": 35,
      "word": "ahmod",
      "loss": 3.2714,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1012,
            -0.1112,
            0.1366,
            -0.0364,
            0.0347,
            0.0531,
            0.0338,
            0.0981,
            0.1385,
            0.0473,
            -0.0259,
            -0.0408,
            0.1305,
            0.005,
            -0.058,
            0.1022
          ],
          "after": [
            -0.06,
            0.0869,
            0.1307,
            0.058,
            0.0183,
            -0.0426,
            0.0946,
            -0.1024,
            -0.0335,
            -0.0236,
            0.0409,
            0.0015,
            0.0141,
            -0.0362,
            -0.0332,
            -0.0236
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0202,
            0.0493,
            -0.0275,
            0.0678,
            0.1065,
            -0.0755,
            0.1502,
            0.0012,
            -0.0807,
            -0.0098,
            0.0069,
            0.0088,
            0.0526,
            -0.0587,
            -0.1587,
            0.0988
          ],
          "after": [
            0.0175,
            0.0702,
            0.0164,
            -0.0891,
            0.0485,
            0.0102,
            0.0553,
            0.1751,
            -0.0471,
            -0.0551,
            0.09,
            -0.0156,
            0.0646,
            0.0396,
            -0.0598,
            0.1534
          ]
        },
        "position_0": {
          "grad": [
            0.1705,
            0.0062,
            0.0842,
            -0.1244,
            -0.0145,
            0.0546,
            -0.2067,
            -0.1151,
            0.1823,
            -0.109,
            0.1512,
            -0.0271,
            -0.155,
            0.2216,
            0.0608,
            -0.1437
          ],
          "after": [
            -0.0161,
            -0.0789,
            -0.2,
            0.0727,
            0.0429,
            -0.0299,
            -0.098,
            0.0722,
            0.1179,
            -0.0899,
            0.0322,
            0.1097,
            0.0319,
            0.013,
            -0.1035,
            0.0612
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            -0.0009,
            0.0001,
            -0.0009,
            -0.001,
            0.0001,
            -0.0005,
            0.0003,
            0.0006,
            0.0007,
            0.0005,
            0.0006,
            -0.0002,
            -0.0006,
            0.0003,
            0.0002
          ],
          "after": [
            0.052,
            0.0125,
            -0.039,
            0.0402,
            0.001,
            0.1551,
            -0.0166,
            -0.0826,
            -0.0112,
            0.0792,
            -0.0299,
            -0.0427,
            0.0928,
            -0.0602,
            -0.0133,
            0.0143
          ]
        }
      }
    },
    {
      "step": 36,
      "word": "nivin",
      "loss": 2.7473,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0598,
            0.0868,
            0.1297,
            0.058,
            0.0183,
            -0.0441,
            0.0931,
            -0.1035,
            -0.0343,
            -0.0239,
            0.0414,
            0.0012,
            0.0134,
            -0.0358,
            -0.0338,
            -0.0245
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0668,
            0.0275,
            -0.0152,
            0.0289,
            0.1097,
            -0.0948,
            0.1758,
            -0.0115,
            -0.0407,
            0.0024,
            0.0158,
            0.0341,
            0.0082,
            -0.0728,
            -0.1215,
            0.0791
          ],
          "after": [
            0.0177,
            0.0698,
            0.0164,
            -0.0889,
            0.0469,
            0.0113,
            0.0547,
            0.1746,
            -0.0464,
            -0.0546,
            0.0904,
            -0.0153,
            0.0644,
            0.0397,
            -0.0586,
            0.1532
          ]
        },
        "position_0": {
          "grad": [
            0.0566,
            -0.098,
            0.018,
            -0.1464,
            -0.1998,
            0.028,
            -0.1736,
            -0.1903,
            0.2308,
            -0.1095,
            -0.0409,
            -0.1933,
            -0.1164,
            0.1495,
            0.1492,
            -0.07
          ],
          "after": [
            -0.0167,
            -0.0786,
            -0.2009,
            0.0741,
            0.0444,
            -0.0305,
            -0.0979,
            0.0728,
            0.1165,
            -0.088,
            0.0312,
            0.1108,
            0.0322,
            0.0117,
            -0.1042,
            0.0617
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0,
            0.0001,
            0.0001,
            -0.0006,
            -0.0001,
            -0.0007,
            -0.0003,
            0.0001,
            -0.0003,
            -0.0004,
            -0.0006,
            0.0002,
            0.0007,
            0.0002,
            0.0001
          ],
          "after": [
            0.0524,
            0.0128,
            -0.0402,
            0.042,
            0.0027,
            0.1555,
            -0.0161,
            -0.0827,
            -0.0132,
            0.0775,
            -0.0312,
            -0.0422,
            0.0921,
            -0.0598,
            -0.0134,
            0.0155
          ]
        }
      }
    },
    {
      "step": 37,
      "word": "milli",
      "loss": 2.7537,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0595,
            0.0867,
            0.1289,
            0.058,
            0.0184,
            -0.0455,
            0.0918,
            -0.1044,
            -0.035,
            -0.0242,
            0.0419,
            0.0009,
            0.0128,
            -0.0354,
            -0.0344,
            -0.0253
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0684,
            0.0505,
            -0.0348,
            0.0513,
            0.0974,
            -0.0778,
            0.1341,
            -0.0422,
            -0.0729,
            0.0023,
            0.036,
            0.0084,
            0.0181,
            -0.0679,
            -0.108,
            0.1003
          ],
          "after": [
            0.0182,
            0.0692,
            0.0165,
            -0.0889,
            0.0451,
            0.0125,
            0.054,
            0.1743,
            -0.0455,
            -0.0542,
            0.0906,
            -0.015,
            0.0642,
            0.0401,
            -0.0573,
            0.1527
          ]
        },
        "position_0": {
          "grad": [
            -0.1099,
            0.1066,
            -0.0027,
            0.1648,
            0.1089,
            -0.0452,
            0.2496,
            0.2202,
            -0.0067,
            -0.0458,
            -0.004,
            -0.006,
            -0.1142,
            -0.1448,
            -0.0092,
            0.1045
          ],
          "after": [
            -0.0165,
            -0.0788,
            -0.2017,
            0.0744,
            0.0447,
            -0.0307,
            -0.0986,
            0.0725,
            0.1153,
            -0.0861,
            0.0304,
            0.1117,
            0.0329,
            0.0111,
            -0.1048,
            0.0617
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0002,
            -0.0002,
            0.0002,
            -0.0007,
            -0.0002,
            -0.0002,
            0.0004,
            0.0,
            0.0002,
            0.0,
            0.0002,
            0.0001,
            -0.0001,
            0.0005,
            0.0004
          ],
          "after": [
            0.053,
            0.0133,
            -0.0408,
            0.0433,
            0.0049,
            0.1561,
            -0.0154,
            -0.0834,
            -0.0151,
            0.0759,
            -0.0323,
            -0.042,
            0.0915,
            -0.0592,
            -0.0142,
            0.016
          ]
        }
      }
    },
    {
      "step": 38,
      "word": "cristiana",
      "loss": 2.7992,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.134,
            -0.0645,
            -0.0477,
            0.0454,
            -0.1667,
            0.0469,
            -0.0636,
            -0.1009,
            0.049,
            -0.1046,
            0.0598,
            -0.1981,
            -0.0036,
            0.074,
            0.053,
            -0.0156
          ],
          "after": [
            -0.0599,
            0.087,
            0.1286,
            0.0577,
            0.0193,
            -0.0468,
            0.0909,
            -0.1048,
            -0.0358,
            -0.0236,
            0.0419,
            0.0016,
            0.0122,
            -0.0356,
            -0.0351,
            -0.026
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0676,
            0.0386,
            0.0085,
            0.0495,
            0.0806,
            -0.0608,
            0.1283,
            -0.035,
            -0.0263,
            0.0109,
            0.0195,
            0.0333,
            0.0065,
            -0.0788,
            -0.0996,
            0.076
          ],
          "after": [
            0.0188,
            0.0685,
            0.0166,
            -0.0891,
            0.0433,
            0.0138,
            0.053,
            0.1742,
            -0.0445,
            -0.0538,
            0.0906,
            -0.0149,
            0.0639,
            0.0406,
            -0.0558,
            0.152
          ]
        },
        "position_0": {
          "grad": [
            -0.0581,
            0.0227,
            0.0133,
            -0.0114,
            -0.022,
            -0.0128,
            0.0739,
            0.0314,
            0.0191,
            0.0033,
            0.1165,
            0.0688,
            -0.032,
            -0.0307,
            -0.0548,
            0.01
          ],
          "after": [
            -0.0161,
            -0.079,
            -0.2025,
            0.0747,
            0.0452,
            -0.0308,
            -0.0993,
            0.0721,
            0.1141,
            -0.0845,
            0.0291,
            0.1122,
            0.0335,
            0.0108,
            -0.105,
            0.0616
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0002,
            -0.0005,
            -0.0006,
            -0.0002,
            -0.0003,
            -0.0005,
            0.0001,
            -0.0001,
            0.0002,
            -0.0004,
            -0.0001,
            -0.0001,
            0.0005,
            0.0002,
            -0.0004
          ],
          "after": [
            0.0529,
            0.014,
            -0.0404,
            0.045,
            0.007,
            0.1571,
            -0.0143,
            -0.0844,
            -0.0165,
            0.0742,
            -0.0326,
            -0.0417,
            0.091,
            -0.0595,
            -0.0151,
            0.0169
          ]
        }
      }
    },
    {
      "step": 39,
      "word": "jaimee",
      "loss": 2.5129,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2211,
            -0.1095,
            0.1012,
            -0.0306,
            0.1481,
            0.0041,
            0.002,
            0.0313,
            0.0193,
            -0.0812,
            0.0502,
            -0.0111,
            -0.1076,
            -0.06,
            -0.0307,
            0.0515
          ],
          "after": [
            -0.0593,
            0.0879,
            0.1276,
            0.0577,
            0.0193,
            -0.0481,
            0.0902,
            -0.1052,
            -0.0367,
            -0.0225,
            0.0416,
            0.0023,
            0.0124,
            -0.0354,
            -0.0356,
            -0.0267
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.337,
            -0.1974,
            0.1384,
            -0.2196,
            -0.8913,
            0.4391,
            -0.3946,
            -0.1244,
            0.4793,
            -0.1936,
            -0.066,
            -0.1385,
            -0.1671,
            0.5411,
            0.7867,
            -0.2057
          ],
          "after": [
            0.0182,
            0.0688,
            0.0162,
            -0.0885,
            0.0444,
            0.0132,
            0.0531,
            0.1748,
            -0.0456,
            -0.0527,
            0.091,
            -0.0142,
            0.0644,
            0.0396,
            -0.0566,
            0.1521
          ]
        },
        "position_0": {
          "grad": [
            -0.1417,
            0.129,
            0.0835,
            0.1021,
            0.1032,
            -0.0499,
            0.1537,
            0.1068,
            -0.1392,
            -0.0979,
            0.1022,
            0.2033,
            0.239,
            -0.2229,
            -0.17,
            0.0933
          ],
          "after": [
            -0.015,
            -0.0797,
            -0.2035,
            0.0744,
            0.0448,
            -0.0305,
            -0.1005,
            0.0713,
            0.1138,
            -0.0825,
            0.0276,
            0.1118,
            0.0333,
            0.0114,
            -0.1043,
            0.0611
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0,
            0.0001,
            0.0,
            -0.0003,
            -0.0,
            0.0,
            -0.0001,
            0.0002,
            -0.0001,
            -0.0001,
            0.0,
            -0.0001,
            0.0001,
            0.0001,
            0.0
          ],
          "after": [
            0.0528,
            0.0145,
            -0.0404,
            0.0464,
            0.0091,
            0.1581,
            -0.0133,
            -0.085,
            -0.018,
            0.0728,
            -0.0327,
            -0.0415,
            0.0908,
            -0.0597,
            -0.016,
            0.0176
          ]
        }
      }
    },
    {
      "step": 40,
      "word": "mitchell",
      "loss": 2.987,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0588,
            0.0886,
            0.1268,
            0.0577,
            0.0193,
            -0.0491,
            0.0895,
            -0.1056,
            -0.0374,
            -0.0215,
            0.0413,
            0.0028,
            0.0125,
            -0.0352,
            -0.036,
            -0.0273
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1407,
            -0.0588,
            0.0952,
            -0.0489,
            -0.2197,
            0.0562,
            -0.0806,
            -0.0009,
            0.1322,
            0.1738,
            0.0955,
            0.0035,
            0.0135,
            -0.0064,
            0.2567,
            -0.0288
          ],
          "after": [
            0.0181,
            0.0693,
            0.0156,
            -0.0879,
            0.0459,
            0.0124,
            0.0533,
            0.1753,
            -0.047,
            -0.0525,
            0.0909,
            -0.0137,
            0.0647,
            0.0387,
            -0.0577,
            0.1523
          ]
        },
        "position_0": {
          "grad": [
            -0.0791,
            0.087,
            0.0022,
            0.1207,
            0.0858,
            -0.0307,
            0.1822,
            0.1424,
            -0.0173,
            -0.0309,
            0.0128,
            0.0069,
            -0.0659,
            -0.0921,
            -0.0033,
            0.0838
          ],
          "after": [
            -0.0136,
            -0.0806,
            -0.2044,
            0.0737,
            0.0439,
            -0.0301,
            -0.102,
            0.0701,
            0.1135,
            -0.0807,
            0.0262,
            0.1113,
            0.0333,
            0.0123,
            -0.1037,
            0.0603
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0003,
            -0.0002,
            -0.0004,
            0.0001,
            -0.0001,
            -0.0001,
            0.0007,
            -0.0004,
            -0.0001,
            -0.0001,
            -0.0002,
            -0.0,
            -0.0,
            0.0002,
            -0.0002
          ],
          "after": [
            0.0527,
            0.0154,
            -0.04,
            0.0481,
            0.0108,
            0.1592,
            -0.0124,
            -0.0867,
            -0.0186,
            0.0719,
            -0.0325,
            -0.041,
            0.0905,
            -0.0599,
            -0.017,
            0.0184
          ]
        }
      }
    },
    {
      "step": 41,
      "word": "nairah",
      "loss": 2.505,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2815,
            -0.1743,
            0.2519,
            -0.0503,
            0.1052,
            0.0311,
            0.0747,
            0.0682,
            0.1281,
            -0.0676,
            0.0343,
            -0.0984,
            -0.013,
            -0.0363,
            -0.0448,
            0.0774
          ],
          "after": [
            -0.0574,
            0.0901,
            0.1249,
            0.058,
            0.0188,
            -0.0502,
            0.0886,
            -0.1062,
            -0.0388,
            -0.0202,
            0.0408,
            0.0038,
            0.0127,
            -0.0348,
            -0.0361,
            -0.0281
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0739,
            0.0771,
            -0.0191,
            0.0587,
            0.0941,
            -0.1285,
            0.1914,
            -0.0386,
            -0.0719,
            0.0147,
            0.0036,
            0.0468,
            0.0222,
            -0.0933,
            -0.1552,
            0.1074
          ],
          "after": [
            0.0183,
            0.0694,
            0.0152,
            -0.0875,
            0.047,
            0.0123,
            0.0531,
            0.1759,
            -0.0479,
            -0.0525,
            0.0907,
            -0.0134,
            0.0649,
            0.0381,
            -0.0584,
            0.1521
          ]
        },
        "position_0": {
          "grad": [
            0.0915,
            -0.1028,
            -0.0028,
            -0.1439,
            -0.2275,
            0.0177,
            -0.1614,
            -0.1551,
            0.2174,
            -0.0964,
            -0.0606,
            -0.2039,
            -0.1206,
            0.1599,
            0.1703,
            -0.0857
          ],
          "after": [
            -0.0129,
            -0.0809,
            -0.2052,
            0.0737,
            0.0446,
            -0.0298,
            -0.1028,
            0.0697,
            0.1125,
            -0.0786,
            0.0253,
            0.1118,
            0.0337,
            0.0124,
            -0.104,
            0.06
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0001,
            0.0003,
            -0.0004,
            0.0004,
            -0.0,
            -0.0004,
            -0.0006,
            -0.0001,
            0.0001,
            0.0004,
            -0.0006,
            -0.0002,
            0.0006,
            0.0,
            -0.0008
          ],
          "after": [
            0.0523,
            0.0163,
            -0.0403,
            0.0499,
            0.0118,
            0.1601,
            -0.0112,
            -0.0871,
            -0.0188,
            0.0709,
            -0.033,
            -0.0398,
            0.0907,
            -0.0609,
            -0.018,
            0.0199
          ]
        }
      }
    },
    {
      "step": 42,
      "word": "lorena",
      "loss": 2.4597,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1073,
            0.0628,
            0.0292,
            0.0257,
            -0.0877,
            -0.0092,
            0.0579,
            -0.0378,
            -0.0456,
            0.0218,
            0.0124,
            -0.072,
            0.0976,
            0.0252,
            0.0035,
            -0.0428
          ],
          "after": [
            -0.0566,
            0.091,
            0.1231,
            0.0581,
            0.0188,
            -0.051,
            0.0876,
            -0.1066,
            -0.0397,
            -0.0192,
            0.0403,
            0.0049,
            0.0123,
            -0.0346,
            -0.0362,
            -0.0287
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1434,
            -0.1301,
            0.0366,
            -0.1281,
            0.0365,
            0.0636,
            -0.219,
            0.1713,
            0.0678,
            -0.2602,
            0.0553,
            -0.0028,
            -0.1485,
            -0.1284,
            0.3524,
            -0.089
          ],
          "after": [
            0.0189,
            0.07,
            0.0147,
            -0.0868,
            0.0478,
            0.0119,
            0.0534,
            0.1756,
            -0.049,
            -0.0513,
            0.0904,
            -0.0132,
            0.0656,
            0.0381,
            -0.0597,
            0.1522
          ]
        },
        "position_0": {
          "grad": [
            0.0521,
            0.0312,
            -0.1148,
            -0.1876,
            -0.0074,
            0.0876,
            -0.249,
            0.11,
            0.0691,
            0.0519,
            -0.0914,
            0.0196,
            -0.1731,
            0.1317,
            0.1958,
            -0.2773
          ],
          "after": [
            -0.0126,
            -0.0814,
            -0.2053,
            0.0746,
            0.0452,
            -0.0302,
            -0.1027,
            0.069,
            0.1113,
            -0.0771,
            0.025,
            0.1122,
            0.0346,
            0.012,
            -0.1052,
            0.061
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0004,
            0.0003,
            -0.0004,
            -0.0005,
            0.0,
            -0.0003,
            -0.0001,
            0.0002,
            -0.0005,
            -0.0001,
            -0.0004,
            0.0002,
            0.0003,
            0.0002,
            -0.0002
          ],
          "after": [
            0.0516,
            0.0177,
            -0.0411,
            0.0518,
            0.0132,
            0.1608,
            -0.0099,
            -0.0873,
            -0.0193,
            0.0706,
            -0.0333,
            -0.0382,
            0.0906,
            -0.0621,
            -0.019,
            0.0214
          ]
        }
      }
    },
    {
      "step": 43,
      "word": "gentrie",
      "loss": 2.805,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0559,
            0.0918,
            0.1216,
            0.0582,
            0.0188,
            -0.0518,
            0.0867,
            -0.1069,
            -0.0404,
            -0.0184,
            0.0399,
            0.0059,
            0.012,
            -0.0345,
            -0.0363,
            -0.0291
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.5753,
            -0.3616,
            0.0288,
            -0.0765,
            -0.2701,
            0.1386,
            -0.62,
            0.1986,
            0.1934,
            -0.1494,
            0.0082,
            -0.243,
            0.037,
            0.4738,
            0.3941,
            -0.4048
          ],
          "after": [
            0.0178,
            0.0717,
            0.0141,
            -0.0859,
            0.0491,
            0.0111,
            0.0548,
            0.1744,
            -0.0505,
            -0.0498,
            0.09,
            -0.012,
            0.0662,
            0.0368,
            -0.0615,
            0.1536
          ]
        },
        "position_0": {
          "grad": [
            -0.0828,
            -0.0169,
            -0.0146,
            0.0366,
            -0.0519,
            -0.0607,
            -0.0137,
            -0.0509,
            -0.0246,
            0.0655,
            0.0912,
            0.0312,
            -0.0813,
            -0.0642,
            -0.094,
            0.0481
          ],
          "after": [
            -0.0119,
            -0.0817,
            -0.2053,
            0.0751,
            0.0461,
            -0.0301,
            -0.1026,
            0.0685,
            0.1103,
            -0.0762,
            0.0243,
            0.1124,
            0.0357,
            0.0119,
            -0.1057,
            0.0616
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0,
            -0.0003,
            -0.0003,
            -0.0004,
            0.0002,
            -0.0002,
            -0.0,
            -0.0001,
            -0.0001,
            -0.0001,
            -0.0004,
            0.0,
            0.0001,
            0.0004,
            -0.0002
          ],
          "after": [
            0.0506,
            0.0189,
            -0.0412,
            0.0538,
            0.0148,
            0.1611,
            -0.0085,
            -0.0874,
            -0.0197,
            0.0705,
            -0.0334,
            -0.0364,
            0.0904,
            -0.0633,
            -0.0204,
            0.023
          ]
        }
      }
    },
    {
      "step": 44,
      "word": "torrion",
      "loss": 2.5813,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0553,
            0.0925,
            0.1202,
            0.0582,
            0.0188,
            -0.0524,
            0.0859,
            -0.1072,
            -0.0411,
            -0.0177,
            0.0395,
            0.0068,
            0.0117,
            -0.0343,
            -0.0364,
            -0.0295
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0544,
            0.0529,
            -0.0154,
            0.0568,
            0.0612,
            -0.1169,
            0.1737,
            -0.0289,
            -0.0792,
            0.0398,
            0.0036,
            0.0488,
            0.0687,
            -0.0936,
            -0.1637,
            0.0741
          ],
          "after": [
            0.017,
            0.073,
            0.0137,
            -0.0853,
            0.0501,
            0.0108,
            0.0557,
            0.1735,
            -0.0516,
            -0.0486,
            0.0897,
            -0.0112,
            0.0663,
            0.036,
            -0.0628,
            0.1545
          ]
        },
        "position_0": {
          "grad": [
            0.0174,
            -0.1595,
            0.0461,
            0.0385,
            -0.0215,
            0.0619,
            0.1229,
            -0.0211,
            -0.129,
            -0.0535,
            -0.105,
            -0.2714,
            0.1527,
            0.0456,
            0.0141,
            0.1382
          ],
          "after": [
            -0.0114,
            -0.0814,
            -0.2055,
            0.0755,
            0.047,
            -0.0304,
            -0.1029,
            0.0682,
            0.1101,
            -0.0751,
            0.0242,
            0.1135,
            0.0361,
            0.0117,
            -0.1063,
            0.0615
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0001,
            -0.0002,
            -0.0005,
            0.0002,
            0.0002,
            -0.0005,
            -0.0002,
            0.0001,
            0.0001,
            -0.0,
            -0.0004,
            -0.0003,
            0.0003,
            0.0001,
            -0.0004
          ],
          "after": [
            0.0492,
            0.0201,
            -0.0409,
            0.056,
            0.0159,
            0.1611,
            -0.0068,
            -0.0871,
            -0.0201,
            0.0702,
            -0.0334,
            -0.0343,
            0.0908,
            -0.0648,
            -0.0216,
            0.0246
          ]
        }
      }
    },
    {
      "step": 45,
      "word": "savian",
      "loss": 2.3756,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1897,
            0.0171,
            0.0055,
            0.1392,
            0.0339,
            -0.0171,
            -0.0626,
            -0.103,
            -0.0228,
            -0.0993,
            -0.005,
            -0.0145,
            0.1059,
            -0.1655,
            -0.1681,
            0.2192
          ],
          "after": [
            -0.0541,
            0.093,
            0.1191,
            0.0574,
            0.0186,
            -0.0529,
            0.0856,
            -0.107,
            -0.0415,
            -0.0163,
            0.0392,
            0.0076,
            0.0109,
            -0.0332,
            -0.0356,
            -0.0306
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0604,
            0.0604,
            -0.0175,
            0.0437,
            0.0886,
            -0.1401,
            0.1923,
            -0.0298,
            -0.0943,
            0.0341,
            -0.0008,
            0.0574,
            0.0314,
            -0.0897,
            -0.1524,
            0.0802
          ],
          "after": [
            0.0165,
            0.074,
            0.0134,
            -0.085,
            0.0507,
            0.0111,
            0.0561,
            0.1729,
            -0.0521,
            -0.0478,
            0.0894,
            -0.0108,
            0.0663,
            0.0355,
            -0.0635,
            0.1551
          ]
        },
        "position_0": {
          "grad": [
            -0.0427,
            0.1309,
            -0.0385,
            0.0574,
            -0.0535,
            0.0615,
            0.0197,
            0.1336,
            -0.0958,
            0.147,
            -0.0837,
            0.1448,
            0.1317,
            -0.0515,
            0.0507,
            -0.0017
          ],
          "after": [
            -0.0108,
            -0.0816,
            -0.2055,
            0.0755,
            0.0482,
            -0.0311,
            -0.1032,
            0.0674,
            0.1102,
            -0.075,
            0.0245,
            0.114,
            0.0359,
            0.0116,
            -0.107,
            0.0615
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0008,
            -0.0002,
            0.0005,
            -0.0002,
            0.0012,
            -0.0006,
            0.0011,
            0.0001,
            0.0007,
            0.0001,
            0.0012,
            -0.0007,
            0.0,
            -0.0004,
            0.0004
          ],
          "after": [
            0.0478,
            0.022,
            -0.0402,
            0.0572,
            0.0171,
            0.1594,
            -0.0048,
            -0.0884,
            -0.0207,
            0.0692,
            -0.0337,
            -0.0341,
            0.092,
            -0.0662,
            -0.0222,
            0.0256
          ]
        }
      }
    },
    {
      "step": 46,
      "word": "benjamine",
      "loss": 3.0772,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1673,
            0.0473,
            0.0009,
            0.0924,
            0.0279,
            -0.0313,
            0.1508,
            0.1889,
            0.0581,
            -0.005,
            0.0399,
            0.0475,
            -0.0702,
            -0.1455,
            -0.0049,
            0.1291
          ],
          "after": [
            -0.0525,
            0.0932,
            0.118,
            0.056,
            0.0183,
            -0.0531,
            0.0846,
            -0.1076,
            -0.0423,
            -0.0151,
            0.0387,
            0.008,
            0.0106,
            -0.0315,
            -0.0349,
            -0.032
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2631,
            -0.1391,
            -0.0935,
            -0.2213,
            0.1249,
            0.1353,
            -0.2986,
            0.0441,
            0.0247,
            -0.0977,
            0.0997,
            -0.2887,
            -0.1189,
            0.1295,
            0.0014,
            -0.3714
          ],
          "after": [
            0.0154,
            0.0752,
            0.0135,
            -0.084,
            0.0509,
            0.0108,
            0.0569,
            0.1722,
            -0.0527,
            -0.0466,
            0.0887,
            -0.0094,
            0.0668,
            0.0347,
            -0.0642,
            0.1566
          ]
        },
        "position_0": {
          "grad": [
            -0.0222,
            0.0062,
            0.1313,
            -0.0017,
            0.0289,
            -0.023,
            0.0374,
            0.0206,
            -0.1325,
            -0.07,
            -0.0275,
            0.0937,
            0.1207,
            -0.0897,
            -0.0275,
            0.1843
          ],
          "after": [
            -0.0101,
            -0.0818,
            -0.2061,
            0.0755,
            0.0489,
            -0.0316,
            -0.1035,
            0.0667,
            0.1109,
            -0.0746,
            0.0249,
            0.114,
            0.0354,
            0.012,
            -0.1075,
            0.0607
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0007,
            -0.0002,
            0.0004,
            -0.0008,
            0.0007,
            -0.0015,
            0.0001,
            0.0002,
            -0.0004,
            0.0002,
            -0.0011,
            -0.0004,
            -0.0,
            0.001,
            -0.0001
          ],
          "after": [
            0.046,
            0.0246,
            -0.0392,
            0.0579,
            0.0188,
            0.157,
            -0.0019,
            -0.0896,
            -0.0215,
            0.0689,
            -0.0343,
            -0.0328,
            0.0935,
            -0.0673,
            -0.0237,
            0.0265
          ]
        }
      }
    },
    {
      "step": 47,
      "word": "airess",
      "loss": 2.9102,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1796,
            -0.1449,
            0.0775,
            0.0269,
            0.0419,
            0.0527,
            0.0045,
            0.0676,
            -0.0151,
            -0.0763,
            0.0582,
            -0.0055,
            -0.1399,
            -0.0327,
            0.0675,
            0.0097
          ],
          "after": [
            -0.0505,
            0.0941,
            0.1167,
            0.0547,
            0.0178,
            -0.0536,
            0.0837,
            -0.1084,
            -0.0428,
            -0.0135,
            0.0378,
            0.0085,
            0.0111,
            -0.0298,
            -0.0346,
            -0.0332
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.146,
            -0.1597,
            0.0524,
            -0.1473,
            0.0818,
            0.0973,
            -0.2164,
            0.1477,
            0.0905,
            -0.2459,
            0.0528,
            0.0055,
            -0.2035,
            -0.1465,
            0.3657,
            -0.0994
          ],
          "after": [
            0.0149,
            0.0769,
            0.0134,
            -0.0827,
            0.051,
            0.0103,
            0.0581,
            0.1708,
            -0.0535,
            -0.0447,
            0.0878,
            -0.0082,
            0.068,
            0.0345,
            -0.0655,
            0.1582
          ]
        },
        "position_0": {
          "grad": [
            0.2422,
            -0.051,
            0.0326,
            -0.213,
            -0.1042,
            0.093,
            -0.2599,
            -0.1497,
            0.238,
            -0.0765,
            0.1097,
            -0.1183,
            -0.2067,
            0.2674,
            0.1331,
            -0.1824
          ],
          "after": [
            -0.0108,
            -0.0818,
            -0.2069,
            0.0765,
            0.0502,
            -0.0325,
            -0.1031,
            0.0666,
            0.1106,
            -0.0738,
            0.0247,
            0.1145,
            0.0357,
            0.0112,
            -0.1085,
            0.0607
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0003,
            0.0002,
            0.0004,
            -0.0004,
            -0.0,
            0.0005,
            -0.0001,
            -0.0003,
            -0.0001,
            0.0001,
            -0.0,
            0.0002,
            -0.0001,
            0.0002,
            0.0
          ],
          "after": [
            0.0448,
            0.0263,
            -0.0387,
            0.058,
            0.0208,
            0.1551,
            0.0001,
            -0.0906,
            -0.0218,
            0.0688,
            -0.035,
            -0.0316,
            0.0945,
            -0.0682,
            -0.0253,
            0.0273
          ]
        }
      }
    },
    {
      "step": 48,
      "word": "knute",
      "loss": 3.0852,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0488,
            0.0949,
            0.1156,
            0.0536,
            0.0173,
            -0.054,
            0.0829,
            -0.109,
            -0.0432,
            -0.0122,
            0.0371,
            0.0088,
            0.0115,
            -0.0283,
            -0.0344,
            -0.0343
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2546,
            -0.3036,
            0.1342,
            -0.2128,
            -0.2104,
            0.1907,
            -0.4847,
            0.1179,
            0.1981,
            -0.2139,
            -0.0407,
            -0.0717,
            -0.1984,
            0.3397,
            0.2981,
            -0.0537
          ],
          "after": [
            0.0138,
            0.0792,
            0.0128,
            -0.0809,
            0.0515,
            0.0092,
            0.06,
            0.1692,
            -0.0549,
            -0.0424,
            0.0873,
            -0.0069,
            0.0697,
            0.0334,
            -0.0671,
            0.1597
          ]
        },
        "position_0": {
          "grad": [
            -0.187,
            0.1088,
            -0.1761,
            0.1005,
            0.1684,
            -0.1654,
            0.2652,
            0.0497,
            -0.1651,
            0.0203,
            0.0126,
            0.0448,
            0.1545,
            -0.1195,
            -0.2336,
            -0.0263
          ],
          "after": [
            -0.0104,
            -0.0822,
            -0.2066,
            0.0768,
            0.0503,
            -0.0323,
            -0.1034,
            0.0664,
            0.1109,
            -0.0732,
            0.0245,
            0.1147,
            0.0354,
            0.0111,
            -0.1083,
            0.0609
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            0.0013,
            0.0006,
            0.0008,
            0.0024,
            0.0008,
            -0.0017,
            0.0004,
            -0.0013,
            -0.0,
            0.0003,
            -0.0003,
            0.0,
            -0.0002,
            -0.0019,
            -0.0008
          ],
          "after": [
            0.0423,
            0.0262,
            -0.0393,
            0.0573,
            0.0201,
            0.1525,
            0.0029,
            -0.0919,
            -0.0202,
            0.0687,
            -0.0361,
            -0.0303,
            0.0954,
            -0.0687,
            -0.0245,
            0.0287
          ]
        }
      }
    },
    {
      "step": 49,
      "word": "sultana",
      "loss": 2.653,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1194,
            0.0385,
            -0.009,
            0.0399,
            -0.137,
            -0.1051,
            0.0315,
            -0.1947,
            -0.0814,
            -0.0688,
            -0.0107,
            -0.1473,
            0.0743,
            0.115,
            0.0583,
            -0.0917
          ],
          "after": [
            -0.0478,
            0.0954,
            0.1147,
            0.0523,
            0.0177,
            -0.0537,
            0.0821,
            -0.1088,
            -0.0431,
            -0.0105,
            0.0365,
            0.0099,
            0.0114,
            -0.0278,
            -0.0346,
            -0.0348
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.078,
            0.1161,
            -0.0503,
            0.0725,
            0.107,
            -0.2011,
            0.2243,
            -0.0297,
            -0.14,
            0.0624,
            0.0073,
            0.0962,
            0.0574,
            -0.1214,
            -0.178,
            0.1204
          ],
          "after": [
            0.013,
            0.0808,
            0.0125,
            -0.0796,
            0.0516,
            0.0089,
            0.0611,
            0.1679,
            -0.0556,
            -0.0406,
            0.0868,
            -0.0061,
            0.071,
            0.0328,
            -0.0682,
            0.1606
          ]
        },
        "position_0": {
          "grad": [
            -0.0838,
            0.1587,
            -0.0073,
            0.1327,
            0.0147,
            0.0393,
            0.0811,
            0.128,
            -0.1215,
            0.1125,
            -0.0408,
            0.1642,
            0.1808,
            -0.0819,
            -0.0097,
            0.0511
          ],
          "after": [
            -0.0097,
            -0.0831,
            -0.2063,
            0.0765,
            0.0502,
            -0.0323,
            -0.104,
            0.0657,
            0.1116,
            -0.0734,
            0.0245,
            0.1143,
            0.0346,
            0.0112,
            -0.1081,
            0.0608
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            0.0003,
            0.0002,
            -0.0,
            0.0005,
            -0.0003,
            0.0002,
            -0.0003,
            -0.0002,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0003,
            -0.0001,
            -0.0001,
            -0.0003
          ],
          "after": [
            0.0401,
            0.0257,
            -0.0402,
            0.0566,
            0.0191,
            0.1507,
            0.0052,
            -0.0926,
            -0.0186,
            0.0687,
            -0.0369,
            -0.0292,
            0.0957,
            -0.069,
            -0.0237,
            0.0302
          ]
        }
      }
    },
    {
      "step": 50,
      "word": "danai",
      "loss": 2.5447,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.207,
            -0.2438,
            0.0108,
            -0.1331,
            -0.1099,
            0.015,
            -0.1396,
            0.0013,
            0.1991,
            -0.1573,
            0.0413,
            -0.0914,
            -0.2957,
            0.0652,
            0.1685,
            -0.0589
          ],
          "after": [
            -0.0462,
            0.0969,
            0.1139,
            0.0522,
            0.0186,
            -0.0536,
            0.0821,
            -0.1086,
            -0.0442,
            -0.008,
            0.0357,
            0.0112,
            0.0128,
            -0.0277,
            -0.0355,
            -0.0351
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0463,
            0.1393,
            -0.0623,
            0.0642,
            0.1158,
            -0.193,
            0.2614,
            -0.0372,
            -0.1534,
            0.0631,
            0.0033,
            0.0486,
            0.0734,
            -0.0881,
            -0.2075,
            0.1024
          ],
          "after": [
            0.0125,
            0.0817,
            0.0124,
            -0.0787,
            0.0515,
            0.0093,
            0.0616,
            0.167,
            -0.0556,
            -0.0393,
            0.0863,
            -0.0057,
            0.0718,
            0.0325,
            -0.0687,
            0.1611
          ]
        },
        "position_0": {
          "grad": [
            0.0836,
            -0.0519,
            -0.0484,
            -0.1101,
            0.0292,
            -0.1291,
            -0.0391,
            -0.0932,
            0.108,
            0.0146,
            -0.2409,
            -0.1558,
            0.0083,
            0.2476,
            0.1583,
            0.0513
          ],
          "after": [
            -0.0096,
            -0.0837,
            -0.2059,
            0.0768,
            0.05,
            -0.0316,
            -0.1043,
            0.0655,
            0.1118,
            -0.0736,
            0.0256,
            0.1145,
            0.0338,
            0.0105,
            -0.1086,
            0.0606
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            0.0005,
            0.0003,
            -0.0002,
            0.001,
            -0.0002,
            0.001,
            -0.0006,
            -0.0002,
            -0.0002,
            0.0,
            -0.0002,
            0.0003,
            -0.0001,
            -0.0008,
            -0.0004
          ],
          "after": [
            0.0382,
            0.0247,
            -0.0414,
            0.0563,
            0.0176,
            0.1493,
            0.0063,
            -0.0924,
            -0.0168,
            0.069,
            -0.0377,
            -0.0279,
            0.0956,
            -0.0691,
            -0.0223,
            0.0319
          ]
        }
      }
    },
    {
      "step": 51,
      "word": "azzan",
      "loss": 3.3917,
      "learning_rate": 0.0029,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2376,
            0.1532,
            -0.1573,
            0.1046,
            0.347,
            -0.2751,
            0.2408,
            -0.089,
            0.2164,
            -0.0817,
            -0.0118,
            0.3492,
            0.0024,
            -0.1531,
            -0.4407,
            0.3599
          ],
          "after": [
            -0.0441,
            0.0975,
            0.1141,
            0.0514,
            0.0177,
            -0.0521,
            0.081,
            -0.1081,
            -0.0461,
            -0.0054,
            0.0351,
            0.0107,
            0.014,
            -0.0267,
            -0.0344,
            -0.0365
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1084,
            0.1726,
            -0.1105,
            0.084,
            0.1301,
            -0.2329,
            0.321,
            -0.0088,
            -0.2025,
            0.0561,
            -0.0045,
            0.0889,
            0.108,
            -0.1048,
            -0.2869,
            0.1411
          ],
          "after": [
            0.0123,
            0.0818,
            0.0128,
            -0.0782,
            0.0511,
            0.0104,
            0.0614,
            0.1663,
            -0.0549,
            -0.0385,
            0.086,
            -0.0056,
            0.0721,
            0.0325,
            -0.0685,
            0.1611
          ]
        },
        "position_0": {
          "grad": [
            0.2371,
            0.0427,
            0.0765,
            -0.1595,
            0.0057,
            0.0829,
            -0.1736,
            -0.1696,
            0.1888,
            -0.1118,
            0.1824,
            -0.0586,
            -0.1288,
            0.2635,
            0.0741,
            -0.1194
          ],
          "after": [
            -0.0104,
            -0.0844,
            -0.2059,
            0.0777,
            0.0498,
            -0.0315,
            -0.1042,
            0.0659,
            0.1113,
            -0.0731,
            0.0257,
            0.1149,
            0.0336,
            0.009,
            -0.1094,
            0.0608
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            -0.0012,
            0.0,
            -0.0008,
            0.001,
            0.0006,
            -0.0018,
            0.0003,
            0.0007,
            0.0012,
            0.0004,
            0.0012,
            -0.0018,
            -0.0003,
            0.0002,
            -0.0001
          ],
          "after": [
            0.0358,
            0.0251,
            -0.0425,
            0.0568,
            0.0155,
            0.1475,
            0.0084,
            -0.0926,
            -0.0164,
            0.068,
            -0.0392,
            -0.0281,
            0.0973,
            -0.0688,
            -0.0213,
            0.0334
          ]
        }
      }
    },
    {
      "step": 52,
      "word": "issabelle",
      "loss": 3.0106,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0764,
            -0.0169,
            0.2097,
            0.0399,
            0.023,
            -0.0018,
            -0.0731,
            0.0746,
            -0.0779,
            -0.1699,
            -0.0795,
            0.1673,
            0.1262,
            -0.0745,
            -0.128,
            0.3129
          ],
          "after": [
            -0.0421,
            0.098,
            0.1131,
            0.0505,
            0.0169,
            -0.0509,
            0.0804,
            -0.108,
            -0.0473,
            -0.0023,
            0.0352,
            0.0096,
            0.0143,
            -0.0255,
            -0.033,
            -0.0386
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0724,
            -0.1993,
            -0.0878,
            -0.1318,
            -0.1271,
            0.2503,
            -0.1275,
            0.2533,
            0.1125,
            -0.0626,
            0.0066,
            -0.3104,
            -0.1187,
            0.0798,
            0.2279,
            -0.3173
          ],
          "after": [
            0.012,
            0.0826,
            0.0134,
            -0.0774,
            0.0511,
            0.0106,
            0.0615,
            0.1646,
            -0.0548,
            -0.0375,
            0.0856,
            -0.0045,
            0.0728,
            0.0324,
            -0.0688,
            0.162
          ]
        },
        "position_0": {
          "grad": [
            -0.0563,
            -0.1467,
            0.0441,
            -0.1155,
            0.0353,
            0.0984,
            -0.1318,
            0.0033,
            0.1023,
            -0.0301,
            0.0191,
            -0.0486,
            -0.2012,
            0.0476,
            0.1001,
            -0.0537
          ],
          "after": [
            -0.011,
            -0.0844,
            -0.2061,
            0.0789,
            0.0494,
            -0.0319,
            -0.1036,
            0.0663,
            0.1105,
            -0.0726,
            0.0257,
            0.1154,
            0.0341,
            0.0075,
            -0.1104,
            0.0613
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            -0.0006,
            0.0003,
            0.0003,
            -0.0002,
            0.0008,
            -0.0024,
            0.0001,
            0.0004,
            -0.0007,
            -0.0005,
            0.0004,
            -0.0005,
            0.0017,
            -0.0002,
            0.0005
          ],
          "after": [
            0.0326,
            0.0261,
            -0.0439,
            0.0569,
            0.0139,
            0.145,
            0.0114,
            -0.0928,
            -0.0166,
            0.0679,
            -0.0394,
            -0.0286,
            0.0993,
            -0.0705,
            -0.0203,
            0.0342
          ]
        }
      }
    },
    {
      "step": 53,
      "word": "abrahim",
      "loss": 2.9919,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1955,
            -0.0416,
            0.3193,
            -0.0359,
            0.1809,
            0.0161,
            0.0015,
            0.1453,
            0.0132,
            -0.0584,
            -0.0984,
            0.2433,
            0.2688,
            -0.1764,
            -0.2746,
            0.4642
          ],
          "after": [
            -0.0397,
            0.0987,
            0.111,
            0.0499,
            0.0154,
            -0.0499,
            0.0799,
            -0.1084,
            -0.0484,
            0.0008,
            0.036,
            0.0077,
            0.0135,
            -0.0236,
            -0.0308,
            -0.0414
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0719,
            0.1344,
            -0.0475,
            0.0695,
            0.1354,
            -0.1813,
            0.2326,
            -0.0479,
            -0.1847,
            0.0588,
            -0.0176,
            0.0935,
            0.0844,
            -0.0848,
            -0.2247,
            0.1474
          ],
          "after": [
            0.0119,
            0.0829,
            0.014,
            -0.0769,
            0.0507,
            0.0112,
            0.0612,
            0.1633,
            -0.054,
            -0.0369,
            0.0854,
            -0.0039,
            0.073,
            0.0324,
            -0.0687,
            0.1623
          ]
        },
        "position_0": {
          "grad": [
            0.2102,
            -0.0336,
            0.0267,
            -0.1883,
            -0.0797,
            0.0655,
            -0.2057,
            -0.1435,
            0.193,
            -0.0732,
            0.1165,
            -0.1115,
            -0.1595,
            0.2274,
            0.1175,
            -0.1335
          ],
          "after": [
            -0.0123,
            -0.0843,
            -0.2064,
            0.0807,
            0.0495,
            -0.0327,
            -0.1026,
            0.0671,
            0.1091,
            -0.0717,
            0.0252,
            0.1163,
            0.035,
            0.0056,
            -0.1119,
            0.0622
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0007,
            -0.0005,
            -0.0007,
            0.0005,
            0.0002,
            -0.0013,
            0.0002,
            0.0008,
            0.0,
            -0.0003,
            0.0001,
            -0.0006,
            0.0006,
            -0.0005,
            0.0
          ],
          "after": [
            0.0293,
            0.0276,
            -0.0443,
            0.0577,
            0.0122,
            0.1427,
            0.0146,
            -0.0933,
            -0.0178,
            0.0677,
            -0.039,
            -0.0292,
            0.1015,
            -0.0724,
            -0.0189,
            0.0348
          ]
        }
      }
    },
    {
      "step": 54,
      "word": "aislyn",
      "loss": 2.3662,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1533,
            -0.14,
            0.0482,
            0.0486,
            0.0246,
            0.0586,
            0.0126,
            0.0837,
            -0.0263,
            -0.0692,
            0.0554,
            -0.0081,
            -0.1151,
            -0.0108,
            0.0907,
            -0.0004
          ],
          "after": [
            -0.0372,
            0.0999,
            0.109,
            0.0491,
            0.014,
            -0.0493,
            0.0794,
            -0.1092,
            -0.0493,
            0.0039,
            0.0363,
            0.0061,
            0.0132,
            -0.0219,
            -0.0293,
            -0.0439
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0759,
            0.094,
            -0.0548,
            0.0434,
            0.1075,
            -0.1789,
            0.2356,
            -0.019,
            -0.1531,
            0.0945,
            0.0102,
            0.0982,
            0.0473,
            -0.1268,
            -0.1668,
            0.1202
          ],
          "after": [
            0.012,
            0.0828,
            0.0148,
            -0.0766,
            0.0501,
            0.0123,
            0.0605,
            0.1623,
            -0.0528,
            -0.0367,
            0.0852,
            -0.0037,
            0.0731,
            0.0328,
            -0.0682,
            0.1623
          ]
        },
        "position_0": {
          "grad": [
            0.261,
            -0.1037,
            0.0119,
            -0.2332,
            -0.1298,
            0.0906,
            -0.3078,
            -0.1479,
            0.286,
            -0.0686,
            0.1095,
            -0.1405,
            -0.2032,
            0.2912,
            0.1682,
            -0.2151
          ],
          "after": [
            -0.0143,
            -0.0838,
            -0.2067,
            0.0831,
            0.0504,
            -0.0339,
            -0.1009,
            0.0684,
            0.107,
            -0.0705,
            0.0243,
            0.1176,
            0.0364,
            0.0031,
            -0.1138,
            0.0638
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0004,
            0.0004,
            0.0004,
            -0.0001,
            -0.0001,
            0.0007,
            0.0001,
            -0.0005,
            -0.0001,
            -0.0,
            0.0,
            0.0004,
            -0.0002,
            -0.0001,
            -0.0
          ],
          "after": [
            0.0266,
            0.0284,
            -0.0451,
            0.0579,
            0.0108,
            0.1409,
            0.0169,
            -0.0939,
            -0.0181,
            0.0677,
            -0.0387,
            -0.0298,
            0.1031,
            -0.0739,
            -0.0177,
            0.0354
          ]
        }
      }
    },
    {
      "step": 55,
      "word": "aerys",
      "loss": 2.9243,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0074,
            -0.1151,
            -0.071,
            0.1058,
            -0.1093,
            -0.0165,
            -0.0338,
            -0.1869,
            0.0369,
            0.0901,
            -0.0808,
            -0.0399,
            -0.0094,
            -0.0226,
            0.0515,
            -0.173
          ],
          "after": [
            -0.0351,
            0.1015,
            0.1076,
            0.0478,
            0.0133,
            -0.0487,
            0.0791,
            -0.1091,
            -0.0502,
            0.0059,
            0.0371,
            0.005,
            0.0131,
            -0.0203,
            -0.0281,
            -0.0455
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1305,
            -0.3498,
            -0.0206,
            -0.2153,
            0.1774,
            0.1527,
            -0.4917,
            0.0745,
            0.378,
            -0.0198,
            0.0023,
            0.0664,
            -0.1652,
            0.1099,
            0.0407,
            -0.1101
          ],
          "after": [
            0.0117,
            0.0837,
            0.0156,
            -0.0757,
            0.0492,
            0.0128,
            0.0607,
            0.1612,
            -0.0531,
            -0.0365,
            0.085,
            -0.0037,
            0.0737,
            0.0328,
            -0.0679,
            0.1626
          ]
        },
        "position_0": {
          "grad": [
            0.2919,
            -0.12,
            -0.0012,
            -0.2555,
            -0.159,
            0.0891,
            -0.3311,
            -0.1704,
            0.3156,
            -0.1027,
            0.1421,
            -0.1769,
            -0.2217,
            0.3253,
            0.2004,
            -0.2248
          ],
          "after": [
            -0.017,
            -0.0829,
            -0.207,
            0.086,
            0.052,
            -0.0355,
            -0.0986,
            0.0701,
            0.1044,
            -0.069,
            0.023,
            0.1194,
            0.0383,
            0.0001,
            -0.1162,
            0.0659
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0001,
            -0.0001,
            0.0002,
            -0.0006,
            0.0003,
            0.0001,
            -0.0001,
            -0.0002,
            0.0001,
            0.0003,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0008,
            0.0
          ],
          "after": [
            0.0247,
            0.029,
            -0.0458,
            0.0579,
            0.01,
            0.1389,
            0.0188,
            -0.0942,
            -0.0182,
            0.0675,
            -0.0389,
            -0.0302,
            0.1045,
            -0.075,
            -0.0175,
            0.0359
          ]
        }
      }
    },
    {
      "step": 56,
      "word": "malaiya",
      "loss": 2.2149,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1234,
            0.0464,
            -0.0449,
            -0.0966,
            0.0892,
            -0.0709,
            -0.0559,
            0.1398,
            -0.1636,
            0.0674,
            -0.0133,
            0.0375,
            -0.0986,
            0.022,
            0.1862,
            -0.2856
          ],
          "after": [
            -0.0329,
            0.1027,
            0.1066,
            0.0473,
            0.0123,
            -0.0479,
            0.0792,
            -0.1095,
            -0.0501,
            0.0071,
            0.0379,
            0.0038,
            0.0134,
            -0.0191,
            -0.0279,
            -0.046
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0931,
            0.1318,
            -0.05,
            0.0622,
            0.1204,
            -0.2064,
            0.241,
            -0.0486,
            -0.1978,
            0.1005,
            0.0151,
            0.0846,
            0.079,
            -0.1139,
            -0.2113,
            0.1437
          ],
          "after": [
            0.0118,
            0.0842,
            0.0164,
            -0.0751,
            0.0481,
            0.0138,
            0.0605,
            0.1604,
            -0.0527,
            -0.0367,
            0.0847,
            -0.0041,
            0.074,
            0.0331,
            -0.0672,
            0.1624
          ]
        },
        "position_0": {
          "grad": [
            -0.0611,
            0.0278,
            0.0113,
            0.1177,
            0.0288,
            -0.0182,
            0.1475,
            0.1858,
            0.0232,
            -0.0209,
            -0.0082,
            -0.0005,
            -0.1201,
            -0.0837,
            0.0472,
            0.0343
          ],
          "after": [
            -0.0191,
            -0.0822,
            -0.2073,
            0.088,
            0.0532,
            -0.0367,
            -0.0971,
            0.0709,
            0.102,
            -0.0676,
            0.0218,
            0.1209,
            0.0402,
            -0.0022,
            -0.1184,
            0.0677
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0005,
            -0.0002,
            0.0003,
            0.0002,
            -0.0001,
            0.0007,
            -0.0001,
            -0.0006,
            -0.0002,
            0.0,
            -0.0005,
            0.0007,
            0.0001,
            -0.0003,
            -0.0
          ],
          "after": [
            0.0231,
            0.0291,
            -0.046,
            0.0575,
            0.0092,
            0.1372,
            0.02,
            -0.0943,
            -0.0175,
            0.0676,
            -0.0391,
            -0.0299,
            0.105,
            -0.0762,
            -0.017,
            0.0363
          ]
        }
      }
    },
    {
      "step": 57,
      "word": "kieon",
      "loss": 2.6066,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0309,
            0.1036,
            0.1057,
            0.0468,
            0.0114,
            -0.0471,
            0.0792,
            -0.1099,
            -0.05,
            0.0082,
            0.0386,
            0.0028,
            0.0136,
            -0.018,
            -0.0277,
            -0.0465
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0552,
            -0.0384,
            -0.0782,
            0.316,
            -0.078,
            0.1477,
            -0.1077,
            0.3542,
            -0.0899,
            0.1023,
            0.0563,
            0.3876,
            -0.0937,
            -0.2436,
            0.1413,
            0.123
          ],
          "after": [
            0.012,
            0.0847,
            0.0173,
            -0.0755,
            0.0474,
            0.0142,
            0.0606,
            0.1584,
            -0.0521,
            -0.0373,
            0.0842,
            -0.0056,
            0.0746,
            0.034,
            -0.0669,
            0.1619
          ]
        },
        "position_0": {
          "grad": [
            -0.218,
            0.1348,
            -0.1457,
            0.0772,
            0.1939,
            -0.189,
            0.2785,
            0.0521,
            -0.203,
            0.053,
            0.0069,
            0.0727,
            0.1305,
            -0.182,
            -0.3113,
            -0.0249
          ],
          "after": [
            -0.0201,
            -0.0822,
            -0.2068,
            0.0894,
            0.0532,
            -0.0366,
            -0.0965,
            0.0713,
            0.1007,
            -0.0666,
            0.0208,
            0.1219,
            0.0415,
            -0.0036,
            -0.119,
            0.0692
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0002,
            -0.0003,
            0.0007,
            0.0,
            -0.0001,
            -0.0008,
            0.0004,
            -0.0001,
            -0.0001,
            -0.0002,
            0.0008,
            -0.0008,
            0.0001,
            0.0006,
            0.0007
          ],
          "after": [
            0.0217,
            0.0289,
            -0.0457,
            0.0566,
            0.0085,
            0.1359,
            0.0215,
            -0.0949,
            -0.0168,
            0.0677,
            -0.0389,
            -0.0305,
            0.1063,
            -0.0773,
            -0.0172,
            0.036
          ]
        }
      }
    },
    {
      "step": 58,
      "word": "ayansh",
      "loss": 2.6216,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0995,
            -0.0037,
            -0.0696,
            0.0161,
            -0.2044,
            0.0473,
            0.0493,
            -0.0783,
            0.0319,
            0.0016,
            -0.1831,
            0.0742,
            0.0665,
            0.1468,
            0.0744,
            0.1648
          ],
          "after": [
            -0.0297,
            0.1045,
            0.1053,
            0.0463,
            0.0116,
            -0.0468,
            0.079,
            -0.1099,
            -0.0501,
            0.0091,
            0.0403,
            0.0016,
            0.0135,
            -0.0179,
            -0.0278,
            -0.0474
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0856,
            0.1007,
            -0.0632,
            0.0544,
            0.1133,
            -0.2019,
            0.2511,
            -0.0076,
            -0.1672,
            0.0922,
            -0.0183,
            0.103,
            0.0624,
            -0.1258,
            -0.2029,
            0.1504
          ],
          "after": [
            0.0124,
            0.0848,
            0.0184,
            -0.0761,
            0.0465,
            0.0152,
            0.0602,
            0.1567,
            -0.0511,
            -0.0382,
            0.0838,
            -0.0073,
            0.0749,
            0.0351,
            -0.0662,
            0.161
          ]
        },
        "position_0": {
          "grad": [
            0.2301,
            -0.0948,
            -0.0478,
            -0.1909,
            -0.1417,
            0.0537,
            -0.2494,
            -0.1297,
            0.2496,
            -0.1043,
            0.1137,
            -0.1343,
            -0.1694,
            0.2428,
            0.1647,
            -0.1558
          ],
          "after": [
            -0.0217,
            -0.0818,
            -0.2061,
            0.0913,
            0.0539,
            -0.0368,
            -0.0954,
            0.0722,
            0.0988,
            -0.0653,
            0.0194,
            0.1232,
            0.0431,
            -0.0055,
            -0.12,
            0.0711
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0005,
            -0.0002,
            -0.0004,
            0.0001,
            0.0005,
            -0.0013,
            -0.0001,
            0.0005,
            0.0001,
            0.0001,
            -0.0,
            -0.0008,
            0.0005,
            0.0001,
            -0.0007
          ],
          "after": [
            0.0201,
            0.0292,
            -0.0451,
            0.0561,
            0.0077,
            0.1342,
            0.0235,
            -0.0953,
            -0.0168,
            0.0678,
            -0.039,
            -0.031,
            0.108,
            -0.0787,
            -0.0174,
            0.0364
          ]
        }
      }
    },
    {
      "step": 59,
      "word": "berklee",
      "loss": 2.8609,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0285,
            0.1052,
            0.105,
            0.0459,
            0.0117,
            -0.0464,
            0.0788,
            -0.1099,
            -0.0502,
            0.0099,
            0.0418,
            0.0007,
            0.0135,
            -0.0179,
            -0.0279,
            -0.0481
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.4031,
            -0.5972,
            -0.0163,
            -0.42,
            -0.3998,
            0.7895,
            -0.8468,
            0.153,
            0.7468,
            -0.1801,
            -0.0307,
            -0.5399,
            -0.3044,
            0.6031,
            0.6986,
            -0.7289
          ],
          "after": [
            0.0116,
            0.0864,
            0.0194,
            -0.0754,
            0.0467,
            0.014,
            0.0612,
            0.1547,
            -0.0522,
            -0.0382,
            0.0837,
            -0.007,
            0.0762,
            0.0346,
            -0.067,
            0.1621
          ]
        },
        "position_0": {
          "grad": [
            -0.0893,
            0.0785,
            0.2489,
            0.0585,
            0.1433,
            -0.0306,
            0.1272,
            0.0273,
            -0.2487,
            -0.0743,
            0.0097,
            0.1878,
            0.1912,
            -0.1729,
            -0.1522,
            0.2746
          ],
          "after": [
            -0.0227,
            -0.0818,
            -0.2068,
            0.0927,
            0.0537,
            -0.0368,
            -0.0948,
            0.0728,
            0.098,
            -0.0637,
            0.0182,
            0.1236,
            0.0438,
            -0.0066,
            -0.1204,
            0.0717
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0002,
            -0.0001,
            0.0,
            -0.0002,
            0.0,
            0.0003,
            -0.0001,
            -0.0,
            0.0004,
            -0.0001,
            -0.0,
            0.0002,
            0.0002,
            -0.0002,
            -0.0
          ],
          "after": [
            0.0186,
            0.0293,
            -0.0445,
            0.0557,
            0.0072,
            0.1327,
            0.0251,
            -0.0955,
            -0.0167,
            0.0674,
            -0.039,
            -0.0314,
            0.1094,
            -0.0802,
            -0.0174,
            0.0368
          ]
        }
      }
    },
    {
      "step": 60,
      "word": "emmakate",
      "loss": 3.1448,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2679,
            -0.167,
            0.0346,
            0.131,
            0.1343,
            -0.0422,
            0.0675,
            0.145,
            -0.0946,
            -0.2308,
            -0.2263,
            -0.074,
            0.1547,
            -0.0568,
            -0.2871,
            0.2278
          ],
          "after": [
            -0.0268,
            0.1066,
            0.1045,
            0.0448,
            0.0113,
            -0.0459,
            0.0783,
            -0.1105,
            -0.0498,
            0.0118,
            0.0443,
            0.0001,
            0.0127,
            -0.0175,
            -0.0269,
            -0.0493
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2587,
            0.0214,
            0.3067,
            -0.3245,
            -0.2851,
            0.232,
            -0.2355,
            0.1125,
            0.231,
            -0.0325,
            -0.0048,
            -0.3899,
            -0.122,
            0.2662,
            0.1914,
            -0.3616
          ],
          "after": [
            0.0104,
            0.0877,
            0.0191,
            -0.0739,
            0.0475,
            0.0125,
            0.0624,
            0.1526,
            -0.0536,
            -0.0381,
            0.0836,
            -0.0057,
            0.0777,
            0.0335,
            -0.0679,
            0.1639
          ]
        },
        "position_0": {
          "grad": [
            -0.006,
            -0.0702,
            -0.0387,
            0.0281,
            0.0078,
            0.0285,
            -0.0605,
            -0.1255,
            0.0646,
            0.0581,
            -0.044,
            0.0137,
            -0.0412,
            -0.0408,
            0.0316,
            -0.0932
          ],
          "after": [
            -0.0236,
            -0.0815,
            -0.2072,
            0.0938,
            0.0535,
            -0.037,
            -0.0942,
            0.0738,
            0.0971,
            -0.0627,
            0.0174,
            0.124,
            0.0446,
            -0.0074,
            -0.1208,
            0.0725
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0002,
            0.0,
            0.0001,
            -0.0,
            0.0003,
            -0.0005,
            0.0001,
            0.0002,
            -0.0001,
            -0.0002,
            -0.0,
            -0.0001,
            0.0003,
            -0.0001,
            0.0002
          ],
          "after": [
            0.0171,
            0.0295,
            -0.0439,
            0.0552,
            0.0069,
            0.1311,
            0.0267,
            -0.0959,
            -0.017,
            0.0672,
            -0.0386,
            -0.0317,
            0.1106,
            -0.0818,
            -0.0173,
            0.0369
          ]
        }
      }
    },
    {
      "step": 61,
      "word": "avaneesh",
      "loss": 2.7546,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1502,
            -0.0055,
            -0.0583,
            0.1204,
            0.0592,
            0.0129,
            -0.0712,
            -0.0364,
            -0.0174,
            -0.0256,
            -0.0143,
            0.0765,
            0.0818,
            -0.1175,
            -0.1772,
            0.2479
          ],
          "after": [
            -0.0248,
            0.1078,
            0.1044,
            0.0431,
            0.0107,
            -0.0456,
            0.0783,
            -0.1109,
            -0.0493,
            0.0137,
            0.0466,
            -0.0007,
            0.0118,
            -0.0165,
            -0.0255,
            -0.051
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2943,
            -0.2087,
            0.1417,
            -0.0795,
            -0.5438,
            0.4847,
            -0.4938,
            -0.1531,
            0.3345,
            -0.2175,
            -0.0204,
            -0.2577,
            -0.0217,
            0.4958,
            0.5005,
            -0.2177
          ],
          "after": [
            0.0086,
            0.0894,
            0.0184,
            -0.0724,
            0.0493,
            0.0102,
            0.0642,
            0.1513,
            -0.0556,
            -0.0372,
            0.0836,
            -0.004,
            0.0791,
            0.0316,
            -0.0696,
            0.1658
          ]
        },
        "position_0": {
          "grad": [
            0.1471,
            -0.0229,
            -0.0372,
            -0.1367,
            -0.0344,
            0.0347,
            -0.1507,
            -0.1056,
            0.1446,
            -0.065,
            0.0984,
            -0.0511,
            -0.1023,
            0.1371,
            0.0657,
            -0.0727
          ],
          "after": [
            -0.0248,
            -0.0811,
            -0.2073,
            0.0952,
            0.0536,
            -0.0374,
            -0.0932,
            0.0751,
            0.0959,
            -0.0614,
            0.0163,
            0.1244,
            0.0456,
            -0.0085,
            -0.1214,
            0.0735
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0008,
            0.0,
            0.0006,
            -0.0004,
            0.0004,
            0.0007,
            -0.0002,
            -0.0001,
            -0.0001,
            -0.0,
            -0.0004,
            0.0004,
            0.0,
            -0.0001,
            0.0003
          ],
          "after": [
            0.016,
            0.0289,
            -0.0435,
            0.0542,
            0.0069,
            0.1293,
            0.0276,
            -0.096,
            -0.0171,
            0.0672,
            -0.0383,
            -0.0315,
            0.1113,
            -0.0832,
            -0.0172,
            0.0367
          ]
        }
      }
    },
    {
      "step": 62,
      "word": "amenadiel",
      "loss": 2.4955,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1701,
            0.0328,
            -0.0525,
            0.076,
            0.2353,
            -0.1115,
            0.1306,
            0.1706,
            0.0887,
            -0.0088,
            -0.0603,
            0.1317,
            -0.0147,
            -0.0981,
            -0.0632,
            0.3137
          ],
          "after": [
            -0.0225,
            0.1087,
            0.1045,
            0.0412,
            0.0092,
            -0.0447,
            0.0776,
            -0.1118,
            -0.0494,
            0.0153,
            0.0488,
            -0.0019,
            0.011,
            -0.0152,
            -0.0241,
            -0.0532
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2024,
            -0.0241,
            -0.0534,
            0.1578,
            -0.3293,
            0.3259,
            -0.1848,
            0.2075,
            0.1882,
            -0.0028,
            0.0508,
            0.0074,
            -0.084,
            -0.0802,
            0.3747,
            -0.1355
          ],
          "after": [
            0.0066,
            0.0909,
            0.0179,
            -0.0716,
            0.0515,
            0.0076,
            0.066,
            0.1496,
            -0.0577,
            -0.0364,
            0.0834,
            -0.0025,
            0.0806,
            0.0302,
            -0.0716,
            0.1678
          ]
        },
        "position_0": {
          "grad": [
            0.1232,
            -0.0126,
            -0.0371,
            -0.1141,
            -0.0289,
            0.0313,
            -0.1283,
            -0.0846,
            0.1239,
            -0.0537,
            0.0849,
            -0.0339,
            -0.0877,
            0.1132,
            0.0509,
            -0.0622
          ],
          "after": [
            -0.0263,
            -0.0808,
            -0.2072,
            0.0969,
            0.0537,
            -0.0378,
            -0.092,
            0.0764,
            0.0945,
            -0.0601,
            0.0149,
            0.125,
            0.0467,
            -0.0097,
            -0.1221,
            0.0745
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0002,
            0.0003,
            0.0002,
            0.0001,
            0.0001,
            0.0002,
            -0.0002,
            -0.0001,
            -0.0001,
            -0.0,
            -0.0001,
            0.0001,
            -0.0,
            -0.0001,
            0.0
          ],
          "after": [
            0.0151,
            0.0282,
            -0.0436,
            0.0531,
            0.0069,
            0.1277,
            0.0284,
            -0.0958,
            -0.0171,
            0.0673,
            -0.0379,
            -0.0313,
            0.1117,
            -0.0844,
            -0.0169,
            0.0365
          ]
        }
      }
    },
    {
      "step": 63,
      "word": "renardo",
      "loss": 2.839,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0078,
            -0.0471,
            -0.0063,
            0.0323,
            0.1782,
            0.1364,
            0.0277,
            -0.0294,
            0.0656,
            -0.0519,
            0.0472,
            0.2304,
            -0.093,
            -0.1333,
            0.0006,
            0.3305
          ],
          "after": [
            -0.0206,
            0.1097,
            0.1047,
            0.0394,
            0.0073,
            -0.0446,
            0.0769,
            -0.1125,
            -0.0498,
            0.017,
            0.0505,
            -0.0038,
            0.0107,
            -0.0134,
            -0.0228,
            -0.0558
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0479,
            -0.0729,
            0.0829,
            -0.0595,
            0.3759,
            -0.0741,
            -0.1153,
            0.0038,
            0.1074,
            -0.0322,
            0.1054,
            0.1969,
            -0.2024,
            -0.0796,
            -0.177,
            0.0462
          ],
          "after": [
            0.005,
            0.0924,
            0.0173,
            -0.0707,
            0.0526,
            0.0056,
            0.0677,
            0.148,
            -0.0598,
            -0.0356,
            0.0826,
            -0.0018,
            0.0826,
            0.0291,
            -0.0731,
            0.1693
          ]
        },
        "position_0": {
          "grad": [
            0.0213,
            -0.0391,
            0.0702,
            -0.0728,
            0.0407,
            0.0891,
            0.0179,
            -0.0637,
            0.0536,
            0.0039,
            -0.034,
            0.0371,
            -0.1577,
            -0.0812,
            0.0936,
            0.1099
          ],
          "after": [
            -0.0277,
            -0.0803,
            -0.2075,
            0.0985,
            0.0537,
            -0.0388,
            -0.0911,
            0.0778,
            0.0932,
            -0.0589,
            0.0139,
            0.1253,
            0.0481,
            -0.0106,
            -0.1231,
            0.075
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0002,
            0.0001,
            0.0001,
            0.0006,
            0.0004,
            -0.0001,
            -0.0003,
            0.0,
            0.0001,
            0.0001,
            0.0006,
            -0.0006,
            -0.0,
            -0.0001,
            -0.0003
          ],
          "after": [
            0.0138,
            0.0273,
            -0.0438,
            0.0521,
            0.0064,
            0.1258,
            0.029,
            -0.0952,
            -0.0171,
            0.0673,
            -0.0377,
            -0.0318,
            0.1126,
            -0.0854,
            -0.0166,
            0.0366
          ]
        }
      }
    },
    {
      "step": 64,
      "word": "brekken",
      "loss": 3.0536,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0189,
            0.1106,
            0.1048,
            0.0378,
            0.0056,
            -0.0446,
            0.0762,
            -0.1131,
            -0.0502,
            0.0184,
            0.0519,
            -0.0054,
            0.0105,
            -0.0119,
            -0.0218,
            -0.0581
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0936,
            -0.2197,
            -0.0373,
            0.1728,
            -0.104,
            0.6138,
            -0.3765,
            0.0809,
            0.3177,
            -0.0857,
            0.1958,
            0.2047,
            -0.26,
            -0.0555,
            0.3755,
            0.0435
          ],
          "after": [
            0.0039,
            0.0943,
            0.0168,
            -0.0704,
            0.0537,
            0.0028,
            0.0697,
            0.1464,
            -0.0622,
            -0.0345,
            0.0809,
            -0.0017,
            0.0851,
            0.0283,
            -0.0749,
            0.1706
          ]
        },
        "position_0": {
          "grad": [
            -0.099,
            0.0861,
            0.2828,
            0.0466,
            0.1653,
            -0.0314,
            0.1353,
            0.0157,
            -0.2477,
            -0.0853,
            0.0131,
            0.1975,
            0.1905,
            -0.1759,
            -0.1629,
            0.2775
          ],
          "after": [
            -0.0285,
            -0.0803,
            -0.209,
            0.0998,
            0.0528,
            -0.0394,
            -0.0907,
            0.079,
            0.0928,
            -0.0574,
            0.013,
            0.1248,
            0.0487,
            -0.0107,
            -0.1233,
            0.0744
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0006,
            0.0003,
            -0.0,
            0.0011,
            -0.0003,
            0.0006,
            0.0014,
            -0.0001,
            0.0012,
            0.0003,
            0.0003,
            -0.0007,
            -0.001,
            -0.0003,
            0.0007
          ],
          "after": [
            0.0137,
            0.0261,
            -0.0445,
            0.0513,
            0.0051,
            0.1246,
            0.0292,
            -0.0965,
            -0.0169,
            0.066,
            -0.0381,
            -0.0325,
            0.1141,
            -0.0851,
            -0.016,
            0.036
          ]
        }
      }
    },
    {
      "step": 65,
      "word": "kamry",
      "loss": 2.8524,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0911,
            0.0552,
            0.0232,
            0.2063,
            0.0738,
            -0.052,
            0.284,
            0.2414,
            0.0469,
            -0.0609,
            0.1083,
            -0.0045,
            -0.0934,
            -0.134,
            0.0387,
            0.07
          ],
          "after": [
            -0.0172,
            0.111,
            0.1048,
            0.0354,
            0.0039,
            -0.0443,
            0.0745,
            -0.1146,
            -0.0507,
            0.02,
            0.0524,
            -0.0068,
            0.0107,
            -0.0099,
            -0.021,
            -0.0602
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0564,
            0.1218,
            -0.0885,
            0.0382,
            0.1177,
            -0.2317,
            0.2519,
            0.0107,
            -0.2493,
            0.1424,
            -0.0214,
            0.0322,
            0.1343,
            -0.0885,
            -0.2327,
            0.1514
          ],
          "after": [
            0.003,
            0.0955,
            0.0168,
            -0.0703,
            0.0544,
            0.0009,
            0.071,
            0.1449,
            -0.0637,
            -0.0342,
            0.0796,
            -0.0017,
            0.0868,
            0.0278,
            -0.076,
            0.1713
          ]
        },
        "position_0": {
          "grad": [
            -0.2017,
            0.1423,
            -0.0808,
            0.105,
            0.1667,
            -0.1751,
            0.3121,
            0.0592,
            -0.1962,
            0.0079,
            -0.003,
            0.0531,
            0.1133,
            -0.1425,
            -0.2654,
            -0.0351
          ],
          "after": [
            -0.0284,
            -0.0808,
            -0.2099,
            0.1005,
            0.0512,
            -0.0389,
            -0.0911,
            0.0798,
            0.0931,
            -0.0562,
            0.0122,
            0.1242,
            0.0489,
            -0.0105,
            -0.1225,
            0.074
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0001,
            -0.0002,
            -0.0001,
            0.001,
            -0.0001,
            0.0002,
            -0.0003,
            -0.0007,
            0.0007,
            0.0005,
            0.0004,
            0.0005,
            -0.0005,
            -0.0007,
            -0.0003
          ],
          "after": [
            0.0131,
            0.025,
            -0.0446,
            0.0506,
            0.0032,
            0.1237,
            0.0292,
            -0.0973,
            -0.0159,
            0.0642,
            -0.0393,
            -0.0335,
            0.1149,
            -0.0843,
            -0.0149,
            0.0358
          ]
        }
      }
    },
    {
      "step": 66,
      "word": "wylin",
      "loss": 2.8153,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0157,
            0.1114,
            0.1048,
            0.0334,
            0.0024,
            -0.0441,
            0.073,
            -0.1158,
            -0.0512,
            0.0214,
            0.0529,
            -0.0079,
            0.0109,
            -0.0082,
            -0.0204,
            -0.062
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.089,
            0.1106,
            -0.0905,
            0.0556,
            0.127,
            -0.2444,
            0.3237,
            -0.0152,
            -0.2191,
            0.1188,
            0.0242,
            0.066,
            0.0731,
            -0.147,
            -0.2318,
            0.1987
          ],
          "after": [
            0.0025,
            0.0963,
            0.0171,
            -0.0704,
            0.0548,
            -0.0003,
            0.0717,
            0.1438,
            -0.0645,
            -0.0344,
            0.0783,
            -0.0019,
            0.088,
            0.0277,
            -0.0766,
            0.1715
          ]
        },
        "position_0": {
          "grad": [
            -0.2101,
            0.4129,
            0.1384,
            0.3246,
            0.3641,
            -0.2328,
            0.5725,
            0.1258,
            -0.4431,
            -0.0501,
            0.0479,
            0.2519,
            0.2899,
            -0.3758,
            -0.5263,
            0.1889
          ],
          "after": [
            -0.0277,
            -0.0827,
            -0.2113,
            0.0999,
            0.0485,
            -0.0373,
            -0.0927,
            0.0799,
            0.0945,
            -0.0549,
            0.0113,
            0.1228,
            0.0482,
            -0.0092,
            -0.1203,
            0.0731
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.0003,
            0.0003,
            0.0011,
            -0.0006,
            -0.0012,
            -0.0007,
            0.0013,
            -0.0003,
            -0.0008,
            -0.0005,
            0.0014,
            0.0003,
            -0.0001,
            -0.0,
            0.0009
          ],
          "after": [
            0.0121,
            0.0245,
            -0.0452,
            0.049,
            0.0021,
            0.1244,
            0.0297,
            -0.0993,
            -0.0147,
            0.0636,
            -0.0394,
            -0.0356,
            0.1152,
            -0.0835,
            -0.0138,
            0.0347
          ]
        }
      }
    },
    {
      "step": 67,
      "word": "trixie",
      "loss": 2.7414,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0144,
            0.1118,
            0.1048,
            0.0316,
            0.0011,
            -0.0438,
            0.0718,
            -0.1168,
            -0.0516,
            0.0226,
            0.0533,
            -0.009,
            0.0111,
            -0.0067,
            -0.0198,
            -0.0636
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0327,
            -0.0724,
            -0.0956,
            0.1313,
            -0.3612,
            0.0536,
            -0.0832,
            0.1716,
            0.0722,
            0.0871,
            -0.0588,
            0.0231,
            0.1599,
            -0.0342,
            0.3154,
            0.1029
          ],
          "after": [
            0.0022,
            0.0971,
            0.0176,
            -0.0708,
            0.0558,
            -0.0014,
            0.0724,
            0.1421,
            -0.0653,
            -0.0349,
            0.0776,
            -0.0021,
            0.0884,
            0.0277,
            -0.0777,
            0.1714
          ]
        },
        "position_0": {
          "grad": [
            -0.0261,
            -0.1375,
            0.0798,
            0.0859,
            -0.0032,
            0.0731,
            0.1802,
            0.0166,
            -0.1943,
            -0.0529,
            -0.1555,
            -0.2727,
            0.2061,
            0.012,
            -0.0151,
            0.1701
          ],
          "after": [
            -0.027,
            -0.0837,
            -0.2128,
            0.099,
            0.0462,
            -0.0364,
            -0.0945,
            0.08,
            0.0962,
            -0.0535,
            0.0113,
            0.1225,
            0.047,
            -0.0082,
            -0.1184,
            0.0717
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0002,
            0.0004,
            0.0003,
            -0.0003,
            0.0002,
            -0.0,
            -0.0002,
            0.0005,
            0.0002,
            -0.0,
            0.0001,
            -0.0,
            0.0007,
            -0.001,
            0.0007
          ],
          "after": [
            0.0111,
            0.0238,
            -0.0464,
            0.0473,
            0.0013,
            0.1247,
            0.0301,
            -0.1008,
            -0.0143,
            0.0627,
            -0.0395,
            -0.0376,
            0.1156,
            -0.0836,
            -0.0121,
            0.0331
          ]
        }
      }
    },
    {
      "step": 68,
      "word": "breslynn",
      "loss": 2.7218,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0133,
            0.112,
            0.1048,
            0.0301,
            0.0001,
            -0.0436,
            0.0707,
            -0.1177,
            -0.052,
            0.0236,
            0.0536,
            -0.0098,
            0.0113,
            -0.0055,
            -0.0193,
            -0.0649
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1986,
            -0.0023,
            0.0338,
            0.1349,
            0.257,
            0.1076,
            -0.0075,
            0.079,
            0.0101,
            -0.0025,
            0.1416,
            0.3361,
            -0.2814,
            -0.2213,
            0.0075,
            0.1849
          ],
          "after": [
            0.0024,
            0.0978,
            0.018,
            -0.0715,
            0.0562,
            -0.0025,
            0.0729,
            0.1404,
            -0.066,
            -0.0353,
            0.0762,
            -0.0032,
            0.0897,
            0.0282,
            -0.0786,
            0.1708
          ]
        },
        "position_0": {
          "grad": [
            -0.031,
            0.0087,
            0.1873,
            0.0103,
            0.0554,
            -0.015,
            0.0581,
            0.0075,
            -0.1651,
            -0.0753,
            -0.0181,
            0.1114,
            0.1287,
            -0.1031,
            -0.0616,
            0.2051
          ],
          "after": [
            -0.0262,
            -0.0847,
            -0.2148,
            0.0983,
            0.044,
            -0.0355,
            -0.0962,
            0.08,
            0.0981,
            -0.0518,
            0.0113,
            0.122,
            0.0456,
            -0.007,
            -0.1166,
            0.0698
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0003,
            -0.0001,
            0.0003,
            0.0006,
            0.0004,
            -0.0007,
            -0.0001,
            -0.0001,
            -0.0003,
            0.0004,
            -0.0002,
            -0.0005,
            0.0,
            -0.0004,
            -0.0001
          ],
          "after": [
            0.0098,
            0.023,
            -0.0472,
            0.0456,
            0.0003,
            0.1246,
            0.0308,
            -0.1021,
            -0.0139,
            0.0624,
            -0.0403,
            -0.039,
            0.1164,
            -0.0837,
            -0.0103,
            0.032
          ]
        }
      }
    },
    {
      "step": 69,
      "word": "bianka",
      "loss": 2.7379,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2599,
            0.2166,
            -0.1181,
            -0.0051,
            -0.3091,
            -0.0621,
            0.1315,
            -0.1571,
            -0.045,
            0.0833,
            -0.0138,
            -0.2957,
            0.1942,
            0.1948,
            0.0362,
            -0.1219
          ],
          "after": [
            -0.0133,
            0.1112,
            0.1054,
            0.0288,
            0.0004,
            -0.0431,
            0.0692,
            -0.1179,
            -0.052,
            0.0239,
            0.054,
            -0.0094,
            0.0105,
            -0.0054,
            -0.019,
            -0.0657
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0942,
            0.1505,
            0.0037,
            0.0252,
            0.1405,
            -0.2474,
            0.2565,
            -0.019,
            -0.196,
            0.1038,
            0.0118,
            0.0969,
            0.0544,
            -0.0962,
            -0.2503,
            0.1568
          ],
          "after": [
            0.0029,
            0.098,
            0.0183,
            -0.0722,
            0.0562,
            -0.003,
            0.073,
            0.139,
            -0.0662,
            -0.0361,
            0.075,
            -0.0043,
            0.0906,
            0.0288,
            -0.0789,
            0.1701
          ]
        },
        "position_0": {
          "grad": [
            -0.0199,
            -0.0161,
            0.2101,
            -0.0067,
            0.0338,
            -0.0159,
            0.0203,
            0.0111,
            -0.1656,
            -0.092,
            -0.047,
            0.1241,
            0.1421,
            -0.1191,
            -0.0556,
            0.234
          ],
          "after": [
            -0.0256,
            -0.0855,
            -0.2173,
            0.0977,
            0.042,
            -0.0347,
            -0.0976,
            0.08,
            0.1001,
            -0.0499,
            0.0116,
            0.1211,
            0.044,
            -0.0057,
            -0.1149,
            0.0675
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0002,
            -0.0003,
            0.001,
            -0.0013,
            0.0006,
            -0.0009,
            0.0008,
            -0.0002,
            0.0004,
            0.0004,
            0.0013,
            -0.0002,
            -0.0007,
            0.0009,
            0.0005
          ],
          "after": [
            0.0089,
            0.0224,
            -0.0474,
            0.0432,
            0.0003,
            0.1238,
            0.032,
            -0.1039,
            -0.0133,
            0.0617,
            -0.0417,
            -0.0413,
            0.1172,
            -0.0831,
            -0.0096,
            0.0305
          ]
        }
      }
    },
    {
      "step": 70,
      "word": "mordchai",
      "loss": 2.8566,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1509,
            -0.1819,
            0.0299,
            -0.0521,
            0.1133,
            0.0646,
            -0.1174,
            0.1275,
            0.1139,
            -0.0775,
            0.0266,
            0.0623,
            -0.1863,
            -0.0423,
            0.0016,
            0.0436
          ],
          "after": [
            -0.0127,
            0.1114,
            0.1057,
            0.028,
            0.0003,
            -0.0431,
            0.0684,
            -0.1185,
            -0.0527,
            0.0247,
            0.0541,
            -0.0092,
            0.0107,
            -0.0052,
            -0.0188,
            -0.0666
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0616,
            0.1215,
            -0.0443,
            0.065,
            0.1425,
            -0.171,
            0.2247,
            0.0084,
            -0.1985,
            0.0728,
            -0.0113,
            0.0546,
            0.1074,
            -0.1043,
            -0.2927,
            0.1603
          ],
          "after": [
            0.0034,
            0.0979,
            0.0187,
            -0.073,
            0.0559,
            -0.0031,
            0.0728,
            0.1378,
            -0.0659,
            -0.037,
            0.074,
            -0.0054,
            0.0911,
            0.0295,
            -0.0787,
            0.169
          ]
        },
        "position_0": {
          "grad": [
            -0.0674,
            0.075,
            0.0499,
            0.0966,
            0.1115,
            -0.0017,
            0.1795,
            0.1284,
            -0.0096,
            -0.0244,
            0.0452,
            0.0561,
            -0.084,
            -0.0908,
            -0.0089,
            0.079
          ],
          "after": [
            -0.0247,
            -0.0864,
            -0.2197,
            0.0968,
            0.0398,
            -0.0339,
            -0.0993,
            0.0795,
            0.1019,
            -0.0481,
            0.0116,
            0.1201,
            0.0428,
            -0.0043,
            -0.1134,
            0.0653
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0002,
            -0.0005,
            0.0002,
            0.0002,
            -0.0005,
            -0.0002,
            0.0001,
            -0.0006,
            0.0005,
            0.0003,
            0.0001,
            -0.0003,
            -0.0001,
            0.0006,
            -0.0008
          ],
          "after": [
            0.0074,
            0.0222,
            -0.0467,
            0.0409,
            0.0003,
            0.1237,
            0.0331,
            -0.1057,
            -0.012,
            0.0606,
            -0.0434,
            -0.0433,
            0.1183,
            -0.0824,
            -0.0095,
            0.03
          ]
        }
      }
    },
    {
      "step": 71,
      "word": "haoyu",
      "loss": 2.9348,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.04,
            -0.0428,
            0.0957,
            -0.0188,
            0.0606,
            -0.15,
            -0.1034,
            -0.0248,
            -0.1029,
            0.1254,
            -0.0213,
            0.2395,
            -0.0017,
            -0.106,
            -0.0685,
            -0.0715
          ],
          "after": [
            -0.0124,
            0.1118,
            0.1056,
            0.0275,
            -0.0001,
            -0.0422,
            0.0682,
            -0.1189,
            -0.0527,
            0.0246,
            0.0544,
            -0.01,
            0.0109,
            -0.0045,
            -0.0183,
            -0.0671
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0057,
            0.0912,
            -0.0774,
            0.0323,
            0.0769,
            -0.1813,
            0.1784,
            0.0243,
            -0.1925,
            0.0875,
            -0.0034,
            0.0143,
            0.1156,
            -0.0731,
            -0.2147,
            0.1014
          ],
          "after": [
            0.0039,
            0.0975,
            0.0193,
            -0.0738,
            0.0555,
            -0.0028,
            0.0723,
            0.1367,
            -0.0652,
            -0.0382,
            0.0731,
            -0.0064,
            0.0911,
            0.0304,
            -0.0782,
            0.1679
          ]
        },
        "position_0": {
          "grad": [
            -0.0595,
            -0.108,
            0.0772,
            0.0276,
            -0.0885,
            0.0698,
            0.0091,
            0.076,
            0.1413,
            0.0449,
            -0.0684,
            -0.1277,
            0.1819,
            0.0674,
            0.068,
            0.0842
          ],
          "after": [
            -0.0238,
            -0.0868,
            -0.2221,
            0.096,
            0.0383,
            -0.0337,
            -0.1008,
            0.0788,
            0.103,
            -0.0468,
            0.0119,
            0.1197,
            0.0414,
            -0.0033,
            -0.1123,
            0.0631
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            0.0008,
            -0.0,
            -0.0003,
            0.0018,
            -0.0013,
            0.0018,
            0.0005,
            -0.0014,
            0.0019,
            0.001,
            0.0005,
            0.0015,
            -0.0014,
            -0.0009,
            -0.0
          ],
          "after": [
            0.0056,
            0.0212,
            -0.0462,
            0.0393,
            -0.001,
            0.1248,
            0.0329,
            -0.1076,
            -0.0097,
            0.0582,
            -0.0462,
            -0.0455,
            0.1178,
            -0.0805,
            -0.0087,
            0.0297
          ]
        }
      }
    },
    {
      "step": 72,
      "word": "frankie",
      "loss": 2.7841,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0539,
            0.0288,
            -0.0626,
            -0.0087,
            -0.1222,
            -0.0235,
            -0.0129,
            -0.0984,
            0.0501,
            -0.0375,
            -0.0221,
            -0.0691,
            -0.0176,
            0.0969,
            0.0007,
            -0.0119
          ],
          "after": [
            -0.0123,
            0.1119,
            0.1057,
            0.0271,
            0.0001,
            -0.0413,
            0.0681,
            -0.1189,
            -0.053,
            0.0247,
            0.0547,
            -0.0104,
            0.0111,
            -0.0043,
            -0.0179,
            -0.0675
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1918,
            -0.1076,
            -0.1561,
            0.1294,
            -0.267,
            -0.0133,
            -0.1442,
            0.1607,
            -0.078,
            -0.0002,
            -0.0586,
            -0.0914,
            0.1347,
            0.0509,
            0.2818,
            -0.0937
          ],
          "after": [
            0.0038,
            0.0975,
            0.0204,
            -0.0748,
            0.0557,
            -0.0025,
            0.0721,
            0.1351,
            -0.0644,
            -0.0391,
            0.0727,
            -0.007,
            0.0906,
            0.0309,
            -0.0782,
            0.1671
          ]
        },
        "position_0": {
          "grad": [
            -0.1928,
            0.3411,
            0.1144,
            0.2436,
            0.3575,
            -0.2148,
            0.2328,
            -0.0012,
            -0.502,
            -0.0351,
            0.1046,
            0.4191,
            0.2714,
            -0.2447,
            -0.2843,
            0.2123
          ],
          "after": [
            -0.0224,
            -0.0882,
            -0.2245,
            0.0945,
            0.0358,
            -0.0324,
            -0.1025,
            0.0782,
            0.1052,
            -0.0456,
            0.0117,
            0.1181,
            0.0394,
            -0.0018,
            -0.1105,
            0.0606
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            -0.0003,
            -0.0002,
            0.0002,
            -0.0,
            0.0003,
            -0.0007,
            0.0003,
            -0.0,
            -0.0001,
            0.0001,
            0.0002,
            -0.0005,
            -0.0001,
            0.0004,
            -0.0
          ],
          "after": [
            0.0041,
            0.0207,
            -0.0454,
            0.0377,
            -0.002,
            0.1255,
            0.0332,
            -0.1096,
            -0.0076,
            0.0562,
            -0.0488,
            -0.0475,
            0.1178,
            -0.0788,
            -0.0083,
            0.0294
          ]
        }
      }
    },
    {
      "step": 73,
      "word": "josselin",
      "loss": 2.7196,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0122,
            0.1121,
            0.1059,
            0.0267,
            0.0002,
            -0.0406,
            0.0681,
            -0.1189,
            -0.0532,
            0.0248,
            0.0551,
            -0.0107,
            0.0113,
            -0.0042,
            -0.0176,
            -0.0678
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0046,
            0.1745,
            0.0986,
            0.1137,
            -0.0352,
            -0.0096,
            -0.0765,
            -0.0774,
            -0.0216,
            -0.1553,
            -0.0428,
            -0.086,
            0.1357,
            0.1702,
            -0.1361,
            0.1177
          ],
          "after": [
            0.0037,
            0.097,
            0.021,
            -0.076,
            0.0559,
            -0.0023,
            0.072,
            0.134,
            -0.0637,
            -0.0393,
            0.0726,
            -0.0073,
            0.0897,
            0.0311,
            -0.078,
            0.1662
          ]
        },
        "position_0": {
          "grad": [
            -0.1219,
            0.0997,
            0.0937,
            0.1406,
            0.1478,
            -0.0597,
            0.155,
            0.0516,
            -0.196,
            -0.0659,
            0.0823,
            0.1975,
            0.2672,
            -0.2018,
            -0.177,
            0.0911
          ],
          "after": [
            -0.0207,
            -0.0898,
            -0.227,
            0.0927,
            0.0331,
            -0.031,
            -0.1044,
            0.0775,
            0.1075,
            -0.0441,
            0.0112,
            0.1162,
            0.037,
            0.0001,
            -0.1085,
            0.0582
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0001,
            -0.0,
            0.0002,
            -0.0002,
            -0.0,
            -0.0005,
            0.0002,
            -0.0,
            -0.0001,
            -0.0001,
            -0.0004,
            0.0002,
            0.0003,
            -0.0003,
            -0.0002
          ],
          "after": [
            0.0022,
            0.0202,
            -0.0447,
            0.0362,
            -0.0028,
            0.1262,
            0.0337,
            -0.1115,
            -0.0058,
            0.0546,
            -0.0509,
            -0.0489,
            0.1176,
            -0.0777,
            -0.0078,
            0.0293
          ]
        }
      }
    },
    {
      "step": 74,
      "word": "saila",
      "loss": 2.3304,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3041,
            -0.0681,
            -0.1411,
            -0.1137,
            -0.2108,
            -0.0656,
            -0.0166,
            -0.0282,
            -0.1165,
            0.0716,
            0.0096,
            -0.5618,
            0.1749,
            0.2659,
            0.1564,
            -0.3359
          ],
          "after": [
            -0.0131,
            0.1125,
            0.1067,
            0.0271,
            0.0011,
            -0.0396,
            0.0681,
            -0.1188,
            -0.0528,
            0.0245,
            0.0553,
            -0.0092,
            0.0107,
            -0.0054,
            -0.0179,
            -0.0672
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.024,
            0.0793,
            -0.0375,
            0.0082,
            0.067,
            -0.142,
            0.1707,
            -0.0434,
            -0.1359,
            0.0684,
            0.031,
            0.0019,
            0.0443,
            -0.0583,
            -0.1586,
            0.0572
          ],
          "after": [
            0.0037,
            0.0964,
            0.0217,
            -0.0771,
            0.056,
            -0.0018,
            0.0717,
            0.1333,
            -0.0628,
            -0.0398,
            0.0724,
            -0.0076,
            0.0889,
            0.0313,
            -0.0775,
            0.1653
          ]
        },
        "position_0": {
          "grad": [
            0.0447,
            0.0319,
            -0.0116,
            0.0178,
            -0.2545,
            0.1246,
            -0.1346,
            0.1285,
            0.0598,
            0.1482,
            -0.1333,
            0.0019,
            0.065,
            0.0496,
            0.1903,
            -0.1513
          ],
          "after": [
            -0.0195,
            -0.0913,
            -0.2291,
            0.0911,
            0.0319,
            -0.0305,
            -0.1057,
            0.0764,
            0.1093,
            -0.0437,
            0.0113,
            0.1145,
            0.0347,
            0.0015,
            -0.1074,
            0.0566
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0019,
            0.0007,
            -0.0,
            0.0016,
            -0.0015,
            0.0011,
            -0.0016,
            -0.001,
            -0.0,
            0.0009,
            -0.0001,
            0.001,
            -0.0006,
            -0.001,
            -0.0007
          ],
          "after": [
            0.0001,
            0.0182,
            -0.0452,
            0.035,
            -0.0045,
            0.128,
            0.0336,
            -0.1114,
            -0.0033,
            0.0533,
            -0.0538,
            -0.05,
            0.1166,
            -0.0762,
            -0.0064,
            0.0299
          ]
        }
      }
    },
    {
      "step": 75,
      "word": "kionna",
      "loss": 2.2982,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2024,
            0.189,
            -0.0743,
            -0.0041,
            -0.1575,
            -0.029,
            0.1468,
            -0.0472,
            -0.131,
            0.1396,
            0.0059,
            -0.2138,
            0.2273,
            0.0661,
            0.0348,
            -0.1138
          ],
          "after": [
            -0.0146,
            0.112,
            0.1077,
            0.0274,
            0.0025,
            -0.0386,
            0.0674,
            -0.1185,
            -0.0517,
            0.0233,
            0.0554,
            -0.0073,
            0.0093,
            -0.0067,
            -0.0183,
            -0.0664
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1073,
            0.0954,
            -0.0072,
            0.0226,
            0.109,
            -0.1889,
            0.2418,
            0.017,
            -0.1354,
            0.0904,
            0.0075,
            0.074,
            0.0439,
            -0.1122,
            -0.2114,
            0.1292
          ],
          "after": [
            0.0039,
            0.0956,
            0.0222,
            -0.078,
            0.0558,
            -0.001,
            0.071,
            0.1326,
            -0.0617,
            -0.0405,
            0.0721,
            -0.008,
            0.088,
            0.0317,
            -0.0768,
            0.1642
          ]
        },
        "position_0": {
          "grad": [
            -0.0624,
            0.0006,
            -0.1664,
            -0.0432,
            0.0197,
            -0.0698,
            0.0859,
            -0.0299,
            0.0044,
            0.0272,
            -0.0162,
            -0.0762,
            0.018,
            -0.006,
            -0.1068,
            -0.1419
          ],
          "after": [
            -0.0182,
            -0.0925,
            -0.2301,
            0.0898,
            0.0307,
            -0.0297,
            -0.107,
            0.0755,
            0.1108,
            -0.0435,
            0.0115,
            0.1133,
            0.0328,
            0.0028,
            -0.1061,
            0.0558
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0007,
            0.0007,
            0.0002,
            -0.0005,
            -0.0001,
            0.0002,
            -0.0004,
            -0.0002,
            0.0,
            -0.0001,
            -0.0001,
            0.0001,
            0.0,
            0.0002,
            0.0
          ],
          "after": [
            -0.0014,
            0.0159,
            -0.0466,
            0.0337,
            -0.0056,
            0.1297,
            0.0333,
            -0.1109,
            -0.001,
            0.0521,
            -0.0562,
            -0.0508,
            0.1156,
            -0.0749,
            -0.0054,
            0.0304
          ]
        }
      }
    },
    {
      "step": 76,
      "word": "jennings",
      "loss": 2.7781,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0158,
            0.1116,
            0.1086,
            0.0277,
            0.0036,
            -0.0377,
            0.0669,
            -0.1182,
            -0.0508,
            0.0224,
            0.0555,
            -0.0057,
            0.0081,
            -0.0078,
            -0.0187,
            -0.0657
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0785,
            -0.1301,
            0.0654,
            -0.0092,
            0.2208,
            -0.0023,
            -0.1449,
            -0.0607,
            0.1397,
            0.0894,
            0.1362,
            0.1478,
            -0.1971,
            -0.0976,
            -0.1101,
            0.0179
          ],
          "after": [
            0.0039,
            0.0952,
            0.0225,
            -0.0788,
            0.0552,
            -0.0003,
            0.0707,
            0.1322,
            -0.0611,
            -0.0415,
            0.0711,
            -0.0087,
            0.0879,
            0.0323,
            -0.076,
            0.1632
          ]
        },
        "position_0": {
          "grad": [
            -0.0792,
            0.02,
            0.0466,
            0.0796,
            0.0412,
            -0.0309,
            0.0647,
            0.0634,
            -0.0919,
            -0.0711,
            0.0413,
            0.1209,
            0.1824,
            -0.1569,
            -0.101,
            0.0289
          ],
          "after": [
            -0.0168,
            -0.0937,
            -0.2312,
            0.0885,
            0.0296,
            -0.0288,
            -0.1082,
            0.0745,
            0.1124,
            -0.043,
            0.0115,
            0.1118,
            0.0306,
            0.0043,
            -0.1047,
            0.0549
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0,
            0.0,
            0.0001,
            -0.0003,
            -0.0,
            0.0003,
            0.0003,
            0.0002,
            -0.0002,
            -0.0005,
            -0.0006,
            0.0006,
            0.0002,
            -0.0,
            0.0001
          ],
          "after": [
            -0.0025,
            0.014,
            -0.0479,
            0.0325,
            -0.0063,
            0.1312,
            0.0329,
            -0.1109,
            0.0009,
            0.0513,
            -0.0575,
            -0.051,
            0.1143,
            -0.074,
            -0.0045,
            0.0307
          ]
        }
      }
    },
    {
      "step": 77,
      "word": "arlington",
      "loss": 2.7878,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0207,
            0.01,
            -0.0087,
            -0.0158,
            0.0443,
            0.1091,
            0.0806,
            -0.0555,
            0.0315,
            -0.0154,
            0.0727,
            0.0635,
            -0.0518,
            -0.0911,
            0.0636,
            0.1191
          ],
          "after": [
            -0.0169,
            0.1112,
            0.1095,
            0.0281,
            0.0044,
            -0.0376,
            0.066,
            -0.1178,
            -0.0502,
            0.0217,
            0.0551,
            -0.0045,
            0.0072,
            -0.0083,
            -0.0192,
            -0.0655
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1282,
            0.0994,
            -0.0125,
            0.0583,
            0.1315,
            -0.1684,
            0.223,
            -0.0224,
            -0.123,
            0.0922,
            0.0346,
            0.1011,
            0.0489,
            -0.1386,
            -0.2274,
            0.1625
          ],
          "after": [
            0.0043,
            0.0947,
            0.0227,
            -0.0797,
            0.0545,
            0.0007,
            0.0701,
            0.132,
            -0.0603,
            -0.0427,
            0.0701,
            -0.0096,
            0.0877,
            0.0332,
            -0.0749,
            0.162
          ]
        },
        "position_0": {
          "grad": [
            0.1505,
            -0.0517,
            -0.0048,
            -0.1395,
            -0.0933,
            0.0722,
            -0.1976,
            -0.0862,
            0.1748,
            0.0059,
            0.0851,
            -0.1029,
            -0.1509,
            0.151,
            0.1049,
            -0.1604
          ],
          "after": [
            -0.0162,
            -0.0945,
            -0.2321,
            0.0879,
            0.029,
            -0.0285,
            -0.1088,
            0.0741,
            0.1132,
            -0.0425,
            0.0111,
            0.111,
            0.0292,
            0.0051,
            -0.1038,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0008,
            0.0001,
            0.001,
            0.0006,
            -0.001,
            0.001,
            0.0,
            -0.0005,
            -0.0002,
            0.0001,
            0.0014,
            0.0006,
            -0.0009,
            -0.0005,
            0.0011
          ],
          "after": [
            -0.0029,
            0.0116,
            -0.0491,
            0.0307,
            -0.0074,
            0.1333,
            0.032,
            -0.1109,
            0.003,
            0.0508,
            -0.0588,
            -0.0523,
            0.1127,
            -0.0724,
            -0.0034,
            0.0299
          ]
        }
      }
    },
    {
      "step": 78,
      "word": "jupiter",
      "loss": 3.0455,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0178,
            0.1108,
            0.1102,
            0.0284,
            0.0051,
            -0.0375,
            0.0653,
            -0.1174,
            -0.0497,
            0.021,
            0.0548,
            -0.0034,
            0.0065,
            -0.0087,
            -0.0197,
            -0.0652
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0122,
            -0.1566,
            -0.0174,
            -0.1049,
            -0.2302,
            0.0529,
            -0.0834,
            0.0507,
            0.1375,
            0.0543,
            -0.0272,
            0.0151,
            -0.0825,
            0.06,
            0.2395,
            0.0545
          ],
          "after": [
            0.0046,
            0.0946,
            0.023,
            -0.0801,
            0.0543,
            0.0014,
            0.0697,
            0.1317,
            -0.06,
            -0.0439,
            0.0694,
            -0.0105,
            0.0877,
            0.0338,
            -0.0744,
            0.1608
          ]
        },
        "position_0": {
          "grad": [
            -0.1396,
            0.1349,
            0.0512,
            0.1477,
            0.2083,
            -0.0412,
            0.1942,
            0.0282,
            -0.2096,
            -0.0915,
            0.133,
            0.2478,
            0.2906,
            -0.2255,
            -0.2002,
            0.1562
          ],
          "after": [
            -0.0151,
            -0.0956,
            -0.2331,
            0.0869,
            0.0277,
            -0.028,
            -0.1098,
            0.0735,
            0.1145,
            -0.0416,
            0.0101,
            0.1095,
            0.0272,
            0.0065,
            -0.1025,
            0.0541
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0016,
            0.0,
            0.0009,
            -0.0008,
            0.002,
            -0.0009,
            -0.001,
            -0.0003,
            0.0013,
            0.0012,
            0.0011,
            -0.0022,
            -0.0004,
            -0.0004,
            -0.0005
          ],
          "after": [
            -0.003,
            0.011,
            -0.0502,
            0.0283,
            -0.0077,
            0.1333,
            0.0318,
            -0.11,
            0.0051,
            0.0493,
            -0.0612,
            -0.0542,
            0.1132,
            -0.0707,
            -0.0021,
            0.0296
          ]
        }
      }
    },
    {
      "step": 79,
      "word": "aleisha",
      "loss": 2.4527,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0764,
            0.2262,
            -0.1504,
            -0.0428,
            -0.0508,
            -0.0509,
            -0.0097,
            0.0512,
            -0.2471,
            0.1114,
            -0.0559,
            -0.0264,
            0.0828,
            0.0841,
            0.1269,
            -0.2321
          ],
          "after": [
            -0.0189,
            0.1095,
            0.1115,
            0.0289,
            0.0059,
            -0.0371,
            0.0647,
            -0.1173,
            -0.048,
            0.0199,
            0.0549,
            -0.0024,
            0.0056,
            -0.0095,
            -0.0206,
            -0.0644
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0645,
            -0.0376,
            0.0874,
            0.0683,
            0.2553,
            0.2824,
            -0.0951,
            0.1521,
            0.0681,
            0.0526,
            -0.0191,
            0.2317,
            -0.2481,
            -0.0367,
            -0.0832,
            -0.0647
          ],
          "after": [
            0.0051,
            0.0947,
            0.0229,
            -0.0807,
            0.0536,
            0.0014,
            0.0695,
            0.1307,
            -0.0598,
            -0.0452,
            0.0689,
            -0.0118,
            0.0886,
            0.0343,
            -0.0738,
            0.16
          ]
        },
        "position_0": {
          "grad": [
            0.1962,
            -0.0879,
            0.021,
            -0.1595,
            -0.1205,
            0.0846,
            -0.2824,
            -0.0909,
            0.2563,
            -0.0018,
            0.0978,
            -0.1515,
            -0.1786,
            0.2001,
            0.1297,
            -0.2216
          ],
          "after": [
            -0.0149,
            -0.0963,
            -0.2341,
            0.0865,
            0.0271,
            -0.0279,
            -0.11,
            0.0735,
            0.1149,
            -0.0409,
            0.0089,
            0.1087,
            0.026,
            0.0071,
            -0.1018,
            0.0542
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0001,
            -0.0001,
            -0.0001,
            0.0002,
            -0.0005,
            0.0004,
            -0.0003,
            -0.0002,
            0.0003,
            0.0005,
            -0.0006,
            0.0004,
            -0.0001,
            -0.0001,
            -0.0002
          ],
          "after": [
            -0.0033,
            0.0105,
            -0.051,
            0.0263,
            -0.0081,
            0.1337,
            0.0313,
            -0.1089,
            0.0072,
            0.0478,
            -0.0638,
            -0.0554,
            0.1133,
            -0.0692,
            -0.0008,
            0.0296
          ]
        }
      }
    },
    {
      "step": 80,
      "word": "kemorah",
      "loss": 2.6484,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.127,
            -0.1273,
            0.0893,
            0.0359,
            0.0294,
            0.0639,
            0.0008,
            0.1164,
            0.0952,
            -0.0266,
            -0.0601,
            -0.0386,
            0.1041,
            0.0127,
            -0.0367,
            0.1242
          ],
          "after": [
            -0.0193,
            0.109,
            0.1122,
            0.0291,
            0.0065,
            -0.0372,
            0.0642,
            -0.1177,
            -0.0471,
            0.019,
            0.0554,
            -0.0015,
            0.0043,
            -0.0102,
            -0.0212,
            -0.0641
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0808,
            -0.1186,
            -0.0099,
            -0.0162,
            0.2779,
            0.0578,
            -0.187,
            -0.1073,
            0.1399,
            -0.0098,
            0.1555,
            0.1946,
            -0.0928,
            -0.0327,
            -0.2714,
            -0.0629
          ],
          "after": [
            0.0053,
            0.0951,
            0.0229,
            -0.0811,
            0.0525,
            0.0013,
            0.0696,
            0.1304,
            -0.06,
            -0.0462,
            0.0677,
            -0.0134,
            0.0896,
            0.0349,
            -0.0728,
            0.1594
          ]
        },
        "position_0": {
          "grad": [
            -0.0454,
            0.0023,
            -0.1381,
            -0.0504,
            -0.0169,
            -0.0347,
            0.0668,
            -0.0395,
            0.0231,
            0.0185,
            -0.0115,
            -0.0944,
            -0.0054,
            0.025,
            -0.0504,
            -0.1288
          ],
          "after": [
            -0.0146,
            -0.0969,
            -0.2343,
            0.0864,
            0.0266,
            -0.0278,
            -0.1103,
            0.0735,
            0.1152,
            -0.0403,
            0.0078,
            0.1083,
            0.025,
            0.0075,
            -0.101,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0002,
            0.0001,
            -0.0001,
            -0.0005,
            0.0,
            -0.0002,
            -0.0001,
            0.0002,
            -0.0004,
            -0.0002,
            0.0001,
            -0.0007,
            -0.0,
            0.0011,
            0.0001
          ],
          "after": [
            -0.0031,
            0.0101,
            -0.0517,
            0.0246,
            -0.0082,
            0.134,
            0.031,
            -0.1079,
            0.0087,
            0.0469,
            -0.0658,
            -0.0565,
            0.1138,
            -0.0678,
            -0.0008,
            0.0295
          ]
        }
      }
    },
    {
      "step": 81,
      "word": "denisse",
      "loss": 2.62,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0197,
            0.1085,
            0.1128,
            0.0293,
            0.007,
            -0.0372,
            0.0638,
            -0.118,
            -0.0463,
            0.0183,
            0.0558,
            -0.0007,
            0.0033,
            -0.0108,
            -0.0218,
            -0.0637
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1315,
            -0.1573,
            0.0432,
            -0.2027,
            -0.0885,
            0.2422,
            -0.5232,
            -0.1247,
            0.2996,
            -0.1146,
            0.1274,
            -0.2098,
            -0.0885,
            0.252,
            0.2718,
            -0.2175
          ],
          "after": [
            0.0051,
            0.0959,
            0.0227,
            -0.0809,
            0.0517,
            0.0007,
            0.0706,
            0.1305,
            -0.0609,
            -0.0467,
            0.0659,
            -0.0142,
            0.0908,
            0.0348,
            -0.0725,
            0.1595
          ]
        },
        "position_0": {
          "grad": [
            0.0062,
            -0.04,
            -0.0064,
            -0.0809,
            0.0273,
            -0.0808,
            -0.0409,
            -0.0432,
            0.0858,
            0.0119,
            -0.1525,
            -0.0952,
            -0.0179,
            0.1584,
            0.1196,
            0.052
          ],
          "after": [
            -0.0144,
            -0.0972,
            -0.2345,
            0.0866,
            0.026,
            -0.0272,
            -0.1104,
            0.0738,
            0.1153,
            -0.0399,
            0.0077,
            0.1083,
            0.0242,
            0.0074,
            -0.1007,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0003,
            -0.0,
            0.0005,
            0.0002,
            -0.0001,
            0.0007,
            0.0003,
            -0.0003,
            -0.0001,
            -0.0001,
            -0.0003,
            0.0006,
            -0.0002,
            -0.0003,
            0.0002
          ],
          "after": [
            -0.0031,
            0.0096,
            -0.0524,
            0.0228,
            -0.0084,
            0.1344,
            0.0304,
            -0.1073,
            0.0102,
            0.0462,
            -0.0673,
            -0.0572,
            0.1139,
            -0.0665,
            -0.0004,
            0.0292
          ]
        }
      }
    },
    {
      "step": 82,
      "word": "zada",
      "loss": 2.945,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.4414,
            0.0687,
            -0.558,
            0.0436,
            -0.2555,
            -0.0401,
            0.0449,
            0.0807,
            -0.172,
            0.1754,
            -0.3179,
            -0.4801,
            0.3976,
            0.3732,
            0.3701,
            0.1037
          ],
          "after": [
            -0.0214,
            0.1078,
            0.1152,
            0.0292,
            0.0084,
            -0.037,
            0.0632,
            -0.1186,
            -0.0448,
            0.0167,
            0.0579,
            0.0013,
            0.001,
            -0.0129,
            -0.0235,
            -0.0638
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.001,
            0.1079,
            -0.0789,
            0.042,
            0.0115,
            -0.1196,
            0.1873,
            -0.0044,
            -0.1604,
            0.0719,
            -0.0025,
            -0.0593,
            0.1369,
            -0.0175,
            -0.1488,
            0.0453
          ],
          "after": [
            0.0049,
            0.0962,
            0.0228,
            -0.0808,
            0.051,
            0.0004,
            0.0711,
            0.1307,
            -0.0612,
            -0.0473,
            0.0645,
            -0.0148,
            0.0913,
            0.0348,
            -0.0719,
            0.1594
          ]
        },
        "position_0": {
          "grad": [
            -0.0399,
            0.107,
            -0.0462,
            0.3074,
            0.0389,
            -0.117,
            0.3954,
            -0.0293,
            -0.0124,
            0.0033,
            -0.0274,
            0.146,
            0.0353,
            0.0086,
            -0.0085,
            0.0428
          ],
          "after": [
            -0.014,
            -0.0979,
            -0.2344,
            0.0858,
            0.0255,
            -0.0261,
            -0.1114,
            0.0741,
            0.1153,
            -0.0396,
            0.0076,
            0.1078,
            0.0234,
            0.0073,
            -0.1004,
            0.0551
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0002,
            -0.0008,
            0.0011,
            -0.0002,
            0.0012,
            0.0004,
            0.0009,
            -0.0003,
            0.0002,
            0.0001,
            0.0003,
            0.0,
            -0.0001,
            -0.0002,
            0.0001
          ],
          "after": [
            -0.0031,
            0.0089,
            -0.0517,
            0.0203,
            -0.0085,
            0.1337,
            0.0296,
            -0.1076,
            0.0119,
            0.0454,
            -0.0688,
            -0.0579,
            0.114,
            -0.0653,
            0.0,
            0.0288
          ]
        }
      }
    },
    {
      "step": 83,
      "word": "kaydynce",
      "loss": 2.8324,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0349,
            0.0264,
            0.0005,
            -0.0081,
            -0.0195,
            0.0712,
            0.0745,
            0.0205,
            -0.0701,
            0.0385,
            -0.1166,
            0.1156,
            0.0836,
            0.0513,
            0.0295,
            0.1251
          ],
          "after": [
            -0.0229,
            0.1071,
            0.1173,
            0.0292,
            0.0096,
            -0.0372,
            0.0624,
            -0.1192,
            -0.0432,
            0.0151,
            0.0602,
            0.0027,
            -0.0013,
            -0.0148,
            -0.0251,
            -0.0641
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1603,
            0.0263,
            -0.1451,
            -0.1009,
            -0.2706,
            0.0652,
            -0.0408,
            0.1005,
            0.0339,
            0.0834,
            -0.1068,
            -0.313,
            0.1656,
            0.1628,
            0.0509,
            -0.2298
          ],
          "after": [
            0.0043,
            0.0964,
            0.0235,
            -0.0804,
            0.0509,
            0.0001,
            0.0716,
            0.1305,
            -0.0616,
            -0.0482,
            0.0638,
            -0.0144,
            0.0913,
            0.0343,
            -0.0716,
            0.1599
          ]
        },
        "position_0": {
          "grad": [
            -0.0325,
            -0.0113,
            -0.1136,
            -0.0405,
            -0.0323,
            -0.0313,
            0.0572,
            -0.0318,
            0.025,
            0.0058,
            -0.0114,
            -0.0995,
            -0.0202,
            0.0293,
            -0.0173,
            -0.1189
          ],
          "after": [
            -0.0136,
            -0.0984,
            -0.2339,
            0.0851,
            0.0251,
            -0.0251,
            -0.1124,
            0.0745,
            0.1153,
            -0.0393,
            0.0077,
            0.1077,
            0.0228,
            0.0072,
            -0.1001,
            0.0556
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.0001,
            -0.0002,
            -0.0,
            -0.0,
            0.0003,
            -0.0007,
            -0.0003,
            -0.0003,
            -0.0004,
            -0.0,
            -0.0008,
            -0.0002,
            0.0004,
            0.0004,
            -0.0007
          ],
          "after": [
            -0.0037,
            0.0085,
            -0.0509,
            0.0182,
            -0.0085,
            0.133,
            0.0294,
            -0.1076,
            0.0135,
            0.045,
            -0.07,
            -0.0579,
            0.1142,
            -0.0646,
            0.0001,
            0.0292
          ]
        }
      }
    },
    {
      "step": 84,
      "word": "bari",
      "loss": 2.2806,
      "learning_rate": 0.0028,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1213,
            -0.0453,
            0.0223,
            -0.1663,
            0.1551,
            0.0856,
            0.1416,
            -0.2061,
            0.1652,
            -0.0704,
            0.1317,
            0.0689,
            -0.0797,
            -0.0849,
            0.0679,
            0.269
          ],
          "after": [
            -0.0245,
            0.1067,
            0.119,
            0.0301,
            0.0101,
            -0.0379,
            0.061,
            -0.1189,
            -0.0426,
            0.0141,
            0.0615,
            0.0037,
            -0.0029,
            -0.0161,
            -0.0268,
            -0.0651
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.008,
            0.0991,
            -0.0306,
            0.0131,
            0.0148,
            -0.1265,
            0.1767,
            -0.0435,
            -0.1478,
            0.0864,
            0.0006,
            -0.0428,
            0.1366,
            -0.0027,
            -0.1552,
            0.0734
          ],
          "after": [
            0.0038,
            0.0964,
            0.0242,
            -0.0801,
            0.0508,
            0.0,
            0.0717,
            0.1304,
            -0.0616,
            -0.0493,
            0.0632,
            -0.014,
            0.0908,
            0.034,
            -0.071,
            0.1601
          ]
        },
        "position_0": {
          "grad": [
            0.09,
            -0.0974,
            0.1814,
            -0.1486,
            -0.0821,
            0.0073,
            -0.0751,
            -0.0333,
            -0.0571,
            -0.1038,
            -0.0492,
            0.0247,
            0.0755,
            -0.032,
            0.092,
            0.2357
          ],
          "after": [
            -0.0136,
            -0.0985,
            -0.2342,
            0.0851,
            0.0251,
            -0.0242,
            -0.1131,
            0.075,
            0.1155,
            -0.0385,
            0.0079,
            0.1075,
            0.0221,
            0.0071,
            -0.1001,
            0.0553
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0007,
            -0.0002,
            -0.0013,
            0.0018,
            0.0006,
            0.0026,
            -0.001,
            -0.0001,
            0.001,
            0.0008,
            -0.0008,
            0.0003,
            -0.0007,
            -0.0006,
            -0.0005
          ],
          "after": [
            -0.0034,
            0.0075,
            -0.0499,
            0.0177,
            -0.0097,
            0.1319,
            0.0279,
            -0.1067,
            0.0151,
            0.0439,
            -0.072,
            -0.0573,
            0.1142,
            -0.0634,
            0.0006,
            0.03
          ]
        }
      }
    },
    {
      "step": 85,
      "word": "darya",
      "loss": 2.4736,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.4359,
            0.0732,
            -0.183,
            -0.1146,
            -0.0198,
            0.0471,
            0.221,
            -0.1565,
            -0.0543,
            0.0348,
            0.0834,
            -0.2641,
            0.1715,
            0.0338,
            0.1238,
            0.0527
          ],
          "after": [
            -0.0271,
            0.106,
            0.1212,
            0.0316,
            0.0105,
            -0.0387,
            0.0589,
            -0.118,
            -0.0419,
            0.0131,
            0.0621,
            0.0053,
            -0.0049,
            -0.0174,
            -0.0286,
            -0.0661
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0064,
            0.1021,
            -0.0423,
            0.0477,
            0.0568,
            -0.1475,
            0.189,
            -0.0172,
            -0.1818,
            0.0907,
            0.0065,
            -0.0247,
            0.1186,
            -0.0512,
            -0.1778,
            0.0559
          ],
          "after": [
            0.0034,
            0.096,
            0.0249,
            -0.08,
            0.0507,
            0.0003,
            0.0715,
            0.1305,
            -0.0612,
            -0.0506,
            0.0627,
            -0.0136,
            0.09,
            0.0338,
            -0.0701,
            0.1602
          ]
        },
        "position_0": {
          "grad": [
            0.0131,
            -0.0275,
            -0.0145,
            -0.1364,
            0.0565,
            -0.0968,
            -0.037,
            -0.0745,
            0.0983,
            0.0139,
            -0.1897,
            -0.112,
            -0.0401,
            0.2062,
            0.171,
            0.0809
          ],
          "after": [
            -0.0136,
            -0.0985,
            -0.2344,
            0.0856,
            0.0248,
            -0.023,
            -0.1136,
            0.0758,
            0.1154,
            -0.0379,
            0.009,
            0.1077,
            0.0216,
            0.0065,
            -0.1006,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0002,
            0.0001,
            0.0002,
            0.0,
            -0.0,
            0.0001,
            -0.0,
            -0.0001,
            -0.0003,
            -0.0001,
            -0.0001,
            0.0001,
            0.0001,
            -0.0001,
            -0.0001
          ],
          "after": [
            -0.0033,
            0.0066,
            -0.0491,
            0.0171,
            -0.0108,
            0.131,
            0.0265,
            -0.1059,
            0.0166,
            0.0432,
            -0.0736,
            -0.0566,
            0.1141,
            -0.0624,
            0.0012,
            0.0308
          ]
        }
      }
    },
    {
      "step": 86,
      "word": "ellieanne",
      "loss": 2.3833,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0508,
            -0.004,
            -0.0028,
            -0.0074,
            -0.0703,
            -0.0372,
            -0.019,
            -0.0522,
            0.0699,
            -0.0874,
            -0.028,
            0.0255,
            -0.0621,
            0.0144,
            0.0171,
            0.0334
          ],
          "after": [
            -0.0291,
            0.1055,
            0.123,
            0.033,
            0.0112,
            -0.0392,
            0.0572,
            -0.117,
            -0.0416,
            0.0128,
            0.0628,
            0.0065,
            -0.0064,
            -0.0185,
            -0.0302,
            -0.067
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2704,
            0.0823,
            0.3463,
            0.0207,
            -0.325,
            0.0924,
            -0.3262,
            -0.0058,
            -0.0089,
            -0.2966,
            -0.0288,
            -0.2531,
            0.054,
            0.2645,
            0.2097,
            -0.3817
          ],
          "after": [
            0.0023,
            0.0955,
            0.0243,
            -0.08,
            0.0512,
            0.0003,
            0.0719,
            0.1305,
            -0.0608,
            -0.0506,
            0.0624,
            -0.0125,
            0.0891,
            0.033,
            -0.0698,
            0.1612
          ]
        },
        "position_0": {
          "grad": [
            0.0634,
            -0.1652,
            0.0081,
            -0.0526,
            -0.0923,
            0.0702,
            -0.1968,
            -0.0885,
            0.1823,
            0.1023,
            -0.0584,
            -0.103,
            -0.1367,
            0.0526,
            0.0967,
            -0.2242
          ],
          "after": [
            -0.0138,
            -0.098,
            -0.2346,
            0.0861,
            0.025,
            -0.0223,
            -0.1136,
            0.0767,
            0.1148,
            -0.0379,
            0.0102,
            0.1082,
            0.0215,
            0.0058,
            -0.1014,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            -0.001,
            -0.0012,
            0.0001,
            -0.0021,
            0.0006,
            -0.0021,
            -0.0001,
            0.0002,
            -0.0001,
            -0.0005,
            -0.0014,
            -0.0004,
            0.0017,
            0.0015,
            -0.0008
          ],
          "after": [
            -0.0044,
            0.0066,
            -0.047,
            0.0164,
            -0.0103,
            0.1298,
            0.0264,
            -0.1052,
            0.0176,
            0.0427,
            -0.0743,
            -0.055,
            0.1143,
            -0.0631,
            0.0004,
            0.0322
          ]
        }
      }
    },
    {
      "step": 87,
      "word": "gretel",
      "loss": 2.8323,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0308,
            0.105,
            0.1245,
            0.0341,
            0.0118,
            -0.0397,
            0.0558,
            -0.1162,
            -0.0414,
            0.0125,
            0.0634,
            0.0076,
            -0.0077,
            -0.0195,
            -0.0316,
            -0.0678
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0229,
            -0.2864,
            0.2117,
            -0.0296,
            0.1049,
            0.4848,
            -0.6157,
            0.0697,
            0.4175,
            -0.2887,
            0.1342,
            0.3798,
            -0.7024,
            0.0683,
            0.4454,
            -0.012
          ],
          "after": [
            0.0013,
            0.0958,
            0.023,
            -0.0799,
            0.0514,
            -0.0006,
            0.0731,
            0.1303,
            -0.0614,
            -0.0495,
            0.0615,
            -0.0127,
            0.0903,
            0.0322,
            -0.0703,
            0.162
          ]
        },
        "position_0": {
          "grad": [
            -0.0537,
            -0.0156,
            -0.0295,
            0.0637,
            -0.0632,
            -0.0965,
            -0.0109,
            -0.0363,
            -0.0421,
            0.0746,
            0.0453,
            -0.0269,
            -0.0505,
            -0.0298,
            -0.1307,
            0.044
          ],
          "after": [
            -0.0138,
            -0.0974,
            -0.2346,
            0.0864,
            0.0254,
            -0.0213,
            -0.1135,
            0.0777,
            0.1144,
            -0.0385,
            0.011,
            0.1087,
            0.0216,
            0.0053,
            -0.1016,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0001,
            -0.0009,
            -0.0005,
            -0.0001,
            0.0,
            0.0008,
            0.0,
            -0.0006,
            -0.0002,
            -0.0006,
            -0.0009,
            0.0011,
            -0.0003,
            0.0009,
            -0.001
          ],
          "after": [
            -0.0054,
            0.0066,
            -0.0442,
            0.0164,
            -0.0099,
            0.1287,
            0.026,
            -0.1046,
            0.0191,
            0.0424,
            -0.0743,
            -0.053,
            0.1136,
            -0.0635,
            -0.001,
            0.0343
          ]
        }
      }
    },
    {
      "step": 88,
      "word": "mahnoor",
      "loss": 2.7984,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0601,
            -0.0427,
            0.0627,
            0.0906,
            -0.06,
            0.0993,
            0.0577,
            0.119,
            0.0492,
            0.0788,
            -0.07,
            -0.0756,
            0.1764,
            0.0182,
            -0.0213,
            0.1152
          ],
          "after": [
            -0.0321,
            0.1048,
            0.1256,
            0.0345,
            0.0125,
            -0.0406,
            0.0543,
            -0.1159,
            -0.0414,
            0.0118,
            0.0643,
            0.0088,
            -0.0094,
            -0.0204,
            -0.0327,
            -0.0688
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0943,
            0.0917,
            -0.0327,
            0.0754,
            0.1089,
            -0.1999,
            0.2345,
            0.0297,
            -0.2075,
            0.0882,
            0.0017,
            0.0725,
            0.1153,
            -0.1165,
            -0.2991,
            0.1471
          ],
          "after": [
            0.0007,
            0.0958,
            0.022,
            -0.0801,
            0.0514,
            -0.001,
            0.0738,
            0.13,
            -0.0614,
            -0.0488,
            0.0607,
            -0.0129,
            0.0911,
            0.0318,
            -0.0702,
            0.1624
          ]
        },
        "position_0": {
          "grad": [
            0.0043,
            -0.01,
            0.0191,
            0.0564,
            0.0074,
            0.0317,
            0.0749,
            0.1426,
            0.112,
            0.0017,
            -0.0075,
            -0.0386,
            -0.1635,
            -0.008,
            0.0914,
            -0.0252
          ],
          "after": [
            -0.0139,
            -0.0969,
            -0.2348,
            0.0864,
            0.0256,
            -0.0205,
            -0.1137,
            0.078,
            0.1138,
            -0.0389,
            0.0117,
            0.1093,
            0.0222,
            0.0049,
            -0.1021,
            0.0552
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0,
            -0.0003,
            0.0003,
            0.0002,
            0.0001,
            0.0005,
            0.0005,
            -0.0005,
            0.0001,
            0.0001,
            0.0002,
            0.0004,
            -0.0004,
            -0.0001,
            0.0002
          ],
          "after": [
            -0.006,
            0.0065,
            -0.0414,
            0.0161,
            -0.0096,
            0.1277,
            0.0253,
            -0.1045,
            0.0208,
            0.042,
            -0.0744,
            -0.0514,
            0.1128,
            -0.0635,
            -0.0021,
            0.0358
          ]
        }
      }
    },
    {
      "step": 89,
      "word": "aryo",
      "loss": 2.7139,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0002,
            -0.0318,
            -0.0286,
            0.0293,
            0.0862,
            0.1778,
            0.1601,
            -0.0822,
            0.07,
            -0.0542,
            0.1382,
            0.1349,
            -0.1023,
            -0.1669,
            0.129,
            0.3242
          ],
          "after": [
            -0.0332,
            0.1047,
            0.1267,
            0.0347,
            0.0128,
            -0.0423,
            0.0523,
            -0.1154,
            -0.0418,
            0.0115,
            0.0643,
            0.0094,
            -0.0105,
            -0.0204,
            -0.0342,
            -0.0705
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0325,
            0.0709,
            -0.0465,
            0.064,
            -0.0055,
            -0.1469,
            0.2104,
            0.0212,
            -0.1973,
            0.1352,
            0.0117,
            -0.0241,
            0.1545,
            -0.0803,
            -0.1914,
            0.0761
          ],
          "after": [
            0.0003,
            0.0957,
            0.0214,
            -0.0804,
            0.0514,
            -0.001,
            0.0741,
            0.1296,
            -0.061,
            -0.0488,
            0.0599,
            -0.0131,
            0.0913,
            0.0316,
            -0.0698,
            0.1625
          ]
        },
        "position_0": {
          "grad": [
            0.2881,
            -0.1391,
            0.0434,
            -0.3674,
            -0.1207,
            0.1469,
            -0.4551,
            -0.2004,
            0.4135,
            0.0104,
            0.2135,
            -0.2046,
            -0.3537,
            0.3135,
            0.1938,
            -0.3312
          ],
          "after": [
            -0.0149,
            -0.096,
            -0.2351,
            0.0876,
            0.0264,
            -0.0206,
            -0.1128,
            0.079,
            0.1123,
            -0.0393,
            0.0113,
            0.1104,
            0.0236,
            0.0037,
            -0.1031,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0003,
            0.0002,
            -0.0001,
            -0.0,
            -0.0001,
            0.0008,
            0.0008,
            -0.0003,
            0.0006,
            0.0007,
            0.0001,
            0.0009,
            -0.001,
            -0.0009,
            -0.0005
          ],
          "after": [
            -0.0064,
            0.0068,
            -0.0392,
            0.0159,
            -0.0094,
            0.127,
            0.0244,
            -0.105,
            0.0226,
            0.0412,
            -0.0753,
            -0.0502,
            0.1115,
            -0.0627,
            -0.0024,
            0.0376
          ]
        }
      }
    },
    {
      "step": 90,
      "word": "miles",
      "loss": 2.4733,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0342,
            0.1047,
            0.1276,
            0.0348,
            0.013,
            -0.0438,
            0.0506,
            -0.1149,
            -0.0421,
            0.0112,
            0.0643,
            0.0099,
            -0.0114,
            -0.0205,
            -0.0354,
            -0.0719
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0523,
            -0.2543,
            0.0424,
            -0.2671,
            0.0454,
            0.2035,
            -0.2513,
            0.1942,
            0.1256,
            -0.1842,
            -0.1951,
            -0.1445,
            -0.0372,
            0.0632,
            0.2903,
            -0.3044
          ],
          "after": [
            0.0001,
            0.0962,
            0.0207,
            -0.0798,
            0.0513,
            -0.0015,
            0.0747,
            0.1285,
            -0.061,
            -0.0481,
            0.0603,
            -0.0129,
            0.0917,
            0.0313,
            -0.0699,
            0.1634
          ]
        },
        "position_0": {
          "grad": [
            -0.0083,
            0.0068,
            0.0242,
            0.0879,
            0.0411,
            0.0458,
            0.1192,
            0.1887,
            0.1239,
            0.0015,
            0.0034,
            -0.0156,
            -0.1881,
            -0.0266,
            0.0763,
            -0.0066
          ],
          "after": [
            -0.0157,
            -0.0952,
            -0.2354,
            0.0883,
            0.0268,
            -0.0209,
            -0.1123,
            0.0791,
            0.1107,
            -0.0397,
            0.011,
            0.1114,
            0.0253,
            0.0028,
            -0.1042,
            0.0572
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0001,
            -0.0001,
            -0.0001,
            0.0003,
            0.0,
            0.0,
            0.0008,
            -0.0002,
            0.0003,
            -0.0,
            -0.0004,
            0.0004,
            -0.0006,
            0.0001,
            0.0
          ],
          "after": [
            -0.0065,
            0.0069,
            -0.0372,
            0.0159,
            -0.0093,
            0.1263,
            0.0235,
            -0.1062,
            0.0243,
            0.0403,
            -0.0761,
            -0.0488,
            0.1101,
            -0.0615,
            -0.0027,
            0.0391
          ]
        }
      }
    },
    {
      "step": 91,
      "word": "evalise",
      "loss": 2.6954,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.08,
            0.1765,
            -0.0653,
            -0.0274,
            0.0933,
            -0.0299,
            -0.058,
            0.156,
            -0.0864,
            0.0114,
            -0.0363,
            0.1505,
            -0.0159,
            -0.0399,
            0.0218,
            -0.1337
          ],
          "after": [
            -0.0348,
            0.1039,
            0.1286,
            0.0351,
            0.0128,
            -0.0449,
            0.0495,
            -0.1152,
            -0.0419,
            0.011,
            0.0645,
            0.0098,
            -0.0122,
            -0.0203,
            -0.0365,
            -0.0728
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1754,
            0.2543,
            0.3755,
            -0.0456,
            -0.1697,
            0.1281,
            -0.0892,
            -0.1504,
            0.0186,
            -0.1456,
            0.0467,
            -0.4944,
            0.1479,
            0.2922,
            0.1044,
            -0.3608
          ],
          "after": [
            -0.0006,
            0.096,
            0.0188,
            -0.0792,
            0.0515,
            -0.0021,
            0.0754,
            0.1282,
            -0.061,
            -0.047,
            0.0603,
            -0.0116,
            0.0915,
            0.0304,
            -0.0702,
            0.1649
          ]
        },
        "position_0": {
          "grad": [
            0.0626,
            -0.156,
            -0.0187,
            -0.049,
            -0.0377,
            0.0747,
            -0.1919,
            -0.132,
            0.1737,
            0.1252,
            -0.0396,
            -0.0832,
            -0.1443,
            0.0293,
            0.0827,
            -0.223
          ],
          "after": [
            -0.0167,
            -0.094,
            -0.2356,
            0.0891,
            0.0273,
            -0.0216,
            -0.1114,
            0.0797,
            0.1089,
            -0.0408,
            0.0109,
            0.1125,
            0.0271,
            0.0019,
            -0.1054,
            0.0587
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0005,
            -0.0001,
            0.0007,
            -0.0002,
            0.0003,
            0.0001,
            0.0005,
            -0.0004,
            -0.0001,
            0.0001,
            -0.0007,
            0.0004,
            -0.0003,
            0.0,
            0.0002
          ],
          "after": [
            -0.0062,
            0.0066,
            -0.0354,
            0.0152,
            -0.0091,
            0.1255,
            0.0228,
            -0.1077,
            0.0261,
            0.0396,
            -0.0768,
            -0.0471,
            0.1085,
            -0.0602,
            -0.0029,
            0.0403
          ]
        }
      }
    },
    {
      "step": 92,
      "word": "igor",
      "loss": 3.1497,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0353,
            0.1031,
            0.1295,
            0.0354,
            0.0127,
            -0.0458,
            0.0484,
            -0.1154,
            -0.0418,
            0.0107,
            0.0646,
            0.0098,
            -0.0128,
            -0.0202,
            -0.0375,
            -0.0736
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0828,
            0.0923,
            -0.084,
            0.0042,
            0.0591,
            -0.1476,
            0.2155,
            -0.0109,
            -0.1707,
            0.1088,
            0.0074,
            -0.0321,
            0.1402,
            -0.0937,
            -0.2576,
            0.1084
          ],
          "after": [
            -0.0009,
            0.0956,
            0.0176,
            -0.0787,
            0.0516,
            -0.0023,
            0.0756,
            0.128,
            -0.0606,
            -0.0465,
            0.0604,
            -0.0104,
            0.091,
            0.0298,
            -0.07,
            0.1659
          ]
        },
        "position_0": {
          "grad": [
            -0.1249,
            -0.274,
            0.0544,
            -0.2334,
            0.1862,
            0.1745,
            -0.2557,
            -0.0007,
            0.1467,
            -0.0092,
            0.0615,
            -0.0952,
            -0.3049,
            0.0358,
            0.0896,
            -0.1124
          ],
          "after": [
            -0.017,
            -0.0921,
            -0.2361,
            0.0905,
            0.0271,
            -0.0229,
            -0.1101,
            0.0802,
            0.107,
            -0.0416,
            0.0106,
            0.1138,
            0.0294,
            0.001,
            -0.1067,
            0.0603
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0002,
            0.0002,
            0.0003,
            -0.0001,
            0.0,
            -0.0002,
            0.0002,
            0.0,
            0.0002,
            0.0001,
            0.0003,
            -0.0002,
            0.0,
            -0.0002,
            -0.0002
          ],
          "after": [
            -0.0063,
            0.0065,
            -0.034,
            0.0144,
            -0.0089,
            0.1248,
            0.0222,
            -0.1091,
            0.0277,
            0.0389,
            -0.0777,
            -0.0458,
            0.1074,
            -0.0591,
            -0.003,
            0.0414
          ]
        }
      }
    },
    {
      "step": 93,
      "word": "thoren",
      "loss": 2.5636,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0357,
            0.1025,
            0.1302,
            0.0356,
            0.0126,
            -0.0466,
            0.0476,
            -0.1156,
            -0.0417,
            0.0105,
            0.0648,
            0.0098,
            -0.0133,
            -0.0201,
            -0.0383,
            -0.0742
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0148,
            -0.093,
            0.0885,
            0.0281,
            0.0916,
            0.1366,
            -0.2476,
            0.0155,
            0.1028,
            -0.2803,
            -0.0766,
            0.0191,
            -0.1002,
            0.1692,
            0.0332,
            0.0703
          ],
          "after": [
            -0.0013,
            0.0955,
            0.0162,
            -0.0784,
            0.0515,
            -0.0028,
            0.0762,
            0.1277,
            -0.0605,
            -0.0451,
            0.0608,
            -0.0094,
            0.0909,
            0.0289,
            -0.0699,
            0.1666
          ]
        },
        "position_0": {
          "grad": [
            0.0092,
            -0.2184,
            0.0592,
            0.089,
            -0.1276,
            0.0611,
            0.1149,
            0.0546,
            -0.0954,
            -0.1048,
            -0.1705,
            -0.3577,
            0.1802,
            0.0851,
            0.0907,
            0.1182
          ],
          "after": [
            -0.0174,
            -0.0897,
            -0.2367,
            0.0914,
            0.0273,
            -0.0244,
            -0.1093,
            0.0804,
            0.1056,
            -0.0417,
            0.011,
            0.1159,
            0.0309,
            0.0,
            -0.108,
            0.0613
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0002,
            -0.0002,
            -0.0001,
            -0.0001,
            0.0001,
            -0.0006,
            -0.0004,
            0.0,
            -0.0007,
            -0.0004,
            -0.0006,
            -0.0002,
            0.0006,
            0.0006,
            -0.0001
          ],
          "after": [
            -0.0065,
            0.0065,
            -0.0326,
            0.0137,
            -0.0086,
            0.1241,
            0.022,
            -0.11,
            0.029,
            0.0388,
            -0.0779,
            -0.0443,
            0.1066,
            -0.0587,
            -0.0035,
            0.0424
          ]
        }
      }
    },
    {
      "step": 94,
      "word": "hector",
      "loss": 2.909,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0361,
            0.102,
            0.1308,
            0.0358,
            0.0125,
            -0.0473,
            0.0468,
            -0.1157,
            -0.0416,
            0.0103,
            0.0649,
            0.0098,
            -0.0138,
            -0.02,
            -0.039,
            -0.0747
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0767,
            -0.0456,
            0.2367,
            -0.162,
            0.2599,
            -0.0211,
            -0.2063,
            -0.1003,
            0.2554,
            0.1002,
            0.2113,
            0.142,
            -0.1227,
            -0.0008,
            -0.12,
            -0.1635
          ],
          "after": [
            -0.0018,
            0.0955,
            0.0143,
            -0.0776,
            0.0509,
            -0.0032,
            0.077,
            0.1279,
            -0.061,
            -0.0442,
            0.0601,
            -0.0089,
            0.0911,
            0.0281,
            -0.0697,
            0.1676
          ]
        },
        "position_0": {
          "grad": [
            -0.0484,
            -0.0952,
            0.0709,
            0.0395,
            -0.0969,
            0.0651,
            0.0048,
            0.0815,
            0.1214,
            0.0362,
            -0.0642,
            -0.1375,
            0.1641,
            0.0928,
            0.0799,
            0.0778
          ],
          "after": [
            -0.0175,
            -0.0874,
            -0.2375,
            0.0921,
            0.0279,
            -0.0259,
            -0.1086,
            0.0803,
            0.1041,
            -0.042,
            0.0117,
            0.1181,
            0.0318,
            -0.0011,
            -0.1095,
            0.0619
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.001,
            0.001,
            -0.0,
            0.0007,
            -0.0007,
            0.0003,
            0.0001,
            0.0005,
            0.0009,
            0.0001,
            -0.0007,
            0.0017,
            -0.0,
            -0.002,
            0.0008
          ],
          "after": [
            -0.0073,
            0.0057,
            -0.0326,
            0.0132,
            -0.0088,
            0.1241,
            0.0217,
            -0.1109,
            0.0296,
            0.038,
            -0.0784,
            -0.0426,
            0.1048,
            -0.0583,
            -0.0025,
            0.0425
          ]
        }
      }
    },
    {
      "step": 95,
      "word": "anuel",
      "loss": 2.6093,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0594,
            -0.023,
            -0.1807,
            0.0344,
            -0.233,
            -0.0376,
            -0.0608,
            -0.0139,
            0.0484,
            -0.1225,
            -0.0933,
            -0.1319,
            0.0593,
            0.1133,
            0.1027,
            0.0262
          ],
          "after": [
            -0.0366,
            0.1017,
            0.1321,
            0.0357,
            0.0133,
            -0.0477,
            0.0465,
            -0.1158,
            -0.0418,
            0.0109,
            0.0655,
            0.0102,
            -0.0144,
            -0.0205,
            -0.04,
            -0.0753
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2752,
            -0.1841,
            0.0234,
            0.0095,
            -0.247,
            0.0988,
            0.1007,
            0.0097,
            -0.0445,
            -0.2031,
            -0.0346,
            -0.0602,
            -0.0271,
            -0.1958,
            0.3494,
            -0.0446
          ],
          "after": [
            -0.0014,
            0.096,
            0.0126,
            -0.0769,
            0.0509,
            -0.0037,
            0.0776,
            0.128,
            -0.0613,
            -0.0428,
            0.0597,
            -0.0083,
            0.0913,
            0.0279,
            -0.07,
            0.1686
          ]
        },
        "position_0": {
          "grad": [
            0.1778,
            -0.0818,
            0.0004,
            -0.2396,
            0.0004,
            0.0888,
            -0.3554,
            -0.1573,
            0.2943,
            0.0073,
            0.2198,
            -0.059,
            -0.207,
            0.1848,
            0.077,
            -0.1935
          ],
          "after": [
            -0.0182,
            -0.0852,
            -0.2383,
            0.0933,
            0.0284,
            -0.0277,
            -0.1072,
            0.0808,
            0.1022,
            -0.0423,
            0.0113,
            0.1201,
            0.033,
            -0.0025,
            -0.1109,
            0.063
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0005,
            -0.0,
            -0.0002,
            -0.0004,
            -0.0001,
            0.0002,
            -0.0001,
            0.0002,
            -0.0005,
            -0.0003,
            0.0,
            -0.0001,
            0.0002,
            0.0004,
            0.0001
          ],
          "after": [
            -0.0078,
            0.0055,
            -0.0326,
            0.0129,
            -0.0087,
            0.1242,
            0.0213,
            -0.1115,
            0.0299,
            0.0377,
            -0.0783,
            -0.0411,
            0.1032,
            -0.0582,
            -0.0019,
            0.0425
          ]
        }
      }
    },
    {
      "step": 96,
      "word": "emmerie",
      "loss": 2.5591,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.037,
            0.1014,
            0.1331,
            0.0357,
            0.014,
            -0.048,
            0.0462,
            -0.1159,
            -0.0419,
            0.0114,
            0.066,
            0.0105,
            -0.0149,
            -0.0208,
            -0.0409,
            -0.0757
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2502,
            -0.1883,
            0.3928,
            -0.1189,
            -0.5341,
            0.2348,
            -0.3755,
            0.039,
            0.297,
            -0.2577,
            -0.0331,
            -0.3192,
            -0.0647,
            0.0747,
            0.8241,
            -0.5951
          ],
          "after": [
            -0.0018,
            0.097,
            0.01,
            -0.076,
            0.0519,
            -0.0046,
            0.0786,
            0.128,
            -0.0622,
            -0.0409,
            0.0595,
            -0.007,
            0.0917,
            0.0276,
            -0.0716,
            0.1706
          ]
        },
        "position_0": {
          "grad": [
            0.0376,
            -0.1243,
            -0.0463,
            -0.0374,
            -0.0219,
            0.0848,
            -0.1618,
            -0.1307,
            0.1296,
            0.1278,
            -0.0251,
            -0.0409,
            -0.113,
            0.0034,
            0.062,
            -0.1861
          ],
          "after": [
            -0.019,
            -0.0828,
            -0.2387,
            0.0946,
            0.0289,
            -0.0295,
            -0.1057,
            0.0818,
            0.1002,
            -0.0433,
            0.0111,
            0.122,
            0.0344,
            -0.0038,
            -0.1124,
            0.0645
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0002,
            -0.0003,
            0.0003,
            -0.0005,
            -0.0001,
            -0.0003,
            0.0003,
            -0.0002,
            -0.0001,
            -0.0001,
            -0.0003,
            0.0001,
            0.0002,
            0.0005,
            0.0
          ],
          "after": [
            -0.0083,
            0.0054,
            -0.0323,
            0.0125,
            -0.0083,
            0.1243,
            0.0212,
            -0.1123,
            0.0304,
            0.0376,
            -0.0781,
            -0.0396,
            0.1019,
            -0.0582,
            -0.0018,
            0.0425
          ]
        }
      }
    },
    {
      "step": 97,
      "word": "erian",
      "loss": 2.1096,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0955,
            0.0052,
            -0.0531,
            0.0239,
            -0.1169,
            -0.1166,
            0.0392,
            -0.0763,
            0.085,
            -0.1119,
            0.0251,
            -0.0061,
            -0.0791,
            -0.0272,
            0.0604,
            0.0018
          ],
          "after": [
            -0.037,
            0.1011,
            0.1342,
            0.0355,
            0.015,
            -0.0476,
            0.0458,
            -0.1156,
            -0.0424,
            0.0125,
            0.0663,
            0.0108,
            -0.0151,
            -0.021,
            -0.0418,
            -0.0761
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.116,
            0.2892,
            0.6147,
            -0.1226,
            0.1925,
            -0.1355,
            0.2841,
            -0.0867,
            0.0208,
            -0.0415,
            0.0769,
            -0.1327,
            -0.0634,
            0.0062,
            -0.2595,
            -0.1628
          ],
          "after": [
            -0.0018,
            0.097,
            0.0065,
            -0.0748,
            0.0524,
            -0.0051,
            0.079,
            0.1283,
            -0.0631,
            -0.039,
            0.0589,
            -0.0057,
            0.0922,
            0.0273,
            -0.0725,
            0.1727
          ]
        },
        "position_0": {
          "grad": [
            0.0797,
            -0.2394,
            -0.0985,
            -0.1178,
            -0.1117,
            0.1128,
            -0.2964,
            -0.1621,
            0.2393,
            0.1732,
            -0.066,
            -0.1279,
            -0.206,
            0.033,
            0.1445,
            -0.3077
          ],
          "after": [
            -0.0199,
            -0.0801,
            -0.2386,
            0.096,
            0.0298,
            -0.0317,
            -0.1037,
            0.0832,
            0.098,
            -0.0451,
            0.0112,
            0.124,
            0.036,
            -0.005,
            -0.114,
            0.0666
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0004,
            -0.0002,
            0.0,
            -0.0001,
            -0.0002,
            -0.0003,
            0.0005,
            0.0,
            -0.0004,
            -0.0002,
            0.0004,
            -0.0001,
            -0.0002,
            0.0004,
            0.0002
          ],
          "after": [
            -0.0087,
            0.0057,
            -0.0318,
            0.012,
            -0.0079,
            0.1246,
            0.0212,
            -0.1134,
            0.0308,
            0.0378,
            -0.0777,
            -0.0386,
            0.1008,
            -0.058,
            -0.002,
            0.0424
          ]
        }
      }
    },
    {
      "step": 98,
      "word": "blakeleigh",
      "loss": 3.1764,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.174,
            0.0182,
            0.0187,
            -0.0822,
            0.0918,
            -0.0962,
            0.1226,
            0.0103,
            -0.0516,
            -0.1007,
            -0.0189,
            -0.0049,
            0.0635,
            -0.1555,
            -0.0525,
            -0.042
          ],
          "after": [
            -0.0366,
            0.1008,
            0.1351,
            0.0358,
            0.0155,
            -0.0467,
            0.0448,
            -0.1154,
            -0.0426,
            0.014,
            0.0667,
            0.0111,
            -0.0155,
            -0.0205,
            -0.0424,
            -0.0763
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2644,
            -0.2357,
            -0.091,
            0.0189,
            -0.186,
            0.3734,
            -0.4173,
            -0.038,
            0.1535,
            -0.1473,
            -0.0944,
            -0.2123,
            0.0673,
            0.3096,
            0.1444,
            -0.2801
          ],
          "after": [
            -0.0026,
            0.0976,
            0.0038,
            -0.0739,
            0.0532,
            -0.0063,
            0.0801,
            0.1287,
            -0.0641,
            -0.037,
            0.0589,
            -0.004,
            0.0925,
            0.0263,
            -0.0735,
            0.175
          ]
        },
        "position_0": {
          "grad": [
            -0.033,
            0.0653,
            0.1161,
            0.0266,
            0.0802,
            -0.0354,
            0.1046,
            0.0008,
            -0.1562,
            -0.0729,
            0.0088,
            0.1237,
            0.1308,
            -0.108,
            -0.0823,
            0.2016
          ],
          "after": [
            -0.0206,
            -0.078,
            -0.239,
            0.0971,
            0.0302,
            -0.0333,
            -0.1023,
            0.0844,
            0.0964,
            -0.0462,
            0.0112,
            0.1253,
            0.0371,
            -0.0056,
            -0.1152,
            0.0677
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            0.0001,
            0.0011,
            0.0,
            0.0005,
            0.0014,
            -0.0002,
            -0.0009,
            0.001,
            0.0001,
            0.0003,
            0.0,
            -0.0003,
            0.0003,
            -0.001,
            0.0001
          ],
          "after": [
            -0.0086,
            0.0058,
            -0.0326,
            0.0117,
            -0.0079,
            0.1237,
            0.0214,
            -0.1136,
            0.0301,
            0.0379,
            -0.0776,
            -0.0378,
            0.1001,
            -0.0581,
            -0.0014,
            0.0422
          ]
        }
      }
    },
    {
      "step": 99,
      "word": "madina",
      "loss": 2.3548,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0733,
            0.2087,
            -0.0576,
            -0.0353,
            0.1857,
            -0.0361,
            0.168,
            -0.0062,
            -0.068,
            0.099,
            -0.0828,
            0.0651,
            0.1395,
            0.0312,
            -0.0563,
            0.2444
          ],
          "after": [
            -0.036,
            0.0996,
            0.1361,
            0.0364,
            0.0153,
            -0.0458,
            0.0433,
            -0.1152,
            -0.0425,
            0.0147,
            0.0675,
            0.0111,
            -0.0163,
            -0.0202,
            -0.0428,
            -0.0772
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1302,
            0.1726,
            -0.0486,
            0.0611,
            0.1998,
            -0.26,
            0.3099,
            0.0103,
            -0.2467,
            0.0664,
            0.0245,
            0.036,
            0.1125,
            -0.1051,
            -0.3563,
            0.1759
          ],
          "after": [
            -0.0028,
            0.0977,
            0.0015,
            -0.0733,
            0.0535,
            -0.0068,
            0.0804,
            0.129,
            -0.0645,
            -0.0355,
            0.0588,
            -0.0027,
            0.0924,
            0.0257,
            -0.0738,
            0.1767
          ]
        },
        "position_0": {
          "grad": [
            -0.0576,
            0.1379,
            0.0292,
            0.1266,
            0.1328,
            -0.0073,
            0.2647,
            0.1358,
            -0.0381,
            0.0016,
            0.0449,
            0.0986,
            -0.0917,
            -0.1208,
            -0.0287,
            0.0847
          ],
          "after": [
            -0.0209,
            -0.0767,
            -0.2395,
            0.0976,
            0.0301,
            -0.0347,
            -0.1017,
            0.0849,
            0.0952,
            -0.0471,
            0.011,
            0.1261,
            0.0383,
            -0.0059,
            -0.1161,
            0.0685
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0,
            -0.0,
            0.0006,
            -0.0012,
            0.0005,
            -0.0014,
            -0.0,
            0.0,
            0.0002,
            -0.0002,
            -0.0002,
            -0.0002,
            0.0007,
            0.0004,
            -0.0001
          ],
          "after": [
            -0.0089,
            0.006,
            -0.0332,
            0.0109,
            -0.0071,
            0.1226,
            0.0222,
            -0.1137,
            0.0294,
            0.0378,
            -0.0773,
            -0.0369,
            0.0996,
            -0.0588,
            -0.0013,
            0.0421
          ]
        }
      }
    },
    {
      "step": 100,
      "word": "fatumata",
      "loss": 3.3302,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0851,
            -0.2771,
            0.073,
            0.2581,
            0.0407,
            0.2337,
            0.2491,
            0.0932,
            -0.2858,
            -0.1501,
            -0.269,
            -0.3356,
            0.2898,
            -0.0614,
            -0.0839,
            0.4358
          ],
          "after": [
            -0.0352,
            0.0998,
            0.1366,
            0.0353,
            0.0149,
            -0.0463,
            0.041,
            -0.1155,
            -0.041,
            0.0161,
            0.0695,
            0.0121,
            -0.0181,
            -0.0196,
            -0.0427,
            -0.079
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1653,
            0.2276,
            -0.0147,
            0.0872,
            0.2526,
            -0.3042,
            0.281,
            -0.0089,
            -0.257,
            0.0602,
            0.0516,
            0.0955,
            0.1126,
            -0.1274,
            -0.4074,
            0.2048
          ],
          "after": [
            -0.0026,
            0.0972,
            -0.0003,
            -0.073,
            0.0533,
            -0.0066,
            0.0803,
            0.1293,
            -0.0642,
            -0.0344,
            0.0584,
            -0.0018,
            0.092,
            0.0255,
            -0.0734,
            0.1776
          ]
        },
        "position_0": {
          "grad": [
            -0.1504,
            0.3555,
            0.1335,
            0.293,
            0.244,
            -0.1994,
            0.3536,
            -0.0145,
            -0.4743,
            -0.0841,
            0.0956,
            0.3367,
            0.273,
            -0.2115,
            -0.267,
            0.239
          ],
          "after": [
            -0.0207,
            -0.0767,
            -0.2405,
            0.0972,
            0.029,
            -0.0349,
            -0.102,
            0.0854,
            0.0953,
            -0.0475,
            0.0105,
            0.1258,
            0.0386,
            -0.0055,
            -0.116,
            0.0685
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0004,
            0.0004,
            -0.0006,
            -0.0011,
            -0.0003,
            -0.0002,
            -0.0001,
            0.0005,
            0.0002,
            -0.0,
            -0.0002,
            -0.0007,
            0.0,
            0.0005,
            -0.0003
          ],
          "after": [
            -0.0091,
            0.0064,
            -0.0343,
            0.0107,
            -0.0058,
            0.1218,
            0.0231,
            -0.1137,
            0.0285,
            0.0375,
            -0.077,
            -0.036,
            0.0996,
            -0.0594,
            -0.0014,
            0.0423
          ]
        }
      }
    },
    {
      "step": 101,
      "word": "amberle",
      "loss": 2.5406,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0686,
            0.0628,
            0.0537,
            0.0743,
            0.1942,
            -0.0202,
            0.1578,
            0.163,
            0.0882,
            -0.0324,
            0.0533,
            0.137,
            -0.113,
            -0.119,
            -0.0918,
            0.1603
          ],
          "after": [
            -0.0343,
            0.0997,
            0.1369,
            0.034,
            0.0139,
            -0.0466,
            0.0384,
            -0.1164,
            -0.0402,
            0.0175,
            0.0709,
            0.0126,
            -0.0192,
            -0.0187,
            -0.0423,
            -0.081
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2058,
            -0.3905,
            -0.1645,
            -0.2102,
            -0.1888,
            0.3983,
            -0.3765,
            0.1356,
            0.2295,
            -0.1363,
            -0.1202,
            -0.4556,
            -0.0637,
            0.0935,
            0.4796,
            -0.5304
          ],
          "after": [
            -0.003,
            0.0978,
            -0.0015,
            -0.0722,
            0.0534,
            -0.0072,
            0.0808,
            0.129,
            -0.0645,
            -0.033,
            0.0587,
            -0.0,
            0.0918,
            0.0251,
            -0.0738,
            0.1794
          ]
        },
        "position_0": {
          "grad": [
            0.0872,
            0.0685,
            -0.016,
            -0.1171,
            0.0955,
            0.0221,
            -0.1121,
            -0.1229,
            0.0783,
            0.014,
            0.1929,
            0.0609,
            -0.0757,
            0.0469,
            -0.0358,
            -0.0396
          ],
          "after": [
            -0.0208,
            -0.077,
            -0.2413,
            0.0972,
            0.0278,
            -0.0352,
            -0.1019,
            0.0862,
            0.0953,
            -0.0478,
            0.0092,
            0.1253,
            0.039,
            -0.0053,
            -0.1159,
            0.0686
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0002,
            -0.0002,
            -0.0002,
            -0.0001,
            0.0001,
            -0.0002,
            -0.0001,
            -0.0,
            -0.0002,
            -0.0003,
            -0.0004,
            -0.0001,
            0.0005,
            0.0001,
            -0.0005
          ],
          "after": [
            -0.0096,
            0.0069,
            -0.0349,
            0.0107,
            -0.0047,
            0.1211,
            0.0239,
            -0.1136,
            0.0277,
            0.0375,
            -0.0763,
            -0.035,
            0.0997,
            -0.0603,
            -0.0017,
            0.043
          ]
        }
      }
    },
    {
      "step": 102,
      "word": "kessa",
      "loss": 2.5062,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2966,
            0.0827,
            -0.1506,
            0.0078,
            -0.0743,
            -0.0219,
            0.0598,
            0.0388,
            -0.1528,
            0.0486,
            -0.0315,
            -0.2739,
            0.1803,
            0.1245,
            -0.0004,
            -0.173
          ],
          "after": [
            -0.0344,
            0.0993,
            0.1377,
            0.0328,
            0.0133,
            -0.0467,
            0.0359,
            -0.1173,
            -0.0388,
            0.0184,
            0.0722,
            0.0138,
            -0.0207,
            -0.0184,
            -0.0419,
            -0.0822
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.209,
            -0.2602,
            -0.0068,
            -0.0759,
            0.3044,
            0.2096,
            -0.3698,
            -0.1512,
            0.3625,
            -0.0406,
            0.2229,
            0.2106,
            -0.2488,
            0.0202,
            -0.131,
            -0.1793
          ],
          "after": [
            -0.0038,
            0.0989,
            -0.0024,
            -0.0712,
            0.0529,
            -0.0081,
            0.0818,
            0.1294,
            -0.0655,
            -0.0317,
            0.0579,
            0.001,
            0.0923,
            0.0248,
            -0.074,
            0.1813
          ]
        },
        "position_0": {
          "grad": [
            -0.093,
            0.0466,
            -0.1382,
            0.0948,
            -0.0728,
            -0.1154,
            0.201,
            0.0345,
            -0.0729,
            -0.0595,
            -0.0556,
            -0.1158,
            0.1035,
            0.0109,
            -0.1101,
            -0.095
          ],
          "after": [
            -0.0206,
            -0.0773,
            -0.2414,
            0.0969,
            0.027,
            -0.0349,
            -0.1023,
            0.0869,
            0.0954,
            -0.0478,
            0.0084,
            0.1253,
            0.0391,
            -0.0051,
            -0.1154,
            0.0689
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0006,
            0.0004,
            0.0004,
            -0.0001,
            0.0002,
            -0.0006,
            -0.0005,
            0.0,
            -0.0005,
            -0.0002,
            -0.0004,
            0.0003,
            0.0007,
            -0.0003,
            -0.0003
          ],
          "after": [
            -0.0104,
            0.0068,
            -0.0358,
            0.0103,
            -0.0036,
            0.1204,
            0.0249,
            -0.113,
            0.027,
            0.0379,
            -0.0756,
            -0.0338,
            0.0996,
            -0.0616,
            -0.0017,
            0.0438
          ]
        }
      }
    },
    {
      "step": 103,
      "word": "rukaya",
      "loss": 2.8551,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.003,
            0.151,
            0.0752,
            -0.0512,
            -0.0181,
            0.0872,
            0.1323,
            0.0614,
            -0.0949,
            0.0904,
            -0.1095,
            0.1749,
            0.1714,
            0.0656,
            -0.0449,
            0.1749
          ],
          "after": [
            -0.0346,
            0.0983,
            0.1381,
            0.0321,
            0.0128,
            -0.0473,
            0.0332,
            -0.1183,
            -0.0372,
            0.0186,
            0.074,
            0.0143,
            -0.0226,
            -0.0184,
            -0.0414,
            -0.0836
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1105,
            0.1978,
            -0.0115,
            0.0523,
            0.1817,
            -0.3099,
            0.3098,
            0.0162,
            -0.268,
            0.1378,
            0.0353,
            0.0765,
            0.1125,
            -0.1225,
            -0.3415,
            0.2022
          ],
          "after": [
            -0.0043,
            0.0993,
            -0.0032,
            -0.0705,
            0.0522,
            -0.0083,
            0.0821,
            0.1296,
            -0.0658,
            -0.031,
            0.057,
            0.0017,
            0.0925,
            0.0247,
            -0.0736,
            0.1826
          ]
        },
        "position_0": {
          "grad": [
            0.056,
            -0.0326,
            0.0574,
            -0.13,
            0.178,
            0.104,
            0.0171,
            -0.1298,
            0.1046,
            -0.0026,
            0.01,
            0.074,
            -0.159,
            -0.0638,
            0.0697,
            0.1238
          ],
          "after": [
            -0.0206,
            -0.0775,
            -0.2417,
            0.097,
            0.0257,
            -0.0351,
            -0.1027,
            0.0879,
            0.0952,
            -0.0478,
            0.0076,
            0.125,
            0.0396,
            -0.0048,
            -0.1152,
            0.0689
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0003,
            0.0,
            -0.0002,
            -0.0002,
            -0.0002,
            0.0,
            -0.0001,
            0.0002,
            -0.0001,
            0.0001,
            -0.0,
            0.0001,
            0.0,
            0.0,
            -0.0001
          ],
          "after": [
            -0.0109,
            0.0066,
            -0.0367,
            0.0101,
            -0.0025,
            0.1199,
            0.0258,
            -0.1124,
            0.0261,
            0.0383,
            -0.0751,
            -0.0328,
            0.0995,
            -0.0628,
            -0.0017,
            0.0446
          ]
        }
      }
    },
    {
      "step": 104,
      "word": "dimitry",
      "loss": 2.6752,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0347,
            0.0974,
            0.1384,
            0.0315,
            0.0124,
            -0.0478,
            0.031,
            -0.1192,
            -0.0358,
            0.0188,
            0.0754,
            0.0147,
            -0.0243,
            -0.0185,
            -0.041,
            -0.0849
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1303,
            0.1693,
            -0.0539,
            0.0638,
            0.2103,
            -0.2024,
            0.2994,
            -0.0095,
            -0.2317,
            0.0765,
            0.0134,
            0.044,
            0.1326,
            -0.0911,
            -0.367,
            0.2007
          ],
          "after": [
            -0.0043,
            0.0993,
            -0.0038,
            -0.0701,
            0.0511,
            -0.008,
            0.082,
            0.1299,
            -0.0655,
            -0.0307,
            0.0562,
            0.0022,
            0.0923,
            0.0249,
            -0.0727,
            0.1832
          ]
        },
        "position_0": {
          "grad": [
            -0.0701,
            0.1237,
            -0.0016,
            -0.0076,
            0.1796,
            -0.0898,
            0.147,
            -0.0526,
            -0.0872,
            -0.0025,
            -0.1124,
            0.0676,
            0.0641,
            0.0464,
            -0.019,
            0.1826
          ],
          "after": [
            -0.0204,
            -0.0781,
            -0.2419,
            0.0971,
            0.024,
            -0.0349,
            -0.1033,
            0.089,
            0.0953,
            -0.0477,
            0.0075,
            0.1246,
            0.0399,
            -0.0047,
            -0.1149,
            0.0683
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            -0.0004,
            0.0004,
            -0.0011,
            0.0009,
            0.0002,
            0.0011,
            -0.0005,
            0.0001,
            0.0006,
            0.0007,
            0.0002,
            -0.0006,
            -0.0008,
            0.0002,
            0.0002
          ],
          "after": [
            -0.0104,
            0.0067,
            -0.0379,
            0.0108,
            -0.0022,
            0.1193,
            0.0259,
            -0.1115,
            0.0252,
            0.0381,
            -0.0755,
            -0.0321,
            0.0998,
            -0.0631,
            -0.0019,
            0.0451
          ]
        }
      }
    },
    {
      "step": 105,
      "word": "conlin",
      "loss": 2.474,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0348,
            0.0967,
            0.1387,
            0.0309,
            0.0121,
            -0.0482,
            0.029,
            -0.12,
            -0.0346,
            0.019,
            0.0767,
            0.015,
            -0.0257,
            -0.0185,
            -0.0406,
            -0.086
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1373,
            0.1351,
            -0.0898,
            0.0469,
            0.1056,
            -0.2251,
            0.3353,
            0.0459,
            -0.2046,
            0.0939,
            0.0168,
            0.0732,
            0.125,
            -0.148,
            -0.2706,
            0.2238
          ],
          "after": [
            -0.004,
            0.0989,
            -0.004,
            -0.07,
            0.05,
            -0.0074,
            0.0813,
            0.1299,
            -0.0648,
            -0.0308,
            0.0554,
            0.0025,
            0.0917,
            0.0254,
            -0.0716,
            0.1833
          ]
        },
        "position_0": {
          "grad": [
            -0.137,
            0.0529,
            0.0367,
            0.1056,
            -0.0401,
            -0.0707,
            0.1896,
            0.0637,
            -0.1056,
            0.0175,
            0.0868,
            0.1497,
            0.051,
            -0.0942,
            -0.1395,
            0.0515
          ],
          "after": [
            -0.0197,
            -0.0788,
            -0.2423,
            0.0969,
            0.0227,
            -0.0343,
            -0.1042,
            0.0896,
            0.0956,
            -0.0478,
            0.007,
            0.1238,
            0.0399,
            -0.0043,
            -0.1143,
            0.0677
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0003,
            -0.0001,
            0.0002,
            -0.0001,
            -0.0001,
            0.0006,
            0.0001,
            -0.0002,
            0.0001,
            0.0002,
            -0.0001,
            0.0004,
            -0.0003,
            0.0,
            0.0003
          ],
          "after": [
            -0.0099,
            0.0065,
            -0.0388,
            0.0113,
            -0.0018,
            0.1189,
            0.0256,
            -0.1107,
            0.0247,
            0.0379,
            -0.0761,
            -0.0314,
            0.0998,
            -0.0631,
            -0.0021,
            0.0453
          ]
        }
      }
    },
    {
      "step": 106,
      "word": "diary",
      "loss": 2.5854,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0574,
            -0.0318,
            0.0176,
            -0.0928,
            0.1969,
            0.1622,
            0.0491,
            -0.1841,
            0.1537,
            -0.0195,
            0.1239,
            0.2017,
            -0.194,
            -0.0955,
            0.0472,
            0.2736
          ],
          "after": [
            -0.0351,
            0.0962,
            0.1389,
            0.031,
            0.0111,
            -0.0494,
            0.0272,
            -0.1198,
            -0.0343,
            0.0193,
            0.0771,
            0.0148,
            -0.0262,
            -0.0181,
            -0.0405,
            -0.0876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0539,
            0.126,
            -0.0758,
            0.0723,
            0.1256,
            -0.1941,
            0.2745,
            -0.007,
            -0.2427,
            0.1138,
            0.0229,
            0.0057,
            0.1351,
            -0.0989,
            -0.2861,
            0.1276
          ],
          "after": [
            -0.0035,
            0.0983,
            -0.0039,
            -0.0701,
            0.0488,
            -0.0065,
            0.0804,
            0.13,
            -0.0638,
            -0.0312,
            0.0547,
            0.0027,
            0.0909,
            0.0261,
            -0.0702,
            0.1831
          ]
        },
        "position_0": {
          "grad": [
            -0.0809,
            0.0914,
            -0.0202,
            -0.0402,
            0.1681,
            -0.1242,
            0.1145,
            -0.0459,
            -0.05,
            -0.0149,
            -0.1809,
            0.032,
            0.0401,
            0.0951,
            0.0353,
            0.1927
          ],
          "after": [
            -0.0188,
            -0.0796,
            -0.2426,
            0.0969,
            0.0209,
            -0.0333,
            -0.1053,
            0.0904,
            0.096,
            -0.0478,
            0.0073,
            0.123,
            0.0399,
            -0.0042,
            -0.1139,
            0.0666
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0004,
            -0.0004,
            0.0004,
            -0.0002,
            -0.0,
            -0.0003,
            0.0008,
            -0.0005,
            0.0005,
            0.0002,
            0.0009,
            -0.0002,
            -0.001,
            0.0006,
            0.0002
          ],
          "after": [
            -0.0094,
            0.0067,
            -0.0391,
            0.0113,
            -0.0013,
            0.1186,
            0.0256,
            -0.1109,
            0.0248,
            0.0373,
            -0.0769,
            -0.0316,
            0.0999,
            -0.0623,
            -0.0027,
            0.0452
          ]
        }
      }
    },
    {
      "step": 107,
      "word": "giavonni",
      "loss": 2.9317,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2268,
            -0.0372,
            0.0294,
            0.0221,
            0.2225,
            0.0553,
            -0.0555,
            0.0116,
            -0.1129,
            -0.054,
            -0.0052,
            0.1579,
            0.1128,
            -0.2462,
            -0.1097,
            0.2626
          ],
          "after": [
            -0.0346,
            0.0959,
            0.1389,
            0.031,
            0.0094,
            -0.0507,
            0.0258,
            -0.1198,
            -0.0335,
            0.0198,
            0.0775,
            0.0141,
            -0.027,
            -0.0168,
            -0.04,
            -0.0896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1533,
            0.0935,
            -0.0259,
            0.0363,
            0.1459,
            -0.1843,
            0.2519,
            0.012,
            -0.1601,
            0.0773,
            0.0119,
            0.1035,
            0.0752,
            -0.121,
            -0.2724,
            0.1731
          ],
          "after": [
            -0.0028,
            0.0975,
            -0.0038,
            -0.0703,
            0.0475,
            -0.0054,
            0.0793,
            0.13,
            -0.0625,
            -0.0318,
            0.054,
            0.0026,
            0.09,
            0.027,
            -0.0686,
            0.1826
          ]
        },
        "position_0": {
          "grad": [
            -0.0491,
            0.0491,
            -0.0137,
            0.0767,
            -0.0048,
            -0.0839,
            0.0663,
            -0.0305,
            -0.1118,
            0.0752,
            0.022,
            0.0412,
            0.0029,
            -0.07,
            -0.1501,
            0.0619
          ],
          "after": [
            -0.0178,
            -0.0805,
            -0.2427,
            0.0966,
            0.0195,
            -0.0321,
            -0.1063,
            0.0912,
            0.0966,
            -0.0482,
            0.0075,
            0.1223,
            0.0399,
            -0.004,
            -0.1131,
            0.0656
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0004,
            -0.0,
            0.0005,
            0.0001,
            -0.0005,
            0.0011,
            0.0002,
            -0.0007,
            0.0006,
            0.0003,
            0.0008,
            0.0006,
            -0.0005,
            -0.0004,
            0.0004
          ],
          "after": [
            -0.0087,
            0.0066,
            -0.0394,
            0.0109,
            -0.001,
            0.1187,
            0.025,
            -0.1111,
            0.0256,
            0.0363,
            -0.0779,
            -0.0323,
            0.0996,
            -0.0611,
            -0.0029,
            0.0447
          ]
        }
      }
    },
    {
      "step": 108,
      "word": "kyrin",
      "loss": 2.3354,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0342,
            0.0957,
            0.1389,
            0.031,
            0.008,
            -0.0518,
            0.0247,
            -0.1197,
            -0.0329,
            0.0203,
            0.0778,
            0.0135,
            -0.0277,
            -0.0156,
            -0.0395,
            -0.0913
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1018,
            0.1036,
            -0.0743,
            0.0223,
            0.0772,
            -0.2097,
            0.3041,
            0.0295,
            -0.2112,
            0.111,
            -0.027,
            -0.0032,
            0.1514,
            -0.0939,
            -0.2195,
            0.1742
          ],
          "after": [
            -0.0018,
            0.0966,
            -0.0035,
            -0.0705,
            0.0462,
            -0.004,
            0.0778,
            0.1298,
            -0.0611,
            -0.0327,
            0.0535,
            0.0026,
            0.0888,
            0.028,
            -0.0669,
            0.1818
          ]
        },
        "position_0": {
          "grad": [
            -0.087,
            0.0798,
            -0.1578,
            0.0538,
            -0.0381,
            -0.0801,
            0.2227,
            -0.0029,
            -0.0923,
            -0.0258,
            -0.0283,
            -0.0924,
            0.0753,
            -0.0104,
            -0.1235,
            -0.086
          ],
          "after": [
            -0.0167,
            -0.0815,
            -0.2421,
            0.0962,
            0.0184,
            -0.0307,
            -0.1077,
            0.0918,
            0.0973,
            -0.0484,
            0.0077,
            0.1219,
            0.0397,
            -0.0037,
            -0.112,
            0.0649
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0002,
            0.0001,
            0.0004,
            0.0009,
            -0.0011,
            0.0014,
            0.0011,
            -0.0003,
            -0.0005,
            -0.0004,
            0.0014,
            0.0004,
            -0.0011,
            -0.0004,
            0.0008
          ],
          "after": [
            -0.0077,
            0.0066,
            -0.0397,
            0.0101,
            -0.0012,
            0.1196,
            0.0237,
            -0.1123,
            0.0266,
            0.0358,
            -0.0783,
            -0.0339,
            0.0991,
            -0.0593,
            -0.0028,
            0.0436
          ]
        }
      }
    },
    {
      "step": 109,
      "word": "kesean",
      "loss": 2.2676,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0353,
            0.0057,
            -0.0254,
            0.0591,
            -0.0672,
            -0.0914,
            -0.014,
            -0.0689,
            0.0391,
            -0.1346,
            -0.0183,
            0.0033,
            -0.0519,
            0.008,
            -0.0046,
            0.0019
          ],
          "after": [
            -0.0338,
            0.0955,
            0.139,
            0.0306,
            0.0071,
            -0.0523,
            0.0237,
            -0.1194,
            -0.0325,
            0.0215,
            0.0782,
            0.013,
            -0.0281,
            -0.0146,
            -0.0391,
            -0.0927
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0467,
            -0.1534,
            0.1091,
            -0.1025,
            0.1017,
            0.2921,
            -0.6119,
            -0.1552,
            0.4398,
            -0.2799,
            0.2029,
            -0.0103,
            -0.1456,
            -0.0037,
            0.3594,
            -0.3242
          ],
          "after": [
            -0.0012,
            0.0963,
            -0.0036,
            -0.0704,
            0.045,
            -0.0035,
            0.0775,
            0.1304,
            -0.0607,
            -0.0325,
            0.0521,
            0.0025,
            0.0882,
            0.0288,
            -0.066,
            0.1818
          ]
        },
        "position_0": {
          "grad": [
            -0.0723,
            0.0193,
            -0.1343,
            0.044,
            -0.0754,
            -0.0767,
            0.1331,
            0.0215,
            -0.0326,
            -0.041,
            -0.0493,
            -0.1148,
            0.0532,
            0.025,
            -0.0757,
            -0.1063
          ],
          "after": [
            -0.0155,
            -0.0824,
            -0.241,
            0.0957,
            0.0177,
            -0.0291,
            -0.1091,
            0.0923,
            0.098,
            -0.0483,
            0.0082,
            0.1219,
            0.0393,
            -0.0036,
            -0.1108,
            0.0646
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0,
            0.0,
            -0.0001,
            -0.0001,
            0.0,
            0.0001,
            0.0,
            0.0002,
            -0.0001,
            -0.0001,
            0.0003,
            -0.0,
            0.0,
            0.0001,
            0.0002
          ],
          "after": [
            -0.0068,
            0.0067,
            -0.0401,
            0.0095,
            -0.0013,
            0.1204,
            0.0227,
            -0.1132,
            0.0272,
            0.0355,
            -0.0786,
            -0.0355,
            0.0986,
            -0.0578,
            -0.0027,
            0.0425
          ]
        }
      }
    },
    {
      "step": 110,
      "word": "addie",
      "loss": 2.4636,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1115,
            0.1053,
            -0.066,
            0.0179,
            0.2253,
            -0.1306,
            0.1434,
            -0.0201,
            0.0066,
            -0.0473,
            -0.1614,
            0.0899,
            0.1021,
            0.0809,
            -0.0266,
            0.3072
          ],
          "after": [
            -0.0331,
            0.0948,
            0.1394,
            0.0301,
            0.0055,
            -0.052,
            0.0223,
            -0.119,
            -0.0322,
            0.0227,
            0.0793,
            0.0123,
            -0.0288,
            -0.0142,
            -0.0386,
            -0.0947
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2365,
            -0.0911,
            -0.0618,
            0.2686,
            -0.4518,
            0.0185,
            -0.3314,
            0.1426,
            -0.0434,
            -0.1462,
            -0.1859,
            -0.0243,
            0.1741,
            0.158,
            0.2089,
            -0.0591
          ],
          "after": [
            -0.0012,
            0.0962,
            -0.0035,
            -0.0711,
            0.0448,
            -0.003,
            0.0777,
            0.1302,
            -0.0603,
            -0.0319,
            0.0519,
            0.0026,
            0.0872,
            0.0292,
            -0.0656,
            0.1819
          ]
        },
        "position_0": {
          "grad": [
            0.1413,
            0.0009,
            -0.0277,
            -0.1957,
            -0.0028,
            0.0472,
            -0.2392,
            -0.1151,
            0.2189,
            -0.0101,
            0.2392,
            -0.0346,
            -0.1391,
            0.1186,
            0.0418,
            -0.1198
          ],
          "after": [
            -0.015,
            -0.0832,
            -0.24,
            0.0959,
            0.0171,
            -0.028,
            -0.1098,
            0.0932,
            0.0981,
            -0.0482,
            0.0076,
            0.122,
            0.0394,
            -0.0038,
            -0.1099,
            0.0647
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0,
            0.0006,
            0.0011,
            0.0003,
            -0.0006,
            -0.0013,
            0.0009,
            -0.0004,
            -0.0008,
            -0.0004,
            0.0002,
            0.0004,
            -0.0004,
            -0.001,
            0.0002
          ],
          "after": [
            -0.0065,
            0.0067,
            -0.041,
            0.0081,
            -0.0016,
            0.1216,
            0.0224,
            -0.1148,
            0.0281,
            0.0359,
            -0.0784,
            -0.037,
            0.0979,
            -0.0562,
            -0.0019,
            0.0413
          ]
        }
      }
    },
    {
      "step": 111,
      "word": "immanuel",
      "loss": 2.6733,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.028,
            0.0324,
            -0.0645,
            0.0263,
            -0.0782,
            -0.068,
            0.0184,
            -0.0428,
            0.0248,
            -0.0432,
            -0.0001,
            -0.0176,
            -0.0004,
            -0.0048,
            0.0179,
            0.0052
          ],
          "after": [
            -0.0324,
            0.0941,
            0.14,
            0.0296,
            0.0044,
            -0.0514,
            0.0211,
            -0.1185,
            -0.0321,
            0.024,
            0.0803,
            0.0117,
            -0.0294,
            -0.0137,
            -0.0383,
            -0.0964
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0002,
            -0.1382,
            -0.083,
            0.1012,
            -0.1887,
            0.1648,
            0.0324,
            -0.046,
            -0.0587,
            -0.0224,
            -0.0245,
            -0.1858,
            0.0504,
            0.0563,
            0.0897,
            -0.0608
          ],
          "after": [
            -0.0013,
            0.0964,
            -0.0031,
            -0.0721,
            0.045,
            -0.0029,
            0.0779,
            0.1303,
            -0.0599,
            -0.0313,
            0.0518,
            0.0031,
            0.0862,
            0.0293,
            -0.0653,
            0.1821
          ]
        },
        "position_0": {
          "grad": [
            -0.0723,
            -0.1203,
            -0.0177,
            -0.143,
            0.1195,
            0.0791,
            -0.1206,
            0.0079,
            0.0773,
            0.0054,
            0.0227,
            -0.0089,
            -0.1519,
            -0.0189,
            0.0121,
            -0.047
          ],
          "after": [
            -0.0143,
            -0.0835,
            -0.239,
            0.0965,
            0.0162,
            -0.0274,
            -0.1102,
            0.0939,
            0.0979,
            -0.0481,
            0.007,
            0.1221,
            0.0399,
            -0.004,
            -0.1092,
            0.0649
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0004,
            0.0002,
            0.0002,
            0.0004,
            -0.0003,
            0.0,
            0.0004,
            0.0,
            -0.0002,
            0.0,
            0.0007,
            -0.0003,
            -0.0005,
            -0.0001,
            0.0002
          ],
          "after": [
            -0.0061,
            0.0072,
            -0.042,
            0.0068,
            -0.0021,
            0.1228,
            0.0221,
            -0.1164,
            0.029,
            0.0364,
            -0.0783,
            -0.0388,
            0.0976,
            -0.0543,
            -0.0012,
            0.0402
          ]
        }
      }
    },
    {
      "step": 112,
      "word": "caedyn",
      "loss": 2.4568,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0601,
            -0.1628,
            -0.0513,
            0.0809,
            -0.1495,
            -0.0098,
            -0.1469,
            -0.1114,
            0.0995,
            0.0695,
            -0.0179,
            -0.1438,
            -0.0606,
            0.0618,
            0.0725,
            -0.2753
          ],
          "after": [
            -0.032,
            0.0943,
            0.1407,
            0.0287,
            0.004,
            -0.0509,
            0.0206,
            -0.1176,
            -0.0324,
            0.0247,
            0.0812,
            0.0117,
            -0.0297,
            -0.0136,
            -0.0383,
            -0.0972
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0581,
            -0.0339,
            -0.1328,
            0.1625,
            0.1257,
            0.3266,
            -0.014,
            0.23,
            0.0613,
            0.0521,
            -0.0286,
            0.3003,
            -0.1894,
            -0.1632,
            0.1295,
            0.1973
          ],
          "after": [
            -0.0012,
            0.0967,
            -0.0024,
            -0.0734,
            0.0449,
            -0.0034,
            0.078,
            0.1294,
            -0.0597,
            -0.031,
            0.0518,
            0.0028,
            0.0859,
            0.0298,
            -0.0653,
            0.1819
          ]
        },
        "position_0": {
          "grad": [
            -0.1157,
            0.0524,
            0.0039,
            0.0684,
            -0.0379,
            -0.0658,
            0.1712,
            0.0442,
            -0.0864,
            0.0341,
            0.0946,
            0.1423,
            0.0316,
            -0.0912,
            -0.1208,
            0.0359
          ],
          "after": [
            -0.0133,
            -0.0839,
            -0.2382,
            0.0968,
            0.0156,
            -0.0266,
            -0.1108,
            0.0943,
            0.098,
            -0.0483,
            0.0061,
            0.1218,
            0.0402,
            -0.0038,
            -0.1082,
            0.065
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0001,
            0.0002,
            -0.0003,
            -0.0001,
            -0.0002,
            -0.0003,
            -0.0005,
            -0.0001,
            -0.0002,
            0.0,
            -0.0004,
            -0.0,
            0.0003,
            0.0001,
            -0.0004
          ],
          "after": [
            -0.0058,
            0.0076,
            -0.0432,
            0.0059,
            -0.0025,
            0.124,
            0.0221,
            -0.1173,
            0.0298,
            0.037,
            -0.0782,
            -0.04,
            0.0973,
            -0.053,
            -0.0006,
            0.0396
          ]
        }
      }
    },
    {
      "step": 113,
      "word": "tyreik",
      "loss": 2.8027,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0316,
            0.0944,
            0.1413,
            0.0279,
            0.0037,
            -0.0504,
            0.0203,
            -0.1168,
            -0.0327,
            0.0253,
            0.082,
            0.0116,
            -0.03,
            -0.0136,
            -0.0383,
            -0.0978
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1811,
            -0.1617,
            0.0202,
            -0.1324,
            0.1719,
            0.03,
            -0.1357,
            0.0756,
            0.0542,
            -0.1745,
            -0.0093,
            0.0447,
            -0.1451,
            -0.1409,
            0.3022,
            0.0077
          ],
          "after": [
            -0.0006,
            0.0974,
            -0.0019,
            -0.0741,
            0.0445,
            -0.004,
            0.0783,
            0.1284,
            -0.0595,
            -0.0301,
            0.0519,
            0.0024,
            0.086,
            0.0306,
            -0.0657,
            0.1817
          ]
        },
        "position_0": {
          "grad": [
            0.0228,
            -0.2091,
            0.0613,
            0.1017,
            -0.1559,
            0.0571,
            0.1141,
            0.0608,
            -0.1052,
            -0.1109,
            -0.2054,
            -0.3531,
            0.1585,
            0.0961,
            0.1026,
            0.103
          ],
          "after": [
            -0.0125,
            -0.0836,
            -0.2378,
            0.0968,
            0.0156,
            -0.0262,
            -0.1116,
            0.0944,
            0.0984,
            -0.0477,
            0.0061,
            0.1225,
            0.0401,
            -0.004,
            -0.1077,
            0.0648
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0002,
            0.0003,
            -0.0,
            -0.0005,
            -0.0005,
            0.0001,
            0.0003,
            0.0,
            0.0002,
            -0.0003,
            0.0004,
            -0.0,
            -0.0,
            0.0003,
            0.0005
          ],
          "after": [
            -0.0053,
            0.0079,
            -0.0444,
            0.0051,
            -0.0025,
            0.1254,
            0.022,
            -0.1184,
            0.0305,
            0.0374,
            -0.0777,
            -0.0414,
            0.0971,
            -0.0519,
            -0.0004,
            0.0386
          ]
        }
      }
    },
    {
      "step": 114,
      "word": "marty",
      "loss": 2.4278,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1418,
            -0.0793,
            -0.064,
            -0.1496,
            0.0827,
            0.0035,
            0.0525,
            -0.208,
            0.1855,
            -0.1276,
            0.0778,
            -0.0325,
            -0.054,
            0.0078,
            0.1432,
            0.1851
          ],
          "after": [
            -0.0317,
            0.0948,
            0.1421,
            0.0281,
            0.0031,
            -0.05,
            0.0197,
            -0.1153,
            -0.0338,
            0.0266,
            0.0822,
            0.0117,
            -0.03,
            -0.0135,
            -0.039,
            -0.0989
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0528,
            0.1262,
            -0.0871,
            0.0771,
            0.0405,
            -0.1798,
            0.2214,
            0.0077,
            -0.2408,
            0.1042,
            0.0018,
            -0.0343,
            0.2026,
            -0.0743,
            -0.2341,
            0.0996
          ],
          "after": [
            0.0,
            0.0976,
            -0.0012,
            -0.0749,
            0.0441,
            -0.0041,
            0.0782,
            0.1275,
            -0.059,
            -0.0298,
            0.0519,
            0.0022,
            0.0856,
            0.0315,
            -0.0658,
            0.1813
          ]
        },
        "position_0": {
          "grad": [
            -0.0154,
            0.0333,
            0.0228,
            0.092,
            0.0156,
            0.0165,
            0.171,
            0.1691,
            0.0807,
            -0.0209,
            0.0219,
            0.02,
            -0.1819,
            -0.0502,
            0.0818,
            0.0226
          ],
          "after": [
            -0.0118,
            -0.0834,
            -0.2376,
            0.0964,
            0.0156,
            -0.0259,
            -0.1127,
            0.0939,
            0.0985,
            -0.0472,
            0.0061,
            0.1231,
            0.0404,
            -0.004,
            -0.1076,
            0.0645
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0003,
            0.0001,
            -0.0002,
            0.0,
            -0.0001,
            -0.0002,
            -0.0,
            -0.0001,
            -0.0002,
            -0.0001,
            -0.0,
            -0.0002,
            0.0,
            0.0002,
            -0.0
          ],
          "after": [
            -0.0049,
            0.0083,
            -0.0456,
            0.0046,
            -0.0025,
            0.1267,
            0.0221,
            -0.1194,
            0.0312,
            0.0379,
            -0.0773,
            -0.0426,
            0.097,
            -0.051,
            -0.0003,
            0.0378
          ]
        }
      }
    },
    {
      "step": 115,
      "word": "makelle",
      "loss": 2.4244,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1717,
            0.058,
            -0.0549,
            0.0589,
            0.0033,
            0.0554,
            0.1656,
            0.0597,
            -0.0612,
            0.0353,
            -0.0222,
            0.0115,
            -0.0577,
            -0.1111,
            -0.1399,
            -0.0939
          ],
          "after": [
            -0.0313,
            0.0949,
            0.143,
            0.028,
            0.0026,
            -0.05,
            0.0185,
            -0.1143,
            -0.0345,
            0.0274,
            0.0826,
            0.0117,
            -0.0298,
            -0.013,
            -0.0389,
            -0.0995
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.261,
            -0.477,
            -0.1642,
            -0.1327,
            -0.2031,
            0.503,
            -0.5756,
            -0.0015,
            0.2667,
            -0.2117,
            -0.053,
            -0.314,
            -0.0159,
            0.1804,
            0.5105,
            -0.5406
          ],
          "after": [
            -0.0001,
            0.0989,
            -0.0002,
            -0.0752,
            0.0442,
            -0.0051,
            0.079,
            0.1267,
            -0.059,
            -0.0288,
            0.0522,
            0.0027,
            0.0852,
            0.0318,
            -0.0665,
            0.1821
          ]
        },
        "position_0": {
          "grad": [
            -0.03,
            0.0516,
            0.0203,
            0.0784,
            0.0693,
            0.0203,
            0.1459,
            0.1222,
            0.0414,
            -0.0019,
            0.0322,
            0.0722,
            -0.1142,
            -0.0687,
            0.0179,
            0.0306
          ],
          "after": [
            -0.0111,
            -0.0834,
            -0.2374,
            0.0959,
            0.0153,
            -0.0258,
            -0.1139,
            0.0929,
            0.0984,
            -0.0467,
            0.0059,
            0.1234,
            0.041,
            -0.0038,
            -0.1075,
            0.0642
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0,
            -0.0003,
            0.0001,
            0.0001,
            0.0001,
            0.0001,
            -0.0,
            -0.0002,
            -0.0001,
            0.0001,
            -0.0,
            -0.0,
            -0.0001,
            0.0,
            -0.0001
          ],
          "after": [
            -0.0047,
            0.0087,
            -0.0464,
            0.0041,
            -0.0026,
            0.1277,
            0.0221,
            -0.1201,
            0.0319,
            0.0384,
            -0.077,
            -0.0436,
            0.0969,
            -0.0501,
            -0.0002,
            0.0372
          ]
        }
      }
    },
    {
      "step": 116,
      "word": "nahshon",
      "loss": 2.7015,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0614,
            -0.016,
            0.088,
            0.1263,
            -0.0198,
            0.0904,
            0.1198,
            0.1361,
            0.0217,
            0.0852,
            -0.0511,
            -0.0497,
            0.2133,
            0.0239,
            -0.0417,
            0.1665
          ],
          "after": [
            -0.0308,
            0.0951,
            0.1434,
            0.0271,
            0.0023,
            -0.0504,
            0.0171,
            -0.114,
            -0.0351,
            0.0277,
            0.0831,
            0.0119,
            -0.0304,
            -0.0127,
            -0.0387,
            -0.1004
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1188,
            0.0981,
            -0.047,
            0.074,
            0.1271,
            -0.2092,
            0.2678,
            0.0474,
            -0.1854,
            0.0539,
            -0.0114,
            0.1121,
            0.0773,
            -0.1345,
            -0.2923,
            0.191
          ],
          "after": [
            0.0001,
            0.0998,
            0.0008,
            -0.0757,
            0.0439,
            -0.0055,
            0.0793,
            0.1258,
            -0.0587,
            -0.0281,
            0.0525,
            0.0029,
            0.0848,
            0.0324,
            -0.0668,
            0.1823
          ]
        },
        "position_0": {
          "grad": [
            0.0254,
            -0.0454,
            -0.0278,
            -0.0167,
            -0.212,
            -0.0134,
            -0.0895,
            -0.0373,
            0.1373,
            -0.0468,
            -0.1044,
            -0.1694,
            -0.0225,
            0.1235,
            0.1282,
            -0.0422
          ],
          "after": [
            -0.0105,
            -0.0833,
            -0.2372,
            0.0955,
            0.0158,
            -0.0256,
            -0.1147,
            0.0922,
            0.0981,
            -0.0459,
            0.0062,
            0.1242,
            0.0416,
            -0.004,
            -0.1078,
            0.0641
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0,
            -0.0004,
            0.0003,
            0.0013,
            -0.0001,
            0.0009,
            0.0003,
            -0.0004,
            -0.0004,
            -0.0002,
            0.0005,
            -0.0002,
            -0.0008,
            -0.001,
            0.0007
          ],
          "after": [
            -0.0047,
            0.009,
            -0.0465,
            0.0035,
            -0.0035,
            0.1286,
            0.0216,
            -0.121,
            0.0329,
            0.0393,
            -0.0765,
            -0.0448,
            0.097,
            -0.0487,
            0.0006,
            0.036
          ]
        }
      }
    },
    {
      "step": 117,
      "word": "brantley",
      "loss": 2.5699,
      "learning_rate": 0.0027,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0539,
            0.0303,
            -0.1084,
            0.0608,
            -0.145,
            -0.0372,
            -0.0115,
            -0.0353,
            0.0191,
            -0.0232,
            -0.0493,
            -0.0931,
            0.0374,
            0.0926,
            0.0061,
            -0.002
          ],
          "after": [
            -0.0305,
            0.0951,
            0.1442,
            0.0261,
            0.0025,
            -0.0506,
            0.0158,
            -0.1136,
            -0.0358,
            0.028,
            0.0839,
            0.0123,
            -0.0311,
            -0.0128,
            -0.0385,
            -0.1012
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1636,
            -0.1505,
            -0.0865,
            -0.0144,
            -0.1013,
            0.1699,
            -0.1417,
            0.0571,
            -0.0018,
            0.0363,
            -0.1253,
            -0.2104,
            0.1112,
            0.1816,
            0.0957,
            -0.2084
          ],
          "after": [
            -0.0002,
            0.101,
            0.002,
            -0.0761,
            0.044,
            -0.0063,
            0.0797,
            0.1248,
            -0.0584,
            -0.0276,
            0.0534,
            0.0036,
            0.084,
            0.0324,
            -0.0671,
            0.183
          ]
        },
        "position_0": {
          "grad": [
            -0.0037,
            0.0322,
            0.122,
            0.0105,
            0.0269,
            -0.0283,
            0.0781,
            0.0018,
            -0.1591,
            -0.0667,
            -0.0164,
            0.1017,
            0.1211,
            -0.0978,
            -0.0405,
            0.2075
          ],
          "after": [
            -0.0101,
            -0.0833,
            -0.2375,
            0.0951,
            0.0162,
            -0.0253,
            -0.1156,
            0.0916,
            0.0982,
            -0.045,
            0.0065,
            0.1245,
            0.0418,
            -0.0038,
            -0.108,
            0.0634
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0007,
            -0.0005,
            -0.0004,
            0.0003,
            0.0003,
            -0.0002,
            0.0006,
            0.0002,
            0.0003,
            -0.0,
            0.0001,
            -0.0006,
            -0.0001,
            0.0004,
            -0.0003
          ],
          "after": [
            -0.0047,
            0.0099,
            -0.046,
            0.0033,
            -0.0044,
            0.1291,
            0.0212,
            -0.1223,
            0.0336,
            0.0396,
            -0.076,
            -0.0459,
            0.0975,
            -0.0475,
            0.0009,
            0.0353
          ]
        }
      }
    },
    {
      "step": 118,
      "word": "bryli",
      "loss": 2.3727,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0303,
            0.0951,
            0.1449,
            0.0251,
            0.0027,
            -0.0507,
            0.0148,
            -0.1132,
            -0.0363,
            0.0283,
            0.0845,
            0.0127,
            -0.0317,
            -0.0129,
            -0.0384,
            -0.1019
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0524,
            0.1287,
            -0.0626,
            0.0675,
            0.0237,
            -0.2017,
            0.2359,
            -0.0201,
            -0.234,
            0.1146,
            0.0241,
            0.0112,
            0.1696,
            -0.0834,
            -0.2284,
            0.1561
          ],
          "after": [
            -0.0003,
            0.1016,
            0.0031,
            -0.0766,
            0.0439,
            -0.0065,
            0.0797,
            0.1241,
            -0.0577,
            -0.0276,
            0.054,
            0.0041,
            0.0829,
            0.0326,
            -0.067,
            0.1832
          ]
        },
        "position_0": {
          "grad": [
            -0.0049,
            0.028,
            0.1945,
            0.0375,
            0.0181,
            -0.0449,
            0.1115,
            0.0127,
            -0.2266,
            -0.1205,
            -0.019,
            0.1272,
            0.1871,
            -0.1296,
            -0.0461,
            0.3111
          ],
          "after": [
            -0.0096,
            -0.0834,
            -0.2387,
            0.0947,
            0.0164,
            -0.0248,
            -0.1166,
            0.0911,
            0.0988,
            -0.0434,
            0.0069,
            0.1244,
            0.0414,
            -0.0033,
            -0.1079,
            0.0619
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0003,
            0.001,
            -0.0009,
            0.0029,
            0.0005,
            0.0011,
            -0.0013,
            0.0,
            -0.0009,
            -0.0002,
            -0.0005,
            -0.0,
            0.0,
            -0.0022,
            -0.0009
          ],
          "after": [
            -0.0047,
            0.0103,
            -0.0467,
            0.0038,
            -0.0067,
            0.1292,
            0.0204,
            -0.1223,
            0.0342,
            0.0407,
            -0.0753,
            -0.0465,
            0.0979,
            -0.0464,
            0.0026,
            0.0354
          ]
        }
      }
    },
    {
      "step": 119,
      "word": "evella",
      "loss": 2.6703,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0995,
            0.1542,
            -0.0429,
            -0.0136,
            -0.1096,
            -0.0356,
            0.0822,
            0.0243,
            -0.0751,
            0.1063,
            0.0405,
            -0.1214,
            0.156,
            0.06,
            -0.0196,
            -0.0687
          ],
          "after": [
            -0.0304,
            0.0945,
            0.1457,
            0.0244,
            0.0032,
            -0.0507,
            0.0135,
            -0.113,
            -0.0364,
            0.0279,
            0.0848,
            0.0133,
            -0.0327,
            -0.0133,
            -0.0382,
            -0.1023
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2425,
            0.3352,
            0.6078,
            -0.1002,
            0.3387,
            0.1923,
            -0.0328,
            -0.0988,
            0.3684,
            -0.0907,
            0.3358,
            0.1383,
            -0.6296,
            -0.1168,
            0.113,
            -0.1461
          ],
          "after": [
            0.0002,
            0.1014,
            0.0024,
            -0.0767,
            0.0432,
            -0.007,
            0.0798,
            0.1238,
            -0.0578,
            -0.0273,
            0.053,
            0.0042,
            0.0837,
            0.0331,
            -0.0671,
            0.1837
          ]
        },
        "position_0": {
          "grad": [
            0.0507,
            -0.2041,
            -0.032,
            -0.0187,
            -0.0938,
            0.0753,
            -0.2268,
            -0.1027,
            0.1958,
            0.0702,
            -0.0265,
            -0.1274,
            -0.0999,
            0.0333,
            0.0996,
            -0.241
          ],
          "after": [
            -0.0095,
            -0.0828,
            -0.2395,
            0.0944,
            0.0169,
            -0.0248,
            -0.117,
            0.091,
            0.0989,
            -0.0425,
            0.0073,
            0.1247,
            0.0414,
            -0.003,
            -0.1083,
            0.0613
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0002,
            0.0001,
            0.0001,
            0.0001,
            -0.0,
            0.0001,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0002,
            0.0002,
            -0.0001,
            -0.0003,
            -0.0,
            0.0001
          ],
          "after": [
            -0.0047,
            0.0105,
            -0.0473,
            0.0043,
            -0.0088,
            0.1292,
            0.0196,
            -0.1223,
            0.0348,
            0.0418,
            -0.075,
            -0.0471,
            0.0984,
            -0.0453,
            0.0041,
            0.0355
          ]
        }
      }
    },
    {
      "step": 120,
      "word": "westley",
      "loss": 2.7496,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0304,
            0.0939,
            0.1463,
            0.0238,
            0.0037,
            -0.0506,
            0.0125,
            -0.1128,
            -0.0365,
            0.0276,
            0.0851,
            0.0139,
            -0.0336,
            -0.0136,
            -0.038,
            -0.1027
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1189,
            -0.3605,
            -0.0327,
            -0.3244,
            -0.0178,
            0.4745,
            -0.6446,
            0.0872,
            0.5473,
            0.0897,
            -0.1079,
            0.0222,
            -0.0966,
            0.306,
            0.3442,
            -0.2734
          ],
          "after": [
            0.0004,
            0.102,
            0.0019,
            -0.0758,
            0.0427,
            -0.0083,
            0.0808,
            0.1233,
            -0.059,
            -0.0273,
            0.0527,
            0.0043,
            0.0845,
            0.0328,
            -0.0678,
            0.1846
          ]
        },
        "position_0": {
          "grad": [
            -0.0433,
            0.2171,
            -0.0342,
            0.2625,
            0.056,
            -0.1532,
            0.3997,
            0.0896,
            -0.2769,
            -0.0773,
            -0.0485,
            0.0778,
            0.2273,
            -0.1308,
            -0.2236,
            0.1613
          ],
          "after": [
            -0.0092,
            -0.083,
            -0.2401,
            0.0933,
            0.0172,
            -0.024,
            -0.1181,
            0.0906,
            0.0996,
            -0.0413,
            0.0078,
            0.1247,
            0.0407,
            -0.0023,
            -0.1078,
            0.0604
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0006,
            -0.0002,
            -0.0006,
            0.0004,
            -0.0,
            -0.0004,
            0.0001,
            0.0003,
            0.0006,
            -0.0002,
            0.0004,
            -0.0001,
            0.0005,
            -0.0007,
            0.0001
          ],
          "after": [
            -0.005,
            0.0113,
            -0.0477,
            0.0051,
            -0.0107,
            0.1293,
            0.0192,
            -0.1223,
            0.035,
            0.0422,
            -0.0745,
            -0.0479,
            0.0989,
            -0.0448,
            0.0059,
            0.0354
          ]
        }
      }
    },
    {
      "step": 121,
      "word": "maciah",
      "loss": 2.5114,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.5362,
            0.0571,
            0.4079,
            0.1373,
            -0.0023,
            0.0597,
            0.3496,
            0.1617,
            0.0429,
            0.1616,
            0.12,
            0.3894,
            0.0546,
            -0.2983,
            -0.2723,
            0.3081
          ],
          "after": [
            -0.029,
            0.0931,
            0.1453,
            0.0226,
            0.0041,
            -0.0509,
            0.0103,
            -0.1134,
            -0.0368,
            0.0265,
            0.0847,
            0.0133,
            -0.0346,
            -0.0126,
            -0.0368,
            -0.1037
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0656,
            0.1313,
            -0.0411,
            0.0462,
            0.1374,
            -0.1614,
            0.2047,
            -0.0075,
            -0.1732,
            0.0687,
            0.0224,
            0.0293,
            0.0929,
            -0.086,
            -0.2854,
            0.1517
          ],
          "after": [
            0.0007,
            0.1022,
            0.0016,
            -0.0752,
            0.042,
            -0.0092,
            0.0813,
            0.1228,
            -0.0596,
            -0.0275,
            0.0523,
            0.0043,
            0.085,
            0.0327,
            -0.0679,
            0.1851
          ]
        },
        "position_0": {
          "grad": [
            -0.031,
            0.0946,
            0.0009,
            0.0631,
            0.1143,
            0.0226,
            0.1859,
            0.1089,
            0.0316,
            0.0326,
            0.0601,
            0.1189,
            -0.131,
            -0.0952,
            0.0004,
            0.0457
          ],
          "after": [
            -0.0088,
            -0.0835,
            -0.2406,
            0.0922,
            0.017,
            -0.0235,
            -0.1195,
            0.0898,
            0.1001,
            -0.0405,
            0.008,
            0.1244,
            0.0405,
            -0.0014,
            -0.1074,
            0.0595
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0002,
            -0.0,
            0.0008,
            -0.0011,
            -0.0,
            -0.001,
            0.0007,
            0.0001,
            -0.0006,
            -0.0006,
            0.0007,
            -0.0001,
            0.0003,
            0.0002,
            0.0008
          ],
          "after": [
            -0.0055,
            0.012,
            -0.0479,
            0.0052,
            -0.0118,
            0.1294,
            0.0194,
            -0.1229,
            0.035,
            0.043,
            -0.0734,
            -0.0492,
            0.0994,
            -0.0446,
            0.0072,
            0.0347
          ]
        }
      }
    },
    {
      "step": 122,
      "word": "rayann",
      "loss": 2.1035,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0265,
            0.0095,
            -0.0614,
            -0.0585,
            -0.1788,
            -0.0844,
            0.0405,
            -0.0579,
            0.0113,
            -0.0325,
            -0.124,
            0.0405,
            0.0709,
            0.1288,
            0.0922,
            0.0692
          ],
          "after": [
            -0.0278,
            0.0925,
            0.1446,
            0.0218,
            0.0051,
            -0.0507,
            0.0082,
            -0.1136,
            -0.0371,
            0.0257,
            0.085,
            0.0126,
            -0.0356,
            -0.0123,
            -0.0361,
            -0.1048
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1001,
            0.1111,
            -0.0066,
            0.0588,
            0.0988,
            -0.2214,
            0.2511,
            0.0027,
            -0.1643,
            0.1161,
            0.0585,
            0.0863,
            0.079,
            -0.1324,
            -0.239,
            0.1334
          ],
          "after": [
            0.0012,
            0.1022,
            0.0013,
            -0.0749,
            0.0411,
            -0.0095,
            0.0814,
            0.1225,
            -0.0599,
            -0.0281,
            0.0517,
            0.004,
            0.0852,
            0.033,
            -0.0676,
            0.1853
          ]
        },
        "position_0": {
          "grad": [
            0.0989,
            -0.0862,
            0.005,
            -0.2343,
            0.0671,
            0.1006,
            -0.0897,
            -0.1223,
            0.2158,
            0.0164,
            -0.0529,
            -0.0196,
            -0.2441,
            -0.0309,
            0.1424,
            0.0202
          ],
          "after": [
            -0.0088,
            -0.0836,
            -0.241,
            0.092,
            0.0166,
            -0.0235,
            -0.1204,
            0.0897,
            0.1,
            -0.0399,
            0.0084,
            0.1242,
            0.041,
            -0.0006,
            -0.1075,
            0.0587
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0,
            0.0,
            0.0,
            -0.0001,
            0.0001,
            -0.0001,
            0.0,
            0.0,
            0.0001,
            -0.0,
            0.0,
            0.0,
            0.0001,
            0.0,
            0.0001
          ],
          "after": [
            -0.0059,
            0.0126,
            -0.0481,
            0.0052,
            -0.0126,
            0.1294,
            0.0196,
            -0.1235,
            0.0349,
            0.0437,
            -0.0724,
            -0.0502,
            0.0998,
            -0.0445,
            0.0084,
            0.0339
          ]
        }
      }
    },
    {
      "step": 123,
      "word": "aizah",
      "loss": 2.4745,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3793,
            -0.11,
            0.3878,
            0.0606,
            0.1216,
            0.0682,
            0.1034,
            0.1593,
            0.0516,
            -0.0358,
            0.0288,
            -0.0266,
            0.1611,
            -0.0238,
            -0.176,
            0.1335
          ],
          "after": [
            -0.0259,
            0.0924,
            0.1427,
            0.0208,
            0.0055,
            -0.0509,
            0.0061,
            -0.1144,
            -0.0376,
            0.0252,
            0.0851,
            0.0121,
            -0.0371,
            -0.0119,
            -0.0348,
            -0.1061
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0594,
            0.1135,
            -0.0647,
            0.0582,
            0.1159,
            -0.155,
            0.2048,
            -0.0134,
            -0.1762,
            0.0423,
            0.0194,
            0.0109,
            0.0827,
            -0.0654,
            -0.2855,
            0.1052
          ],
          "after": [
            0.0018,
            0.1018,
            0.0013,
            -0.0748,
            0.0402,
            -0.0095,
            0.0812,
            0.1222,
            -0.0597,
            -0.0288,
            0.0511,
            0.0038,
            0.0852,
            0.0334,
            -0.0669,
            0.1852
          ]
        },
        "position_0": {
          "grad": [
            0.1614,
            0.046,
            -0.0165,
            -0.2346,
            0.0602,
            0.0458,
            -0.2558,
            -0.1581,
            0.2015,
            0.0204,
            0.2492,
            0.0378,
            -0.1479,
            0.0787,
            -0.0056,
            -0.1325
          ],
          "after": [
            -0.0095,
            -0.0839,
            -0.2413,
            0.0925,
            0.016,
            -0.0238,
            -0.1207,
            0.0901,
            0.0995,
            -0.0395,
            0.0078,
            0.1239,
            0.0418,
            -0.0001,
            -0.1076,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0002,
            0.0003,
            -0.0007,
            -0.0005,
            -0.0004,
            -0.0002,
            -0.0004,
            0.0002,
            0.0006,
            0.0004,
            -0.0004,
            -0.0001,
            -0.0001,
            0.0012,
            -0.0006
          ],
          "after": [
            -0.0059,
            0.0133,
            -0.0487,
            0.0058,
            -0.013,
            0.1297,
            0.0199,
            -0.1237,
            0.0347,
            0.0437,
            -0.0721,
            -0.0508,
            0.1003,
            -0.0443,
            0.0086,
            0.0338
          ]
        }
      }
    },
    {
      "step": 124,
      "word": "sloane",
      "loss": 2.4401,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0154,
            0.0347,
            -0.1168,
            0.0468,
            -0.1519,
            -0.1206,
            0.0299,
            -0.0408,
            0.045,
            -0.0321,
            0.0075,
            -0.0828,
            0.0042,
            -0.0025,
            0.0623,
            -0.0208
          ],
          "after": [
            -0.0242,
            0.0921,
            0.1415,
            0.0197,
            0.0064,
            -0.0504,
            0.0041,
            -0.115,
            -0.0383,
            0.025,
            0.0852,
            0.0119,
            -0.0384,
            -0.0116,
            -0.034,
            -0.1071
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2223,
            -0.0334,
            -0.0969,
            -0.0425,
            -0.5093,
            0.2107,
            -0.3609,
            -0.0348,
            0.0073,
            0.0331,
            -0.0146,
            -0.3908,
            0.1588,
            0.2558,
            0.2426,
            -0.2059
          ],
          "after": [
            0.0018,
            0.1017,
            0.0015,
            -0.0745,
            0.0404,
            -0.0098,
            0.0815,
            0.1221,
            -0.0596,
            -0.0294,
            0.0506,
            0.0046,
            0.0848,
            0.0331,
            -0.0667,
            0.1856
          ]
        },
        "position_0": {
          "grad": [
            0.0084,
            0.0857,
            0.0453,
            0.0953,
            -0.1985,
            0.0955,
            -0.0442,
            0.0991,
            -0.0219,
            0.0538,
            -0.1007,
            0.0155,
            0.1097,
            0.0658,
            0.1538,
            -0.077
          ],
          "after": [
            -0.0101,
            -0.0844,
            -0.2418,
            0.0926,
            0.0162,
            -0.0244,
            -0.1208,
            0.0902,
            0.0991,
            -0.0394,
            0.0076,
            0.1236,
            0.0422,
            0.0001,
            -0.1082,
            0.0582
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0001,
            -0.0,
            -0.0002,
            -0.0008,
            0.0003,
            -0.0016,
            -0.0003,
            0.0004,
            -0.0004,
            -0.0003,
            -0.0009,
            -0.0002,
            0.0006,
            0.0008,
            0.0003
          ],
          "after": [
            -0.006,
            0.0138,
            -0.0491,
            0.0065,
            -0.0129,
            0.1297,
            0.0209,
            -0.1236,
            0.034,
            0.044,
            -0.0714,
            -0.0506,
            0.1008,
            -0.0447,
            0.0083,
            0.0334
          ]
        }
      }
    },
    {
      "step": 125,
      "word": "future",
      "loss": 3.1983,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0227,
            0.092,
            0.1405,
            0.0188,
            0.0072,
            -0.05,
            0.0025,
            -0.1154,
            -0.0388,
            0.0248,
            0.0852,
            0.0117,
            -0.0395,
            -0.0113,
            -0.0333,
            -0.1079
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0337,
            -0.1121,
            -0.1509,
            -0.09,
            -0.2925,
            0.1944,
            -0.2835,
            0.1255,
            0.1413,
            0.0134,
            -0.0686,
            -0.1607,
            0.0548,
            0.1779,
            0.4386,
            -0.0251
          ],
          "after": [
            0.0016,
            0.1018,
            0.002,
            -0.0741,
            0.0411,
            -0.0105,
            0.0822,
            0.1215,
            -0.0598,
            -0.03,
            0.0506,
            0.0055,
            0.0843,
            0.0324,
            -0.0672,
            0.1859
          ]
        },
        "position_0": {
          "grad": [
            -0.1585,
            0.3698,
            0.0837,
            0.4051,
            0.2855,
            -0.2086,
            0.424,
            -0.0254,
            -0.5871,
            -0.0784,
            0.1368,
            0.3985,
            0.374,
            -0.1881,
            -0.2833,
            0.3424
          ],
          "after": [
            -0.01,
            -0.0859,
            -0.2425,
            0.0916,
            0.0154,
            -0.024,
            -0.1218,
            0.0903,
            0.1001,
            -0.0389,
            0.0069,
            0.1222,
            0.0416,
            0.0009,
            -0.1078,
            0.0572
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0004,
            0.0002,
            -0.0001,
            -0.0002,
            -0.0009,
            -0.0002,
            0.0004,
            0.0002,
            -0.0005,
            -0.0004,
            -0.0006,
            0.0009,
            0.0003,
            -0.0001,
            0.0005
          ],
          "after": [
            -0.0059,
            0.0139,
            -0.0497,
            0.0072,
            -0.0126,
            0.1305,
            0.0219,
            -0.1239,
            0.0332,
            0.0448,
            -0.0704,
            -0.0501,
            0.1005,
            -0.0453,
            0.0081,
            0.0326
          ]
        }
      }
    },
    {
      "step": 126,
      "word": "aiden",
      "loss": 2.135,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1959,
            -0.1487,
            0.1137,
            -0.0449,
            0.0227,
            -0.0055,
            -0.0543,
            0.0671,
            -0.0231,
            -0.0604,
            0.0627,
            -0.1162,
            -0.0869,
            -0.0143,
            0.0167,
            -0.1548
          ],
          "after": [
            -0.0209,
            0.0925,
            0.1393,
            0.0182,
            0.0078,
            -0.0496,
            0.0013,
            -0.1161,
            -0.0392,
            0.025,
            0.0849,
            0.0119,
            -0.0402,
            -0.011,
            -0.0327,
            -0.1083
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2783,
            -0.2186,
            0.1196,
            -0.2673,
            0.0118,
            0.053,
            -0.221,
            -0.0266,
            0.1988,
            -0.1199,
            0.0751,
            0.1073,
            -0.2102,
            -0.1353,
            0.3321,
            -0.0365
          ],
          "after": [
            0.0022,
            0.1023,
            0.0022,
            -0.0729,
            0.0417,
            -0.0111,
            0.0831,
            0.1211,
            -0.0604,
            -0.0301,
            0.0502,
            0.0061,
            0.0844,
            0.0321,
            -0.0681,
            0.1863
          ]
        },
        "position_0": {
          "grad": [
            0.1602,
            -0.0269,
            -0.0291,
            -0.2862,
            0.0001,
            0.0466,
            -0.3365,
            -0.1481,
            0.2737,
            0.0098,
            0.2222,
            -0.032,
            -0.1958,
            0.0983,
            0.0392,
            -0.1958
          ],
          "after": [
            -0.0105,
            -0.0871,
            -0.2431,
            0.0916,
            0.0148,
            -0.0239,
            -0.122,
            0.0909,
            0.1003,
            -0.0386,
            0.0055,
            0.1211,
            0.0415,
            0.0012,
            -0.1076,
            0.0569
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0005,
            0.0002,
            -0.0005,
            0.0002,
            -0.0002,
            0.0,
            0.0001,
            0.0002,
            -0.0004,
            -0.0,
            0.0007,
            -0.0004,
            -0.0003,
            0.0003,
            0.0001
          ],
          "after": [
            -0.0052,
            0.0143,
            -0.0504,
            0.0082,
            -0.0126,
            0.1313,
            0.0228,
            -0.1243,
            0.0323,
            0.0457,
            -0.0694,
            -0.05,
            0.1006,
            -0.0455,
            0.0077,
            0.0319
          ]
        }
      }
    },
    {
      "step": 127,
      "word": "jahniya",
      "loss": 2.5012,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0069,
            0.0227,
            0.1204,
            0.0416,
            -0.0448,
            0.0844,
            0.1064,
            0.0683,
            -0.0177,
            0.156,
            -0.051,
            -0.1206,
            0.2128,
            0.0587,
            -0.052,
            0.0684
          ],
          "after": [
            -0.0194,
            0.0928,
            0.1378,
            0.0175,
            0.0084,
            -0.0497,
            -0.0002,
            -0.117,
            -0.0394,
            0.0243,
            0.085,
            0.0125,
            -0.0414,
            -0.011,
            -0.0321,
            -0.1088
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1151,
            0.1029,
            -0.0328,
            0.0422,
            0.1266,
            -0.1829,
            0.2449,
            0.0039,
            -0.1536,
            0.0591,
            0.0097,
            0.0704,
            0.0652,
            -0.1043,
            -0.2402,
            0.1664
          ],
          "after": [
            0.003,
            0.1026,
            0.0024,
            -0.0719,
            0.042,
            -0.0113,
            0.0835,
            0.1207,
            -0.0605,
            -0.0304,
            0.0498,
            0.0065,
            0.0844,
            0.0322,
            -0.0685,
            0.1863
          ]
        },
        "position_0": {
          "grad": [
            -0.0897,
            0.1115,
            0.0769,
            0.2377,
            0.0077,
            -0.0649,
            0.2264,
            0.1041,
            -0.3087,
            -0.0579,
            -0.0008,
            0.1682,
            0.3225,
            -0.2014,
            -0.1369,
            0.124
          ],
          "after": [
            -0.0106,
            -0.0885,
            -0.2438,
            0.0909,
            0.0142,
            -0.0234,
            -0.1226,
            0.0911,
            0.1011,
            -0.0379,
            0.0043,
            0.1197,
            0.0407,
            0.0021,
            -0.1069,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0005,
            -0.0009,
            -0.0001,
            -0.0007,
            -0.0001,
            -0.0011,
            -0.0002,
            0.0003,
            -0.0005,
            -0.0002,
            -0.001,
            -0.0005,
            0.0009,
            0.001,
            -0.001
          ],
          "after": [
            -0.0055,
            0.0151,
            -0.05,
            0.0092,
            -0.0121,
            0.132,
            0.0241,
            -0.1244,
            0.0313,
            0.047,
            -0.0684,
            -0.0493,
            0.101,
            -0.0464,
            0.0068,
            0.0321
          ]
        }
      }
    },
    {
      "step": 128,
      "word": "karder",
      "loss": 2.3134,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1072,
            -0.032,
            -0.005,
            -0.192,
            0.1678,
            0.0203,
            0.0354,
            -0.1836,
            0.1736,
            -0.0668,
            0.0836,
            0.0224,
            -0.1109,
            -0.0877,
            0.0489,
            0.1162
          ],
          "after": [
            -0.0184,
            0.0932,
            0.1365,
            0.018,
            0.0083,
            -0.05,
            -0.0016,
            -0.1169,
            -0.0404,
            0.024,
            0.0846,
            0.0129,
            -0.0421,
            -0.0106,
            -0.0317,
            -0.1094
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1562,
            -0.0876,
            0.1576,
            -0.1192,
            -0.0791,
            0.021,
            -0.2056,
            -0.059,
            0.1473,
            -0.0984,
            -0.0296,
            0.1114,
            -0.1095,
            0.1033,
            0.0295,
            0.0721
          ],
          "after": [
            0.0041,
            0.103,
            0.0022,
            -0.0708,
            0.0424,
            -0.0116,
            0.0841,
            0.1207,
            -0.061,
            -0.0303,
            0.0496,
            0.0065,
            0.0846,
            0.032,
            -0.0689,
            0.1861
          ]
        },
        "position_0": {
          "grad": [
            -0.0498,
            0.058,
            -0.161,
            -0.0149,
            -0.02,
            -0.0356,
            0.1316,
            -0.0251,
            -0.0415,
            0.0301,
            -0.0222,
            -0.058,
            0.0294,
            -0.0147,
            -0.0827,
            -0.1147
          ],
          "after": [
            -0.0105,
            -0.0899,
            -0.2438,
            0.0903,
            0.0137,
            -0.0229,
            -0.1233,
            0.0914,
            0.102,
            -0.0376,
            0.0034,
            0.1187,
            0.0399,
            0.0029,
            -0.1062,
            0.0561
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0001,
            0.0,
            0.0002,
            0.0,
            0.0001,
            0.0001,
            0.0,
            0.0,
            0.0001,
            0.0,
            0.0001,
            0.0001,
            -0.0001,
            -0.0003,
            0.0001
          ],
          "after": [
            -0.0059,
            0.0157,
            -0.0497,
            0.0098,
            -0.0117,
            0.1326,
            0.0251,
            -0.1245,
            0.0303,
            0.048,
            -0.0675,
            -0.0488,
            0.1013,
            -0.0471,
            0.0062,
            0.0322
          ]
        }
      }
    },
    {
      "step": 129,
      "word": "emerlee",
      "loss": 2.3275,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0175,
            0.0936,
            0.1354,
            0.0184,
            0.0083,
            -0.0501,
            -0.0028,
            -0.1169,
            -0.0412,
            0.0238,
            0.0842,
            0.0132,
            -0.0427,
            -0.0103,
            -0.0314,
            -0.11
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3111,
            -0.2017,
            0.4952,
            -0.2083,
            -0.5168,
            0.9379,
            -0.4857,
            -0.034,
            0.7536,
            -0.1417,
            0.0057,
            -0.2489,
            -0.5476,
            0.5452,
            1.0536,
            -0.8753
          ],
          "after": [
            0.0042,
            0.1038,
            0.0008,
            -0.0692,
            0.0436,
            -0.0133,
            0.0853,
            0.1208,
            -0.0627,
            -0.0298,
            0.0494,
            0.0071,
            0.086,
            0.0305,
            -0.0706,
            0.1876
          ]
        },
        "position_0": {
          "grad": [
            0.0476,
            -0.1668,
            -0.0428,
            -0.089,
            -0.0807,
            0.0743,
            -0.2278,
            -0.1074,
            0.1931,
            0.0835,
            -0.0423,
            -0.1312,
            -0.1341,
            0.0076,
            0.0888,
            -0.2499
          ],
          "after": [
            -0.0106,
            -0.0905,
            -0.2436,
            0.0901,
            0.0136,
            -0.0228,
            -0.1235,
            0.092,
            0.1022,
            -0.0377,
            0.0028,
            0.1182,
            0.0396,
            0.0036,
            -0.1058,
            0.0566
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0,
            -0.0002,
            0.0,
            -0.0004,
            0.0002,
            0.0,
            0.0001,
            0.0,
            0.0002,
            -0.0001,
            -0.0002,
            -0.0001,
            0.0001,
            0.0003,
            -0.0003
          ],
          "after": [
            -0.0062,
            0.0162,
            -0.0493,
            0.0104,
            -0.0112,
            0.1329,
            0.026,
            -0.1247,
            0.0295,
            0.0487,
            -0.0667,
            -0.0483,
            0.1016,
            -0.0478,
            0.0055,
            0.0326
          ]
        }
      }
    },
    {
      "step": 130,
      "word": "kwabena",
      "loss": 2.8987,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.237,
            0.0472,
            0.4253,
            -0.1466,
            0.1598,
            -0.0795,
            0.1504,
            -0.0377,
            -0.1974,
            -0.1534,
            0.0298,
            0.234,
            0.1593,
            -0.3382,
            -0.074,
            0.3002
          ],
          "after": [
            -0.0161,
            0.0937,
            0.1332,
            0.0196,
            0.0077,
            -0.0499,
            -0.0043,
            -0.1167,
            -0.041,
            0.0245,
            0.0837,
            0.0128,
            -0.0438,
            -0.0087,
            -0.0308,
            -0.1113
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0284,
            -0.1027,
            -0.0447,
            -0.0308,
            -0.0437,
            0.1487,
            -0.1051,
            0.0404,
            0.1581,
            -0.1495,
            -0.0721,
            -0.1682,
            -0.1542,
            0.038,
            0.0297,
            -0.1123
          ],
          "after": [
            0.0042,
            0.1048,
            -0.0003,
            -0.0678,
            0.0448,
            -0.015,
            0.0865,
            0.1207,
            -0.0644,
            -0.0289,
            0.0496,
            0.008,
            0.0876,
            0.0292,
            -0.0721,
            0.189
          ]
        },
        "position_0": {
          "grad": [
            -0.0724,
            0.1192,
            -0.1167,
            0.0727,
            0.1064,
            -0.0223,
            0.2285,
            -0.0283,
            -0.1344,
            0.0294,
            0.0385,
            0.0582,
            0.1007,
            -0.06,
            -0.1473,
            -0.0007
          ],
          "after": [
            -0.0104,
            -0.0914,
            -0.2428,
            0.0897,
            0.0131,
            -0.0226,
            -0.1241,
            0.0926,
            0.1028,
            -0.0381,
            0.0021,
            0.1176,
            0.0391,
            0.0043,
            -0.105,
            0.0571
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0009,
            0.0005,
            0.0006,
            -0.0002,
            -0.0006,
            0.0013,
            0.0002,
            -0.0008,
            0.0002,
            -0.0002,
            -0.0003,
            0.0005,
            -0.0001,
            -0.0001,
            0.0004
          ],
          "after": [
            -0.0063,
            0.0159,
            -0.0494,
            0.0103,
            -0.0106,
            0.1336,
            0.0261,
            -0.1251,
            0.0297,
            0.0491,
            -0.0658,
            -0.0476,
            0.1015,
            -0.0483,
            0.005,
            0.0325
          ]
        }
      }
    },
    {
      "step": 131,
      "word": "harfateh",
      "loss": 2.7463,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1035,
            -0.1108,
            0.1333,
            -0.0708,
            0.2497,
            0.091,
            0.2319,
            -0.1204,
            -0.014,
            -0.1114,
            -0.0306,
            0.0155,
            0.0224,
            -0.1905,
            -0.0597,
            0.3542
          ],
          "after": [
            -0.0147,
            0.0942,
            0.1309,
            0.0209,
            0.0063,
            -0.0501,
            -0.0066,
            -0.1161,
            -0.0408,
            0.0256,
            0.0835,
            0.0124,
            -0.0448,
            -0.0067,
            -0.03,
            -0.1132
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1648,
            -0.1876,
            -0.1216,
            -0.0321,
            -0.1895,
            0.0532,
            -0.1464,
            0.0275,
            -0.0034,
            0.0388,
            -0.0765,
            -0.0854,
            -0.0532,
            0.0786,
            0.157,
            -0.0742
          ],
          "after": [
            0.0038,
            0.106,
            -0.0009,
            -0.0664,
            0.0462,
            -0.0165,
            0.0877,
            0.1205,
            -0.0658,
            -0.0282,
            0.05,
            0.009,
            0.0891,
            0.0279,
            -0.0735,
            0.1903
          ]
        },
        "position_0": {
          "grad": [
            -0.0257,
            -0.0074,
            0.0385,
            0.0202,
            -0.0397,
            0.063,
            0.061,
            0.0591,
            0.0371,
            0.0548,
            -0.0718,
            -0.0681,
            0.1373,
            0.0404,
            0.0342,
            0.084
          ],
          "after": [
            -0.0101,
            -0.0922,
            -0.2424,
            0.0893,
            0.0129,
            -0.0228,
            -0.1248,
            0.0929,
            0.1031,
            -0.0387,
            0.0018,
            0.1173,
            0.0383,
            0.0049,
            -0.1044,
            0.0572
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0005,
            0.0002,
            0.0005,
            -0.0001,
            0.0003,
            0.0003,
            -0.0004,
            0.0,
            0.0011,
            0.0003,
            0.0004,
            0.0002,
            0.0005,
            -0.0014,
            0.0001
          ],
          "after": [
            -0.0073,
            0.0153,
            -0.0498,
            0.0099,
            -0.0101,
            0.134,
            0.026,
            -0.1251,
            0.0299,
            0.0485,
            -0.0653,
            -0.0472,
            0.1012,
            -0.0491,
            0.0054,
            0.0324
          ]
        }
      }
    },
    {
      "step": 132,
      "word": "camren",
      "loss": 2.0468,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0259,
            0.0654,
            0.1225,
            0.1255,
            0.0201,
            -0.049,
            0.2438,
            0.1603,
            0.0678,
            -0.0358,
            0.1218,
            0.004,
            -0.1231,
            -0.094,
            0.0091,
            -0.0238
          ],
          "after": [
            -0.0134,
            0.0944,
            0.1285,
            0.0214,
            0.005,
            -0.0501,
            -0.0093,
            -0.1162,
            -0.0409,
            0.0268,
            0.0827,
            0.012,
            -0.0452,
            -0.0046,
            -0.0295,
            -0.1147
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0013,
            -0.13,
            0.0342,
            0.0334,
            0.0639,
            0.1657,
            -0.2878,
            0.0382,
            0.081,
            -0.2158,
            -0.1389,
            0.0323,
            -0.0746,
            0.1894,
            0.1173,
            0.0371
          ],
          "after": [
            0.0035,
            0.1074,
            -0.0015,
            -0.0654,
            0.0472,
            -0.0181,
            0.0891,
            0.1201,
            -0.0672,
            -0.0269,
            0.051,
            0.0097,
            0.0905,
            0.0264,
            -0.075,
            0.1914
          ]
        },
        "position_0": {
          "grad": [
            -0.1025,
            0.0529,
            -0.0344,
            0.0364,
            -0.0298,
            -0.0428,
            0.1466,
            0.0312,
            -0.0736,
            0.0836,
            0.0766,
            0.1463,
            0.0265,
            -0.0771,
            -0.1121,
            0.0271
          ],
          "after": [
            -0.0095,
            -0.093,
            -0.2419,
            0.0888,
            0.0128,
            -0.0227,
            -0.1256,
            0.0931,
            0.1036,
            -0.0397,
            0.0012,
            0.1166,
            0.0375,
            0.0056,
            -0.1036,
            0.0573
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0001,
            -0.0002,
            0.0002,
            0.0,
            0.0002,
            -0.0001,
            -0.0001,
            -0.0003,
            0.0,
            0.0001,
            -0.0006,
            0.0002,
            -0.0,
            -0.0001,
            -0.0003
          ],
          "after": [
            -0.0085,
            0.0147,
            -0.0499,
            0.0093,
            -0.0096,
            0.1342,
            0.026,
            -0.1251,
            0.0303,
            0.0479,
            -0.065,
            -0.0465,
            0.1009,
            -0.0498,
            0.0059,
            0.0326
          ]
        }
      }
    },
    {
      "step": 133,
      "word": "enrico",
      "loss": 2.8571,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0123,
            0.0946,
            0.1265,
            0.0217,
            0.004,
            -0.05,
            -0.0116,
            -0.1163,
            -0.041,
            0.0278,
            0.0819,
            0.0117,
            -0.0455,
            -0.0028,
            -0.029,
            -0.1161
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.179,
            0.3071,
            0.5589,
            -0.0837,
            0.3258,
            -0.2194,
            0.409,
            -0.0808,
            -0.0203,
            -0.0505,
            0.1377,
            -0.039,
            -0.0855,
            -0.1106,
            -0.3778,
            -0.0587
          ],
          "after": [
            0.0037,
            0.1078,
            -0.0033,
            -0.0642,
            0.0475,
            -0.019,
            0.0897,
            0.1202,
            -0.0684,
            -0.0257,
            0.0513,
            0.0104,
            0.0919,
            0.0254,
            -0.0757,
            0.1924
          ]
        },
        "position_0": {
          "grad": [
            0.0549,
            -0.2321,
            -0.0231,
            -0.1028,
            -0.1434,
            0.0811,
            -0.2808,
            -0.1132,
            0.2422,
            0.0637,
            -0.0563,
            -0.1879,
            -0.1787,
            0.0509,
            0.157,
            -0.3071
          ],
          "after": [
            -0.0092,
            -0.093,
            -0.2413,
            0.0888,
            0.0132,
            -0.023,
            -0.1258,
            0.0937,
            0.1035,
            -0.0409,
            0.0009,
            0.1166,
            0.0374,
            0.006,
            -0.1034,
            0.0581
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0005,
            0.002,
            0.0009,
            -0.0028,
            -0.0008,
            -0.0015,
            0.0018,
            0.0002,
            -0.0005,
            -0.0001,
            0.0019,
            0.0006,
            -0.0001,
            -0.0011,
            0.0
          ],
          "after": [
            -0.0086,
            0.0146,
            -0.0518,
            0.008,
            -0.0078,
            0.135,
            0.0268,
            -0.1264,
            0.0305,
            0.0479,
            -0.0646,
            -0.0473,
            0.1,
            -0.0502,
            0.0069,
            0.0327
          ]
        }
      }
    },
    {
      "step": 134,
      "word": "mikiyas",
      "loss": 2.6382,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1133,
            -0.0516,
            -0.0053,
            0.135,
            -0.0193,
            0.1092,
            -0.0213,
            0.1435,
            0.0156,
            -0.0368,
            -0.1199,
            0.1985,
            -0.0344,
            -0.0725,
            0.0864,
            0.1072
          ],
          "after": [
            -0.011,
            0.095,
            0.1248,
            0.0214,
            0.0031,
            -0.0506,
            -0.0136,
            -0.1169,
            -0.0412,
            0.0288,
            0.082,
            0.0109,
            -0.0457,
            -0.0009,
            -0.0289,
            -0.1175
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1048,
            0.1157,
            -0.0205,
            0.0295,
            0.17,
            -0.2117,
            0.2592,
            0.0141,
            -0.2228,
            0.1168,
            0.0028,
            0.0579,
            0.0732,
            -0.1101,
            -0.2756,
            0.1778
          ],
          "after": [
            0.0041,
            0.1079,
            -0.0047,
            -0.0634,
            0.0474,
            -0.0195,
            0.0899,
            0.1202,
            -0.069,
            -0.025,
            0.0515,
            0.0109,
            0.093,
            0.0247,
            -0.0759,
            0.193
          ]
        },
        "position_0": {
          "grad": [
            -0.0359,
            0.0516,
            -0.009,
            0.041,
            0.0906,
            0.031,
            0.1321,
            0.0973,
            0.0576,
            0.0334,
            0.0457,
            0.0875,
            -0.1376,
            -0.0761,
            0.0117,
            0.0311
          ],
          "after": [
            -0.0088,
            -0.0931,
            -0.2408,
            0.0886,
            0.0132,
            -0.0234,
            -0.1262,
            0.0938,
            0.1033,
            -0.0421,
            0.0005,
            0.1163,
            0.0375,
            0.0066,
            -0.1032,
            0.0587
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0003,
            -0.0014,
            -0.0008,
            -0.0027,
            0.0006,
            -0.002,
            -0.0002,
            0.0023,
            -0.001,
            -0.0005,
            -0.0011,
            -0.0009,
            0.0021,
            0.0016,
            -0.0004
          ],
          "after": [
            -0.0091,
            0.0148,
            -0.0521,
            0.0077,
            -0.005,
            0.1352,
            0.0284,
            -0.1274,
            0.0286,
            0.0487,
            -0.0636,
            -0.0472,
            0.1001,
            -0.0521,
            0.0068,
            0.0331
          ]
        }
      }
    },
    {
      "step": 135,
      "word": "treasure",
      "loss": 2.804,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.054,
            0.168,
            0.0934,
            0.0788,
            -0.0535,
            0.0748,
            0.014,
            0.0779,
            -0.0549,
            -0.0009,
            -0.1156,
            0.144,
            0.1085,
            -0.0234,
            -0.0338,
            0.0509
          ],
          "after": [
            -0.0098,
            0.0945,
            0.1231,
            0.0206,
            0.0026,
            -0.0515,
            -0.0152,
            -0.1178,
            -0.041,
            0.0297,
            0.0826,
            0.0098,
            -0.0463,
            0.0007,
            -0.0287,
            -0.1188
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1728,
            -0.13,
            -0.0735,
            -0.073,
            -0.0097,
            0.5437,
            -0.4427,
            0.1737,
            0.2615,
            -0.0375,
            -0.014,
            -0.0627,
            -0.2471,
            0.1468,
            0.4663,
            -0.2113
          ],
          "after": [
            0.004,
            0.1083,
            -0.0058,
            -0.0624,
            0.0473,
            -0.0208,
            0.0907,
            0.1195,
            -0.0699,
            -0.0243,
            0.0517,
            0.0115,
            0.0944,
            0.0238,
            -0.0767,
            0.1938
          ]
        },
        "position_0": {
          "grad": [
            0.0199,
            -0.1688,
            0.0452,
            0.0408,
            -0.1382,
            0.0641,
            0.0472,
            0.0315,
            -0.0555,
            -0.0928,
            -0.1742,
            -0.2847,
            0.0696,
            0.0677,
            0.1024,
            0.0494
          ],
          "after": [
            -0.0086,
            -0.0927,
            -0.2405,
            0.0883,
            0.0137,
            -0.024,
            -0.1266,
            0.0937,
            0.1032,
            -0.0426,
            0.0009,
            0.1169,
            0.0375,
            0.0069,
            -0.1034,
            0.0592
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0003,
            -0.0002,
            -0.0001,
            0.0002,
            -0.0,
            0.0001,
            0.0002,
            -0.0001,
            -0.0,
            -0.0001,
            -0.0004,
            0.0001,
            0.0,
            -0.0004,
            -0.0002
          ],
          "after": [
            -0.0098,
            0.0152,
            -0.0522,
            0.0074,
            -0.0028,
            0.1354,
            0.0297,
            -0.1283,
            0.0271,
            0.0494,
            -0.0626,
            -0.0468,
            0.1,
            -0.0538,
            0.007,
            0.0337
          ]
        }
      }
    },
    {
      "step": 136,
      "word": "fleur",
      "loss": 3.1623,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0087,
            0.0942,
            0.1216,
            0.02,
            0.0021,
            -0.0522,
            -0.0167,
            -0.1186,
            -0.0409,
            0.0305,
            0.0831,
            0.0088,
            -0.0467,
            0.0021,
            -0.0285,
            -0.1199
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0199,
            -0.0258,
            0.1292,
            0.083,
            0.3275,
            0.2642,
            -0.122,
            0.173,
            0.1648,
            0.1317,
            0.0258,
            0.2986,
            -0.3092,
            -0.072,
            -0.0942,
            -0.1641
          ],
          "after": [
            0.004,
            0.1087,
            -0.007,
            -0.0618,
            0.0467,
            -0.0223,
            0.0915,
            0.1181,
            -0.0711,
            -0.0242,
            0.0518,
            0.0113,
            0.0964,
            0.0232,
            -0.0772,
            0.1949
          ]
        },
        "position_0": {
          "grad": [
            -0.1425,
            0.314,
            0.029,
            0.3881,
            0.2902,
            -0.2236,
            0.2866,
            -0.0157,
            -0.5095,
            -0.0847,
            0.1209,
            0.391,
            0.3508,
            -0.129,
            -0.2689,
            0.292
          ],
          "after": [
            -0.0078,
            -0.0933,
            -0.2405,
            0.087,
            0.0132,
            -0.0235,
            -0.1275,
            0.0938,
            0.1042,
            -0.0426,
            0.0007,
            0.1163,
            0.0367,
            0.0075,
            -0.1027,
            0.0587
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            0.0016,
            0.0016,
            0.0014,
            0.0016,
            -0.0005,
            0.0028,
            -0.0012,
            -0.0012,
            -0.0004,
            -0.0001,
            0.0009,
            0.0015,
            -0.0017,
            -0.0043,
            -0.0009
          ],
          "after": [
            -0.0113,
            0.0143,
            -0.0536,
            0.0061,
            -0.0017,
            0.136,
            0.0294,
            -0.1282,
            0.0268,
            0.0503,
            -0.0617,
            -0.047,
            0.0989,
            -0.0539,
            0.0092,
            0.035
          ]
        }
      }
    },
    {
      "step": 137,
      "word": "jinan",
      "loss": 2.1799,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.06,
            0.0683,
            -0.2346,
            0.1157,
            -0.2757,
            -0.1665,
            0.0537,
            -0.0418,
            -0.0121,
            -0.0095,
            0.0061,
            -0.1764,
            0.0717,
            0.0726,
            0.1623,
            -0.0477
          ],
          "after": [
            -0.008,
            0.0935,
            0.1211,
            0.0189,
            0.0027,
            -0.0519,
            -0.0181,
            -0.119,
            -0.0408,
            0.0312,
            0.0835,
            0.0085,
            -0.0474,
            0.003,
            -0.029,
            -0.1208
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.117,
            0.1082,
            -0.1034,
            0.033,
            0.1,
            -0.2329,
            0.3486,
            0.0614,
            -0.2086,
            0.1278,
            -0.0109,
            0.0772,
            0.1298,
            -0.1228,
            -0.2404,
            0.1725
          ],
          "after": [
            0.0042,
            0.1087,
            -0.0078,
            -0.0614,
            0.0459,
            -0.0232,
            0.0917,
            0.1167,
            -0.0716,
            -0.0245,
            0.0519,
            0.0109,
            0.0977,
            0.023,
            -0.0774,
            0.1954
          ]
        },
        "position_0": {
          "grad": [
            -0.0897,
            0.0683,
            0.0815,
            0.2783,
            -0.0119,
            -0.0776,
            0.1925,
            0.1404,
            -0.3267,
            -0.084,
            -0.014,
            0.1572,
            0.4326,
            -0.1998,
            -0.1533,
            0.1152
          ],
          "after": [
            -0.0068,
            -0.094,
            -0.2408,
            0.0852,
            0.0127,
            -0.0228,
            -0.1287,
            0.0932,
            0.1057,
            -0.042,
            0.0006,
            0.1154,
            0.0349,
            0.0087,
            -0.1017,
            0.0581
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0002,
            0.0002,
            -0.0002,
            0.0005,
            -0.0002,
            0.0,
            0.0002,
            -0.0002,
            -0.0009,
            -0.0002,
            0.0002,
            -0.0001,
            -0.0006,
            0.0003,
            -0.0002
          ],
          "after": [
            -0.0123,
            0.0137,
            -0.055,
            0.0052,
            -0.001,
            0.1366,
            0.0292,
            -0.1283,
            0.0268,
            0.0517,
            -0.0606,
            -0.0474,
            0.098,
            -0.0536,
            0.0109,
            0.0363
          ]
        }
      }
    },
    {
      "step": 138,
      "word": "sukayna",
      "loss": 2.4135,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0482,
            0.0739,
            0.0043,
            -0.0474,
            0.0286,
            0.0415,
            0.0867,
            -0.0352,
            -0.1671,
            0.1119,
            0.0026,
            0.1462,
            0.0737,
            0.0461,
            0.0144,
            0.0771
          ],
          "after": [
            -0.0075,
            0.0927,
            0.1206,
            0.0181,
            0.0031,
            -0.0519,
            -0.0197,
            -0.1193,
            -0.0399,
            0.0312,
            0.0839,
            0.0078,
            -0.0482,
            0.0036,
            -0.0295,
            -0.1217
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1107,
            0.1233,
            -0.0253,
            0.0429,
            0.1516,
            -0.2937,
            0.3042,
            0.0551,
            -0.2584,
            0.1453,
            0.0146,
            0.1104,
            0.0808,
            -0.153,
            -0.2631,
            0.2
          ],
          "after": [
            0.0048,
            0.1085,
            -0.0085,
            -0.0612,
            0.045,
            -0.0235,
            0.0915,
            0.1152,
            -0.0716,
            -0.0252,
            0.052,
            0.0103,
            0.0987,
            0.0232,
            -0.0772,
            0.1955
          ]
        },
        "position_0": {
          "grad": [
            0.0114,
            0.1085,
            0.0417,
            0.1033,
            -0.1216,
            0.085,
            0.0002,
            0.0649,
            -0.0477,
            0.049,
            -0.0645,
            0.0405,
            0.1199,
            0.0479,
            0.1153,
            -0.0291
          ],
          "after": [
            -0.006,
            -0.0949,
            -0.2413,
            0.0834,
            0.0128,
            -0.0225,
            -0.1297,
            0.0925,
            0.1071,
            -0.0418,
            0.0008,
            0.1145,
            0.0332,
            0.0096,
            -0.1012,
            0.0576
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0002,
            -0.0003,
            -0.0001,
            -0.0001,
            0.0003,
            -0.0011,
            -0.0007,
            -0.0001,
            -0.0002,
            0.0003,
            -0.0008,
            -0.0002,
            0.0005,
            0.0003,
            -0.001
          ],
          "after": [
            -0.0137,
            0.0133,
            -0.0559,
            0.0044,
            -0.0004,
            0.1369,
            0.0295,
            -0.1278,
            0.0269,
            0.0531,
            -0.0599,
            -0.0471,
            0.0975,
            -0.0536,
            0.0123,
            0.0382
          ]
        }
      }
    },
    {
      "step": 139,
      "word": "brennox",
      "loss": 3.08,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0071,
            0.0919,
            0.1203,
            0.0175,
            0.0034,
            -0.0519,
            -0.021,
            -0.1195,
            -0.0391,
            0.0312,
            0.0842,
            0.0073,
            -0.0489,
            0.0041,
            -0.03,
            -0.1225
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2344,
            0.0235,
            0.1124,
            0.129,
            0.345,
            0.0917,
            0.0617,
            0.1114,
            0.0045,
            0.0303,
            0.1005,
            0.4117,
            -0.3017,
            -0.2022,
            0.0166,
            0.1668
          ],
          "after": [
            0.0058,
            0.1083,
            -0.0093,
            -0.0614,
            0.0436,
            -0.0239,
            0.0912,
            0.1135,
            -0.0717,
            -0.0258,
            0.0516,
            0.0089,
            0.1001,
            0.0238,
            -0.077,
            0.1953
          ]
        },
        "position_0": {
          "grad": [
            0.0037,
            -0.0256,
            0.1189,
            -0.0033,
            -0.0223,
            -0.0134,
            0.0354,
            0.0185,
            -0.1027,
            -0.0866,
            -0.0345,
            0.0445,
            0.1133,
            -0.0567,
            0.0119,
            0.2036
          ],
          "after": [
            -0.0053,
            -0.0956,
            -0.2422,
            0.0818,
            0.0129,
            -0.0222,
            -0.1307,
            0.0918,
            0.1085,
            -0.0412,
            0.001,
            0.1136,
            0.0315,
            0.0105,
            -0.1008,
            0.0567
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0003,
            0.0001,
            -0.0001,
            0.0004,
            0.0005,
            -0.0008,
            -0.0,
            -0.0,
            0.0,
            0.0001,
            0.0005,
            -0.0009,
            -0.0002,
            0.0006,
            -0.0
          ],
          "after": [
            -0.0144,
            0.0133,
            -0.0568,
            0.0038,
            0.0,
            0.1367,
            0.0301,
            -0.1274,
            0.0271,
            0.0543,
            -0.0596,
            -0.0472,
            0.0976,
            -0.0535,
            0.0132,
            0.0398
          ]
        }
      }
    },
    {
      "step": 140,
      "word": "brant",
      "loss": 2.6703,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1179,
            0.0188,
            -0.2263,
            0.1549,
            -0.3586,
            -0.0819,
            -0.0251,
            -0.0634,
            0.0488,
            -0.08,
            -0.1056,
            -0.2572,
            0.118,
            0.2687,
            0.0975,
            -0.0033
          ],
          "after": [
            -0.0071,
            0.0912,
            0.1207,
            0.0162,
            0.0049,
            -0.0515,
            -0.0221,
            -0.1194,
            -0.0387,
            0.0316,
            0.085,
            0.0075,
            -0.0499,
            0.0035,
            -0.0308,
            -0.1231
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0339,
            0.1372,
            -0.0948,
            0.0664,
            -0.0068,
            -0.2218,
            0.2736,
            0.0273,
            -0.2679,
            0.1176,
            -0.0144,
            -0.0413,
            0.2379,
            -0.0644,
            -0.2364,
            0.1147
          ],
          "after": [
            0.0068,
            0.1078,
            -0.0097,
            -0.0619,
            0.0424,
            -0.0239,
            0.0906,
            0.112,
            -0.0712,
            -0.0268,
            0.0513,
            0.0078,
            0.1009,
            0.0245,
            -0.0766,
            0.195
          ]
        },
        "position_0": {
          "grad": [
            0.0503,
            -0.0745,
            0.1818,
            0.0127,
            -0.065,
            -0.027,
            -0.0401,
            0.0232,
            -0.0816,
            -0.111,
            -0.0463,
            0.0009,
            0.1269,
            -0.049,
            0.032,
            0.2364
          ],
          "after": [
            -0.0049,
            -0.096,
            -0.2438,
            0.0805,
            0.0132,
            -0.0219,
            -0.1314,
            0.0912,
            0.1099,
            -0.04,
            0.0015,
            0.1128,
            0.0297,
            0.0114,
            -0.1005,
            0.0553
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0013,
            -0.0,
            0.001,
            0.0028,
            0.0006,
            0.0024,
            0.0003,
            -0.0005,
            -0.0005,
            0.0005,
            0.0004,
            0.0012,
            -0.0017,
            -0.0008,
            -0.0003
          ],
          "after": [
            -0.0144,
            0.0123,
            -0.0575,
            0.0026,
            -0.0009,
            0.1361,
            0.0296,
            -0.1272,
            0.0275,
            0.0557,
            -0.0599,
            -0.0476,
            0.0969,
            -0.0523,
            0.0143,
            0.0415
          ]
        }
      }
    },
    {
      "step": 141,
      "word": "osbaldo",
      "loss": 3.1586,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0315,
            0.1362,
            -0.1238,
            -0.0397,
            0.0665,
            -0.025,
            -0.1191,
            0.1422,
            -0.0718,
            0.0068,
            -0.0967,
            0.1317,
            -0.0358,
            0.0435,
            0.0635,
            -0.1585
          ],
          "after": [
            -0.007,
            0.09,
            0.1214,
            0.0153,
            0.0059,
            -0.051,
            -0.0225,
            -0.12,
            -0.038,
            0.0319,
            0.0862,
            0.0073,
            -0.0507,
            0.0028,
            -0.0317,
            -0.1233
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0648,
            0.166,
            -0.0187,
            0.0986,
            0.1278,
            -0.2841,
            0.3166,
            0.0269,
            -0.316,
            0.0758,
            0.0393,
            0.092,
            0.151,
            -0.1423,
            -0.3467,
            0.1868
          ],
          "after": [
            0.0078,
            0.1069,
            -0.0101,
            -0.0625,
            0.0412,
            -0.0234,
            0.0896,
            0.1105,
            -0.0702,
            -0.0279,
            0.0508,
            0.0067,
            0.1011,
            0.0254,
            -0.0757,
            0.1943
          ]
        },
        "position_0": {
          "grad": [
            0.0617,
            -0.0743,
            0.0345,
            -0.0301,
            0.0153,
            -0.0488,
            -0.1122,
            -0.0196,
            -0.0327,
            0.0965,
            -0.0105,
            0.1018,
            -0.0428,
            -0.0186,
            0.0304,
            -0.0924
          ],
          "after": [
            -0.0049,
            -0.0961,
            -0.2453,
            0.0794,
            0.0134,
            -0.0214,
            -0.1318,
            0.0907,
            0.1111,
            -0.0396,
            0.0019,
            0.1119,
            0.0282,
            0.0123,
            -0.1004,
            0.0543
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0007,
            0.0002,
            0.0004,
            0.0006,
            -0.0001,
            0.0009,
            0.0,
            -0.0003,
            -0.0005,
            0.0002,
            0.0003,
            0.0003,
            -0.0009,
            -0.0003,
            0.0003
          ],
          "after": [
            -0.0144,
            0.0109,
            -0.0582,
            0.0011,
            -0.002,
            0.1356,
            0.0287,
            -0.1272,
            0.0282,
            0.0574,
            -0.0603,
            -0.0481,
            0.0961,
            -0.0507,
            0.0155,
            0.0426
          ]
        }
      }
    },
    {
      "step": 142,
      "word": "kaspian",
      "loss": 2.5504,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1204,
            0.1996,
            0.0431,
            0.1947,
            -0.1954,
            0.047,
            0.0541,
            0.0213,
            -0.0531,
            -0.0384,
            -0.1336,
            0.2351,
            0.0852,
            0.005,
            0.0592,
            0.0766
          ],
          "after": [
            -0.0065,
            0.0881,
            0.1219,
            0.0136,
            0.0074,
            -0.0508,
            -0.0231,
            -0.1205,
            -0.0372,
            0.0324,
            0.0879,
            0.0065,
            -0.0517,
            0.0022,
            -0.0327,
            -0.1236
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0724,
            0.0802,
            -0.0364,
            0.0056,
            0.0969,
            -0.2105,
            0.2265,
            0.0267,
            -0.1818,
            0.0819,
            0.0072,
            0.0634,
            0.0399,
            -0.0982,
            -0.2036,
            0.1142
          ],
          "after": [
            0.0088,
            0.1061,
            -0.0103,
            -0.0631,
            0.0399,
            -0.0227,
            0.0885,
            0.1092,
            -0.0691,
            -0.0291,
            0.0505,
            0.0056,
            0.1013,
            0.0264,
            -0.0747,
            0.1935
          ]
        },
        "position_0": {
          "grad": [
            -0.0538,
            0.0262,
            -0.1136,
            0.0032,
            -0.0099,
            -0.0179,
            0.0788,
            -0.0125,
            -0.0023,
            0.0089,
            -0.0031,
            -0.0361,
            -0.0049,
            0.0162,
            -0.0472,
            -0.1
          ],
          "after": [
            -0.0046,
            -0.0963,
            -0.246,
            0.0784,
            0.0137,
            -0.0208,
            -0.1322,
            0.0903,
            0.1122,
            -0.0392,
            0.0022,
            0.1112,
            0.027,
            0.013,
            -0.1002,
            0.0538
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0016,
            -0.0008,
            0.0004,
            -0.0005,
            -0.0002,
            0.0001,
            0.0005,
            0.0004,
            0.0007,
            0.0001,
            0.0007,
            -0.0003,
            0.0007,
            -0.0002,
            0.0009
          ],
          "after": [
            -0.0148,
            0.0109,
            -0.0582,
            -0.0004,
            -0.0027,
            0.1354,
            0.0279,
            -0.1275,
            0.0285,
            0.0581,
            -0.0608,
            -0.049,
            0.0957,
            -0.0498,
            0.0165,
            0.0428
          ]
        }
      }
    },
    {
      "step": 143,
      "word": "yehudit",
      "loss": 3.1644,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0062,
            0.0865,
            0.1223,
            0.0122,
            0.0087,
            -0.0507,
            -0.0235,
            -0.1209,
            -0.0364,
            0.0328,
            0.0893,
            0.0059,
            -0.0525,
            0.0017,
            -0.0336,
            -0.1239
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0749,
            0.0705,
            0.077,
            -0.0787,
            0.4538,
            -0.0331,
            -0.1233,
            -0.076,
            0.2218,
            -0.1595,
            0.1049,
            0.1455,
            -0.2057,
            0.0438,
            -0.3078,
            0.0129
          ],
          "after": [
            0.0095,
            0.1051,
            -0.0106,
            -0.0634,
            0.0381,
            -0.022,
            0.0877,
            0.1084,
            -0.0685,
            -0.0296,
            0.0497,
            0.0043,
            0.1018,
            0.0272,
            -0.0735,
            0.1928
          ]
        },
        "position_0": {
          "grad": [
            0.0438,
            -0.0977,
            0.1013,
            -0.0773,
            -0.1004,
            0.1088,
            -0.092,
            0.0257,
            0.0742,
            -0.0023,
            -0.139,
            -0.0388,
            -0.0583,
            0.1473,
            0.1526,
            0.0015
          ],
          "after": [
            -0.0045,
            -0.0962,
            -0.2471,
            0.0779,
            0.0142,
            -0.0209,
            -0.1325,
            0.0899,
            0.1129,
            -0.0389,
            0.0031,
            0.1107,
            0.0261,
            0.0131,
            -0.1004,
            0.0533
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0018,
            0.0013,
            0.0013,
            0.0019,
            0.0002,
            0.002,
            -0.0012,
            0.0001,
            -0.0009,
            0.0003,
            0.0009,
            0.0013,
            -0.0004,
            -0.0017,
            0.0013
          ],
          "after": [
            -0.014,
            0.0097,
            -0.0591,
            -0.0026,
            -0.0041,
            0.1351,
            0.0264,
            -0.1268,
            0.0286,
            0.0595,
            -0.0616,
            -0.0503,
            0.0944,
            -0.0488,
            0.0182,
            0.0419
          ]
        }
      }
    },
    {
      "step": 144,
      "word": "tapanga",
      "loss": 2.7216,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0612,
            0.4342,
            -0.0852,
            0.2102,
            -0.0858,
            -0.2156,
            0.5889,
            0.033,
            -0.5415,
            0.1544,
            -0.0802,
            0.1052,
            0.0641,
            0.0036,
            -0.1003,
            0.2441
          ],
          "after": [
            -0.006,
            0.0836,
            0.123,
            0.01,
            0.0101,
            -0.0495,
            -0.0258,
            -0.1215,
            -0.0339,
            0.0323,
            0.0909,
            0.005,
            -0.0534,
            0.0012,
            -0.0339,
            -0.1248
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0926,
            0.1213,
            -0.0597,
            0.0234,
            0.1082,
            -0.2089,
            0.2252,
            0.0191,
            -0.183,
            0.0948,
            0.0348,
            0.0833,
            0.0687,
            -0.1208,
            -0.2214,
            0.112
          ],
          "after": [
            0.0103,
            0.1041,
            -0.0108,
            -0.0636,
            0.0363,
            -0.0211,
            0.0868,
            0.1076,
            -0.0677,
            -0.0303,
            0.0488,
            0.003,
            0.1022,
            0.0281,
            -0.0721,
            0.192
          ]
        },
        "position_0": {
          "grad": [
            0.0304,
            -0.1679,
            0.0549,
            0.0483,
            -0.0786,
            0.0768,
            0.058,
            0.008,
            -0.0876,
            -0.0862,
            -0.1482,
            -0.2616,
            0.0987,
            0.0741,
            0.0893,
            0.079
          ],
          "after": [
            -0.0046,
            -0.0955,
            -0.2483,
            0.0772,
            0.0149,
            -0.0212,
            -0.1328,
            0.0895,
            0.1138,
            -0.0382,
            0.0044,
            0.111,
            0.0251,
            0.0129,
            -0.1009,
            0.0527
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0004,
            -0.0001,
            0.0006,
            -0.0001,
            0.0004,
            0.0001,
            0.0004,
            -0.0,
            -0.0001,
            -0.0001,
            0.0001,
            -0.0004,
            0.0,
            0.0002,
            0.0001
          ],
          "after": [
            -0.0135,
            0.009,
            -0.0598,
            -0.005,
            -0.0052,
            0.1344,
            0.0251,
            -0.1266,
            0.0288,
            0.0607,
            -0.0622,
            -0.0516,
            0.0936,
            -0.0479,
            0.0196,
            0.0411
          ]
        }
      }
    },
    {
      "step": 145,
      "word": "zyaire",
      "loss": 2.3658,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0795,
            -0.1034,
            0.0948,
            -0.0804,
            0.1128,
            0.021,
            -0.0996,
            -0.0055,
            0.0533,
            0.0356,
            0.1136,
            -0.0432,
            -0.1477,
            0.0155,
            -0.0781,
            -0.1118
          ],
          "after": [
            -0.0057,
            0.0816,
            0.1232,
            0.0085,
            0.0109,
            -0.0486,
            -0.0274,
            -0.1219,
            -0.032,
            0.0317,
            0.0917,
            0.0043,
            -0.0537,
            0.0008,
            -0.0339,
            -0.1252
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0279,
            -0.1525,
            -0.1041,
            -0.0712,
            -0.2726,
            0.1512,
            -0.2474,
            0.0635,
            0.1372,
            0.0424,
            -0.0565,
            -0.1575,
            0.0373,
            0.1599,
            0.3759,
            -0.0265
          ],
          "after": [
            0.0111,
            0.1035,
            -0.0107,
            -0.0637,
            0.0353,
            -0.0206,
            0.0863,
            0.1066,
            -0.0673,
            -0.031,
            0.0483,
            0.0023,
            0.1024,
            0.0285,
            -0.0715,
            0.1914
          ]
        },
        "position_0": {
          "grad": [
            -0.0134,
            0.0984,
            -0.0679,
            0.2306,
            0.0177,
            -0.0601,
            0.2863,
            -0.0173,
            -0.0381,
            0.0978,
            -0.0349,
            0.0885,
            0.0437,
            -0.006,
            -0.0159,
            0.0809
          ],
          "after": [
            -0.0046,
            -0.0953,
            -0.249,
            0.0761,
            0.0155,
            -0.0213,
            -0.1336,
            0.0892,
            0.1145,
            -0.0381,
            0.0056,
            0.1111,
            0.0242,
            0.0128,
            -0.1013,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            0.0001,
            0.001,
            0.0004,
            -0.0006,
            -0.0003,
            -0.0015,
            0.0013,
            0.0006,
            -0.0003,
            0.0,
            0.0006,
            -0.0,
            -0.0005,
            -0.0003,
            0.0011
          ],
          "after": [
            -0.0124,
            0.0083,
            -0.0612,
            -0.0073,
            -0.0059,
            0.1341,
            0.0246,
            -0.1274,
            0.0285,
            0.0621,
            -0.0626,
            -0.0531,
            0.0929,
            -0.0469,
            0.0209,
            0.0396
          ]
        }
      }
    },
    {
      "step": 146,
      "word": "jazly",
      "loss": 2.5211,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0681,
            0.1811,
            -0.0886,
            0.5408,
            -0.0225,
            -0.1481,
            0.5969,
            0.0326,
            -0.1625,
            0.1155,
            0.0028,
            0.1909,
            0.1835,
            -0.1,
            -0.0739,
            0.1813
          ],
          "after": [
            -0.0052,
            0.0792,
            0.1237,
            0.0054,
            0.0116,
            -0.047,
            -0.0303,
            -0.1224,
            -0.0297,
            0.0306,
            0.0924,
            0.0033,
            -0.0545,
            0.0008,
            -0.0335,
            -0.1261
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.07,
            0.0957,
            -0.1295,
            0.0481,
            0.0123,
            -0.1729,
            0.2638,
            0.0444,
            -0.2535,
            0.1108,
            -0.0078,
            -0.0205,
            0.1806,
            -0.0812,
            -0.1909,
            0.1301
          ],
          "after": [
            0.0119,
            0.1028,
            -0.0103,
            -0.0638,
            0.0344,
            -0.0198,
            0.0855,
            0.1056,
            -0.0664,
            -0.032,
            0.048,
            0.0017,
            0.1022,
            0.029,
            -0.0707,
            0.1906
          ]
        },
        "position_0": {
          "grad": [
            -0.0686,
            0.105,
            0.009,
            0.2585,
            0.0395,
            -0.0583,
            0.2124,
            0.0781,
            -0.3136,
            -0.0589,
            0.0505,
            0.1738,
            0.4203,
            -0.1639,
            -0.1412,
            0.1681
          ],
          "after": [
            -0.0043,
            -0.0954,
            -0.2496,
            0.0744,
            0.0158,
            -0.0211,
            -0.1348,
            0.0886,
            0.1159,
            -0.0377,
            0.0064,
            0.1106,
            0.0224,
            0.0132,
            -0.1012,
            0.0509
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0001,
            -0.0002,
            -0.0003,
            -0.0007,
            -0.0001,
            -0.0007,
            -0.0002,
            0.0007,
            -0.0002,
            -0.0001,
            -0.0004,
            -0.0002,
            0.0007,
            0.0003,
            -0.0003
          ],
          "after": [
            -0.0116,
            0.0077,
            -0.0622,
            -0.009,
            -0.0062,
            0.1339,
            0.0244,
            -0.1279,
            0.0276,
            0.0634,
            -0.0629,
            -0.054,
            0.0925,
            -0.0465,
            0.0218,
            0.0385
          ]
        }
      }
    },
    {
      "step": 147,
      "word": "tayron",
      "loss": 2.2881,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0961,
            -0.0495,
            -0.0328,
            -0.0692,
            -0.0755,
            0.0174,
            -0.0027,
            -0.001,
            -0.0544,
            -0.0116,
            -0.134,
            0.0433,
            0.1159,
            0.1824,
            0.0583,
            0.0898
          ],
          "after": [
            -0.0051,
            0.0774,
            0.1242,
            0.0031,
            0.0125,
            -0.0458,
            -0.0327,
            -0.1228,
            -0.0276,
            0.0297,
            0.0936,
            0.0022,
            -0.0557,
            0.0001,
            -0.0335,
            -0.127
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0744,
            0.0854,
            -0.0466,
            0.046,
            0.0504,
            -0.1704,
            0.2474,
            0.0447,
            -0.1885,
            0.1285,
            0.0175,
            0.0186,
            0.1337,
            -0.1093,
            -0.2227,
            0.0942
          ],
          "after": [
            0.0129,
            0.102,
            -0.0098,
            -0.0641,
            0.0335,
            -0.019,
            0.0845,
            0.1046,
            -0.0654,
            -0.0333,
            0.0476,
            0.0011,
            0.1017,
            0.0298,
            -0.0697,
            0.1898
          ]
        },
        "position_0": {
          "grad": [
            0.0482,
            -0.2489,
            0.0689,
            -0.0015,
            -0.173,
            0.091,
            -0.0089,
            0.0097,
            -0.0291,
            -0.1018,
            -0.1712,
            -0.3723,
            0.055,
            0.1152,
            0.1541,
            0.0262
          ],
          "after": [
            -0.0043,
            -0.0947,
            -0.2505,
            0.073,
            0.0167,
            -0.0213,
            -0.1358,
            0.0881,
            0.1171,
            -0.0368,
            0.0078,
            0.1112,
            0.0208,
            0.0132,
            -0.1016,
            0.0499
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0,
            -0.0,
            -0.0003,
            -0.0001,
            0.0,
            -0.0,
            0.0,
            -0.0001,
            -0.0002,
            0.0001,
            0.0,
            -0.0001
          ],
          "after": [
            -0.0111,
            0.0072,
            -0.063,
            -0.0103,
            -0.0065,
            0.1337,
            0.0244,
            -0.1282,
            0.0269,
            0.0645,
            -0.0631,
            -0.0548,
            0.0922,
            -0.0462,
            0.0227,
            0.0377
          ]
        }
      }
    },
    {
      "step": 148,
      "word": "jenova",
      "loss": 2.5101,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1606,
            0.2136,
            -0.0807,
            -0.0646,
            -0.0873,
            -0.0787,
            0.0996,
            0.0043,
            -0.1196,
            0.1655,
            0.1009,
            -0.1791,
            0.1891,
            0.0685,
            -0.0183,
            -0.1243
          ],
          "after": [
            -0.0054,
            0.0751,
            0.1249,
            0.0013,
            0.0135,
            -0.0444,
            -0.0351,
            -0.1232,
            -0.0253,
            0.0281,
            0.0941,
            0.0019,
            -0.0573,
            -0.0008,
            -0.0334,
            -0.1275
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1787,
            -0.1146,
            0.1846,
            -0.0119,
            0.3092,
            0.0548,
            -0.2657,
            -0.0844,
            0.2535,
            0.0219,
            0.2381,
            0.161,
            -0.3033,
            -0.0529,
            -0.1365,
            -0.0983
          ],
          "after": [
            0.0132,
            0.1016,
            -0.0099,
            -0.0643,
            0.0323,
            -0.0183,
            0.084,
            0.1041,
            -0.065,
            -0.0344,
            0.0462,
            0.0003,
            0.1019,
            0.0305,
            -0.0687,
            0.1893
          ]
        },
        "position_0": {
          "grad": [
            -0.0507,
            0.0491,
            0.0045,
            0.1812,
            0.0106,
            -0.0448,
            0.1183,
            0.0715,
            -0.212,
            -0.0527,
            0.0345,
            0.1049,
            0.318,
            -0.1365,
            -0.1049,
            0.0995
          ],
          "after": [
            -0.004,
            -0.0943,
            -0.2512,
            0.0713,
            0.0174,
            -0.0213,
            -0.1368,
            0.0874,
            0.1185,
            -0.0357,
            0.0089,
            0.1114,
            0.0187,
            0.0137,
            -0.1016,
            0.0488
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0,
            0.0,
            -0.0003,
            -0.0002,
            0.0001,
            -0.0002,
            0.0002,
            0.0003,
            0.0008,
            0.0008,
            0.0002,
            -0.0004,
            -0.0005,
            -0.0002,
            -0.0004
          ],
          "after": [
            -0.0105,
            0.0068,
            -0.0638,
            -0.0112,
            -0.0066,
            0.1334,
            0.0245,
            -0.1287,
            0.026,
            0.0648,
            -0.0644,
            -0.0555,
            0.0922,
            -0.0456,
            0.0234,
            0.0373
          ]
        }
      }
    },
    {
      "step": 149,
      "word": "ciarra",
      "loss": 2.1712,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2463,
            0.1318,
            -0.0745,
            -0.1531,
            0.0634,
            0.1249,
            0.1061,
            -0.143,
            0.0765,
            0.169,
            0.1879,
            -0.0053,
            -0.0838,
            -0.0388,
            -0.0158,
            0.0256
          ],
          "after": [
            -0.0064,
            0.0726,
            0.1258,
            0.0004,
            0.0142,
            -0.0439,
            -0.0374,
            -0.1229,
            -0.0237,
            0.026,
            0.0937,
            0.0016,
            -0.0585,
            -0.0013,
            -0.0332,
            -0.128
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0225,
            0.1038,
            -0.0431,
            0.024,
            0.0612,
            -0.1297,
            0.2022,
            -0.0003,
            -0.162,
            0.095,
            0.023,
            -0.0101,
            0.099,
            -0.0883,
            -0.2146,
            0.0665
          ],
          "after": [
            0.0135,
            0.101,
            -0.0098,
            -0.0646,
            0.0311,
            -0.0175,
            0.0833,
            0.1036,
            -0.0643,
            -0.0357,
            0.045,
            -0.0004,
            0.1019,
            0.0313,
            -0.0675,
            0.1887
          ]
        },
        "position_0": {
          "grad": [
            -0.0607,
            0.0097,
            -0.0372,
            -0.0651,
            -0.0679,
            -0.014,
            0.0333,
            0.0181,
            0.0377,
            0.1019,
            0.0801,
            0.0935,
            -0.0801,
            -0.0491,
            -0.0752,
            -0.0528
          ],
          "after": [
            -0.0036,
            -0.094,
            -0.2517,
            0.0701,
            0.0183,
            -0.0212,
            -0.1378,
            0.0867,
            0.1197,
            -0.0354,
            0.0095,
            0.1114,
            0.0171,
            0.0142,
            -0.1013,
            0.048
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0007,
            -0.0002,
            0.0005,
            -0.001,
            0.0003,
            -0.0015,
            0.0009,
            0.0002,
            0.0002,
            0.0001,
            0.0011,
            -0.0011,
            -0.0003,
            0.0014,
            0.0004
          ],
          "after": [
            -0.0098,
            0.007,
            -0.0642,
            -0.0124,
            -0.0063,
            0.1329,
            0.0252,
            -0.1297,
            0.0251,
            0.065,
            -0.0655,
            -0.0569,
            0.093,
            -0.045,
            0.0234,
            0.0367
          ]
        }
      }
    },
    {
      "step": 150,
      "word": "bellanova",
      "loss": 2.6859,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0823,
            0.05,
            -0.0803,
            -0.0281,
            -0.0593,
            0.0039,
            -0.0194,
            -0.1109,
            -0.0017,
            0.0004,
            0.0927,
            -0.1195,
            -0.0044,
            0.0192,
            -0.038,
            -0.0333
          ],
          "after": [
            -0.0074,
            0.0703,
            0.1268,
            -0.0002,
            0.015,
            -0.0434,
            -0.0393,
            -0.1222,
            -0.0223,
            0.0241,
            0.0928,
            0.0016,
            -0.0594,
            -0.0019,
            -0.0329,
            -0.1283
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0027,
            -0.043,
            0.0383,
            -0.0821,
            0.2985,
            0.0397,
            -0.0309,
            -0.0441,
            0.2425,
            -0.0201,
            0.1577,
            0.1098,
            -0.2597,
            -0.1416,
            -0.106,
            -0.0014
          ],
          "after": [
            0.0138,
            0.1006,
            -0.0099,
            -0.0645,
            0.0295,
            -0.0169,
            0.0828,
            0.1034,
            -0.0642,
            -0.0368,
            0.0432,
            -0.0012,
            0.1025,
            0.0324,
            -0.0664,
            0.1882
          ]
        },
        "position_0": {
          "grad": [
            0.0067,
            -0.0319,
            0.0807,
            -0.0548,
            0.006,
            0.0124,
            -0.031,
            -0.0066,
            -0.0183,
            -0.0476,
            0.002,
            0.0472,
            0.0425,
            -0.0465,
            0.0008,
            0.1286
          ],
          "after": [
            -0.0032,
            -0.0937,
            -0.2525,
            0.0691,
            0.019,
            -0.0212,
            -0.1386,
            0.0861,
            0.1207,
            -0.0349,
            0.0099,
            0.1112,
            0.0157,
            0.0148,
            -0.1011,
            0.047
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0008,
            -0.0007,
            -0.0007,
            0.0003,
            0.0009,
            -0.0014,
            -0.0001,
            0.001,
            0.0001,
            0.0,
            -0.0001,
            -0.0016,
            0.0005,
            0.0,
            -0.0008
          ],
          "after": [
            -0.0096,
            0.0077,
            -0.064,
            -0.0128,
            -0.0061,
            0.1318,
            0.0264,
            -0.1306,
            0.0236,
            0.065,
            -0.0665,
            -0.058,
            0.0948,
            -0.0447,
            0.0234,
            0.0368
          ]
        }
      }
    },
    {
      "step": 151,
      "word": "dominic",
      "loss": 2.8823,
      "learning_rate": 0.0026,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0084,
            0.0684,
            0.1276,
            -0.0008,
            0.0157,
            -0.043,
            -0.0409,
            -0.1216,
            -0.0211,
            0.0225,
            0.0921,
            0.0017,
            -0.0602,
            -0.0024,
            -0.0326,
            -0.1286
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0653,
            0.0561,
            -0.0552,
            0.0198,
            0.1175,
            -0.1063,
            0.191,
            0.0359,
            -0.1422,
            0.0688,
            -0.0201,
            0.014,
            0.0892,
            -0.0647,
            -0.1946,
            0.0929
          ],
          "after": [
            0.0142,
            0.1001,
            -0.0098,
            -0.0646,
            0.028,
            -0.0162,
            0.0821,
            0.1031,
            -0.0638,
            -0.0379,
            0.0418,
            -0.002,
            0.1028,
            0.0334,
            -0.0651,
            0.1876
          ]
        },
        "position_0": {
          "grad": [
            -0.0538,
            0.0117,
            0.0007,
            -0.0967,
            0.1289,
            -0.019,
            -0.0093,
            -0.0533,
            0.0694,
            0.0271,
            -0.1023,
            0.0144,
            -0.0385,
            0.0892,
            0.0779,
            0.0889
          ],
          "after": [
            -0.0027,
            -0.0934,
            -0.2531,
            0.0686,
            0.0192,
            -0.0211,
            -0.1392,
            0.0859,
            0.1215,
            -0.0346,
            0.0108,
            0.1111,
            0.0145,
            0.015,
            -0.1012,
            0.0459
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            -0.0008,
            -0.0005,
            0.0019,
            -0.0031,
            -0.0004,
            -0.0025,
            0.0003,
            -0.0007,
            -0.0005,
            0.0001,
            -0.0015,
            0.0002,
            0.0008,
            0.0019,
            -0.0004
          ],
          "after": [
            -0.0107,
            0.0089,
            -0.0635,
            -0.0145,
            -0.0047,
            0.1311,
            0.0284,
            -0.1315,
            0.0228,
            0.0654,
            -0.0675,
            -0.058,
            0.0961,
            -0.0451,
            0.0225,
            0.0373
          ]
        }
      }
    },
    {
      "step": 152,
      "word": "laziyah",
      "loss": 2.4631,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.278,
            0.0296,
            0.1464,
            0.4715,
            0.1197,
            0.0211,
            0.5146,
            0.0817,
            -0.0016,
            0.0596,
            -0.1778,
            0.2697,
            0.1344,
            -0.1841,
            -0.1395,
            0.3552
          ],
          "after": [
            -0.0083,
            0.0666,
            0.1279,
            -0.0028,
            0.0158,
            -0.0428,
            -0.0436,
            -0.1214,
            -0.0201,
            0.0209,
            0.0923,
            0.001,
            -0.0613,
            -0.0021,
            -0.0318,
            -0.1297
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0717,
            0.0607,
            -0.0505,
            0.0307,
            0.0733,
            -0.1148,
            0.1704,
            0.0091,
            -0.1456,
            0.0645,
            0.0057,
            0.0119,
            0.0705,
            -0.0683,
            -0.1638,
            0.0886
          ],
          "after": [
            0.0147,
            0.0996,
            -0.0096,
            -0.0647,
            0.0265,
            -0.0154,
            0.0812,
            0.1028,
            -0.0632,
            -0.039,
            0.0406,
            -0.0027,
            0.1029,
            0.0345,
            -0.0639,
            0.1869
          ]
        },
        "position_0": {
          "grad": [
            0.0069,
            0.0555,
            -0.038,
            -0.1439,
            -0.0028,
            0.0534,
            -0.168,
            0.0811,
            0.038,
            0.0324,
            -0.1081,
            0.0209,
            -0.1389,
            0.0993,
            0.1268,
            -0.2518
          ],
          "after": [
            -0.0023,
            -0.0933,
            -0.2535,
            0.0685,
            0.0193,
            -0.0213,
            -0.1394,
            0.0853,
            0.122,
            -0.0345,
            0.0119,
            0.1109,
            0.0138,
            0.0149,
            -0.1016,
            0.0456
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0004,
            0.0001,
            -0.0029,
            -0.0006,
            -0.002,
            -0.0032,
            -0.0016,
            0.0027,
            -0.0013,
            -0.0005,
            -0.0024,
            -0.0008,
            0.0029,
            0.0003,
            -0.0025
          ],
          "after": [
            -0.0126,
            0.0101,
            -0.0632,
            -0.0139,
            -0.0032,
            0.1321,
            0.0312,
            -0.1311,
            0.0204,
            0.0668,
            -0.0677,
            -0.0566,
            0.0977,
            -0.047,
            0.0216,
            0.0394
          ]
        }
      }
    },
    {
      "step": 153,
      "word": "akin",
      "loss": 2.9106,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.183,
            0.0887,
            -0.1027,
            -0.0263,
            0.0863,
            -0.1562,
            0.2304,
            0.0938,
            0.0834,
            -0.0355,
            -0.2312,
            -0.0273,
            0.0104,
            0.0616,
            -0.3014,
            0.0079
          ],
          "after": [
            -0.0078,
            0.0648,
            0.1284,
            -0.0044,
            0.0157,
            -0.0418,
            -0.0464,
            -0.1217,
            -0.0196,
            0.0196,
            0.0936,
            0.0005,
            -0.0624,
            -0.0021,
            -0.0299,
            -0.1306
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0437,
            0.0496,
            -0.0796,
            -0.0085,
            -0.0018,
            -0.1385,
            0.1855,
            0.0344,
            -0.1518,
            0.092,
            -0.0343,
            -0.0738,
            0.1214,
            -0.0389,
            -0.11,
            0.0781
          ],
          "after": [
            0.0153,
            0.099,
            -0.0092,
            -0.0648,
            0.0253,
            -0.0145,
            0.0802,
            0.1024,
            -0.0625,
            -0.0403,
            0.0397,
            -0.003,
            0.1027,
            0.0355,
            -0.0626,
            0.1862
          ]
        },
        "position_0": {
          "grad": [
            0.1832,
            -0.1721,
            0.1754,
            -0.3821,
            -0.1251,
            0.0857,
            -0.5367,
            -0.1596,
            0.4948,
            -0.0663,
            0.238,
            -0.1322,
            -0.4062,
            0.2014,
            0.1982,
            -0.4723
          ],
          "after": [
            -0.0027,
            -0.0927,
            -0.2546,
            0.0695,
            0.0198,
            -0.0218,
            -0.1385,
            0.0855,
            0.1214,
            -0.034,
            0.0119,
            0.111,
            0.0142,
            0.0142,
            -0.1026,
            0.0466
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0033,
            0.0007,
            -0.0016,
            0.0011,
            -0.0005,
            -0.0012,
            0.0071,
            0.0023,
            -0.0013,
            0.0034,
            0.003,
            0.0023,
            0.0015,
            -0.0052,
            0.0015,
            0.0
          ],
          "after": [
            -0.0121,
            0.0107,
            -0.0617,
            -0.0141,
            -0.0018,
            0.1338,
            0.031,
            -0.1322,
            0.0193,
            0.0655,
            -0.0704,
            -0.0567,
            0.0982,
            -0.046,
            0.0202,
            0.0411
          ]
        }
      }
    },
    {
      "step": 154,
      "word": "adam",
      "loss": 2.9404,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.376,
            0.281,
            0.1066,
            0.312,
            0.262,
            -0.1829,
            0.6018,
            0.3316,
            0.0367,
            -0.1078,
            -0.1695,
            0.2156,
            -0.0094,
            -0.0642,
            0.0805,
            0.5033
          ],
          "after": [
            -0.0064,
            0.0622,
            0.1285,
            -0.0068,
            0.0147,
            -0.0401,
            -0.05,
            -0.1232,
            -0.0192,
            0.0191,
            0.0954,
            -0.0006,
            -0.0632,
            -0.0018,
            -0.0287,
            -0.1326
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0264,
            0.0658,
            -0.1288,
            0.0465,
            -0.0195,
            -0.1248,
            0.1398,
            0.0312,
            -0.2116,
            0.1172,
            -0.0167,
            -0.1024,
            0.1803,
            -0.0213,
            -0.1548,
            0.035
          ],
          "after": [
            0.0157,
            0.0983,
            -0.0085,
            -0.065,
            0.0243,
            -0.0136,
            0.0792,
            0.1019,
            -0.0615,
            -0.0418,
            0.0391,
            -0.0031,
            0.1021,
            0.0364,
            -0.0614,
            0.1855
          ]
        },
        "position_0": {
          "grad": [
            0.1953,
            -0.1587,
            0.1562,
            -0.3902,
            -0.1381,
            0.0784,
            -0.5293,
            -0.1534,
            0.4915,
            -0.0559,
            0.2212,
            -0.1229,
            -0.4374,
            0.1941,
            0.2105,
            -0.4794
          ],
          "after": [
            -0.0037,
            -0.0918,
            -0.2562,
            0.0712,
            0.0208,
            -0.0226,
            -0.1367,
            0.0863,
            0.12,
            -0.0333,
            0.0111,
            0.1115,
            0.0155,
            0.0129,
            -0.1041,
            0.0485
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0019,
            0.0009,
            -0.0006,
            0.0004,
            -0.0004,
            -0.0002,
            0.0015,
            0.0002,
            -0.0024,
            0.0027,
            0.0015,
            0.0001,
            0.0013,
            -0.0012,
            -0.0002,
            -0.0002
          ],
          "after": [
            -0.0107,
            0.0106,
            -0.0601,
            -0.0145,
            -0.0004,
            0.1354,
            0.0303,
            -0.1333,
            0.0199,
            0.0631,
            -0.0736,
            -0.0568,
            0.0977,
            -0.0447,
            0.019,
            0.0428
          ]
        }
      }
    },
    {
      "step": 155,
      "word": "venancio",
      "loss": 2.895,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0348,
            0.041,
            -0.1012,
            -0.0238,
            -0.0784,
            -0.0501,
            -0.0139,
            -0.0531,
            0.0298,
            -0.0079,
            0.0031,
            -0.0352,
            -0.0045,
            0.026,
            0.0268,
            -0.0517
          ],
          "after": [
            -0.0053,
            0.06,
            0.1289,
            -0.0087,
            0.0141,
            -0.0385,
            -0.0531,
            -0.1243,
            -0.019,
            0.0188,
            0.0969,
            -0.0013,
            -0.0639,
            -0.0017,
            -0.0277,
            -0.1342
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0596,
            0.0478,
            0.1547,
            -0.1382,
            0.3537,
            0.0534,
            -0.183,
            -0.0594,
            0.3362,
            -0.0568,
            0.184,
            0.2073,
            -0.3358,
            -0.0714,
            -0.0432,
            -0.0049
          ],
          "after": [
            0.0162,
            0.0976,
            -0.0084,
            -0.0647,
            0.0228,
            -0.0128,
            0.0786,
            0.1017,
            -0.0612,
            -0.0429,
            0.0377,
            -0.0037,
            0.1024,
            0.0374,
            -0.0602,
            0.1849
          ]
        },
        "position_0": {
          "grad": [
            -0.0653,
            -0.0393,
            0.0067,
            0.0248,
            0.051,
            0.0342,
            -0.0838,
            0.0149,
            -0.0233,
            0.0073,
            -0.0959,
            -0.1219,
            0.025,
            -0.0321,
            -0.0522,
            0.0875
          ],
          "after": [
            -0.0044,
            -0.0908,
            -0.2576,
            0.0727,
            0.0214,
            -0.0235,
            -0.1351,
            0.0869,
            0.1188,
            -0.0328,
            0.0107,
            0.1122,
            0.0165,
            0.012,
            -0.1052,
            0.0499
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0015,
            -0.0009,
            -0.0003,
            -0.0016,
            0.0006,
            -0.0034,
            -0.0001,
            0.0006,
            -0.0003,
            -0.0003,
            -0.0001,
            -0.0023,
            0.0011,
            0.0019,
            -0.0019
          ],
          "after": [
            -0.01,
            0.0115,
            -0.058,
            -0.0146,
            0.0015,
            0.1363,
            0.0308,
            -0.1342,
            0.0201,
            0.0613,
            -0.0761,
            -0.0569,
            0.0987,
            -0.044,
            0.0172,
            0.0453
          ]
        }
      }
    },
    {
      "step": 156,
      "word": "kristabelle",
      "loss": 2.8597,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1182,
            -0.1973,
            0.1317,
            0.1209,
            0.084,
            0.0569,
            0.0052,
            0.0155,
            -0.0288,
            -0.1649,
            -0.1186,
            0.0848,
            0.0024,
            -0.122,
            0.0067,
            0.3144
          ],
          "after": [
            -0.004,
            0.0587,
            0.1289,
            -0.0108,
            0.0134,
            -0.0373,
            -0.0557,
            -0.1252,
            -0.0188,
            0.0193,
            0.0987,
            -0.0023,
            -0.0645,
            -0.0011,
            -0.0269,
            -0.1362
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3506,
            -0.1412,
            -0.1117,
            -0.1546,
            -0.3273,
            0.4067,
            -0.1951,
            0.0928,
            0.1486,
            0.1154,
            -0.1182,
            -0.4962,
            0.1587,
            0.1818,
            0.1372,
            -0.3975
          ],
          "after": [
            0.0157,
            0.0974,
            -0.008,
            -0.064,
            0.0221,
            -0.0129,
            0.0783,
            0.1012,
            -0.0612,
            -0.0442,
            0.037,
            -0.0031,
            0.1022,
            0.0377,
            -0.0595,
            0.1852
          ]
        },
        "position_0": {
          "grad": [
            -0.0464,
            0.0214,
            -0.0759,
            -0.0314,
            -0.0059,
            0.0018,
            0.0412,
            -0.0166,
            0.0158,
            0.0285,
            0.0008,
            -0.0142,
            -0.0529,
            -0.0011,
            -0.0305,
            -0.0805
          ],
          "after": [
            -0.0048,
            -0.0901,
            -0.2585,
            0.074,
            0.022,
            -0.0242,
            -0.1338,
            0.0875,
            0.1177,
            -0.0325,
            0.0105,
            0.1129,
            0.0174,
            0.0112,
            -0.1061,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            -0.002,
            -0.0024,
            -0.0016,
            -0.001,
            0.0025,
            -0.0005,
            0.0028,
            0.0006,
            0.0035,
            0.0003,
            -0.0017,
            -0.0014,
            0.0008,
            -0.0002,
            -0.0026
          ],
          "after": [
            -0.0104,
            0.0134,
            -0.0549,
            -0.0138,
            0.0035,
            0.1353,
            0.0313,
            -0.1364,
            0.0199,
            0.0582,
            -0.0785,
            -0.056,
            0.1003,
            -0.0437,
            0.0158,
            0.0487
          ]
        }
      }
    },
    {
      "step": 157,
      "word": "polo",
      "loss": 3.156,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0029,
            0.0577,
            0.1288,
            -0.0125,
            0.0127,
            -0.0364,
            -0.058,
            -0.1261,
            -0.0186,
            0.0197,
            0.1002,
            -0.003,
            -0.065,
            -0.0006,
            -0.0262,
            -0.1379
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0148,
            0.0395,
            -0.1206,
            0.0505,
            -0.1024,
            -0.1393,
            0.1768,
            0.0492,
            -0.1944,
            0.0964,
            -0.0044,
            -0.06,
            0.1688,
            -0.056,
            -0.1519,
            0.07
          ],
          "after": [
            0.0153,
            0.0971,
            -0.0073,
            -0.0635,
            0.0217,
            -0.0127,
            0.0778,
            0.1005,
            -0.0609,
            -0.0456,
            0.0365,
            -0.0024,
            0.1018,
            0.0382,
            -0.0586,
            0.1853
          ]
        },
        "position_0": {
          "grad": [
            0.0482,
            0.2308,
            -0.0763,
            0.2729,
            -0.0109,
            -0.1478,
            0.5126,
            0.1858,
            -0.4749,
            -0.019,
            -0.1074,
            0.0665,
            0.082,
            0.1843,
            -0.0538,
            0.4132
          ],
          "after": [
            -0.0053,
            -0.0902,
            -0.2589,
            0.0744,
            0.0225,
            -0.0242,
            -0.1336,
            0.0872,
            0.1178,
            -0.0321,
            0.0106,
            0.1133,
            0.0181,
            0.0099,
            -0.1066,
            0.0515
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0002,
            0.0002,
            0.0006,
            -0.0005,
            -0.0007,
            0.0035,
            0.0021,
            -0.0017,
            0.0024,
            0.0025,
            0.0003,
            0.0023,
            -0.0028,
            -0.0026,
            -0.001
          ],
          "after": [
            -0.0106,
            0.0148,
            -0.0523,
            -0.0135,
            0.0054,
            0.135,
            0.0308,
            -0.1393,
            0.0207,
            0.0547,
            -0.0819,
            -0.0554,
            0.1003,
            -0.0424,
            0.0158,
            0.0522
          ]
        }
      }
    },
    {
      "step": 158,
      "word": "kallan",
      "loss": 1.936,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0029,
            0.0744,
            -0.1825,
            -0.0628,
            -0.0717,
            -0.062,
            -0.2711,
            0.0786,
            -0.007,
            -0.1913,
            -0.1337,
            0.0902,
            -0.1612,
            0.1178,
            0.152,
            -0.3516
          ],
          "after": [
            -0.002,
            0.0566,
            0.1293,
            -0.0138,
            0.0124,
            -0.0352,
            -0.0592,
            -0.1271,
            -0.0184,
            0.021,
            0.1021,
            -0.004,
            -0.0649,
            -0.0007,
            -0.0263,
            -0.1386
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0606,
            0.1016,
            -0.0764,
            0.0496,
            0.0004,
            -0.1702,
            0.2259,
            0.0145,
            -0.1884,
            0.0908,
            0.0094,
            -0.0038,
            0.1125,
            -0.0995,
            -0.1334,
            0.1455
          ],
          "after": [
            0.015,
            0.0966,
            -0.0066,
            -0.0633,
            0.0214,
            -0.0123,
            0.0771,
            0.0999,
            -0.0603,
            -0.0471,
            0.036,
            -0.0018,
            0.1011,
            0.0388,
            -0.0577,
            0.1851
          ]
        },
        "position_0": {
          "grad": [
            -0.074,
            0.0039,
            -0.1186,
            -0.0477,
            -0.0299,
            -0.0221,
            0.0331,
            -0.0117,
            0.053,
            0.0279,
            -0.0003,
            -0.0507,
            -0.0613,
            0.0193,
            -0.0503,
            -0.1593
          ],
          "after": [
            -0.0055,
            -0.0902,
            -0.2587,
            0.0749,
            0.0231,
            -0.024,
            -0.1336,
            0.087,
            0.1177,
            -0.0319,
            0.0108,
            0.1137,
            0.0188,
            0.0088,
            -0.1069,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0013,
            -0.0003,
            -0.0002,
            -0.0002,
            0.0023,
            -0.0015,
            0.0013,
            0.0024,
            0.0005,
            -0.0008,
            0.0026,
            -0.0014,
            0.0008,
            -0.0002,
            0.0012
          ],
          "after": [
            -0.0105,
            0.0168,
            -0.0499,
            -0.0131,
            0.0071,
            0.1334,
            0.0307,
            -0.1424,
            0.0201,
            0.0514,
            -0.0843,
            -0.0562,
            0.1011,
            -0.0415,
            0.0159,
            0.0544
          ]
        }
      }
    },
    {
      "step": 159,
      "word": "abby",
      "loss": 3.4369,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0806,
            0.1083,
            0.4024,
            0.1462,
            0.2585,
            -0.089,
            0.337,
            0.0783,
            -0.2144,
            -0.2152,
            -0.215,
            0.2598,
            0.3878,
            -0.1212,
            -0.2548,
            0.7802
          ],
          "after": [
            -0.001,
            0.0552,
            0.1285,
            -0.0153,
            0.0113,
            -0.0339,
            -0.061,
            -0.1283,
            -0.0174,
            0.023,
            0.1046,
            -0.0054,
            -0.0661,
            -0.0002,
            -0.0253,
            -0.1406
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0276,
            0.069,
            -0.0568,
            0.0679,
            -0.0513,
            -0.1704,
            0.1483,
            0.0202,
            -0.2425,
            0.1185,
            -0.0279,
            -0.0286,
            0.2067,
            -0.0043,
            -0.1495,
            0.1099
          ],
          "after": [
            0.0149,
            0.096,
            -0.0058,
            -0.0633,
            0.0212,
            -0.0116,
            0.0763,
            0.0992,
            -0.0594,
            -0.0488,
            0.0357,
            -0.0012,
            0.1001,
            0.0394,
            -0.0567,
            0.1847
          ]
        },
        "position_0": {
          "grad": [
            0.1531,
            -0.0751,
            0.0996,
            -0.3093,
            0.0094,
            0.0995,
            -0.3764,
            -0.2016,
            0.3183,
            -0.0279,
            0.3381,
            0.015,
            -0.3271,
            0.1175,
            0.1241,
            -0.311
          ],
          "after": [
            -0.0062,
            -0.0901,
            -0.259,
            0.076,
            0.0235,
            -0.0244,
            -0.1328,
            0.0877,
            0.1171,
            -0.0316,
            0.0097,
            0.1141,
            0.0201,
            0.0074,
            -0.1076,
            0.0532
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0032,
            0.0033,
            0.0025,
            -0.0024,
            -0.0045,
            0.0035,
            -0.0019,
            -0.0052,
            0.0001,
            -0.0007,
            -0.0018,
            0.0072,
            0.002,
            -0.0005,
            0.0003
          ],
          "after": [
            -0.0105,
            0.0166,
            -0.05,
            -0.0142,
            0.0094,
            0.1344,
            0.0297,
            -0.1439,
            0.0218,
            0.0486,
            -0.0859,
            -0.056,
            0.0991,
            -0.0416,
            0.0162,
            0.0561
          ]
        }
      }
    },
    {
      "step": 160,
      "word": "lilian",
      "loss": 2.1632,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0029,
            -0.023,
            -0.0311,
            0.0455,
            -0.0397,
            -0.0578,
            -0.0545,
            -0.0775,
            0.0458,
            -0.1882,
            -0.04,
            0.0381,
            -0.1022,
            0.0168,
            0.0224,
            -0.0459
          ],
          "after": [
            -0.0001,
            0.0541,
            0.1279,
            -0.0168,
            0.0105,
            -0.0324,
            -0.0624,
            -0.129,
            -0.0167,
            0.0256,
            0.1069,
            -0.0068,
            -0.0668,
            0.0001,
            -0.0246,
            -0.1423
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0549,
            0.0781,
            -0.0813,
            0.0272,
            0.0419,
            -0.1187,
            0.203,
            -0.0096,
            -0.1581,
            0.0773,
            0.0062,
            -0.0176,
            0.0995,
            -0.0775,
            -0.1621,
            0.1259
          ],
          "after": [
            0.015,
            0.0953,
            -0.0049,
            -0.0634,
            0.021,
            -0.0108,
            0.0753,
            0.0987,
            -0.0583,
            -0.0504,
            0.0354,
            -0.0007,
            0.099,
            0.04,
            -0.0556,
            0.1842
          ]
        },
        "position_0": {
          "grad": [
            -0.0065,
            0.0542,
            -0.06,
            -0.1506,
            0.0017,
            0.0522,
            -0.1982,
            0.0952,
            0.0357,
            0.0338,
            -0.1068,
            0.0441,
            -0.132,
            0.0919,
            0.1183,
            -0.2719
          ],
          "after": [
            -0.0068,
            -0.0901,
            -0.2589,
            0.0773,
            0.0239,
            -0.0249,
            -0.1318,
            0.0879,
            0.1164,
            -0.0316,
            0.0091,
            0.1143,
            0.0215,
            0.006,
            -0.1085,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            -0.0024,
            -0.0001,
            0.0029,
            -0.0072,
            -0.0008,
            -0.0049,
            0.0049,
            0.0027,
            -0.0011,
            -0.0027,
            0.0057,
            -0.0017,
            0.0007,
            0.0043,
            0.0039
          ],
          "after": [
            -0.01,
            0.0176,
            -0.05,
            -0.0164,
            0.0132,
            0.1356,
            0.0302,
            -0.147,
            0.0222,
            0.0467,
            -0.0855,
            -0.058,
            0.0979,
            -0.042,
            0.0148,
            0.0555
          ]
        }
      }
    },
    {
      "step": 161,
      "word": "bradyn",
      "loss": 2.1425,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0652,
            0.0629,
            -0.0175,
            0.0169,
            0.1912,
            -0.0674,
            0.0788,
            -0.0441,
            -0.0095,
            0.0113,
            -0.0813,
            0.0824,
            0.0667,
            0.1083,
            -0.0122,
            0.2154
          ],
          "after": [
            0.0008,
            0.053,
            0.1275,
            -0.0181,
            0.0092,
            -0.0309,
            -0.0638,
            -0.1294,
            -0.0161,
            0.0277,
            0.1091,
            -0.0082,
            -0.0676,
            -0.0001,
            -0.0239,
            -0.1442
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0528,
            0.0833,
            -0.0672,
            0.0722,
            0.0378,
            -0.1566,
            0.2256,
            0.0368,
            -0.2022,
            0.0971,
            -0.0264,
            0.0045,
            0.149,
            -0.0855,
            -0.1775,
            0.1118
          ],
          "after": [
            0.0151,
            0.0945,
            -0.004,
            -0.0638,
            0.0207,
            -0.0099,
            0.0742,
            0.0981,
            -0.057,
            -0.0522,
            0.0353,
            -0.0002,
            0.0978,
            0.0408,
            -0.0544,
            0.1835
          ]
        },
        "position_0": {
          "grad": [
            0.0132,
            0.0171,
            0.0894,
            -0.0483,
            0.0451,
            0.0057,
            0.0491,
            -0.0298,
            -0.1134,
            -0.0536,
            0.0254,
            0.0867,
            0.1023,
            -0.0869,
            -0.0305,
            0.2426
          ],
          "after": [
            -0.0073,
            -0.0902,
            -0.2593,
            0.0786,
            0.024,
            -0.0254,
            -0.1311,
            0.0882,
            0.1161,
            -0.0312,
            0.0086,
            0.1142,
            0.0224,
            0.0051,
            -0.1092,
            0.0555
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0005,
            0.0003,
            0.0013,
            0.0011,
            0.0021,
            -0.0002,
            0.0007,
            -0.0002,
            0.0015,
            0.0013,
            0.0022,
            -0.0021,
            -0.0005,
            0.0006,
            -0.0002
          ],
          "after": [
            -0.0094,
            0.0183,
            -0.0502,
            -0.0189,
            0.016,
            0.1356,
            0.0306,
            -0.1499,
            0.0225,
            0.0445,
            -0.0859,
            -0.0604,
            0.0977,
            -0.0421,
            0.0134,
            0.055
          ]
        }
      }
    },
    {
      "step": 162,
      "word": "umayma",
      "loss": 2.6052,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2575,
            0.1734,
            -0.0572,
            -0.1789,
            -0.113,
            0.104,
            0.0682,
            0.0076,
            -0.1281,
            0.2084,
            -0.001,
            0.0069,
            0.1832,
            0.1923,
            -0.0159,
            -0.0583
          ],
          "after": [
            0.0009,
            0.0514,
            0.1273,
            -0.0187,
            0.0085,
            -0.03,
            -0.0652,
            -0.1298,
            -0.0151,
            0.0285,
            0.1111,
            -0.0095,
            -0.0689,
            -0.0009,
            -0.0233,
            -0.1456
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0296,
            0.055,
            -0.0959,
            0.0295,
            0.0797,
            -0.1448,
            0.1149,
            0.0387,
            -0.1829,
            0.0994,
            -0.0154,
            -0.0126,
            0.1339,
            -0.0596,
            -0.1745,
            0.0855
          ],
          "after": [
            0.0154,
            0.0937,
            -0.003,
            -0.0642,
            0.0203,
            -0.0089,
            0.073,
            0.0974,
            -0.0556,
            -0.054,
            0.0353,
            0.0002,
            0.0964,
            0.0416,
            -0.0532,
            0.1827
          ]
        },
        "position_0": {
          "grad": [
            0.0744,
            0.0867,
            -0.0448,
            0.0073,
            0.1175,
            -0.0865,
            -0.1797,
            -0.0394,
            0.0412,
            0.0215,
            -0.0718,
            -0.042,
            -0.0192,
            -0.0586,
            -0.1858,
            -0.0173
          ],
          "after": [
            -0.0081,
            -0.0905,
            -0.2594,
            0.0797,
            0.0237,
            -0.0254,
            -0.1301,
            0.0885,
            0.1158,
            -0.031,
            0.0084,
            0.1142,
            0.0233,
            0.0044,
            -0.1092,
            0.0562
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            0.0006,
            0.0003,
            0.0002,
            -0.0006,
            -0.0001,
            0.0004,
            0.0003,
            -0.0,
            0.0006,
            0.0007,
            0.0003,
            -0.0003,
            -0.0006,
            0.0008,
            -0.0003
          ],
          "after": [
            -0.0089,
            0.0185,
            -0.0505,
            -0.0212,
            0.0186,
            0.1357,
            0.0309,
            -0.1525,
            0.0229,
            0.0423,
            -0.0867,
            -0.0626,
            0.0975,
            -0.0419,
            0.0119,
            0.0547
          ]
        }
      }
    },
    {
      "step": 163,
      "word": "sevastian",
      "loss": 2.7189,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.117,
            0.0952,
            -0.0202,
            0.1163,
            -0.1006,
            -0.0015,
            0.0164,
            0.0923,
            -0.0018,
            -0.1182,
            -0.1746,
            0.2359,
            0.0008,
            -0.0465,
            0.0508,
            0.1232
          ],
          "after": [
            0.0013,
            0.0497,
            0.1272,
            -0.0195,
            0.0081,
            -0.0293,
            -0.0664,
            -0.1305,
            -0.0142,
            0.0298,
            0.1135,
            -0.0111,
            -0.07,
            -0.0015,
            -0.023,
            -0.1471
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0005,
            0.1425,
            0.1617,
            -0.0056,
            0.2915,
            -0.0122,
            -0.2098,
            -0.1112,
            0.1594,
            -0.056,
            0.0635,
            0.0744,
            -0.0508,
            0.0718,
            -0.1825,
            -0.0039
          ],
          "after": [
            0.0156,
            0.0926,
            -0.0025,
            -0.0645,
            0.0194,
            -0.0081,
            0.0723,
            0.0973,
            -0.0547,
            -0.0554,
            0.0349,
            0.0004,
            0.0954,
            0.0421,
            -0.0519,
            0.1821
          ]
        },
        "position_0": {
          "grad": [
            0.0064,
            0.0702,
            0.0085,
            0.0358,
            -0.1079,
            0.0661,
            -0.0167,
            0.0532,
            -0.0078,
            0.0445,
            -0.0659,
            0.0266,
            0.0431,
            0.0338,
            0.1007,
            -0.0346
          ],
          "after": [
            -0.0088,
            -0.091,
            -0.2596,
            0.0805,
            0.0239,
            -0.0257,
            -0.1292,
            0.0887,
            0.1155,
            -0.0311,
            0.0084,
            0.1142,
            0.0239,
            0.0038,
            -0.1095,
            0.0569
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            -0.0011,
            -0.0021,
            -0.0003,
            -0.0006,
            -0.0007,
            -0.0003,
            0.0016,
            -0.0006,
            0.001,
            0.0005,
            0.0002,
            -0.001,
            -0.0007,
            0.0013,
            -0.0008
          ],
          "after": [
            -0.0094,
            0.0192,
            -0.0497,
            -0.0229,
            0.021,
            0.136,
            0.0312,
            -0.1554,
            0.0234,
            0.04,
            -0.0876,
            -0.0646,
            0.0978,
            -0.0415,
            0.0101,
            0.0549
          ]
        }
      }
    },
    {
      "step": 164,
      "word": "emmanuelle",
      "loss": 2.6222,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0143,
            0.0219,
            -0.0632,
            -0.0173,
            -0.0677,
            -0.0398,
            -0.0181,
            -0.0526,
            0.0379,
            -0.0124,
            0.0108,
            -0.0102,
            -0.0251,
            0.0069,
            0.0194,
            -0.0401
          ],
          "after": [
            0.0016,
            0.0482,
            0.1273,
            -0.0201,
            0.0081,
            -0.0285,
            -0.0674,
            -0.1308,
            -0.0136,
            0.0308,
            0.1154,
            -0.0126,
            -0.0708,
            -0.002,
            -0.0227,
            -0.1483
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0904,
            -0.1226,
            0.3086,
            -0.1339,
            -0.2154,
            0.2958,
            -0.1565,
            -0.0351,
            0.1217,
            -0.0853,
            -0.12,
            -0.2833,
            -0.0249,
            0.2875,
            0.2478,
            -0.4713
          ],
          "after": [
            0.0155,
            0.092,
            -0.0029,
            -0.0643,
            0.0191,
            -0.0078,
            0.072,
            0.0974,
            -0.0541,
            -0.0562,
            0.0352,
            0.0012,
            0.0945,
            0.0418,
            -0.0511,
            0.1824
          ]
        },
        "position_0": {
          "grad": [
            0.0082,
            -0.0817,
            -0.0114,
            -0.0458,
            0.0024,
            0.0544,
            -0.1258,
            -0.0743,
            0.0903,
            0.0846,
            0.0065,
            -0.0149,
            -0.1001,
            -0.0242,
            0.0315,
            -0.1647
          ],
          "after": [
            -0.0094,
            -0.0911,
            -0.2596,
            0.0813,
            0.024,
            -0.0262,
            -0.1283,
            0.0891,
            0.1151,
            -0.0317,
            0.0084,
            0.1142,
            0.0247,
            0.0033,
            -0.1099,
            0.0578
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0011,
            -0.0011,
            0.0002,
            -0.0005,
            0.0002,
            -0.0003,
            0.0012,
            -0.0006,
            -0.0005,
            -0.0014,
            -0.0015,
            0.0005,
            0.0006,
            0.0003,
            -0.0007
          ],
          "after": [
            -0.01,
            0.0204,
            -0.0484,
            -0.0245,
            0.0232,
            0.1363,
            0.0315,
            -0.1583,
            0.024,
            0.0383,
            -0.0877,
            -0.0657,
            0.0978,
            -0.0414,
            0.0085,
            0.0554
          ]
        }
      }
    },
    {
      "step": 165,
      "word": "suan",
      "loss": 2.5378,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1877,
            -0.0803,
            -0.2826,
            0.153,
            -0.525,
            -0.0145,
            -0.1599,
            -0.089,
            0.1905,
            -0.0334,
            -0.1122,
            -0.2479,
            -0.0232,
            0.3427,
            0.1101,
            -0.087
          ],
          "after": [
            0.0013,
            0.0472,
            0.1282,
            -0.0212,
            0.0096,
            -0.0278,
            -0.0678,
            -0.1308,
            -0.0138,
            0.0319,
            0.1176,
            -0.013,
            -0.0715,
            -0.0036,
            -0.023,
            -0.1492
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0018,
            -0.0101,
            -0.0973,
            0.0099,
            -0.0018,
            -0.0926,
            0.0978,
            0.0335,
            -0.078,
            0.0824,
            -0.0045,
            -0.0115,
            0.0702,
            -0.0535,
            -0.0619,
            0.0518
          ],
          "after": [
            0.0154,
            0.0916,
            -0.003,
            -0.0642,
            0.0188,
            -0.0074,
            0.0715,
            0.0973,
            -0.0535,
            -0.0573,
            0.0354,
            0.0019,
            0.0937,
            0.0417,
            -0.0504,
            0.1826
          ]
        },
        "position_0": {
          "grad": [
            0.011,
            0.1138,
            0.0285,
            0.0864,
            -0.2435,
            0.1269,
            -0.0523,
            0.1232,
            0.0128,
            0.048,
            -0.1402,
            0.0091,
            0.1075,
            0.0976,
            0.2126,
            -0.0659
          ],
          "after": [
            -0.0099,
            -0.0916,
            -0.2598,
            0.0818,
            0.0249,
            -0.0272,
            -0.1273,
            0.0889,
            0.1147,
            -0.0325,
            0.0089,
            0.1142,
            0.0251,
            0.0026,
            -0.1109,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0015,
            0.0006,
            -0.0001,
            0.0017,
            -0.0004,
            -0.001,
            -0.0008,
            0.001,
            0.0008,
            0.0002,
            0.0003,
            0.0004,
            0.0004,
            -0.0012,
            0.0
          ],
          "after": [
            -0.0106,
            0.0206,
            -0.0476,
            -0.0258,
            0.0246,
            0.1366,
            0.0321,
            -0.1604,
            0.0242,
            0.0364,
            -0.0878,
            -0.0668,
            0.0977,
            -0.0415,
            0.0076,
            0.0558
          ]
        }
      }
    },
    {
      "step": 166,
      "word": "junius",
      "loss": 2.7042,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0011,
            0.0463,
            0.129,
            -0.0221,
            0.0109,
            -0.0271,
            -0.0682,
            -0.1308,
            -0.014,
            0.0328,
            0.1194,
            -0.0135,
            -0.072,
            -0.005,
            -0.0232,
            -0.1499
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0481,
            0.0479,
            -0.0959,
            0.002,
            0.0858,
            -0.1347,
            0.1673,
            0.0454,
            -0.1191,
            0.0833,
            -0.027,
            0.0157,
            0.0994,
            -0.0621,
            -0.1536,
            0.1007
          ],
          "after": [
            0.0155,
            0.091,
            -0.0028,
            -0.0641,
            0.0184,
            -0.0069,
            0.0709,
            0.0971,
            -0.0528,
            -0.0584,
            0.0358,
            0.0024,
            0.0927,
            0.0418,
            -0.0495,
            0.1826
          ]
        },
        "position_0": {
          "grad": [
            -0.0821,
            0.1059,
            0.0358,
            0.276,
            0.1508,
            -0.0618,
            0.2658,
            0.0776,
            -0.2946,
            -0.0822,
            0.0538,
            0.1526,
            0.406,
            -0.1695,
            -0.2077,
            0.2156
          ],
          "after": [
            -0.0101,
            -0.0924,
            -0.2601,
            0.0815,
            0.0252,
            -0.0278,
            -0.127,
            0.0885,
            0.1149,
            -0.0327,
            0.0092,
            0.1138,
            0.0246,
            0.0026,
            -0.111,
            0.0591
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0006,
            0.0004,
            -0.0007,
            0.0009,
            0.0003,
            0.0024,
            0.0006,
            -0.0008,
            -0.0006,
            -0.0004,
            0.001,
            0.0003,
            -0.0011,
            -0.0003,
            0.0002
          ],
          "after": [
            -0.0103,
            0.0205,
            -0.0471,
            -0.0266,
            0.0255,
            0.1368,
            0.0319,
            -0.1625,
            0.0246,
            0.0352,
            -0.0877,
            -0.068,
            0.0975,
            -0.0411,
            0.0069,
            0.0561
          ]
        }
      }
    },
    {
      "step": 167,
      "word": "amena",
      "loss": 2.194,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.4132,
            0.1517,
            -0.1634,
            0.1019,
            -0.0352,
            -0.0114,
            0.3126,
            0.2182,
            -0.0211,
            0.0897,
            0.0645,
            -0.4265,
            0.0233,
            0.0648,
            0.0435,
            -0.2039
          ],
          "after": [
            -0.0001,
            0.0451,
            0.1302,
            -0.0232,
            0.012,
            -0.0265,
            -0.0693,
            -0.1316,
            -0.0141,
            0.0332,
            0.1207,
            -0.0127,
            -0.0726,
            -0.0064,
            -0.0236,
            -0.1501
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0014,
            0.1031,
            0.326,
            0.0771,
            0.0197,
            0.2658,
            -0.0173,
            -0.015,
            0.3299,
            -0.0552,
            0.2458,
            0.3058,
            -0.495,
            -0.1209,
            0.279,
            -0.1891
          ],
          "after": [
            0.0156,
            0.0903,
            -0.0035,
            -0.0642,
            0.0181,
            -0.0069,
            0.0704,
            0.0969,
            -0.0527,
            -0.0592,
            0.035,
            0.0022,
            0.093,
            0.0421,
            -0.0492,
            0.183
          ]
        },
        "position_0": {
          "grad": [
            0.1262,
            -0.1035,
            0.0337,
            -0.3154,
            -0.146,
            0.0483,
            -0.4345,
            -0.1109,
            0.3496,
            -0.0085,
            0.2232,
            -0.0435,
            -0.3359,
            0.0932,
            0.134,
            -0.3313
          ],
          "after": [
            -0.0107,
            -0.0927,
            -0.2606,
            0.082,
            0.0259,
            -0.0285,
            -0.126,
            0.0885,
            0.1145,
            -0.0328,
            0.0086,
            0.1135,
            0.0248,
            0.0022,
            -0.1116,
            0.0601
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0004,
            0.0009,
            0.0,
            0.0006,
            0.0002,
            -0.0002,
            -0.0008,
            -0.0005,
            -0.0003,
            0.0005,
            0.0001,
            -0.0006,
            -0.0002,
            -0.0004,
            -0.0011
          ],
          "after": [
            -0.0103,
            0.0202,
            -0.0473,
            -0.0273,
            0.026,
            0.1369,
            0.0319,
            -0.1639,
            0.0252,
            0.0342,
            -0.088,
            -0.0692,
            0.0975,
            -0.0406,
            0.0064,
            0.0568
          ]
        }
      }
    },
    {
      "step": 168,
      "word": "sheldon",
      "loss": 2.6156,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0012,
            0.044,
            0.1312,
            -0.0241,
            0.013,
            -0.026,
            -0.0702,
            -0.1323,
            -0.0142,
            0.0336,
            0.1218,
            -0.012,
            -0.0731,
            -0.0076,
            -0.0239,
            -0.1502
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0617,
            0.0873,
            0.2133,
            0.0608,
            0.0136,
            0.0821,
            -0.0312,
            0.0322,
            0.0764,
            0.1364,
            0.2042,
            0.2461,
            -0.2052,
            -0.0911,
            0.141,
            -0.1809
          ],
          "after": [
            0.0158,
            0.0895,
            -0.0046,
            -0.0646,
            0.0177,
            -0.007,
            0.07,
            0.0966,
            -0.0528,
            -0.0603,
            0.0335,
            0.0015,
            0.0937,
            0.0426,
            -0.0491,
            0.1836
          ]
        },
        "position_0": {
          "grad": [
            -0.0021,
            0.0807,
            0.0231,
            0.0889,
            -0.1376,
            0.0806,
            0.0002,
            0.0805,
            -0.0087,
            0.0227,
            -0.083,
            0.021,
            0.0813,
            0.0615,
            0.1349,
            -0.0108
          ],
          "after": [
            -0.0113,
            -0.0932,
            -0.261,
            0.0823,
            0.027,
            -0.0295,
            -0.1251,
            0.0883,
            0.1141,
            -0.033,
            0.0084,
            0.1132,
            0.0249,
            0.0017,
            -0.1125,
            0.061
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0003,
            -0.0001,
            0.0003,
            -0.0002,
            0.0001,
            0.0004,
            -0.0001,
            -0.0004,
            0.0006,
            0.0006,
            0.0001,
            -0.0,
            -0.0005,
            0.0003,
            -0.0
          ],
          "after": [
            -0.0103,
            0.0198,
            -0.0473,
            -0.028,
            0.0266,
            0.1369,
            0.0317,
            -0.1651,
            0.0259,
            0.0331,
            -0.0885,
            -0.0702,
            0.0975,
            -0.0401,
            0.006,
            0.0575
          ]
        }
      }
    },
    {
      "step": 169,
      "word": "fredrick",
      "loss": 3.0126,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0021,
            0.0431,
            0.1321,
            -0.0249,
            0.0139,
            -0.0256,
            -0.0709,
            -0.1329,
            -0.0143,
            0.0338,
            0.1228,
            -0.0115,
            -0.0735,
            -0.0087,
            -0.0241,
            -0.1504
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0928,
            0.0349,
            0.1247,
            0.0818,
            0.2255,
            0.1822,
            -0.0734,
            0.0486,
            0.1125,
            -0.0383,
            0.0806,
            0.2675,
            -0.3048,
            -0.0584,
            0.1279,
            0.0184
          ],
          "after": [
            0.0162,
            0.0887,
            -0.0058,
            -0.0652,
            0.017,
            -0.0074,
            0.0698,
            0.0961,
            -0.0531,
            -0.0612,
            0.0319,
            0.0003,
            0.0949,
            0.0432,
            -0.0492,
            0.1841
          ]
        },
        "position_0": {
          "grad": [
            -0.0822,
            0.2611,
            -0.0349,
            0.208,
            0.2423,
            -0.1389,
            0.2701,
            -0.052,
            -0.3328,
            -0.0376,
            0.0824,
            0.2196,
            0.2303,
            -0.1029,
            -0.183,
            0.2355
          ],
          "after": [
            -0.0114,
            -0.0944,
            -0.2613,
            0.082,
            0.0271,
            -0.0297,
            -0.1249,
            0.0883,
            0.1144,
            -0.033,
            0.008,
            0.1124,
            0.0244,
            0.0016,
            -0.1127,
            0.0612
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0006,
            -0.001,
            0.0011,
            -0.0026,
            -0.0006,
            -0.0013,
            0.0018,
            0.0001,
            0.0016,
            -0.0008,
            -0.0013,
            -0.0003,
            0.0003,
            0.0012,
            -0.0007
          ],
          "after": [
            -0.0105,
            0.0198,
            -0.0468,
            -0.0291,
            0.0278,
            0.1372,
            0.0319,
            -0.1669,
            0.0264,
            0.0315,
            -0.0885,
            -0.0706,
            0.0977,
            -0.0397,
            0.0051,
            0.0584
          ]
        }
      }
    },
    {
      "step": 170,
      "word": "finn",
      "loss": 2.5068,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0029,
            0.0423,
            0.1328,
            -0.0256,
            0.0146,
            -0.0252,
            -0.0716,
            -0.1335,
            -0.0143,
            0.0341,
            0.1236,
            -0.011,
            -0.0738,
            -0.0095,
            -0.0244,
            -0.1505
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.082,
            0.0237,
            -0.1158,
            0.0449,
            0.0175,
            -0.1043,
            0.2206,
            0.0318,
            -0.1148,
            0.0946,
            -0.035,
            0.0076,
            0.0969,
            -0.0786,
            -0.1168,
            0.1143
          ],
          "after": [
            0.0168,
            0.088,
            -0.0065,
            -0.0658,
            0.0164,
            -0.0076,
            0.0693,
            0.0956,
            -0.0532,
            -0.0622,
            0.0307,
            -0.0008,
            0.0957,
            0.0439,
            -0.0491,
            0.1843
          ]
        },
        "position_0": {
          "grad": [
            -0.1046,
            0.3594,
            -0.0447,
            0.3526,
            0.3228,
            -0.2614,
            0.3601,
            -0.0513,
            -0.4963,
            -0.1299,
            0.0777,
            0.265,
            0.3811,
            -0.0818,
            -0.2638,
            0.3564
          ],
          "after": [
            -0.0111,
            -0.0964,
            -0.2613,
            0.0809,
            0.0262,
            -0.0287,
            -0.1253,
            0.0884,
            0.1156,
            -0.0322,
            0.0073,
            0.111,
            0.0233,
            0.0018,
            -0.1121,
            0.0606
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0018,
            -0.001,
            -0.0006,
            -0.001,
            0.0021,
            -0.0011,
            0.0026,
            0.0021,
            0.0016,
            0.0021,
            0.0016,
            0.0033,
            -0.0008,
            -0.0032,
            0.0006,
            0.0012
          ],
          "after": [
            -0.0097,
            0.0202,
            -0.046,
            -0.0296,
            0.0282,
            0.1379,
            0.0314,
            -0.1691,
            0.0262,
            0.0293,
            -0.0894,
            -0.072,
            0.0981,
            -0.0382,
            0.0041,
            0.0586
          ]
        }
      }
    },
    {
      "step": 171,
      "word": "havik",
      "loss": 2.6535,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0406,
            0.0595,
            -0.2646,
            0.1886,
            0.2471,
            -0.0115,
            -0.0226,
            -0.0164,
            -0.1874,
            0.0221,
            -0.1339,
            -0.176,
            0.3296,
            -0.0596,
            -0.1961,
            0.3597
          ],
          "after": [
            -0.0034,
            0.0414,
            0.1342,
            -0.0268,
            0.0145,
            -0.0248,
            -0.0721,
            -0.1338,
            -0.0136,
            0.0342,
            0.1248,
            -0.0101,
            -0.0752,
            -0.0101,
            -0.0238,
            -0.1513
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0428,
            0.0348,
            -0.089,
            0.05,
            0.0513,
            -0.1079,
            0.2065,
            0.0218,
            -0.1879,
            0.0646,
            -0.0558,
            -0.0335,
            0.1084,
            -0.0658,
            -0.1568,
            0.1162
          ],
          "after": [
            0.0175,
            0.0872,
            -0.0069,
            -0.0666,
            0.0157,
            -0.0075,
            0.0686,
            0.095,
            -0.0529,
            -0.0633,
            0.0299,
            -0.0016,
            0.0961,
            0.0447,
            -0.0489,
            0.1843
          ]
        },
        "position_0": {
          "grad": [
            0.0109,
            -0.0525,
            0.0634,
            0.0856,
            -0.1212,
            0.0865,
            0.0519,
            0.1097,
            0.1006,
            0.0486,
            -0.1505,
            -0.1991,
            0.2177,
            0.1556,
            0.0768,
            0.1417
          ],
          "after": [
            -0.0108,
            -0.098,
            -0.2615,
            0.0798,
            0.0257,
            -0.0282,
            -0.1258,
            0.0882,
            0.1164,
            -0.0318,
            0.0073,
            0.1104,
            0.0219,
            0.0015,
            -0.1118,
            0.0599
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0017,
            0.0035,
            -0.0052,
            0.0068,
            -0.0061,
            0.0028,
            -0.003,
            -0.0015,
            0.0,
            0.0005,
            -0.0026,
            0.0017,
            -0.0014,
            -0.0028,
            -0.0034
          ],
          "after": [
            -0.009,
            0.0198,
            -0.047,
            -0.0279,
            0.0267,
            0.1406,
            0.0303,
            -0.1698,
            0.0266,
            0.0274,
            -0.0905,
            -0.0723,
            0.0978,
            -0.0364,
            0.0043,
            0.0602
          ]
        }
      }
    },
    {
      "step": 172,
      "word": "healani",
      "loss": 2.2775,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0396,
            0.1351,
            -0.0928,
            -0.1346,
            -0.0261,
            -0.0263,
            -0.1802,
            0.0444,
            0.0009,
            -0.0588,
            -0.088,
            0.1594,
            -0.1768,
            0.0186,
            0.0635,
            -0.3146
          ],
          "after": [
            -0.0038,
            0.0402,
            0.1357,
            -0.0274,
            0.0145,
            -0.0244,
            -0.0721,
            -0.1343,
            -0.0131,
            0.0346,
            0.1262,
            -0.0098,
            -0.0758,
            -0.0106,
            -0.0235,
            -0.1514
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0667,
            -0.0889,
            0.2468,
            -0.1714,
            0.2783,
            -0.0195,
            -0.3293,
            -0.1023,
            0.312,
            0.0466,
            0.1866,
            0.1613,
            -0.2249,
            0.0374,
            -0.001,
            -0.2191
          ],
          "after": [
            0.0178,
            0.0869,
            -0.0079,
            -0.0666,
            0.0147,
            -0.0075,
            0.0684,
            0.095,
            -0.0532,
            -0.0644,
            0.0285,
            -0.0027,
            0.097,
            0.0453,
            -0.0486,
            0.1847
          ]
        },
        "position_0": {
          "grad": [
            0.0101,
            -0.0707,
            0.0345,
            0.0023,
            -0.1281,
            0.0564,
            -0.0298,
            0.0858,
            0.1257,
            0.0611,
            -0.1309,
            -0.1631,
            0.1064,
            0.0975,
            0.0649,
            0.0378
          ],
          "after": [
            -0.0107,
            -0.0991,
            -0.2619,
            0.0788,
            0.0258,
            -0.0281,
            -0.1261,
            0.0876,
            0.1169,
            -0.0318,
            0.0077,
            0.1102,
            0.0205,
            0.0009,
            -0.1117,
            0.0591
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0016,
            -0.0005,
            -0.0008,
            -0.0006,
            0.0033,
            -0.0034,
            -0.0014,
            0.0006,
            -0.0005,
            -0.0015,
            0.0008,
            -0.0032,
            0.0032,
            0.0027,
            -0.0014
          ],
          "after": [
            -0.0085,
            0.0202,
            -0.0477,
            -0.0261,
            0.0256,
            0.1416,
            0.0303,
            -0.1698,
            0.0268,
            0.0259,
            -0.0905,
            -0.0729,
            0.0986,
            -0.0361,
            0.0035,
            0.0622
          ]
        }
      }
    },
    {
      "step": 173,
      "word": "elira",
      "loss": 2.266,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.4722,
            0.1058,
            -0.2181,
            -0.0668,
            -0.0595,
            0.0553,
            0.0252,
            -0.0759,
            -0.1681,
            0.0743,
            0.0466,
            -0.4385,
            0.2386,
            0.2001,
            0.0376,
            -0.2514
          ],
          "after": [
            -0.0053,
            0.0387,
            0.1376,
            -0.0276,
            0.0147,
            -0.0243,
            -0.0722,
            -0.1345,
            -0.012,
            0.0345,
            0.1273,
            -0.0084,
            -0.077,
            -0.0118,
            -0.0234,
            -0.1509
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.122,
            0.2965,
            0.6331,
            -0.0944,
            0.1381,
            -0.1109,
            0.2765,
            -0.1128,
            -0.042,
            -0.0905,
            0.0696,
            -0.1201,
            -0.009,
            0.0109,
            -0.1867,
            -0.1867
          ],
          "after": [
            0.0185,
            0.0858,
            -0.01,
            -0.0663,
            0.0135,
            -0.0072,
            0.0679,
            0.0955,
            -0.0533,
            -0.065,
            0.027,
            -0.0033,
            0.0978,
            0.0457,
            -0.0481,
            0.1854
          ]
        },
        "position_0": {
          "grad": [
            0.0302,
            -0.2249,
            -0.0022,
            -0.1066,
            -0.0946,
            0.085,
            -0.3038,
            -0.1015,
            0.2274,
            0.131,
            0.0058,
            -0.1095,
            -0.2245,
            -0.0063,
            0.1083,
            -0.3648
          ],
          "after": [
            -0.0107,
            -0.0994,
            -0.2623,
            0.0783,
            0.0261,
            -0.0283,
            -0.1259,
            0.0875,
            0.1168,
            -0.0326,
            0.008,
            0.1104,
            0.0197,
            0.0004,
            -0.112,
            0.0592
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            -0.0,
            0.0031,
            -0.0008,
            0.0046,
            0.0008,
            -0.0021,
            -0.0018,
            -0.0012,
            -0.0016,
            0.0009,
            0.0015,
            -0.0026,
            0.0,
            -0.0017,
            -0.0038
          ],
          "after": [
            -0.0087,
            0.0205,
            -0.0495,
            -0.0243,
            0.0236,
            0.1422,
            0.0307,
            -0.1693,
            0.0274,
            0.0254,
            -0.091,
            -0.0738,
            0.1001,
            -0.0359,
            0.0035,
            0.0652
          ]
        }
      }
    },
    {
      "step": 174,
      "word": "azzam",
      "loss": 3.0167,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.283,
            0.3388,
            0.1047,
            0.8024,
            0.4293,
            -0.1107,
            0.9837,
            0.3536,
            0.1047,
            -0.0797,
            -0.1874,
            0.4956,
            0.0159,
            -0.2404,
            -0.2902,
            0.7086
          ],
          "after": [
            -0.0058,
            0.0364,
            0.1389,
            -0.0299,
            0.0136,
            -0.0236,
            -0.0741,
            -0.1359,
            -0.0114,
            0.0348,
            0.1289,
            -0.0085,
            -0.0781,
            -0.0119,
            -0.0223,
            -0.1519
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0451,
            0.0209,
            -0.1159,
            0.0385,
            0.0562,
            -0.0874,
            0.1353,
            0.019,
            -0.157,
            0.0559,
            -0.025,
            -0.0274,
            0.1236,
            -0.0347,
            -0.1927,
            0.0673
          ],
          "after": [
            0.0192,
            0.0848,
            -0.0116,
            -0.0662,
            0.0124,
            -0.0069,
            0.0673,
            0.0959,
            -0.0532,
            -0.0657,
            0.0258,
            -0.0038,
            0.0982,
            0.0462,
            -0.0475,
            0.1858
          ]
        },
        "position_0": {
          "grad": [
            0.1211,
            -0.0381,
            0.0701,
            -0.1694,
            -0.0301,
            0.0803,
            -0.2801,
            -0.1179,
            0.2177,
            -0.0291,
            0.2655,
            0.0495,
            -0.2521,
            0.0894,
            0.0967,
            -0.2085
          ],
          "after": [
            -0.0112,
            -0.0996,
            -0.2629,
            0.0782,
            0.0265,
            -0.0289,
            -0.1251,
            0.0879,
            0.1164,
            -0.0331,
            0.0074,
            0.1105,
            0.0196,
            -0.0003,
            -0.1125,
            0.0598
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0001,
            -0.0006,
            0.0001,
            0.0007,
            -0.0004,
            0.0006,
            0.0004,
            -0.0009,
            -0.0009,
            -0.0009,
            -0.0004,
            0.0015,
            0.0003,
            -0.0008,
            0.0004
          ],
          "after": [
            -0.0088,
            0.0207,
            -0.0509,
            -0.0228,
            0.0217,
            0.1429,
            0.0309,
            -0.1689,
            0.0282,
            0.0254,
            -0.091,
            -0.0745,
            0.1009,
            -0.0358,
            0.0038,
            0.0676
          ]
        }
      }
    },
    {
      "step": 175,
      "word": "jeancarlos",
      "loss": 2.7121,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0,
            -0.1805,
            -0.1105,
            0.0442,
            0.0626,
            0.1136,
            -0.0055,
            -0.0822,
            0.1371,
            -0.0883,
            -0.095,
            0.1618,
            -0.1913,
            -0.0865,
            0.0582,
            0.2503
          ],
          "after": [
            -0.0063,
            0.0351,
            0.1403,
            -0.032,
            0.0126,
            -0.0236,
            -0.0758,
            -0.1368,
            -0.0115,
            0.0355,
            0.1307,
            -0.0089,
            -0.0785,
            -0.0117,
            -0.0215,
            -0.1532
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1029,
            -0.0872,
            0.1226,
            0.0133,
            0.2474,
            0.0318,
            -0.219,
            -0.0745,
            0.1934,
            0.001,
            0.1446,
            0.1134,
            -0.2257,
            -0.0596,
            -0.0807,
            -0.0381
          ],
          "after": [
            0.0194,
            0.0842,
            -0.0132,
            -0.0661,
            0.011,
            -0.0066,
            0.0671,
            0.0965,
            -0.0535,
            -0.0663,
            0.0242,
            -0.0045,
            0.099,
            0.0467,
            -0.0468,
            0.1863
          ]
        },
        "position_0": {
          "grad": [
            -0.0527,
            0.0725,
            -0.0119,
            0.1066,
            0.0644,
            -0.0564,
            0.1365,
            0.0427,
            -0.1496,
            -0.0154,
            0.0131,
            0.0973,
            0.203,
            -0.1313,
            -0.135,
            0.0984
          ],
          "after": [
            -0.0114,
            -0.0999,
            -0.2633,
            0.0779,
            0.0266,
            -0.0291,
            -0.1248,
            0.0881,
            0.1163,
            -0.0334,
            0.0069,
            0.1102,
            0.0191,
            -0.0005,
            -0.1125,
            0.0601
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0012,
            -0.0015,
            -0.0003,
            -0.0003,
            0.0005,
            -0.001,
            0.0009,
            0.0004,
            0.0002,
            -0.0004,
            -0.0005,
            -0.0008,
            0.0004,
            0.0013,
            -0.0004
          ],
          "after": [
            -0.0092,
            0.0214,
            -0.0513,
            -0.0214,
            0.0202,
            0.1433,
            0.0313,
            -0.1689,
            0.0288,
            0.0253,
            -0.0907,
            -0.0749,
            0.1019,
            -0.0358,
            0.0035,
            0.0698
          ]
        }
      }
    },
    {
      "step": 176,
      "word": "talayeh",
      "loss": 2.4366,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1197,
            0.115,
            0.0284,
            -0.2035,
            0.1433,
            0.1151,
            -0.1184,
            0.0987,
            -0.1104,
            0.082,
            -0.1053,
            0.4057,
            -0.1218,
            -0.0191,
            0.0268,
            -0.1429
          ],
          "after": [
            -0.0064,
            0.0336,
            0.1414,
            -0.0332,
            0.0113,
            -0.0242,
            -0.077,
            -0.1379,
            -0.0111,
            0.0357,
            0.1326,
            -0.0102,
            -0.0784,
            -0.0114,
            -0.021,
            -0.154
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2012,
            0.0031,
            -0.1334,
            -0.0936,
            -0.4922,
            0.1183,
            -0.3067,
            -0.0415,
            0.2084,
            -0.0832,
            0.0202,
            -0.2872,
            0.0831,
            0.2752,
            0.192,
            -0.1215
          ],
          "after": [
            0.0191,
            0.0837,
            -0.0142,
            -0.0658,
            0.0107,
            -0.0066,
            0.0673,
            0.0973,
            -0.054,
            -0.0666,
            0.0228,
            -0.0044,
            0.0995,
            0.0465,
            -0.0464,
            0.1869
          ]
        },
        "position_0": {
          "grad": [
            0.027,
            -0.1471,
            0.0194,
            0.0558,
            -0.0924,
            0.0537,
            0.0652,
            0.009,
            -0.0855,
            -0.0805,
            -0.1619,
            -0.257,
            0.097,
            0.0528,
            0.0569,
            0.0871
          ],
          "after": [
            -0.0116,
            -0.0998,
            -0.2638,
            0.0774,
            0.027,
            -0.0295,
            -0.1246,
            0.0882,
            0.1164,
            -0.0332,
            0.0069,
            0.1107,
            0.0184,
            -0.0008,
            -0.1127,
            0.0601
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0014,
            0.0009,
            -0.0021,
            0.0002,
            0.0005,
            -0.0036,
            0.0017,
            0.0026,
            -0.0009,
            -0.0007,
            0.0013,
            -0.0025,
            0.0002,
            0.0003,
            -0.0001
          ],
          "after": [
            -0.0089,
            0.0227,
            -0.0521,
            -0.0194,
            0.0188,
            0.1434,
            0.0325,
            -0.1695,
            0.0282,
            0.0257,
            -0.0901,
            -0.0756,
            0.1034,
            -0.0359,
            0.0032,
            0.0718
          ]
        }
      }
    },
    {
      "step": 177,
      "word": "spurgeon",
      "loss": 3.1265,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0065,
            0.0323,
            0.1424,
            -0.0342,
            0.0102,
            -0.0247,
            -0.0781,
            -0.1389,
            -0.0108,
            0.0359,
            0.1343,
            -0.0114,
            -0.0783,
            -0.0112,
            -0.0205,
            -0.1547
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1504,
            -0.1333,
            0.0713,
            0.0001,
            -0.3694,
            -0.0644,
            -0.0968,
            0.0212,
            0.1052,
            -0.0315,
            0.0218,
            -0.1908,
            0.0412,
            0.1811,
            0.1563,
            -0.125
          ],
          "after": [
            0.0184,
            0.0836,
            -0.0153,
            -0.0654,
            0.0112,
            -0.0065,
            0.0677,
            0.0978,
            -0.0547,
            -0.0667,
            0.0215,
            -0.0039,
            0.0998,
            0.0459,
            -0.0464,
            0.1877
          ]
        },
        "position_0": {
          "grad": [
            -0.0137,
            0.1227,
            0.0182,
            0.1177,
            -0.0252,
            0.0968,
            0.0698,
            0.0444,
            -0.0728,
            0.0224,
            -0.0378,
            0.0904,
            0.1155,
            0.0243,
            0.0716,
            0.0576
          ],
          "after": [
            -0.0118,
            -0.1,
            -0.2643,
            0.0768,
            0.0275,
            -0.0303,
            -0.1245,
            0.0881,
            0.1166,
            -0.0332,
            0.0071,
            0.1109,
            0.0177,
            -0.0011,
            -0.1131,
            0.06
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            -0.0002,
            -0.0009,
            0.0002,
            -0.0008,
            -0.0001,
            -0.0007,
            -0.0006,
            0.0005,
            0.0,
            -0.0008,
            -0.0008,
            -0.0005,
            0.0017,
            0.0014,
            -0.0006
          ],
          "after": [
            -0.0093,
            0.0238,
            -0.0523,
            -0.0178,
            0.0179,
            0.1436,
            0.0337,
            -0.1698,
            0.0276,
            0.0259,
            -0.0892,
            -0.076,
            0.1049,
            -0.0366,
            0.0025,
            0.0736
          ]
        }
      }
    },
    {
      "step": 178,
      "word": "jermany",
      "loss": 2.2757,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0446,
            -0.0276,
            -0.0152,
            -0.0034,
            -0.0249,
            -0.0234,
            -0.0415,
            -0.0653,
            0.0525,
            -0.1374,
            -0.0319,
            0.0761,
            -0.1035,
            -0.0185,
            0.0205,
            -0.0175
          ],
          "after": [
            -0.0064,
            0.0313,
            0.1433,
            -0.0351,
            0.0093,
            -0.025,
            -0.0788,
            -0.1394,
            -0.0107,
            0.0367,
            0.1358,
            -0.0125,
            -0.0778,
            -0.011,
            -0.0202,
            -0.1553
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1586,
            -0.1375,
            0.119,
            0.0157,
            0.2757,
            0.0458,
            -0.3313,
            -0.0809,
            0.2467,
            0.0468,
            0.164,
            0.0951,
            -0.2568,
            -0.0418,
            -0.0886,
            -0.0983
          ],
          "after": [
            0.0174,
            0.0839,
            -0.0165,
            -0.0652,
            0.0111,
            -0.0065,
            0.0684,
            0.0986,
            -0.0557,
            -0.0669,
            0.0197,
            -0.0037,
            0.1007,
            0.0454,
            -0.0462,
            0.1885
          ]
        },
        "position_0": {
          "grad": [
            -0.0576,
            0.0984,
            -0.013,
            0.1552,
            0.0984,
            -0.0772,
            0.2026,
            0.0571,
            -0.2173,
            -0.0329,
            0.0264,
            0.1169,
            0.296,
            -0.1621,
            -0.1904,
            0.1539
          ],
          "after": [
            -0.0117,
            -0.1005,
            -0.2647,
            0.0759,
            0.0275,
            -0.0306,
            -0.1249,
            0.0878,
            0.1172,
            -0.033,
            0.0072,
            0.1107,
            0.0164,
            -0.0009,
            -0.1129,
            0.0596
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0004,
            -0.0003,
            0.0007,
            -0.0009,
            0.0005,
            -0.001,
            -0.0004,
            -0.0,
            0.0006,
            0.0,
            -0.0003,
            -0.0003,
            0.0007,
            0.0003,
            0.0003
          ],
          "after": [
            -0.01,
            0.025,
            -0.0524,
            -0.0167,
            0.0173,
            0.1435,
            0.0349,
            -0.1699,
            0.0271,
            0.0259,
            -0.0884,
            -0.0762,
            0.1063,
            -0.0375,
            0.0017,
            0.0751
          ]
        }
      }
    },
    {
      "step": 179,
      "word": "mialani",
      "loss": 2.1581,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0733,
            0.1139,
            -0.0802,
            -0.1326,
            -0.0167,
            -0.0253,
            -0.1771,
            0.0522,
            0.0105,
            -0.0691,
            -0.0829,
            0.192,
            -0.1935,
            -0.0056,
            0.0641,
            -0.2922
          ],
          "after": [
            -0.0062,
            0.03,
            0.1443,
            -0.0355,
            0.0086,
            -0.0251,
            -0.0792,
            -0.1401,
            -0.0107,
            0.0376,
            0.1375,
            -0.0139,
            -0.0769,
            -0.0107,
            -0.0202,
            -0.1552
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0613,
            0.0337,
            -0.0557,
            0.0186,
            0.0618,
            -0.0927,
            0.1481,
            -0.0175,
            -0.1351,
            0.0606,
            -0.0008,
            -0.0006,
            0.0729,
            -0.0692,
            -0.132,
            0.0955
          ],
          "after": [
            0.0167,
            0.084,
            -0.0174,
            -0.0651,
            0.0109,
            -0.0063,
            0.0689,
            0.0994,
            -0.0564,
            -0.0673,
            0.0182,
            -0.0035,
            0.1012,
            0.0452,
            -0.0459,
            0.1891
          ]
        },
        "position_0": {
          "grad": [
            -0.0167,
            0.0067,
            0.0063,
            -0.0208,
            0.0479,
            0.0297,
            0.0648,
            0.0885,
            0.1195,
            0.0505,
            0.0098,
            0.0606,
            -0.2002,
            -0.0688,
            0.0277,
            -0.0249
          ],
          "after": [
            -0.0116,
            -0.1009,
            -0.265,
            0.0752,
            0.0274,
            -0.0311,
            -0.1253,
            0.0872,
            0.1175,
            -0.0331,
            0.0072,
            0.1104,
            0.0158,
            -0.0005,
            -0.1128,
            0.0593
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            -0.0029,
            -0.0002,
            0.0019,
            -0.0035,
            0.0025,
            -0.007,
            0.0019,
            0.0012,
            -0.0008,
            -0.0014,
            0.0017,
            -0.0041,
            0.002,
            0.0035,
            -0.0002
          ],
          "after": [
            -0.0109,
            0.0271,
            -0.0524,
            -0.0166,
            0.0177,
            0.1427,
            0.0373,
            -0.1707,
            0.0261,
            0.0262,
            -0.087,
            -0.077,
            0.1087,
            -0.0389,
            0.0,
            0.0765
          ]
        }
      }
    },
    {
      "step": 180,
      "word": "amaury",
      "loss": 2.3998,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0126,
            0.1399,
            0.044,
            0.1577,
            0.2324,
            -0.1658,
            0.0186,
            0.1699,
            0.1249,
            -0.0629,
            0.022,
            0.0637,
            -0.1684,
            -0.239,
            -0.142,
            0.0203
          ],
          "after": [
            -0.006,
            0.0285,
            0.145,
            -0.0362,
            0.0073,
            -0.0245,
            -0.0795,
            -0.1413,
            -0.0111,
            0.0387,
            0.1388,
            -0.0152,
            -0.0755,
            -0.0097,
            -0.0196,
            -0.1552
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0311,
            0.0332,
            -0.055,
            0.0187,
            0.0715,
            -0.1214,
            0.1221,
            0.0064,
            -0.1465,
            0.0822,
            -0.0054,
            0.005,
            0.0986,
            -0.0702,
            -0.1644,
            0.08
          ],
          "after": [
            0.0162,
            0.084,
            -0.018,
            -0.0651,
            0.0106,
            -0.006,
            0.0691,
            0.1001,
            -0.0566,
            -0.068,
            0.017,
            -0.0034,
            0.1015,
            0.0452,
            -0.0454,
            0.1894
          ]
        },
        "position_0": {
          "grad": [
            0.0833,
            -0.0675,
            0.0332,
            -0.2928,
            -0.0556,
            0.0605,
            -0.325,
            -0.1297,
            0.2351,
            0.0282,
            0.2294,
            0.0566,
            -0.3059,
            0.0238,
            0.0738,
            -0.2784
          ],
          "after": [
            -0.0118,
            -0.1011,
            -0.2655,
            0.0753,
            0.0275,
            -0.0317,
            -0.125,
            0.0872,
            0.1173,
            -0.0334,
            0.0065,
            0.11,
            0.0158,
            -0.0002,
            -0.1129,
            0.0597
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0006,
            0.0013,
            0.0007,
            -0.0001,
            0.0007,
            0.001,
            -0.0004,
            -0.0003,
            0.0007,
            0.0007,
            0.001,
            -0.0004,
            -0.0008,
            -0.0009,
            -0.0002
          ],
          "after": [
            -0.0115,
            0.0287,
            -0.0529,
            -0.0167,
            0.018,
            0.1417,
            0.0392,
            -0.1712,
            0.0255,
            0.0262,
            -0.0862,
            -0.078,
            0.1108,
            -0.0398,
            -0.0012,
            0.0777
          ]
        }
      }
    },
    {
      "step": 181,
      "word": "elmina",
      "loss": 2.1679,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2217,
            0.1993,
            -0.0597,
            -0.1323,
            -0.1285,
            0.0093,
            0.022,
            -0.0879,
            -0.0687,
            0.1488,
            0.1433,
            -0.2498,
            0.1807,
            0.1219,
            -0.0118,
            -0.1996
          ],
          "after": [
            -0.0063,
            0.0266,
            0.1457,
            -0.0365,
            0.0067,
            -0.0239,
            -0.0798,
            -0.142,
            -0.0113,
            0.039,
            0.1394,
            -0.0158,
            -0.075,
            -0.0093,
            -0.0191,
            -0.1548
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1336,
            0.2637,
            0.5268,
            -0.0565,
            0.144,
            -0.1076,
            0.2677,
            -0.079,
            -0.0744,
            -0.0509,
            0.0409,
            -0.0774,
            0.0375,
            -0.0014,
            -0.193,
            -0.1137
          ],
          "after": [
            0.0161,
            0.0834,
            -0.0196,
            -0.0648,
            0.0101,
            -0.0055,
            0.0689,
            0.101,
            -0.0567,
            -0.0684,
            0.0157,
            -0.0031,
            0.1016,
            0.0452,
            -0.0446,
            0.1899
          ]
        },
        "position_0": {
          "grad": [
            0.021,
            -0.19,
            0.0071,
            -0.091,
            -0.0609,
            0.0797,
            -0.252,
            -0.096,
            0.1903,
            0.1179,
            0.0289,
            -0.0669,
            -0.1925,
            -0.0129,
            0.088,
            -0.3071
          ],
          "after": [
            -0.0121,
            -0.1007,
            -0.2659,
            0.0756,
            0.0277,
            -0.0325,
            -0.1244,
            0.0876,
            0.1168,
            -0.0342,
            0.0057,
            0.1098,
            0.0163,
            0.0,
            -0.1132,
            0.0606
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0028,
            0.0027,
            -0.0012,
            -0.006,
            -0.0006,
            -0.0061,
            -0.0043,
            0.0001,
            0.0002,
            0.0026,
            -0.0028,
            -0.0005,
            0.0032,
            0.001,
            -0.0021
          ],
          "after": [
            -0.0124,
            0.0289,
            -0.0544,
            -0.0163,
            0.0196,
            0.141,
            0.0419,
            -0.1702,
            0.0248,
            0.026,
            -0.0868,
            -0.0779,
            0.1127,
            -0.0417,
            -0.0025,
            0.0795
          ]
        }
      }
    },
    {
      "step": 182,
      "word": "korrin",
      "loss": 2.1327,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0067,
            0.0249,
            0.1464,
            -0.0367,
            0.0061,
            -0.0235,
            -0.08,
            -0.1426,
            -0.0114,
            0.0392,
            0.1398,
            -0.0162,
            -0.0745,
            -0.0089,
            -0.0187,
            -0.1545
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0614,
            0.0523,
            -0.0506,
            0.0284,
            0.0215,
            -0.1164,
            0.2189,
            0.0207,
            -0.1577,
            0.0885,
            -0.0319,
            -0.01,
            0.1192,
            -0.0842,
            -0.1664,
            0.1016
          ],
          "after": [
            0.0163,
            0.0827,
            -0.0209,
            -0.0647,
            0.0096,
            -0.0048,
            0.0684,
            0.1017,
            -0.0565,
            -0.069,
            0.0148,
            -0.0028,
            0.1015,
            0.0454,
            -0.0438,
            0.1901
          ]
        },
        "position_0": {
          "grad": [
            -0.0647,
            0.0217,
            -0.1118,
            0.0032,
            -0.0068,
            -0.0175,
            0.109,
            -0.0043,
            0.0014,
            0.0035,
            -0.0056,
            -0.0633,
            -0.0048,
            0.0211,
            -0.0772,
            -0.0862
          ],
          "after": [
            -0.0121,
            -0.1004,
            -0.2657,
            0.0758,
            0.028,
            -0.0332,
            -0.124,
            0.0879,
            0.1163,
            -0.035,
            0.0051,
            0.1099,
            0.0166,
            0.0002,
            -0.1133,
            0.0616
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0018,
            0.0008,
            0.0004,
            0.0006,
            -0.0008,
            -0.0002,
            0.0011,
            0.0011,
            0.0008,
            0.0001,
            -0.0007,
            0.0025,
            -0.0005,
            -0.0004,
            0.001,
            0.002
          ],
          "after": [
            -0.0124,
            0.0287,
            -0.0558,
            -0.0163,
            0.0212,
            0.1406,
            0.044,
            -0.1697,
            0.024,
            0.0259,
            -0.087,
            -0.0786,
            0.1145,
            -0.0431,
            -0.004,
            0.0803
          ]
        }
      }
    },
    {
      "step": 183,
      "word": "jaden",
      "loss": 1.9695,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1043,
            -0.0412,
            -0.0867,
            -0.1103,
            0.1641,
            -0.1209,
            -0.0028,
            -0.0314,
            0.085,
            -0.0772,
            -0.161,
            -0.0168,
            0.0667,
            0.1374,
            0.0896,
            0.2226
          ],
          "after": [
            -0.0067,
            0.0237,
            0.1473,
            -0.0366,
            0.0051,
            -0.0225,
            -0.0803,
            -0.143,
            -0.0118,
            0.0397,
            0.1409,
            -0.0166,
            -0.0743,
            -0.0091,
            -0.0186,
            -0.1546
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2464,
            -0.2023,
            0.1528,
            -0.2839,
            0.0385,
            0.0619,
            -0.3314,
            -0.0335,
            0.3174,
            -0.2287,
            0.0814,
            0.2048,
            -0.3093,
            -0.0757,
            0.4244,
            -0.0583
          ],
          "after": [
            0.017,
            0.0827,
            -0.0223,
            -0.0637,
            0.0091,
            -0.0044,
            0.0684,
            0.1025,
            -0.0569,
            -0.0687,
            0.0137,
            -0.003,
            0.1021,
            0.0458,
            -0.0437,
            0.1904
          ]
        },
        "position_0": {
          "grad": [
            -0.0795,
            0.1352,
            -0.0248,
            0.2227,
            0.142,
            -0.1222,
            0.2845,
            0.0717,
            -0.2955,
            -0.046,
            0.0324,
            0.1475,
            0.4102,
            -0.2017,
            -0.2511,
            0.2098
          ],
          "after": [
            -0.0117,
            -0.1006,
            -0.2655,
            0.0755,
            0.0277,
            -0.0332,
            -0.1242,
            0.0879,
            0.1165,
            -0.0354,
            0.0045,
            0.1095,
            0.0161,
            0.001,
            -0.1126,
            0.0619
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            -0.0009,
            0.0003,
            -0.0011,
            0.0005,
            -0.0004,
            -0.0013,
            -0.0008,
            0.0006,
            -0.0005,
            0.0,
            0.0008,
            -0.0013,
            0.0002,
            0.0004,
            -0.0005
          ],
          "after": [
            -0.0124,
            0.0289,
            -0.0572,
            -0.0158,
            0.0224,
            0.1403,
            0.046,
            -0.169,
            0.023,
            0.026,
            -0.0872,
            -0.0795,
            0.1165,
            -0.0444,
            -0.0054,
            0.0812
          ]
        }
      }
    },
    {
      "step": 184,
      "word": "lo",
      "loss": 3.3672,
      "learning_rate": 0.0025,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0067,
            0.0226,
            0.148,
            -0.0366,
            0.0043,
            -0.0217,
            -0.0804,
            -0.1433,
            -0.0121,
            0.0402,
            0.1417,
            -0.0169,
            -0.0741,
            -0.0092,
            -0.0186,
            -0.1548
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0053,
            -0.0132,
            -0.1375,
            0.0321,
            -0.1365,
            -0.098,
            0.1272,
            0.0094,
            -0.187,
            0.0513,
            -0.0204,
            -0.0892,
            0.1844,
            -0.0518,
            -0.0945,
            0.1002
          ],
          "after": [
            0.0177,
            0.0826,
            -0.0232,
            -0.0629,
            0.0089,
            -0.0039,
            0.0683,
            0.103,
            -0.0569,
            -0.0687,
            0.0128,
            -0.003,
            0.1021,
            0.0463,
            -0.0434,
            0.1904
          ]
        },
        "position_0": {
          "grad": [
            -0.0188,
            0.011,
            -0.0788,
            -0.1988,
            -0.2331,
            0.1544,
            -0.4561,
            0.3666,
            0.1356,
            -0.1401,
            -0.3166,
            -0.0173,
            -0.2972,
            0.3463,
            0.4854,
            -0.5911
          ],
          "after": [
            -0.0114,
            -0.1008,
            -0.2649,
            0.0757,
            0.0283,
            -0.0339,
            -0.1236,
            0.0865,
            0.1164,
            -0.035,
            0.005,
            0.1092,
            0.0163,
            0.0006,
            -0.1134,
            0.0634
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.0005,
            0.0017,
            0.0005,
            -0.0041,
            -0.0034,
            0.0051,
            0.0009,
            -0.003,
            -0.0024,
            0.0015,
            -0.0044,
            0.008,
            -0.001,
            -0.0039,
            -0.0004
          ],
          "after": [
            -0.0127,
            0.0293,
            -0.059,
            -0.0156,
            0.0243,
            0.1412,
            0.0466,
            -0.1687,
            0.0234,
            0.0271,
            -0.088,
            -0.0789,
            0.1158,
            -0.0451,
            -0.0053,
            0.0821
          ]
        }
      }
    },
    {
      "step": 185,
      "word": "amahya",
      "loss": 2.2113,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1612,
            0.2358,
            0.1555,
            0.0209,
            -0.1563,
            0.0672,
            0.3462,
            0.2144,
            0.0915,
            0.1847,
            0.0799,
            -0.3383,
            0.2437,
            0.0979,
            0.0015,
            -0.1204
          ],
          "after": [
            -0.0071,
            0.0209,
            0.1481,
            -0.0366,
            0.004,
            -0.0213,
            -0.0813,
            -0.1444,
            -0.0128,
            0.0398,
            0.1422,
            -0.0164,
            -0.0747,
            -0.0097,
            -0.0186,
            -0.1546
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0487,
            0.017,
            -0.0633,
            0.0262,
            0.0785,
            -0.0978,
            0.1266,
            0.0014,
            -0.1416,
            0.0659,
            -0.0117,
            0.0079,
            0.0738,
            -0.0762,
            -0.1467,
            0.0965
          ],
          "after": [
            0.0184,
            0.0826,
            -0.0238,
            -0.0623,
            0.0087,
            -0.0032,
            0.068,
            0.1035,
            -0.0567,
            -0.0689,
            0.0121,
            -0.003,
            0.102,
            0.0469,
            -0.043,
            0.1903
          ]
        },
        "position_0": {
          "grad": [
            0.0843,
            -0.0892,
            0.031,
            -0.2847,
            -0.1038,
            0.0533,
            -0.3415,
            -0.115,
            0.255,
            0.0207,
            0.2076,
            0.0473,
            -0.3246,
            0.0274,
            0.1063,
            -0.2907
          ],
          "after": [
            -0.0114,
            -0.1006,
            -0.2645,
            0.0765,
            0.0291,
            -0.0347,
            -0.1225,
            0.0858,
            0.1158,
            -0.0347,
            0.0048,
            0.1088,
            0.0171,
            0.0001,
            -0.1144,
            0.0652
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0011,
            0.0009,
            0.0,
            -0.0009,
            0.0006,
            -0.0001,
            -0.0012,
            0.0008,
            0.001,
            0.0018,
            0.0002,
            -0.0012,
            0.0002,
            -0.0007,
            -0.001
          ],
          "after": [
            -0.0131,
            0.0291,
            -0.0609,
            -0.0154,
            0.0262,
            0.1417,
            0.0472,
            -0.168,
            0.0234,
            0.0276,
            -0.0896,
            -0.0784,
            0.1156,
            -0.0458,
            -0.005,
            0.0832
          ]
        }
      }
    },
    {
      "step": 186,
      "word": "zanaiyah",
      "loss": 2.3966,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2308,
            -0.236,
            0.0742,
            -0.1014,
            -0.1313,
            0.094,
            -0.1985,
            0.1034,
            0.2483,
            -0.149,
            -0.1861,
            -0.0512,
            -0.0795,
            0.0803,
            0.0801,
            0.0395
          ],
          "after": [
            -0.0068,
            0.0202,
            0.148,
            -0.0363,
            0.0042,
            -0.0214,
            -0.0816,
            -0.1457,
            -0.0142,
            0.0401,
            0.1433,
            -0.0158,
            -0.075,
            -0.0103,
            -0.0189,
            -0.1546
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0826,
            0.025,
            -0.0344,
            0.0315,
            0.0711,
            -0.0744,
            0.1468,
            -0.0012,
            -0.0922,
            0.0489,
            -0.0089,
            0.0398,
            0.047,
            -0.066,
            -0.131,
            0.0951
          ],
          "after": [
            0.0192,
            0.0825,
            -0.0243,
            -0.0619,
            0.0083,
            -0.0026,
            0.0675,
            0.104,
            -0.0563,
            -0.0692,
            0.0116,
            -0.0031,
            0.1019,
            0.0475,
            -0.0425,
            0.19
          ]
        },
        "position_0": {
          "grad": [
            -0.0311,
            0.0859,
            -0.0272,
            0.1844,
            0.0451,
            -0.042,
            0.2319,
            0.0296,
            -0.0057,
            0.0782,
            -0.0767,
            0.099,
            0.0051,
            -0.0535,
            -0.0156,
            0.0816
          ],
          "after": [
            -0.0113,
            -0.1008,
            -0.2641,
            0.0768,
            0.0296,
            -0.0352,
            -0.1219,
            0.0851,
            0.1153,
            -0.0349,
            0.0048,
            0.1083,
            0.0177,
            -0.0001,
            -0.1151,
            0.0666
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0027,
            -0.0014,
            -0.0011,
            0.0003,
            -0.0004,
            0.0021,
            -0.0042,
            -0.0002,
            0.0019,
            -0.0032,
            -0.0022,
            -0.0011,
            -0.0028,
            0.0028,
            0.0017,
            0.0003
          ],
          "after": [
            -0.0147,
            0.0296,
            -0.062,
            -0.0154,
            0.0278,
            0.1415,
            0.0485,
            -0.1674,
            0.0227,
            0.0292,
            -0.0899,
            -0.0777,
            0.1161,
            -0.0473,
            -0.0053,
            0.0841
          ]
        }
      }
    },
    {
      "step": 187,
      "word": "walton",
      "loss": 2.5852,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0511,
            0.0859,
            -0.119,
            -0.1357,
            0.0058,
            -0.0004,
            -0.1691,
            0.1168,
            -0.0845,
            -0.0387,
            -0.0468,
            0.0962,
            -0.0403,
            0.0633,
            0.1148,
            -0.2799
          ],
          "after": [
            -0.0065,
            0.0194,
            0.1482,
            -0.0357,
            0.0043,
            -0.0215,
            -0.0816,
            -0.1472,
            -0.0151,
            0.0405,
            0.1444,
            -0.0156,
            -0.0751,
            -0.0111,
            -0.0196,
            -0.154
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0558,
            0.0646,
            -0.0542,
            0.0699,
            0.0123,
            -0.1178,
            0.2197,
            -0.0013,
            -0.1756,
            0.0688,
            0.0052,
            -0.0185,
            0.1183,
            -0.0904,
            -0.1687,
            0.1174
          ],
          "after": [
            0.02,
            0.0822,
            -0.0246,
            -0.0618,
            0.0079,
            -0.0018,
            0.0668,
            0.1044,
            -0.0556,
            -0.0697,
            0.0111,
            -0.0032,
            0.1015,
            0.0483,
            -0.0418,
            0.1895
          ]
        },
        "position_0": {
          "grad": [
            0.0303,
            0.2398,
            -0.12,
            0.2181,
            0.1541,
            -0.1432,
            0.4012,
            0.0389,
            -0.2061,
            -0.0179,
            -0.0555,
            0.0086,
            0.2818,
            -0.0759,
            -0.2959,
            0.2114
          ],
          "after": [
            -0.0113,
            -0.1016,
            -0.2631,
            0.0765,
            0.0296,
            -0.0351,
            -0.1221,
            0.0843,
            0.1153,
            -0.035,
            0.005,
            0.1078,
            0.0177,
            -0.0,
            -0.115,
            0.0673
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.004,
            -0.0034,
            0.0034,
            -0.001,
            0.0018,
            0.0037,
            -0.0009,
            0.0009,
            0.0035,
            0.0007,
            0.0032,
            0.0025,
            -0.0001,
            -0.001,
            0.0064
          ],
          "after": [
            -0.016,
            0.0285,
            -0.0617,
            -0.0166,
            0.0294,
            0.1408,
            0.0489,
            -0.1666,
            0.0217,
            0.0293,
            -0.0904,
            -0.078,
            0.1159,
            -0.0486,
            -0.0052,
            0.0827
          ]
        }
      }
    },
    {
      "step": 188,
      "word": "klarity",
      "loss": 2.4198,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.045,
            -0.0711,
            -0.0167,
            -0.0442,
            0.12,
            0.1379,
            0.0428,
            -0.132,
            0.0777,
            -0.0065,
            0.1174,
            0.1028,
            -0.1479,
            -0.1091,
            0.0015,
            0.1478
          ],
          "after": [
            -0.0064,
            0.019,
            0.1485,
            -0.0351,
            0.004,
            -0.0223,
            -0.0816,
            -0.148,
            -0.0162,
            0.0408,
            0.1449,
            -0.0156,
            -0.0747,
            -0.0114,
            -0.0201,
            -0.1538
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0842,
            0.0889,
            -0.0605,
            0.0485,
            0.0285,
            -0.1379,
            0.2336,
            -0.0168,
            -0.2043,
            0.0705,
            -0.0223,
            -0.0251,
            0.1453,
            -0.0942,
            -0.1727,
            0.1511
          ],
          "after": [
            0.021,
            0.0817,
            -0.0247,
            -0.0619,
            0.0076,
            -0.0009,
            0.0659,
            0.1048,
            -0.0547,
            -0.0704,
            0.0107,
            -0.0031,
            0.1008,
            0.0493,
            -0.0409,
            0.1888
          ]
        },
        "position_0": {
          "grad": [
            -0.0628,
            0.047,
            -0.1072,
            0.0207,
            0.0277,
            -0.0237,
            0.1342,
            -0.0035,
            -0.0306,
            0.0172,
            -0.0116,
            -0.0193,
            0.014,
            0.0,
            -0.0941,
            -0.0515
          ],
          "after": [
            -0.0111,
            -0.1024,
            -0.2618,
            0.0762,
            0.0294,
            -0.0348,
            -0.1225,
            0.0837,
            0.1153,
            -0.0351,
            0.0053,
            0.1074,
            0.0177,
            0.0,
            -0.1146,
            0.0681
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0031,
            0.0002,
            0.003,
            -0.0008,
            0.0064,
            0.0021,
            0.0026,
            -0.0003,
            -0.0012,
            -0.0023,
            -0.0016,
            0.0014,
            -0.0002,
            -0.0012,
            -0.0018,
            0.0004
          ],
          "after": [
            -0.0157,
            0.0275,
            -0.0624,
            -0.0174,
            0.0294,
            0.1395,
            0.0488,
            -0.1658,
            0.0214,
            0.0301,
            -0.0901,
            -0.0787,
            0.1158,
            -0.0493,
            -0.0046,
            0.0814
          ]
        }
      }
    },
    {
      "step": 189,
      "word": "jena",
      "loss": 2.0646,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5155,
            0.0611,
            -0.4039,
            -0.0737,
            -0.3337,
            0.041,
            -0.0451,
            -0.0395,
            -0.1942,
            0.2437,
            0.1248,
            -0.547,
            0.2316,
            0.2171,
            0.2208,
            -0.246
          ],
          "after": [
            -0.0074,
            0.0184,
            0.1499,
            -0.0344,
            0.0047,
            -0.0231,
            -0.0815,
            -0.1485,
            -0.0164,
            0.0401,
            0.1448,
            -0.0145,
            -0.0751,
            -0.0124,
            -0.0215,
            -0.1532
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3132,
            -0.2584,
            0.2146,
            0.0181,
            0.3464,
            0.1287,
            -0.6145,
            -0.1089,
            0.5089,
            0.0742,
            0.2768,
            0.1122,
            -0.4775,
            -0.0164,
            -0.0121,
            -0.2919
          ],
          "after": [
            0.0209,
            0.082,
            -0.0253,
            -0.0621,
            0.0067,
            -0.0003,
            0.0659,
            0.1056,
            -0.0549,
            -0.0712,
            0.0094,
            -0.0034,
            0.1012,
            0.0501,
            -0.0402,
            0.1887
          ]
        },
        "position_0": {
          "grad": [
            -0.0908,
            0.1228,
            -0.003,
            0.3097,
            0.1222,
            -0.1651,
            0.3194,
            0.1239,
            -0.3226,
            -0.1213,
            0.001,
            0.0927,
            0.5295,
            -0.1859,
            -0.2858,
            0.2586
          ],
          "after": [
            -0.0105,
            -0.1035,
            -0.2607,
            0.0753,
            0.0289,
            -0.0339,
            -0.1234,
            0.0827,
            0.116,
            -0.0346,
            0.0055,
            0.1068,
            0.0167,
            0.0006,
            -0.1136,
            0.0682
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0002,
            -0.0001,
            -0.0003,
            0.0,
            0.0006,
            -0.0015,
            -0.0007,
            0.0,
            0.0002,
            0.0005,
            0.0004,
            -0.0015,
            0.0,
            0.0008,
            -0.0009
          ],
          "after": [
            -0.0156,
            0.0267,
            -0.063,
            -0.0179,
            0.0294,
            0.1382,
            0.0489,
            -0.1648,
            0.021,
            0.0307,
            -0.0901,
            -0.0794,
            0.116,
            -0.0499,
            -0.0043,
            0.0805
          ]
        }
      }
    },
    {
      "step": 190,
      "word": "breklynn",
      "loss": 2.6186,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0084,
            0.0179,
            0.1511,
            -0.0337,
            0.0053,
            -0.0238,
            -0.0815,
            -0.149,
            -0.0166,
            0.0395,
            0.1447,
            -0.0135,
            -0.0755,
            -0.0132,
            -0.0226,
            -0.1527
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0902,
            0.0079,
            0.1155,
            0.0789,
            0.2013,
            0.1575,
            -0.1108,
            0.0353,
            0.1371,
            -0.0427,
            0.0999,
            0.2604,
            -0.3041,
            -0.0536,
            0.194,
            0.0253
          ],
          "after": [
            0.0211,
            0.0822,
            -0.026,
            -0.0625,
            0.0055,
            -0.0001,
            0.0662,
            0.1062,
            -0.0552,
            -0.0718,
            0.0078,
            -0.0042,
            0.1022,
            0.051,
            -0.0398,
            0.1886
          ]
        },
        "position_0": {
          "grad": [
            -0.0015,
            0.0171,
            0.0856,
            0.0071,
            0.0789,
            0.0003,
            0.0884,
            -0.0011,
            -0.1071,
            -0.0604,
            0.0097,
            0.0544,
            0.1281,
            -0.0773,
            -0.0617,
            0.239
          ],
          "after": [
            -0.01,
            -0.1044,
            -0.2602,
            0.0745,
            0.0282,
            -0.0331,
            -0.1243,
            0.0818,
            0.1167,
            -0.0338,
            0.0056,
            0.1061,
            0.0156,
            0.0014,
            -0.1125,
            0.0679
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.0008,
            0.0005,
            0.0016,
            0.0004,
            0.0005,
            0.0002,
            0.0,
            -0.0005,
            0.0009,
            0.0009,
            -0.0011,
            0.001,
            0.0,
            -0.0022,
            0.0008
          ],
          "after": [
            -0.016,
            0.0257,
            -0.0637,
            -0.0189,
            0.0293,
            0.1369,
            0.049,
            -0.164,
            0.0209,
            0.031,
            -0.0906,
            -0.0797,
            0.116,
            -0.0504,
            -0.0034,
            0.0796
          ]
        }
      }
    },
    {
      "step": 191,
      "word": "cariana",
      "loss": 1.8359,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0941,
            -0.1489,
            -0.0159,
            -0.248,
            -0.0134,
            0.0283,
            -0.1262,
            -0.306,
            0.1632,
            -0.1543,
            0.1156,
            0.0176,
            -0.2915,
            -0.1189,
            0.0965,
            -0.0756
          ],
          "after": [
            -0.0094,
            0.0179,
            0.1522,
            -0.0326,
            0.0059,
            -0.0246,
            -0.0812,
            -0.1483,
            -0.0173,
            0.0397,
            0.1442,
            -0.0127,
            -0.0749,
            -0.0135,
            -0.0239,
            -0.1521
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0232,
            0.0319,
            -0.0495,
            0.0265,
            0.0246,
            -0.069,
            0.1235,
            -0.0134,
            -0.1139,
            0.0673,
            -0.0027,
            -0.0131,
            0.0772,
            -0.0554,
            -0.1155,
            0.0699
          ],
          "after": [
            0.0214,
            0.0823,
            -0.0265,
            -0.0629,
            0.0045,
            0.0002,
            0.0662,
            0.1067,
            -0.0553,
            -0.0725,
            0.0065,
            -0.0048,
            0.1029,
            0.0518,
            -0.0393,
            0.1884
          ]
        },
        "position_0": {
          "grad": [
            -0.0236,
            -0.0086,
            -0.0371,
            0.0182,
            -0.1373,
            -0.0642,
            0.0533,
            0.0459,
            -0.0073,
            0.0872,
            0.0123,
            0.0046,
            -0.009,
            -0.0138,
            -0.045,
            -0.0316
          ],
          "after": [
            -0.0095,
            -0.1052,
            -0.2595,
            0.0737,
            0.0281,
            -0.0322,
            -0.1252,
            0.0809,
            0.1173,
            -0.0336,
            0.0057,
            0.1056,
            0.0147,
            0.0021,
            -0.1115,
            0.0676
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            -0.0024,
            -0.0008,
            0.0012,
            -0.0034,
            0.0004,
            -0.0063,
            0.0002,
            0.0006,
            -0.0012,
            -0.0009,
            -0.0005,
            -0.0025,
            0.003,
            0.0023,
            -0.0001
          ],
          "after": [
            -0.0171,
            0.0258,
            -0.064,
            -0.0202,
            0.0299,
            0.1357,
            0.0502,
            -0.1634,
            0.0206,
            0.0316,
            -0.0905,
            -0.0797,
            0.1166,
            -0.0517,
            -0.0034,
            0.0788
          ]
        }
      }
    },
    {
      "step": 192,
      "word": "krishna",
      "loss": 2.3525,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0839,
            0.0191,
            0.0049,
            -0.0234,
            -0.0283,
            -0.0025,
            -0.0195,
            -0.0801,
            -0.0639,
            0.0355,
            0.0913,
            -0.0767,
            0.0315,
            0.0445,
            -0.0018,
            -0.0604
          ],
          "after": [
            -0.0104,
            0.0179,
            0.1531,
            -0.0315,
            0.0064,
            -0.0252,
            -0.0809,
            -0.1474,
            -0.0177,
            0.0396,
            0.1434,
            -0.0118,
            -0.0745,
            -0.014,
            -0.0251,
            -0.1515
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0707,
            0.0456,
            -0.0576,
            0.0308,
            0.0582,
            -0.1149,
            0.2009,
            0.0068,
            -0.1488,
            0.0571,
            -0.0422,
            0.0061,
            0.0862,
            -0.079,
            -0.1536,
            0.1302
          ],
          "after": [
            0.0218,
            0.0823,
            -0.0268,
            -0.0634,
            0.0035,
            0.0006,
            0.0659,
            0.1072,
            -0.0551,
            -0.0733,
            0.0055,
            -0.0054,
            0.1033,
            0.0528,
            -0.0387,
            0.188
          ]
        },
        "position_0": {
          "grad": [
            -0.0565,
            0.0255,
            -0.104,
            0.0048,
            0.009,
            -0.0165,
            0.1028,
            -0.0078,
            -0.0045,
            0.0172,
            -0.0175,
            -0.0395,
            -0.0098,
            0.0127,
            -0.0682,
            -0.0662
          ],
          "after": [
            -0.0088,
            -0.1059,
            -0.2585,
            0.0731,
            0.028,
            -0.0313,
            -0.1261,
            0.0802,
            0.1179,
            -0.0336,
            0.0058,
            0.1052,
            0.0139,
            0.0026,
            -0.1105,
            0.0675
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0033,
            -0.0025,
            -0.0009,
            -0.0013,
            0.0001,
            0.0021,
            -0.0028,
            -0.0032,
            -0.0019,
            0.0034,
            0.0031,
            -0.0045,
            -0.0019,
            0.0026,
            0.0004,
            -0.0041
          ],
          "after": [
            -0.0192,
            0.0267,
            -0.064,
            -0.0208,
            0.0304,
            0.1341,
            0.0517,
            -0.1619,
            0.021,
            0.0309,
            -0.0918,
            -0.0786,
            0.1175,
            -0.0537,
            -0.0035,
            0.0794
          ]
        }
      }
    },
    {
      "step": 193,
      "word": "anisten",
      "loss": 2.3572,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0113,
            -0.0553,
            -0.1997,
            0.0257,
            -0.3906,
            0.0854,
            -0.1233,
            -0.0169,
            0.1198,
            -0.1706,
            -0.1467,
            -0.0893,
            -0.0211,
            0.1672,
            0.2557,
            0.0408
          ],
          "after": [
            -0.0114,
            0.018,
            0.1544,
            -0.0306,
            0.0079,
            -0.0261,
            -0.0804,
            -0.1466,
            -0.0184,
            0.0403,
            0.1433,
            -0.0109,
            -0.0741,
            -0.0149,
            -0.0269,
            -0.151
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0916,
            -0.2257,
            -0.0104,
            -0.1716,
            -0.2724,
            0.0868,
            -0.2936,
            0.0124,
            0.2448,
            0.0422,
            0.0305,
            0.0317,
            -0.1592,
            0.1107,
            0.3229,
            -0.0476
          ],
          "after": [
            0.0219,
            0.0829,
            -0.0271,
            -0.0632,
            0.0032,
            0.0008,
            0.0661,
            0.1075,
            -0.0554,
            -0.0741,
            0.0046,
            -0.006,
            0.104,
            0.0533,
            -0.0386,
            0.1877
          ]
        },
        "position_0": {
          "grad": [
            0.0969,
            -0.137,
            0.0769,
            -0.2721,
            -0.146,
            0.0685,
            -0.3749,
            -0.096,
            0.2908,
            -0.0215,
            0.2011,
            -0.0136,
            -0.3258,
            0.059,
            0.1468,
            -0.2987
          ],
          "after": [
            -0.0086,
            -0.1062,
            -0.258,
            0.0732,
            0.0283,
            -0.0309,
            -0.1262,
            0.0799,
            0.1178,
            -0.0334,
            0.0052,
            0.1049,
            0.0139,
            0.0029,
            -0.1099,
            0.068
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0017,
            -0.0012,
            -0.0012,
            0.0,
            -0.0,
            0.0008,
            -0.0003,
            -0.0004,
            0.0013,
            0.0013,
            0.0008,
            -0.0016,
            -0.0006,
            0.0011,
            -0.0009
          ],
          "after": [
            -0.0211,
            0.0281,
            -0.0635,
            -0.0209,
            0.0308,
            0.1327,
            0.0529,
            -0.1606,
            0.0215,
            0.03,
            -0.0933,
            -0.0778,
            0.1187,
            -0.0551,
            -0.0039,
            0.0802
          ]
        }
      }
    },
    {
      "step": 194,
      "word": "lenzie",
      "loss": 2.9445,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0122,
            0.0182,
            0.1555,
            -0.0299,
            0.0091,
            -0.0269,
            -0.0799,
            -0.1459,
            -0.0191,
            0.0409,
            0.1432,
            -0.0101,
            -0.0737,
            -0.0157,
            -0.0285,
            -0.1506
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1269,
            -0.2886,
            0.171,
            -0.0753,
            -0.173,
            0.2037,
            -0.6606,
            0.165,
            0.6929,
            -0.1073,
            0.0541,
            0.19,
            -0.3233,
            0.2146,
            0.4843,
            -0.3361
          ],
          "after": [
            0.0216,
            0.084,
            -0.0276,
            -0.0628,
            0.0032,
            0.0006,
            0.0672,
            0.107,
            -0.0568,
            -0.0745,
            0.0036,
            -0.0069,
            0.1052,
            0.0532,
            -0.0393,
            0.1881
          ]
        },
        "position_0": {
          "grad": [
            -0.0202,
            0.0515,
            -0.0613,
            -0.1318,
            -0.0033,
            0.0788,
            -0.1711,
            0.1098,
            0.0309,
            0.0032,
            -0.1057,
            0.0807,
            -0.1491,
            0.0967,
            0.1731,
            -0.2473
          ],
          "after": [
            -0.0084,
            -0.1065,
            -0.2573,
            0.0735,
            0.0286,
            -0.0309,
            -0.1261,
            0.0793,
            0.1177,
            -0.0333,
            0.0051,
            0.1045,
            0.0141,
            0.0028,
            -0.11,
            0.069
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            0.0007,
            0.0012,
            0.0018,
            -0.0002,
            -0.0001,
            0.002,
            0.0021,
            0.0018,
            0.003,
            0.0002,
            0.0043,
            -0.0,
            -0.0009,
            -0.0022,
            0.0026
          ],
          "after": [
            -0.0221,
            0.029,
            -0.0636,
            -0.0217,
            0.0312,
            0.1315,
            0.0535,
            -0.16,
            0.0212,
            0.0282,
            -0.0948,
            -0.0783,
            0.1197,
            -0.0561,
            -0.0036,
            0.0801
          ]
        }
      }
    },
    {
      "step": 195,
      "word": "emilyrose",
      "loss": 2.9678,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0128,
            0.0183,
            0.1565,
            -0.0293,
            0.0102,
            -0.0276,
            -0.0796,
            -0.1454,
            -0.0196,
            0.0413,
            0.1431,
            -0.0094,
            -0.0734,
            -0.0163,
            -0.0299,
            -0.1503
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.126,
            0.2943,
            0.2423,
            -0.0123,
            -0.1944,
            0.0082,
            0.0472,
            -0.0001,
            -0.1815,
            -0.0539,
            -0.0105,
            -0.3817,
            0.2734,
            0.1677,
            -0.0112,
            -0.3239
          ],
          "after": [
            0.021,
            0.0843,
            -0.0287,
            -0.0623,
            0.0036,
            0.0005,
            0.068,
            0.1065,
            -0.0577,
            -0.0746,
            0.0028,
            -0.0068,
            0.1057,
            0.0527,
            -0.0398,
            0.1891
          ]
        },
        "position_0": {
          "grad": [
            -0.009,
            -0.1151,
            0.0094,
            -0.0604,
            -0.0017,
            0.0637,
            -0.1512,
            -0.0912,
            0.1094,
            0.1059,
            0.0522,
            -0.0085,
            -0.1428,
            -0.0401,
            0.0504,
            -0.1852
          ],
          "after": [
            -0.0082,
            -0.1065,
            -0.2567,
            0.074,
            0.0289,
            -0.0311,
            -0.1257,
            0.0791,
            0.1174,
            -0.0337,
            0.0048,
            0.1041,
            0.0146,
            0.0029,
            -0.1101,
            0.0701
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0011,
            0.0002,
            0.0015,
            -0.0011,
            -0.0004,
            0.0008,
            0.0003,
            -0.0011,
            -0.0001,
            0.0015,
            -0.0019,
            0.0016,
            -0.0012,
            -0.0005,
            0.0005
          ],
          "after": [
            -0.0231,
            0.0293,
            -0.0637,
            -0.0228,
            0.0318,
            0.1306,
            0.0538,
            -0.1597,
            0.0214,
            0.0267,
            -0.0966,
            -0.0782,
            0.1202,
            -0.0565,
            -0.0032,
            0.0798
          ]
        }
      }
    },
    {
      "step": 196,
      "word": "franklyn",
      "loss": 2.543,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0901,
            0.0137,
            -0.1643,
            0.1046,
            -0.2323,
            0.0355,
            -0.0132,
            -0.0674,
            0.032,
            -0.0009,
            -0.0498,
            -0.0628,
            0.0228,
            0.1338,
            0.0445,
            0.0117
          ],
          "after": [
            -0.0136,
            0.0184,
            0.1578,
            -0.0291,
            0.0117,
            -0.0284,
            -0.0792,
            -0.1446,
            -0.0202,
            0.0418,
            0.1432,
            -0.0087,
            -0.0732,
            -0.0174,
            -0.0312,
            -0.15
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1037,
            0.0915,
            -0.0434,
            0.0615,
            0.0534,
            -0.1604,
            0.2366,
            -0.002,
            -0.1731,
            0.0702,
            -0.0142,
            0.049,
            0.0934,
            -0.1187,
            -0.1686,
            0.1633
          ],
          "after": [
            0.0208,
            0.0843,
            -0.0294,
            -0.0622,
            0.0039,
            0.0006,
            0.0684,
            0.1061,
            -0.0581,
            -0.0749,
            0.0021,
            -0.0068,
            0.1059,
            0.0525,
            -0.0401,
            0.1897
          ]
        },
        "position_0": {
          "grad": [
            -0.0635,
            0.2338,
            -0.0689,
            0.1782,
            0.2406,
            -0.149,
            0.2252,
            -0.0431,
            -0.2853,
            -0.0521,
            0.0467,
            0.1692,
            0.2262,
            -0.0505,
            -0.1868,
            0.2285
          ],
          "after": [
            -0.0077,
            -0.1071,
            -0.2559,
            0.074,
            0.0283,
            -0.0307,
            -0.1257,
            0.0791,
            0.1177,
            -0.0338,
            0.0044,
            0.1033,
            0.0146,
            0.0031,
            -0.1098,
            0.0706
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0002,
            -0.0013,
            -0.0002,
            -0.0003,
            0.0006,
            0.0007,
            0.0015,
            0.0009,
            -0.0006,
            -0.0002,
            0.0011,
            -0.0007,
            -0.0014,
            0.0018,
            0.0011
          ],
          "after": [
            -0.0233,
            0.0297,
            -0.0633,
            -0.0237,
            0.0323,
            0.1297,
            0.054,
            -0.1599,
            0.0212,
            0.0257,
            -0.098,
            -0.0784,
            0.1207,
            -0.0564,
            -0.0033,
            0.0793
          ]
        }
      }
    },
    {
      "step": 197,
      "word": "shireen",
      "loss": 2.3709,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0143,
            0.0184,
            0.1589,
            -0.0289,
            0.013,
            -0.029,
            -0.0789,
            -0.144,
            -0.0207,
            0.0421,
            0.1433,
            -0.0081,
            -0.0731,
            -0.0182,
            -0.0323,
            -0.1498
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1806,
            -0.2655,
            -0.0899,
            -0.1699,
            -0.4734,
            0.454,
            -0.5814,
            -0.0543,
            0.4687,
            -0.3254,
            -0.1281,
            -0.0463,
            -0.1997,
            0.5123,
            0.7734,
            -0.1668
          ],
          "after": [
            0.0201,
            0.0849,
            -0.0299,
            -0.0615,
            0.0049,
            -0.0001,
            0.0695,
            0.1061,
            -0.0592,
            -0.0741,
            0.0021,
            -0.0067,
            0.1065,
            0.0511,
            -0.0413,
            0.1904
          ]
        },
        "position_0": {
          "grad": [
            0.0058,
            0.077,
            0.0005,
            0.0336,
            -0.1543,
            0.0773,
            -0.0259,
            0.0549,
            0.0047,
            0.049,
            -0.0844,
            0.0246,
            0.0522,
            0.043,
            0.1407,
            -0.0288
          ],
          "after": [
            -0.0073,
            -0.1079,
            -0.2552,
            0.0739,
            0.0284,
            -0.0306,
            -0.1257,
            0.0789,
            0.1179,
            -0.0342,
            0.0044,
            0.1026,
            0.0145,
            0.0032,
            -0.1098,
            0.0711
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0006,
            -0.0003,
            0.0,
            -0.0015,
            -0.0,
            -0.0014,
            0.0003,
            0.0001,
            -0.0001,
            0.0002,
            0.0006,
            -0.001,
            -0.0001,
            0.0021,
            -0.0007
          ],
          "after": [
            -0.0234,
            0.0303,
            -0.0629,
            -0.0245,
            0.0331,
            0.1289,
            0.0545,
            -0.1601,
            0.021,
            0.0249,
            -0.0993,
            -0.0788,
            0.1215,
            -0.0563,
            -0.0041,
            0.0791
          ]
        }
      }
    },
    {
      "step": 198,
      "word": "viktor",
      "loss": 3.3766,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0149,
            0.0185,
            0.1599,
            -0.0287,
            0.0141,
            -0.0296,
            -0.0787,
            -0.1435,
            -0.0211,
            0.0424,
            0.1434,
            -0.0076,
            -0.0729,
            -0.019,
            -0.0333,
            -0.1496
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0717,
            0.1064,
            -0.0592,
            0.0605,
            0.0433,
            -0.1628,
            0.2755,
            0.0141,
            -0.2416,
            0.1216,
            -0.0398,
            -0.036,
            0.192,
            -0.0815,
            -0.2729,
            0.147
          ],
          "after": [
            0.0197,
            0.0852,
            -0.0302,
            -0.0611,
            0.0057,
            -0.0003,
            0.0701,
            0.106,
            -0.0598,
            -0.0738,
            0.0022,
            -0.0066,
            0.1066,
            0.0502,
            -0.042,
            0.1908
          ]
        },
        "position_0": {
          "grad": [
            -0.0878,
            0.0724,
            0.0086,
            0.1508,
            0.1736,
            0.0606,
            0.0305,
            0.0289,
            -0.1373,
            -0.0182,
            -0.1033,
            -0.1388,
            0.1788,
            -0.0063,
            -0.1367,
            0.2344
          ],
          "after": [
            -0.0066,
            -0.1087,
            -0.2547,
            0.0735,
            0.0279,
            -0.0309,
            -0.1257,
            0.0786,
            0.1183,
            -0.0344,
            0.0047,
            0.1023,
            0.0141,
            0.0032,
            -0.1095,
            0.0711
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0017,
            0.0094,
            0.0036,
            0.0003,
            0.0023,
            -0.0038,
            0.0112,
            -0.0013,
            0.0001,
            0.0049,
            -0.0034,
            -0.0007,
            0.0061,
            0.0009,
            -0.0075,
            0.006
          ],
          "after": [
            -0.0228,
            0.0282,
            -0.0637,
            -0.0252,
            0.0333,
            0.1294,
            0.0531,
            -0.1599,
            0.0208,
            0.0228,
            -0.099,
            -0.0789,
            0.1207,
            -0.0566,
            -0.0028,
            0.0774
          ]
        }
      }
    },
    {
      "step": 199,
      "word": "keilee",
      "loss": 2.2884,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0154,
            0.0185,
            0.1607,
            -0.0285,
            0.015,
            -0.0301,
            -0.0784,
            -0.143,
            -0.0215,
            0.0427,
            0.1435,
            -0.0071,
            -0.0728,
            -0.0196,
            -0.0341,
            -0.1494
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3986,
            -0.5075,
            0.0692,
            -0.2986,
            -0.2527,
            0.784,
            -1.1239,
            -0.0744,
            1.0776,
            -0.3315,
            -0.0581,
            0.1366,
            -0.5347,
            0.7108,
            0.7819,
            -0.7166
          ],
          "after": [
            0.0184,
            0.0866,
            -0.0305,
            -0.0598,
            0.0068,
            -0.0019,
            0.0719,
            0.1062,
            -0.0618,
            -0.0725,
            0.0026,
            -0.0068,
            0.1077,
            0.0479,
            -0.0436,
            0.1924
          ]
        },
        "position_0": {
          "grad": [
            -0.0681,
            0.0186,
            -0.1261,
            -0.0371,
            -0.0076,
            -0.0228,
            0.0665,
            -0.0273,
            0.0252,
            0.0236,
            -0.0174,
            -0.0413,
            -0.0345,
            0.0199,
            -0.0788,
            -0.1063
          ],
          "after": [
            -0.0058,
            -0.1095,
            -0.2536,
            0.0732,
            0.0274,
            -0.0309,
            -0.1259,
            0.0784,
            0.1187,
            -0.0347,
            0.005,
            0.1022,
            0.0138,
            0.0032,
            -0.1091,
            0.0712
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0002,
            -0.0004,
            -0.0005,
            -0.0011,
            0.0023,
            -0.0026,
            -0.0001,
            0.0018,
            0.0001,
            -0.0,
            0.0035,
            -0.003,
            0.0008,
            0.0016,
            -0.0009
          ],
          "after": [
            -0.0222,
            0.0266,
            -0.0643,
            -0.0257,
            0.0336,
            0.1292,
            0.0523,
            -0.1596,
            0.02,
            0.021,
            -0.0988,
            -0.0798,
            0.1207,
            -0.057,
            -0.0021,
            0.0761
          ]
        }
      }
    },
    {
      "step": 200,
      "word": "ameliarose",
      "loss": 2.559,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0887,
            -0.1285,
            0.1263,
            0.0409,
            0.2876,
            0.0787,
            0.1527,
            0.0372,
            0.1593,
            -0.048,
            0.053,
            0.2544,
            -0.3047,
            -0.2235,
            -0.0024,
            0.1596
          ],
          "after": [
            -0.0156,
            0.019,
            0.161,
            -0.0285,
            0.015,
            -0.0308,
            -0.0786,
            -0.1427,
            -0.0224,
            0.0431,
            0.1434,
            -0.0073,
            -0.0718,
            -0.0194,
            -0.0348,
            -0.1496
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1474,
            0.1395,
            0.0936,
            0.0064,
            -0.2638,
            0.2405,
            -0.1959,
            -0.0114,
            0.1676,
            0.0088,
            0.0281,
            -0.1014,
            -0.0255,
            0.1347,
            0.3004,
            -0.2773
          ],
          "after": [
            0.0169,
            0.0875,
            -0.0311,
            -0.0587,
            0.0082,
            -0.0035,
            0.0737,
            0.1065,
            -0.0637,
            -0.0714,
            0.0028,
            -0.0067,
            0.1087,
            0.0457,
            -0.0453,
            0.1942
          ]
        },
        "position_0": {
          "grad": [
            0.0487,
            -0.0501,
            0.0365,
            -0.185,
            -0.056,
            0.0409,
            -0.2135,
            -0.0938,
            0.1593,
            0.0339,
            0.1649,
            0.056,
            -0.2196,
            -0.0072,
            0.0687,
            -0.1789
          ],
          "after": [
            -0.0052,
            -0.11,
            -0.2529,
            0.0734,
            0.0273,
            -0.0312,
            -0.1256,
            0.0787,
            0.1187,
            -0.0352,
            0.0047,
            0.102,
            0.0139,
            0.0033,
            -0.1089,
            0.0717
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            0.0,
            -0.0012,
            0.001,
            -0.0017,
            -0.0001,
            -0.0021,
            0.0017,
            0.0002,
            0.0005,
            -0.0011,
            -0.0015,
            0.0008,
            0.0011,
            0.0009,
            -0.0002
          ],
          "after": [
            -0.0219,
            0.0252,
            -0.0645,
            -0.0265,
            0.0343,
            0.129,
            0.0519,
            -0.16,
            0.0193,
            0.0193,
            -0.0982,
            -0.0803,
            0.1205,
            -0.0577,
            -0.0018,
            0.0751
          ]
        }
      }
    },
    {
      "step": 201,
      "word": "vallie",
      "loss": 2.3292,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0889,
            0.1765,
            -0.0499,
            -0.1049,
            0.0757,
            -0.0163,
            -0.1167,
            0.1454,
            -0.1199,
            0.0056,
            -0.0355,
            0.1835,
            -0.0424,
            0.01,
            0.0541,
            -0.2997
          ],
          "after": [
            -0.0155,
            0.0188,
            0.1614,
            -0.0282,
            0.0149,
            -0.0314,
            -0.0785,
            -0.143,
            -0.0227,
            0.0434,
            0.1434,
            -0.0079,
            -0.0708,
            -0.0192,
            -0.0356,
            -0.1492
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0804,
            -0.0673,
            -0.1064,
            0.1054,
            -0.6205,
            -0.0068,
            -0.18,
            0.0371,
            0.1256,
            -0.0064,
            0.026,
            -0.0321,
            0.1124,
            0.068,
            0.3917,
            0.0388
          ],
          "after": [
            0.0153,
            0.0883,
            -0.0313,
            -0.0581,
            0.0104,
            -0.005,
            0.0754,
            0.1065,
            -0.0656,
            -0.0705,
            0.0028,
            -0.0065,
            0.1093,
            0.0436,
            -0.0473,
            0.1957
          ]
        },
        "position_0": {
          "grad": [
            -0.0871,
            0.0688,
            -0.0148,
            0.0815,
            0.1453,
            0.0471,
            -0.0095,
            0.0166,
            -0.1072,
            0.0232,
            -0.1126,
            -0.1019,
            0.1088,
            -0.0425,
            -0.1286,
            0.1746
          ],
          "after": [
            -0.0044,
            -0.1107,
            -0.2522,
            0.0733,
            0.0266,
            -0.0316,
            -0.1254,
            0.0788,
            0.1188,
            -0.0357,
            0.0048,
            0.1021,
            0.0138,
            0.0034,
            -0.1083,
            0.0718
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.002,
            0.0005,
            0.0025,
            -0.0002,
            -0.0,
            0.0002,
            -0.0038,
            0.0024,
            0.003,
            0.0005,
            0.0011,
            0.0022,
            -0.0018,
            -0.0013,
            -0.002,
            0.002
          ],
          "after": [
            -0.021,
            0.0238,
            -0.0654,
            -0.027,
            0.0349,
            0.1288,
            0.0522,
            -0.161,
            0.0176,
            0.0177,
            -0.0981,
            -0.0812,
            0.1208,
            -0.0579,
            -0.001,
            0.0737
          ]
        }
      }
    },
    {
      "step": 202,
      "word": "aarit",
      "loss": 2.5112,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1224,
            -0.235,
            0.2698,
            -0.466,
            0.05,
            0.1409,
            -0.5147,
            -0.2581,
            0.4864,
            -0.1998,
            0.5289,
            0.1225,
            -0.6279,
            -0.045,
            0.0743,
            -0.2892
          ],
          "after": [
            -0.0158,
            0.0194,
            0.161,
            -0.0268,
            0.0146,
            -0.0326,
            -0.0773,
            -0.1424,
            -0.0245,
            0.0445,
            0.1416,
            -0.0086,
            -0.0685,
            -0.0189,
            -0.0366,
            -0.1483
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0157,
            0.0739,
            -0.0403,
            0.0408,
            0.0602,
            -0.0988,
            0.1579,
            -0.0254,
            -0.1595,
            0.0763,
            0.0209,
            -0.0508,
            0.1192,
            -0.0522,
            -0.2101,
            0.0551
          ],
          "after": [
            0.0141,
            0.0889,
            -0.0314,
            -0.0577,
            0.0122,
            -0.006,
            0.0767,
            0.1067,
            -0.0669,
            -0.0699,
            0.0028,
            -0.0063,
            0.1096,
            0.042,
            -0.0488,
            0.1969
          ]
        },
        "position_0": {
          "grad": [
            0.1332,
            -0.1567,
            0.0862,
            -0.3532,
            -0.1854,
            0.0565,
            -0.4772,
            -0.1369,
            0.3535,
            0.0415,
            0.255,
            0.0052,
            -0.444,
            0.035,
            0.174,
            -0.3842
          ],
          "after": [
            -0.0043,
            -0.1108,
            -0.252,
            0.0741,
            0.0267,
            -0.0322,
            -0.1244,
            0.0795,
            0.1184,
            -0.0364,
            0.0041,
            0.1021,
            0.0146,
            0.0034,
            -0.1083,
            0.0726
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0017,
            0.0001,
            -0.0003,
            0.0043,
            0.0006,
            0.0034,
            -0.0007,
            -0.0005,
            -0.0005,
            0.0002,
            -0.0011,
            0.0018,
            -0.0014,
            -0.0018,
            -0.0007
          ],
          "after": [
            -0.0198,
            0.0222,
            -0.0662,
            -0.0274,
            0.0345,
            0.1284,
            0.0519,
            -0.1617,
            0.0164,
            0.0165,
            -0.098,
            -0.0818,
            0.1206,
            -0.0576,
            0.0001,
            0.0727
          ]
        }
      }
    },
    {
      "step": 203,
      "word": "kendale",
      "loss": 2.2845,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1542,
            0.0953,
            -0.0311,
            -0.0782,
            0.0631,
            0.0061,
            -0.0978,
            0.1891,
            0.0024,
            -0.0037,
            -0.0668,
            0.22,
            -0.1019,
            -0.0213,
            0.0806,
            -0.1259
          ],
          "after": [
            -0.0156,
            0.0195,
            0.1607,
            -0.0254,
            0.0142,
            -0.0337,
            -0.0761,
            -0.1425,
            -0.0261,
            0.0455,
            0.1403,
            -0.0098,
            -0.0662,
            -0.0186,
            -0.0377,
            -0.1472
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3156,
            -0.4731,
            0.0324,
            -0.1053,
            0.2318,
            0.3728,
            -0.6229,
            0.0216,
            0.5137,
            -0.0144,
            0.043,
            0.0083,
            -0.3046,
            0.219,
            0.1498,
            -0.549
          ],
          "after": [
            0.0123,
            0.0905,
            -0.0315,
            -0.0571,
            0.0133,
            -0.0075,
            0.0785,
            0.1068,
            -0.0687,
            -0.0694,
            0.0026,
            -0.0061,
            0.1105,
            0.0401,
            -0.0502,
            0.1988
          ]
        },
        "position_0": {
          "grad": [
            -0.0543,
            0.0047,
            -0.1011,
            -0.0077,
            -0.0277,
            -0.0186,
            0.0653,
            -0.0063,
            0.028,
            -0.0015,
            -0.0276,
            -0.0454,
            -0.0405,
            0.0284,
            -0.0376,
            -0.0953
          ],
          "after": [
            -0.0039,
            -0.1109,
            -0.2514,
            0.0748,
            0.0268,
            -0.0326,
            -0.1237,
            0.08,
            0.1179,
            -0.037,
            0.0036,
            0.1023,
            0.0153,
            0.0033,
            -0.1083,
            0.0735
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0021,
            -0.001,
            0.0001,
            -0.0002,
            0.0015,
            -0.0016,
            0.0004,
            0.0021,
            0.0013,
            -0.0003,
            0.0019,
            -0.0015,
            0.0014,
            -0.0002,
            0.0003
          ],
          "after": [
            -0.0188,
            0.0215,
            -0.0665,
            -0.0278,
            0.0342,
            0.1276,
            0.0519,
            -0.1624,
            0.0147,
            0.0151,
            -0.0979,
            -0.0827,
            0.1208,
            -0.0578,
            0.0011,
            0.0718
          ]
        }
      }
    },
    {
      "step": 204,
      "word": "malerie",
      "loss": 2.0201,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0765,
            0.1672,
            -0.0246,
            -0.1203,
            0.0587,
            -0.0094,
            -0.0972,
            0.1236,
            -0.1075,
            0.0311,
            -0.0438,
            0.1794,
            -0.0557,
            -0.0101,
            0.0314,
            -0.2723
          ],
          "after": [
            -0.0153,
            0.0192,
            0.1605,
            -0.0238,
            0.0137,
            -0.0345,
            -0.0749,
            -0.143,
            -0.027,
            0.0462,
            0.1393,
            -0.0111,
            -0.0641,
            -0.0183,
            -0.0388,
            -0.1458
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1368,
            -0.4027,
            -0.0054,
            -0.0095,
            -0.2613,
            0.1371,
            -0.5196,
            0.161,
            0.2763,
            -0.2148,
            -0.0916,
            -0.019,
            -0.159,
            0.0544,
            0.6193,
            -0.3744
          ],
          "after": [
            0.0103,
            0.0926,
            -0.0316,
            -0.0565,
            0.0147,
            -0.009,
            0.0807,
            0.106,
            -0.0707,
            -0.0683,
            0.0027,
            -0.0059,
            0.1115,
            0.0384,
            -0.0521,
            0.2011
          ]
        },
        "position_0": {
          "grad": [
            -0.0212,
            0.0038,
            0.0192,
            -0.019,
            0.0846,
            0.0386,
            0.0846,
            0.1022,
            0.1236,
            0.036,
            -0.0112,
            0.0893,
            -0.2015,
            -0.0529,
            0.029,
            0.016
          ],
          "after": [
            -0.0035,
            -0.111,
            -0.251,
            0.0754,
            0.0267,
            -0.0332,
            -0.1233,
            0.0801,
            0.1172,
            -0.0376,
            0.0032,
            0.1022,
            0.0163,
            0.0034,
            -0.1083,
            0.0742
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0041,
            0.0002,
            0.0005,
            -0.0023,
            -0.0,
            -0.0057,
            0.0011,
            0.0017,
            -0.0015,
            -0.0002,
            0.0016,
            -0.0041,
            0.001,
            0.0038,
            -0.0007
          ],
          "after": [
            -0.0182,
            0.0218,
            -0.0669,
            -0.0283,
            0.0344,
            0.127,
            0.0528,
            -0.1633,
            0.0127,
            0.0143,
            -0.0977,
            -0.0838,
            0.1218,
            -0.0583,
            0.001,
            0.0712
          ]
        }
      }
    },
    {
      "step": 205,
      "word": "mackinley",
      "loss": 2.5866,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0949,
            0.1328,
            0.0648,
            0.1815,
            -0.0043,
            -0.014,
            0.2307,
            0.0932,
            -0.1161,
            0.122,
            0.0244,
            0.1027,
            0.1136,
            -0.0766,
            -0.1606,
            0.0866
          ],
          "after": [
            -0.0148,
            0.0184,
            0.1602,
            -0.023,
            0.0133,
            -0.0352,
            -0.0744,
            -0.1438,
            -0.0275,
            0.0462,
            0.1384,
            -0.0125,
            -0.0626,
            -0.0178,
            -0.0391,
            -0.1448
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1054,
            -0.079,
            -0.1081,
            -0.0818,
            -0.0746,
            0.0913,
            -0.0841,
            0.125,
            -0.0067,
            0.1599,
            -0.1252,
            -0.1812,
            0.1267,
            0.0725,
            -0.0103,
            -0.1576
          ],
          "after": [
            0.0084,
            0.0945,
            -0.0315,
            -0.0557,
            0.016,
            -0.0104,
            0.0826,
            0.1048,
            -0.0723,
            -0.0679,
            0.0034,
            -0.0053,
            0.1121,
            0.0368,
            -0.0538,
            0.2033
          ]
        },
        "position_0": {
          "grad": [
            -0.0243,
            0.0237,
            0.0186,
            0.0185,
            0.1106,
            0.0343,
            0.1062,
            0.078,
            0.0673,
            0.0145,
            -0.0001,
            0.0868,
            -0.121,
            -0.0413,
            0.0062,
            0.0537
          ],
          "after": [
            -0.0031,
            -0.1111,
            -0.2507,
            0.0759,
            0.0262,
            -0.0338,
            -0.123,
            0.0799,
            0.1166,
            -0.0383,
            0.0029,
            0.1019,
            0.0173,
            0.0037,
            -0.1083,
            0.0746
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0054,
            -0.0005,
            0.0013,
            -0.0005,
            0.0004,
            -0.0016,
            -0.002,
            -0.0033,
            -0.0008,
            0.0021,
            0.002,
            -0.0053,
            0.0025,
            0.0026,
            -0.0033,
            -0.004
          ],
          "after": [
            -0.0194,
            0.0222,
            -0.0676,
            -0.0286,
            0.0345,
            0.1269,
            0.0538,
            -0.1631,
            0.0112,
            0.0131,
            -0.0983,
            -0.0835,
            0.1221,
            -0.0595,
            0.0017,
            0.0717
          ]
        }
      }
    },
    {
      "step": 206,
      "word": "ramzi",
      "loss": 2.9854,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1551,
            0.0859,
            0.2241,
            0.1653,
            0.3303,
            -0.0988,
            0.3586,
            0.2037,
            0.0536,
            -0.0569,
            0.1505,
            0.1568,
            -0.0982,
            -0.1933,
            -0.1425,
            0.1007
          ],
          "after": [
            -0.014,
            0.0175,
            0.1593,
            -0.0227,
            0.0121,
            -0.0352,
            -0.0747,
            -0.1451,
            -0.028,
            0.0465,
            0.1371,
            -0.014,
            -0.0611,
            -0.0167,
            -0.0388,
            -0.1441
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.045,
            0.1078,
            -0.1563,
            0.0548,
            0.0343,
            -0.1632,
            0.2624,
            0.018,
            -0.2821,
            0.1335,
            -0.0255,
            -0.0629,
            0.2407,
            -0.0683,
            -0.3245,
            0.1611
          ],
          "after": [
            0.0069,
            0.096,
            -0.031,
            -0.0552,
            0.0171,
            -0.0114,
            0.084,
            0.1037,
            -0.0733,
            -0.0679,
            0.0041,
            -0.0047,
            0.1122,
            0.0356,
            -0.0548,
            0.2049
          ]
        },
        "position_0": {
          "grad": [
            0.07,
            -0.0568,
            0.0657,
            -0.1958,
            0.0414,
            0.1351,
            -0.0073,
            -0.1231,
            0.1864,
            -0.0245,
            -0.0631,
            -0.0101,
            -0.2591,
            -0.0718,
            0.1601,
            0.0285
          ],
          "after": [
            -0.003,
            -0.1111,
            -0.2508,
            0.0767,
            0.0257,
            -0.0348,
            -0.1228,
            0.0802,
            0.1157,
            -0.0387,
            0.0028,
            0.1016,
            0.0186,
            0.0041,
            -0.1087,
            0.075
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0025,
            0.0019,
            0.0007,
            0.0005,
            0.0073,
            0.0017,
            0.0027,
            -0.0019,
            -0.0023,
            0.002,
            -0.0015,
            -0.0006,
            0.0026,
            0.0026,
            -0.0069,
            0.0015
          ],
          "after": [
            -0.0212,
            0.022,
            -0.0685,
            -0.0289,
            0.0332,
            0.1264,
            0.0542,
            -0.1624,
            0.0108,
            0.0114,
            -0.0982,
            -0.0831,
            0.1218,
            -0.0613,
            0.0037,
            0.0717
          ]
        }
      }
    },
    {
      "step": 207,
      "word": "hutton",
      "loss": 3.0765,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0133,
            0.0168,
            0.1585,
            -0.0225,
            0.0111,
            -0.0353,
            -0.0749,
            -0.1462,
            -0.0285,
            0.0468,
            0.136,
            -0.0153,
            -0.0598,
            -0.0157,
            -0.0386,
            -0.1436
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0707,
            0.179,
            -0.148,
            0.0923,
            0.0428,
            -0.2264,
            0.3546,
            0.0535,
            -0.3204,
            0.105,
            -0.0304,
            -0.0505,
            0.2925,
            -0.1249,
            -0.356,
            0.2431
          ],
          "after": [
            0.0058,
            0.0968,
            -0.0303,
            -0.0551,
            0.0179,
            -0.0118,
            0.0847,
            0.1024,
            -0.0737,
            -0.0683,
            0.0048,
            -0.004,
            0.1117,
            0.0348,
            -0.0552,
            0.2058
          ]
        },
        "position_0": {
          "grad": [
            0.0143,
            -0.0252,
            0.0895,
            0.1713,
            -0.0879,
            0.0882,
            0.1099,
            0.1142,
            0.0349,
            -0.0316,
            -0.1205,
            -0.2485,
            0.2988,
            0.178,
            0.0761,
            0.1913
          ],
          "after": [
            -0.003,
            -0.111,
            -0.2512,
            0.077,
            0.0255,
            -0.0361,
            -0.1228,
            0.08,
            0.1148,
            -0.0389,
            0.0031,
            0.1021,
            0.0192,
            0.0039,
            -0.1093,
            0.075
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0095,
            -0.0038,
            0.0064,
            0.0006,
            -0.0032,
            0.0138,
            0.0023,
            -0.0039,
            -0.0033,
            -0.0026,
            -0.0047,
            0.0127,
            -0.0008,
            -0.0037,
            0.0069
          ],
          "after": [
            -0.0229,
            0.0201,
            -0.068,
            -0.0311,
            0.032,
            0.1268,
            0.0528,
            -0.1624,
            0.0116,
            0.011,
            -0.0973,
            -0.0817,
            0.1196,
            -0.0625,
            0.0061,
            0.0703
          ]
        }
      }
    },
    {
      "step": 208,
      "word": "evamaria",
      "loss": 2.4022,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0789,
            0.0312,
            0.1821,
            -0.0827,
            0.2259,
            0.13,
            0.1342,
            -0.0235,
            0.2081,
            0.0218,
            0.144,
            0.1749,
            -0.2501,
            -0.2155,
            -0.0452,
            0.1388
          ],
          "after": [
            -0.0125,
            0.016,
            0.1573,
            -0.0221,
            0.0097,
            -0.036,
            -0.0753,
            -0.1471,
            -0.0296,
            0.0469,
            0.1346,
            -0.0168,
            -0.058,
            -0.0142,
            -0.0382,
            -0.1434
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1761,
            0.301,
            0.4289,
            -0.0089,
            0.1812,
            -0.1674,
            0.2929,
            -0.0354,
            -0.1384,
            0.0006,
            0.0539,
            -0.0101,
            0.0585,
            -0.0612,
            -0.3002,
            -0.0875
          ],
          "after": [
            0.0054,
            0.0969,
            -0.0306,
            -0.055,
            0.0183,
            -0.0119,
            0.0849,
            0.1016,
            -0.0738,
            -0.0686,
            0.0052,
            -0.0034,
            0.1112,
            0.0343,
            -0.0551,
            0.2067
          ]
        },
        "position_0": {
          "grad": [
            0.0228,
            -0.1449,
            0.0037,
            -0.0864,
            -0.01,
            0.0711,
            -0.1853,
            -0.1003,
            0.1361,
            0.1258,
            0.0446,
            0.0054,
            -0.1618,
            -0.0135,
            0.0581,
            -0.2187
          ],
          "after": [
            -0.0031,
            -0.1105,
            -0.2517,
            0.0775,
            0.0254,
            -0.0375,
            -0.1225,
            0.0802,
            0.1138,
            -0.0398,
            0.0032,
            0.1025,
            0.02,
            0.0037,
            -0.1099,
            0.0753
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            -0.0013,
            -0.0002,
            0.0023,
            -0.005,
            0.0002,
            -0.003,
            0.002,
            -0.0003,
            0.001,
            0.0013,
            -0.0041,
            0.0009,
            -0.0002,
            0.0018,
            -0.0008
          ],
          "after": [
            -0.0247,
            0.0186,
            -0.0675,
            -0.0336,
            0.0319,
            0.1272,
            0.052,
            -0.1631,
            0.0124,
            0.0104,
            -0.0969,
            -0.0797,
            0.1175,
            -0.0635,
            0.0078,
            0.0692
          ]
        }
      }
    },
    {
      "step": 209,
      "word": "momen",
      "loss": 2.301,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0119,
            0.0154,
            0.1563,
            -0.0217,
            0.0085,
            -0.0365,
            -0.0757,
            -0.1478,
            -0.0305,
            0.047,
            0.1334,
            -0.018,
            -0.0565,
            -0.0129,
            -0.0379,
            -0.1432
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1092,
            -0.149,
            0.1699,
            -0.1277,
            0.0695,
            0.011,
            -0.093,
            -0.0049,
            0.1841,
            -0.2492,
            0.0928,
            0.0559,
            -0.2414,
            -0.2073,
            0.3361,
            -0.1843
          ],
          "after": [
            0.0052,
            0.0973,
            -0.0313,
            -0.0545,
            0.0185,
            -0.012,
            0.0853,
            0.1009,
            -0.0742,
            -0.0681,
            0.0051,
            -0.003,
            0.1112,
            0.0343,
            -0.0555,
            0.2078
          ]
        },
        "position_0": {
          "grad": [
            -0.0501,
            0.0196,
            0.0387,
            0.0612,
            0.1437,
            0.0601,
            0.1743,
            0.1513,
            0.121,
            -0.0069,
            -0.0045,
            0.0807,
            -0.1701,
            -0.0289,
            0.0394,
            0.0973
          ],
          "after": [
            -0.003,
            -0.1101,
            -0.2522,
            0.0778,
            0.0249,
            -0.039,
            -0.1226,
            0.0798,
            0.1128,
            -0.0405,
            0.0033,
            0.1026,
            0.021,
            0.0037,
            -0.1106,
            0.0755
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0005,
            0.0003,
            0.0001,
            0.0007,
            -0.0015,
            0.0041,
            0.002,
            -0.0018,
            -0.0021,
            -0.0013,
            0.0009,
            0.0028,
            -0.0019,
            -0.0006,
            0.0004
          ],
          "after": [
            -0.0258,
            0.0175,
            -0.0671,
            -0.0358,
            0.0317,
            0.1279,
            0.0508,
            -0.1642,
            0.0136,
            0.0104,
            -0.0962,
            -0.0782,
            0.1154,
            -0.0638,
            0.0094,
            0.0682
          ]
        }
      }
    },
    {
      "step": 210,
      "word": "xandria",
      "loss": 2.4955,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1502,
            0.0518,
            -0.167,
            0.0205,
            -0.3508,
            -0.0418,
            -0.0151,
            -0.1829,
            -0.0863,
            -0.0666,
            -0.0442,
            -0.2301,
            0.1527,
            0.1973,
            0.1099,
            0.0072
          ],
          "after": [
            -0.0117,
            0.0146,
            0.1559,
            -0.0215,
            0.0083,
            -0.0368,
            -0.076,
            -0.1478,
            -0.031,
            0.0474,
            0.1325,
            -0.0186,
            -0.0557,
            -0.0125,
            -0.038,
            -0.143
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0569,
            0.1476,
            -0.0674,
            0.0528,
            0.091,
            -0.1664,
            0.2836,
            0.0067,
            -0.2341,
            0.1101,
            -0.0076,
            0.0106,
            0.1612,
            -0.1093,
            -0.2813,
            0.1517
          ],
          "after": [
            0.0053,
            0.0973,
            -0.0317,
            -0.0542,
            0.0186,
            -0.0118,
            0.0852,
            0.1002,
            -0.0742,
            -0.068,
            0.0051,
            -0.0027,
            0.1109,
            0.0346,
            -0.0554,
            0.2085
          ]
        },
        "position_0": {
          "grad": [
            -0.0373,
            0.1335,
            -0.0278,
            0.3197,
            0.0248,
            -0.1859,
            0.1632,
            0.0302,
            -0.2984,
            -0.1066,
            -0.0767,
            -0.1426,
            0.3777,
            -0.1918,
            -0.2019,
            0.2186
          ],
          "after": [
            -0.0027,
            -0.1101,
            -0.2525,
            0.0773,
            0.0243,
            -0.0394,
            -0.1229,
            0.0794,
            0.1124,
            -0.0405,
            0.0037,
            0.1031,
            0.0212,
            0.0043,
            -0.1106,
            0.0752
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0009,
            0.0003,
            0.0009,
            -0.0007,
            -0.0007,
            -0.0009,
            -0.0001,
            -0.0003,
            0.0001,
            0.0008,
            0.0005,
            -0.0006,
            -0.0001,
            0.0006,
            -0.0001
          ],
          "after": [
            -0.0268,
            0.0167,
            -0.067,
            -0.0379,
            0.0317,
            0.1287,
            0.0499,
            -0.1651,
            0.0147,
            0.0105,
            -0.0958,
            -0.0771,
            0.1136,
            -0.064,
            0.0106,
            0.0673
          ]
        }
      }
    },
    {
      "step": 211,
      "word": "demere",
      "loss": 2.2592,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0115,
            0.014,
            0.1555,
            -0.0213,
            0.0082,
            -0.0371,
            -0.0763,
            -0.1478,
            -0.0314,
            0.0477,
            0.1317,
            -0.0191,
            -0.0549,
            -0.0121,
            -0.0381,
            -0.1429
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1293,
            -0.7119,
            0.5046,
            -0.4304,
            0.1396,
            0.4278,
            -1.0814,
            -0.1314,
            1.1397,
            -0.3702,
            0.2849,
            0.1878,
            -0.9791,
            0.0563,
            1.0395,
            -0.6013
          ],
          "after": [
            0.0056,
            0.0986,
            -0.033,
            -0.0526,
            0.0183,
            -0.0123,
            0.0864,
            0.1003,
            -0.0757,
            -0.0669,
            0.004,
            -0.0029,
            0.1123,
            0.0347,
            -0.0566,
            0.2101
          ]
        },
        "position_0": {
          "grad": [
            -0.0851,
            0.0444,
            -0.0078,
            -0.0519,
            0.1257,
            -0.0596,
            0.0709,
            -0.0164,
            0.0195,
            -0.006,
            -0.1772,
            -0.0095,
            0.0436,
            0.09,
            0.066,
            0.1658
          ],
          "after": [
            -0.0021,
            -0.1103,
            -0.2528,
            0.077,
            0.0235,
            -0.0395,
            -0.1233,
            0.0791,
            0.1121,
            -0.0404,
            0.0045,
            0.1036,
            0.0212,
            0.0045,
            -0.1108,
            0.0746
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            -0.0015,
            0.0022,
            -0.0023,
            0.0007,
            -0.0013,
            -0.0025,
            0.0001,
            0.0018,
            -0.0008,
            0.0006,
            -0.0001,
            -0.0009,
            -0.0004,
            -0.0007,
            0.0002
          ],
          "after": [
            -0.0272,
            0.0163,
            -0.0675,
            -0.039,
            0.0315,
            0.1298,
            0.0495,
            -0.1659,
            0.0151,
            0.0107,
            -0.0957,
            -0.076,
            0.1122,
            -0.064,
            0.0118,
            0.0666
          ]
        }
      }
    },
    {
      "step": 212,
      "word": "kaylonnie",
      "loss": 2.281,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0753,
            -0.0574,
            -0.0088,
            -0.0649,
            -0.0547,
            0.0383,
            -0.0115,
            -0.0081,
            -0.0657,
            0.0606,
            -0.1001,
            0.023,
            0.0961,
            0.0983,
            0.0018,
            0.066
          ],
          "after": [
            -0.0115,
            0.0137,
            0.1553,
            -0.0209,
            0.0083,
            -0.0375,
            -0.0765,
            -0.1478,
            -0.0316,
            0.0477,
            0.1314,
            -0.0195,
            -0.0546,
            -0.0122,
            -0.0383,
            -0.1429
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1391,
            -0.0018,
            -0.1706,
            0.0883,
            -0.3351,
            -0.1493,
            0.0431,
            0.1286,
            -0.2221,
            0.0621,
            0.0081,
            -0.0606,
            0.1751,
            -0.0402,
            0.1388,
            -0.0254
          ],
          "after": [
            0.0056,
            0.0998,
            -0.0339,
            -0.0516,
            0.0187,
            -0.0126,
            0.0873,
            0.0998,
            -0.0766,
            -0.0661,
            0.003,
            -0.0029,
            0.1132,
            0.0348,
            -0.0578,
            0.2115
          ]
        },
        "position_0": {
          "grad": [
            -0.0565,
            0.0178,
            -0.0856,
            -0.0086,
            -0.0101,
            -0.0215,
            0.069,
            -0.0102,
            -0.0011,
            0.0018,
            -0.0111,
            -0.0099,
            -0.0246,
            -0.0066,
            -0.047,
            -0.0606
          ],
          "after": [
            -0.0014,
            -0.1105,
            -0.2526,
            0.0768,
            0.0228,
            -0.0395,
            -0.1237,
            0.0788,
            0.1119,
            -0.0404,
            0.0053,
            0.104,
            0.0213,
            0.0047,
            -0.1108,
            0.0742
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0036,
            -0.0003,
            -0.0014,
            0.0005,
            -0.0014,
            -0.0013,
            -0.003,
            0.0002,
            -0.0014,
            0.0003,
            0.0012,
            -0.003,
            0.0002,
            0.0007,
            0.0003,
            -0.0021
          ],
          "after": [
            -0.0286,
            0.0161,
            -0.0675,
            -0.0401,
            0.0316,
            0.1311,
            0.0495,
            -0.1666,
            0.0159,
            0.0109,
            -0.096,
            -0.0745,
            0.111,
            -0.0643,
            0.0127,
            0.0664
          ]
        }
      }
    },
    {
      "step": 213,
      "word": "druv",
      "loss": 3.7609,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0116,
            0.0134,
            0.1551,
            -0.0206,
            0.0083,
            -0.0378,
            -0.0766,
            -0.1478,
            -0.0317,
            0.0477,
            0.1312,
            -0.0199,
            -0.0543,
            -0.0122,
            -0.0384,
            -0.1429
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0771,
            0.1518,
            -0.2229,
            0.0757,
            -0.1268,
            -0.2493,
            0.3356,
            0.0552,
            -0.3943,
            0.175,
            -0.0868,
            -0.1265,
            0.4254,
            -0.0447,
            -0.3217,
            0.2101
          ],
          "after": [
            0.0053,
            0.1005,
            -0.0341,
            -0.0509,
            0.0193,
            -0.0123,
            0.0877,
            0.099,
            -0.0769,
            -0.0659,
            0.0025,
            -0.0027,
            0.1132,
            0.0351,
            -0.0585,
            0.2123
          ]
        },
        "position_0": {
          "grad": [
            -0.1184,
            0.0433,
            0.0304,
            0.0261,
            0.174,
            -0.0561,
            0.1511,
            0.0056,
            -0.0073,
            -0.0871,
            -0.2199,
            -0.0691,
            0.1102,
            0.1677,
            0.1373,
            0.2832
          ],
          "after": [
            -0.0002,
            -0.1108,
            -0.2526,
            0.0765,
            0.0216,
            -0.0393,
            -0.1243,
            0.0786,
            0.1116,
            -0.0399,
            0.0066,
            0.1045,
            0.0212,
            0.0044,
            -0.1112,
            0.0734
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0059,
            0.0045,
            -0.0017,
            0.0015,
            0.0012,
            -0.0049,
            0.0057,
            0.001,
            -0.0022,
            0.0046,
            -0.0006,
            -0.0011,
            0.0083,
            0.0011,
            -0.0046,
            0.0013
          ],
          "after": [
            -0.0311,
            0.015,
            -0.067,
            -0.0415,
            0.0315,
            0.1334,
            0.0487,
            -0.1675,
            0.0172,
            0.0098,
            -0.0961,
            -0.073,
            0.1088,
            -0.0648,
            0.0144,
            0.066
          ]
        }
      }
    },
    {
      "step": 214,
      "word": "jani",
      "loss": 2.0775,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1312,
            -0.0258,
            -0.3277,
            0.0823,
            -0.6413,
            -0.0996,
            -0.1057,
            -0.1559,
            0.0482,
            -0.1913,
            -0.2231,
            -0.3274,
            0.1739,
            0.3234,
            0.2747,
            0.0743
          ],
          "after": [
            -0.0119,
            0.0133,
            0.1558,
            -0.0206,
            0.0098,
            -0.0376,
            -0.0766,
            -0.1473,
            -0.0319,
            0.0485,
            0.1317,
            -0.0195,
            -0.0544,
            -0.0132,
            -0.0394,
            -0.1431
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0945,
            0.1161,
            -0.1399,
            -0.0065,
            -0.0174,
            -0.1614,
            0.3295,
            0.0402,
            -0.2573,
            0.106,
            -0.0455,
            -0.0123,
            0.2319,
            -0.0454,
            -0.2198,
            0.2061
          ],
          "after": [
            0.0054,
            0.1009,
            -0.034,
            -0.0504,
            0.0198,
            -0.0119,
            0.0877,
            0.0982,
            -0.0768,
            -0.0661,
            0.0022,
            -0.0024,
            0.1129,
            0.0354,
            -0.0587,
            0.2126
          ]
        },
        "position_0": {
          "grad": [
            -0.0891,
            0.1075,
            0.001,
            0.3059,
            0.0794,
            -0.1774,
            0.3093,
            0.1048,
            -0.3531,
            -0.1249,
            -0.0077,
            0.0693,
            0.5236,
            -0.2267,
            -0.246,
            0.2532
          ],
          "after": [
            0.0011,
            -0.1114,
            -0.2525,
            0.0756,
            0.0204,
            -0.0384,
            -0.1254,
            0.078,
            0.1121,
            -0.0388,
            0.0078,
            0.1048,
            0.0202,
            0.0048,
            -0.1109,
            0.0722
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0009,
            -0.0007,
            -0.0022,
            0.0034,
            0.0015,
            0.0017,
            -0.0011,
            -0.0003,
            0.0005,
            0.0006,
            -0.0017,
            -0.0001,
            -0.001,
            -0.0005,
            -0.0009
          ],
          "after": [
            -0.0331,
            0.0139,
            -0.0663,
            -0.042,
            0.0308,
            0.1349,
            0.0479,
            -0.168,
            0.0184,
            0.0087,
            -0.0964,
            -0.0714,
            0.1069,
            -0.065,
            0.0159,
            0.0658
          ]
        }
      }
    },
    {
      "step": 215,
      "word": "kaidence",
      "loss": 2.3933,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0723,
            -0.1852,
            0.0544,
            -0.1306,
            0.1043,
            0.0603,
            -0.2079,
            0.0205,
            0.0484,
            -0.0107,
            0.1142,
            -0.0494,
            -0.1753,
            0.005,
            0.0086,
            -0.1164
          ],
          "after": [
            -0.0121,
            0.0137,
            0.1563,
            -0.0202,
            0.0108,
            -0.0377,
            -0.0761,
            -0.1469,
            -0.0323,
            0.0493,
            0.1318,
            -0.0191,
            -0.0541,
            -0.0142,
            -0.0404,
            -0.143
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0528,
            -0.3144,
            0.021,
            -0.2665,
            -0.3604,
            0.178,
            -0.4557,
            -0.0152,
            0.2943,
            0.0425,
            -0.082,
            -0.2182,
            -0.1143,
            0.2122,
            0.2948,
            -0.3644
          ],
          "after": [
            0.0053,
            0.1018,
            -0.034,
            -0.0491,
            0.0208,
            -0.0118,
            0.0882,
            0.0976,
            -0.0772,
            -0.0663,
            0.0023,
            -0.0017,
            0.1128,
            0.0352,
            -0.0593,
            0.2135
          ]
        },
        "position_0": {
          "grad": [
            -0.0569,
            0.0313,
            -0.1061,
            -0.0392,
            0.008,
            -0.0244,
            0.0666,
            -0.0313,
            -0.0005,
            0.0216,
            -0.0016,
            0.0167,
            -0.04,
            -0.0149,
            -0.0672,
            -0.0785
          ],
          "after": [
            0.0025,
            -0.112,
            -0.252,
            0.075,
            0.0194,
            -0.0375,
            -0.1264,
            0.0777,
            0.1125,
            -0.038,
            0.0088,
            0.105,
            0.0194,
            0.0052,
            -0.1105,
            0.0713
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0008,
            -0.0019,
            -0.0007,
            -0.0015,
            0.0036,
            -0.0022,
            0.0009,
            0.0023,
            0.0017,
            -0.0008,
            -0.0002,
            -0.0024,
            0.0019,
            -0.0001,
            -0.0006
          ],
          "after": [
            -0.0347,
            0.0131,
            -0.0652,
            -0.0423,
            0.0305,
            0.1353,
            0.0475,
            -0.1686,
            0.0187,
            0.0074,
            -0.0963,
            -0.0699,
            0.1057,
            -0.0657,
            0.0173,
            0.0658
          ]
        }
      }
    },
    {
      "step": 216,
      "word": "zared",
      "loss": 2.6237,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1076,
            -0.1632,
            -0.0723,
            -0.1729,
            0.0843,
            0.0291,
            -0.0204,
            -0.2235,
            0.1463,
            -0.1009,
            0.067,
            -0.0462,
            -0.1146,
            -0.0797,
            0.0622,
            0.1482
          ],
          "after": [
            -0.0125,
            0.0146,
            0.1569,
            -0.0194,
            0.0115,
            -0.038,
            -0.0756,
            -0.1458,
            -0.0331,
            0.0503,
            0.1317,
            -0.0186,
            -0.0536,
            -0.0147,
            -0.0414,
            -0.1432
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1596,
            -0.196,
            0.0233,
            -0.0886,
            0.2709,
            0.0159,
            -0.2183,
            0.058,
            0.028,
            -0.2542,
            0.0126,
            0.0416,
            -0.1905,
            -0.1455,
            0.2477,
            -0.0946
          ],
          "after": [
            0.0056,
            0.103,
            -0.034,
            -0.0477,
            0.0212,
            -0.0117,
            0.0889,
            0.0968,
            -0.0775,
            -0.0658,
            0.0024,
            -0.0011,
            0.113,
            0.0353,
            -0.0601,
            0.2144
          ]
        },
        "position_0": {
          "grad": [
            -0.0306,
            0.1063,
            -0.0428,
            0.2464,
            0.0688,
            -0.0635,
            0.3154,
            0.006,
            0.0019,
            0.0707,
            -0.0483,
            0.1211,
            0.0153,
            -0.0712,
            0.0129,
            0.1121
          ],
          "after": [
            0.0038,
            -0.1128,
            -0.2514,
            0.0739,
            0.0182,
            -0.0365,
            -0.1278,
            0.0773,
            0.1128,
            -0.0376,
            0.0098,
            0.1048,
            0.0187,
            0.0057,
            -0.1101,
            0.0704
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0005,
            0.0001,
            -0.0,
            0.0017,
            0.0005,
            -0.0007,
            0.0011,
            0.0003,
            -0.0005,
            0.0001,
            0.0008,
            -0.0014,
            -0.0011,
            0.0007,
            0.0002
          ],
          "after": [
            -0.036,
            0.0126,
            -0.0643,
            -0.0425,
            0.0299,
            0.1355,
            0.0472,
            -0.1695,
            0.0189,
            0.0063,
            -0.0963,
            -0.0688,
            0.1049,
            -0.066,
            0.0183,
            0.0658
          ]
        }
      }
    },
    {
      "step": 217,
      "word": "jrake",
      "loss": 2.2877,
      "learning_rate": 0.0024,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2339,
            0.1355,
            0.0825,
            -0.0342,
            0.0374,
            -0.1751,
            0.1958,
            0.0196,
            -0.0468,
            -0.1076,
            -0.0224,
            -0.0729,
            0.0119,
            -0.0754,
            -0.0996,
            -0.2084
          ],
          "after": [
            -0.0122,
            0.015,
            0.1572,
            -0.0187,
            0.012,
            -0.0374,
            -0.0756,
            -0.145,
            -0.0337,
            0.0516,
            0.1316,
            -0.0181,
            -0.0531,
            -0.0149,
            -0.042,
            -0.143
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1692,
            -0.2054,
            -0.1391,
            -0.0003,
            -0.2511,
            0.2608,
            -0.2585,
            -0.0845,
            0.0237,
            -0.1633,
            -0.0886,
            -0.1493,
            0.1785,
            0.2611,
            -0.018,
            -0.1295
          ],
          "after": [
            0.0054,
            0.1044,
            -0.0337,
            -0.0465,
            0.022,
            -0.0121,
            0.0897,
            0.0966,
            -0.0777,
            -0.0649,
            0.0028,
            -0.0003,
            0.113,
            0.0349,
            -0.0608,
            0.2154
          ]
        },
        "position_0": {
          "grad": [
            -0.0729,
            0.1173,
            -0.0309,
            0.1975,
            0.1417,
            -0.127,
            0.2852,
            0.0432,
            -0.3287,
            -0.0783,
            0.0477,
            0.1353,
            0.4237,
            -0.2152,
            -0.254,
            0.2383
          ],
          "after": [
            0.0053,
            -0.1139,
            -0.2507,
            0.0725,
            0.0168,
            -0.0351,
            -0.1295,
            0.0768,
            0.1137,
            -0.037,
            0.0105,
            0.1043,
            0.0174,
            0.0069,
            -0.1092,
            0.0691
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0022,
            -0.0004,
            0.0013,
            0.0009,
            0.0007,
            0.0013,
            -0.0023,
            -0.0018,
            0.0,
            0.001,
            0.002,
            -0.0003,
            -0.0004,
            0.0011,
            -0.003,
            -0.0018
          ],
          "after": [
            -0.0377,
            0.0122,
            -0.0639,
            -0.043,
            0.0292,
            0.1353,
            0.0473,
            -0.1697,
            0.019,
            0.0052,
            -0.0969,
            -0.0678,
            0.1042,
            -0.0665,
            0.0197,
            0.0662
          ]
        }
      }
    },
    {
      "step": 218,
      "word": "daryus",
      "loss": 2.5956,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0686,
            -0.11,
            -0.0521,
            -0.116,
            0.1483,
            0.03,
            0.0236,
            -0.1953,
            0.0909,
            -0.0922,
            0.0864,
            0.0041,
            -0.0607,
            -0.0812,
            0.0181,
            0.1678
          ],
          "after": [
            -0.0121,
            0.0156,
            0.1576,
            -0.0177,
            0.012,
            -0.037,
            -0.0757,
            -0.1436,
            -0.0344,
            0.0531,
            0.1313,
            -0.0176,
            -0.0526,
            -0.0148,
            -0.0425,
            -0.1431
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0078,
            0.1132,
            -0.0793,
            0.041,
            0.0264,
            -0.1756,
            0.2053,
            0.0399,
            -0.239,
            0.1211,
            0.004,
            -0.0023,
            0.1756,
            -0.0709,
            -0.2317,
            0.1166
          ],
          "after": [
            0.0053,
            0.1053,
            -0.0333,
            -0.0456,
            0.0226,
            -0.0121,
            0.0902,
            0.0961,
            -0.0777,
            -0.0645,
            0.0031,
            0.0004,
            0.1126,
            0.0346,
            -0.061,
            0.2161
          ]
        },
        "position_0": {
          "grad": [
            -0.0864,
            0.0222,
            -0.0065,
            -0.0733,
            0.1212,
            -0.0372,
            0.0435,
            -0.0287,
            0.0331,
            -0.018,
            -0.1472,
            0.006,
            -0.0034,
            0.0852,
            0.103,
            0.1405
          ],
          "after": [
            0.0069,
            -0.1149,
            -0.25,
            0.0715,
            0.0153,
            -0.0338,
            -0.1309,
            0.0766,
            0.1144,
            -0.0363,
            0.0116,
            0.1038,
            0.0163,
            0.0076,
            -0.1087,
            0.0678
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0009,
            -0.0005,
            -0.0004,
            0.0015,
            -0.0001,
            0.002,
            0.0017,
            0.0002,
            0.0018,
            0.0007,
            0.0033,
            -0.0003,
            -0.0016,
            -0.0012,
            0.0015
          ],
          "after": [
            -0.0391,
            0.0117,
            -0.0634,
            -0.0432,
            0.0284,
            0.1351,
            0.0471,
            -0.1704,
            0.0191,
            0.0038,
            -0.0978,
            -0.0677,
            0.1037,
            -0.0665,
            0.0212,
            0.0661
          ]
        }
      }
    },
    {
      "step": 219,
      "word": "auston",
      "loss": 2.3247,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1164,
            0.0988,
            -0.0833,
            0.1518,
            0.1604,
            -0.2281,
            -0.2058,
            -0.079,
            -0.046,
            -0.0061,
            0.0159,
            -0.0887,
            0.1418,
            -0.0762,
            -0.2715,
            0.0363
          ],
          "after": [
            -0.0124,
            0.0159,
            0.1582,
            -0.0173,
            0.0117,
            -0.0356,
            -0.0753,
            -0.1422,
            -0.0349,
            0.0544,
            0.1309,
            -0.017,
            -0.0525,
            -0.0145,
            -0.0419,
            -0.1433
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0497,
            0.0725,
            -0.0261,
            0.0171,
            0.0782,
            -0.1449,
            0.1561,
            0.0243,
            -0.1312,
            0.0739,
            0.0296,
            0.0311,
            0.1062,
            -0.0851,
            -0.1936,
            0.0809
          ],
          "after": [
            0.0053,
            0.106,
            -0.0329,
            -0.0449,
            0.023,
            -0.0119,
            0.0905,
            0.0957,
            -0.0775,
            -0.0644,
            0.0032,
            0.0009,
            0.1121,
            0.0346,
            -0.061,
            0.2165
          ]
        },
        "position_0": {
          "grad": [
            0.1241,
            -0.0849,
            0.1104,
            -0.1724,
            -0.0471,
            0.0535,
            -0.2973,
            -0.1074,
            0.2432,
            -0.0078,
            0.2318,
            0.0159,
            -0.2418,
            0.1,
            0.0669,
            -0.2093
          ],
          "after": [
            0.0077,
            -0.1154,
            -0.25,
            0.071,
            0.0141,
            -0.0329,
            -0.1317,
            0.0767,
            0.1146,
            -0.0356,
            0.0117,
            0.1033,
            0.0158,
            0.0079,
            -0.1084,
            0.0671
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0006,
            -0.0008,
            -0.0004,
            0.0008,
            -0.0006,
            0.0015,
            0.0005,
            0.0011,
            0.0009,
            0.0001,
            0.0011,
            0.0004,
            -0.0002,
            -0.0003,
            0.001
          ],
          "after": [
            -0.0403,
            0.0111,
            -0.0628,
            -0.0433,
            0.0276,
            0.1352,
            0.0467,
            -0.1711,
            0.0188,
            0.0023,
            -0.0985,
            -0.0678,
            0.1032,
            -0.0665,
            0.0226,
            0.0659
          ]
        }
      }
    },
    {
      "step": 220,
      "word": "raylei",
      "loss": 2.3762,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1037,
            -0.0906,
            -0.0042,
            -0.0762,
            -0.0774,
            0.0371,
            -0.0399,
            0.0173,
            -0.0821,
            0.0833,
            -0.1406,
            0.0396,
            0.1342,
            0.1543,
            0.0027,
            0.0712
          ],
          "after": [
            -0.0129,
            0.0164,
            0.1587,
            -0.0168,
            0.0117,
            -0.0346,
            -0.0749,
            -0.1411,
            -0.035,
            0.0551,
            0.1311,
            -0.0166,
            -0.0528,
            -0.0147,
            -0.0415,
            -0.1436
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0217,
            -0.2083,
            0.1367,
            -0.0391,
            0.1167,
            0.1074,
            -0.2308,
            0.0468,
            0.1114,
            0.0096,
            -0.14,
            0.025,
            -0.0882,
            0.153,
            -0.0615,
            -0.1727
          ],
          "after": [
            0.0052,
            0.107,
            -0.0329,
            -0.0442,
            0.0231,
            -0.0119,
            0.091,
            0.095,
            -0.0774,
            -0.0643,
            0.0039,
            0.0013,
            0.1118,
            0.0343,
            -0.061,
            0.2171
          ]
        },
        "position_0": {
          "grad": [
            0.0786,
            -0.1158,
            0.0883,
            -0.184,
            -0.0576,
            0.1252,
            -0.1027,
            -0.088,
            0.2584,
            -0.0319,
            -0.0965,
            -0.0707,
            -0.2779,
            -0.0031,
            0.1934,
            -0.0587
          ],
          "after": [
            0.008,
            -0.1155,
            -0.2505,
            0.071,
            0.0133,
            -0.0326,
            -0.1322,
            0.0772,
            0.1142,
            -0.0349,
            0.0122,
            0.1031,
            0.0158,
            0.0081,
            -0.1086,
            0.0666
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0017,
            0.0008,
            0.0015,
            -0.0029,
            0.0027,
            0.0009,
            0.0007,
            -0.0017,
            0.0012,
            -0.0001,
            -0.0008,
            0.0004,
            -0.0002,
            0.0011,
            -0.0018,
            -0.0005
          ],
          "after": [
            -0.0408,
            0.0104,
            -0.0627,
            -0.0426,
            0.0263,
            0.135,
            0.0463,
            -0.1712,
            0.0183,
            0.0011,
            -0.0988,
            -0.068,
            0.1028,
            -0.0668,
            0.024,
            0.0658
          ]
        }
      }
    },
    {
      "step": 221,
      "word": "latif",
      "loss": 3.0928,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0485,
            -0.1724,
            0.0991,
            0.2827,
            -0.0709,
            0.1456,
            0.2672,
            0.0507,
            -0.2742,
            -0.1635,
            -0.2141,
            -0.3527,
            0.2966,
            0.0635,
            0.0324,
            0.2757
          ],
          "after": [
            -0.0131,
            0.0173,
            0.1588,
            -0.0171,
            0.0118,
            -0.0344,
            -0.0751,
            -0.1403,
            -0.0343,
            0.0564,
            0.132,
            -0.0154,
            -0.0538,
            -0.0151,
            -0.0412,
            -0.1444
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0599,
            0.1677,
            -0.0953,
            0.074,
            0.0099,
            -0.1743,
            0.2869,
            -0.0024,
            -0.2957,
            0.0928,
            0.0092,
            -0.0311,
            0.2085,
            -0.0856,
            -0.2834,
            0.1709
          ],
          "after": [
            0.0053,
            0.1075,
            -0.0326,
            -0.0438,
            0.0232,
            -0.0116,
            0.0911,
            0.0945,
            -0.077,
            -0.0645,
            0.0044,
            0.0017,
            0.1113,
            0.0342,
            -0.0606,
            0.2174
          ]
        },
        "position_0": {
          "grad": [
            0.0047,
            0.1005,
            -0.079,
            -0.1337,
            0.0132,
            0.0936,
            -0.1636,
            0.1341,
            0.0117,
            0.0034,
            -0.1431,
            0.1168,
            -0.1409,
            0.1192,
            0.1736,
            -0.2443
          ],
          "after": [
            0.0083,
            -0.116,
            -0.2504,
            0.0713,
            0.0125,
            -0.0328,
            -0.1323,
            0.0771,
            0.1139,
            -0.0343,
            0.013,
            0.1026,
            0.0161,
            0.008,
            -0.1093,
            0.0666
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            0.001,
            0.0022,
            -0.0033,
            0.0076,
            -0.0037,
            0.0037,
            -0.0013,
            -0.0028,
            0.0021,
            0.0003,
            0.0002,
            0.001,
            -0.0033,
            -0.0058,
            -0.0035
          ],
          "after": [
            -0.041,
            0.0097,
            -0.0633,
            -0.0411,
            0.0241,
            0.1357,
            0.0455,
            -0.1709,
            0.0186,
            -0.0004,
            -0.0992,
            -0.0682,
            0.1023,
            -0.0661,
            0.0264,
            0.0665
          ]
        }
      }
    },
    {
      "step": 222,
      "word": "radley",
      "loss": 2.4121,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1983,
            0.047,
            0.0167,
            -0.0333,
            0.2493,
            -0.0958,
            0.1053,
            -0.0261,
            -0.0072,
            -0.0473,
            -0.1242,
            0.0706,
            0.0947,
            0.0008,
            -0.0148,
            0.2339
          ],
          "after": [
            -0.0129,
            0.018,
            0.1589,
            -0.0172,
            0.0113,
            -0.0338,
            -0.0755,
            -0.1395,
            -0.0336,
            0.0577,
            0.1331,
            -0.0146,
            -0.0549,
            -0.0154,
            -0.0409,
            -0.1456
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0376,
            -0.1936,
            0.1271,
            -0.0018,
            0.0948,
            0.0745,
            -0.2125,
            0.0669,
            0.0887,
            0.0043,
            -0.146,
            0.036,
            -0.0692,
            0.1399,
            -0.0602,
            -0.1709
          ],
          "after": [
            0.0053,
            0.1083,
            -0.0327,
            -0.0435,
            0.0231,
            -0.0115,
            0.0914,
            0.0937,
            -0.0768,
            -0.0647,
            0.0055,
            0.0019,
            0.1109,
            0.0338,
            -0.0602,
            0.2179
          ]
        },
        "position_0": {
          "grad": [
            0.073,
            -0.0852,
            0.0761,
            -0.1742,
            0.003,
            0.1278,
            -0.0538,
            -0.1027,
            0.2164,
            -0.0239,
            -0.0703,
            -0.0265,
            -0.2515,
            -0.0198,
            0.1626,
            -0.019
          ],
          "after": [
            0.0082,
            -0.116,
            -0.2508,
            0.0719,
            0.0119,
            -0.0334,
            -0.1323,
            0.0774,
            0.1132,
            -0.0337,
            0.0139,
            0.1023,
            0.0168,
            0.0079,
            -0.1103,
            0.0666
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0004,
            0.0015,
            -0.0008,
            0.0015,
            -0.0009,
            -0.0009,
            -0.0004,
            -0.0,
            0.0003,
            0.0,
            -0.0001,
            0.0002,
            0.0003,
            -0.0014,
            -0.001
          ],
          "after": [
            -0.0411,
            0.0091,
            -0.0642,
            -0.0397,
            0.022,
            0.1366,
            0.0449,
            -0.1706,
            0.0189,
            -0.0019,
            -0.0996,
            -0.0684,
            0.1019,
            -0.0656,
            0.0286,
            0.0673
          ]
        }
      }
    },
    {
      "step": 223,
      "word": "averey",
      "loss": 2.2231,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1182,
            0.066,
            -0.1421,
            0.1457,
            0.2053,
            -0.0183,
            0.0513,
            -0.043,
            -0.1935,
            0.0178,
            -0.1283,
            -0.0767,
            0.2492,
            -0.127,
            -0.1917,
            0.3081
          ],
          "after": [
            -0.0124,
            0.0184,
            0.1594,
            -0.0177,
            0.0104,
            -0.0333,
            -0.076,
            -0.1387,
            -0.0324,
            0.0588,
            0.1345,
            -0.0138,
            -0.0564,
            -0.0153,
            -0.04,
            -0.1471
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0184,
            -0.2366,
            0.2472,
            -0.0358,
            0.2521,
            0.4401,
            -0.7486,
            -0.0958,
            0.5597,
            -0.2056,
            0.2048,
            0.155,
            -0.6888,
            0.1,
            0.4272,
            -0.2733
          ],
          "after": [
            0.0053,
            0.1094,
            -0.0332,
            -0.0431,
            0.0227,
            -0.0121,
            0.0924,
            0.0935,
            -0.0773,
            -0.0642,
            0.0055,
            0.0018,
            0.1117,
            0.0332,
            -0.0603,
            0.2188
          ]
        },
        "position_0": {
          "grad": [
            0.1403,
            -0.0866,
            0.1082,
            -0.2025,
            -0.0422,
            0.0484,
            -0.3229,
            -0.1107,
            0.2639,
            0.0179,
            0.2395,
            0.0241,
            -0.2686,
            0.0864,
            0.0423,
            -0.2354
          ],
          "after": [
            0.0076,
            -0.1159,
            -0.2516,
            0.0729,
            0.0115,
            -0.0342,
            -0.1318,
            0.0781,
            0.1122,
            -0.0333,
            0.0139,
            0.1019,
            0.0178,
            0.0076,
            -0.1112,
            0.0671
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            -0.0002,
            0.0004,
            -0.0011,
            0.0006,
            0.0014,
            -0.0026,
            -0.0002,
            0.0008,
            -0.0011,
            0.0009,
            0.001,
            -0.0031,
            -0.0003,
            0.0017,
            -0.0007
          ],
          "after": [
            -0.0408,
            0.0087,
            -0.0651,
            -0.0381,
            0.02,
            0.137,
            0.0447,
            -0.1702,
            0.019,
            -0.0028,
            -0.1001,
            -0.0688,
            0.102,
            -0.0651,
            0.0301,
            0.0682
          ]
        }
      }
    },
    {
      "step": 224,
      "word": "saanya",
      "loss": 2.4196,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3209,
            0.1454,
            -0.0772,
            -0.1385,
            -0.4662,
            0.1353,
            -0.3376,
            -0.129,
            0.2206,
            0.126,
            0.3819,
            -0.1252,
            -0.2624,
            0.246,
            0.0677,
            -0.3274
          ],
          "after": [
            -0.0127,
            0.0182,
            0.16,
            -0.0178,
            0.0107,
            -0.0334,
            -0.0756,
            -0.1376,
            -0.0322,
            0.0591,
            0.1345,
            -0.0128,
            -0.0571,
            -0.016,
            -0.0394,
            -0.1478
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0558,
            0.0131,
            -0.0063,
            0.0139,
            0.0773,
            -0.1024,
            0.1311,
            0.0132,
            -0.0989,
            0.0687,
            0.0276,
            0.0688,
            0.0187,
            -0.0931,
            -0.1065,
            0.066
          ],
          "after": [
            0.0054,
            0.1103,
            -0.0337,
            -0.0428,
            0.0221,
            -0.0125,
            0.0932,
            0.0933,
            -0.0776,
            -0.064,
            0.0055,
            0.0015,
            0.1123,
            0.033,
            -0.0603,
            0.2194
          ]
        },
        "position_0": {
          "grad": [
            0.0261,
            0.0719,
            -0.0132,
            -0.0092,
            -0.2234,
            0.0637,
            -0.0763,
            0.0607,
            0.0275,
            0.0969,
            -0.1324,
            0.015,
            0.0175,
            0.02,
            0.1645,
            -0.0987
          ],
          "after": [
            0.0069,
            -0.1159,
            -0.2522,
            0.0737,
            0.0118,
            -0.0351,
            -0.1312,
            0.0784,
            0.1112,
            -0.0334,
            0.0144,
            0.1015,
            0.0186,
            0.0072,
            -0.1124,
            0.0677
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0003,
            0.0001,
            0.0001,
            0.0007,
            -0.0005,
            0.0007,
            0.0001,
            -0.0007,
            0.0002,
            -0.0,
            -0.0001,
            0.0005,
            -0.0005,
            -0.0003,
            0.0
          ],
          "after": [
            -0.0406,
            0.0084,
            -0.0659,
            -0.0368,
            0.0183,
            0.1374,
            0.0445,
            -0.17,
            0.0192,
            -0.0036,
            -0.1006,
            -0.069,
            0.102,
            -0.0645,
            0.0315,
            0.0689
          ]
        }
      }
    },
    {
      "step": 225,
      "word": "jaycee",
      "loss": 2.169,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1108,
            -0.0746,
            -0.0221,
            -0.0801,
            -0.0574,
            0.0421,
            -0.028,
            0.0108,
            -0.104,
            0.0728,
            -0.1322,
            0.0351,
            0.1528,
            0.1647,
            -0.005,
            0.0895
          ],
          "after": [
            -0.0133,
            0.0183,
            0.1606,
            -0.0177,
            0.0111,
            -0.0337,
            -0.0753,
            -0.1367,
            -0.0316,
            0.0591,
            0.1348,
            -0.012,
            -0.058,
            -0.017,
            -0.039,
            -0.1485
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0389,
            -0.3217,
            -0.0822,
            -0.1179,
            -0.7052,
            0.3038,
            -0.4154,
            -0.1671,
            0.3386,
            -0.1677,
            -0.2222,
            -0.1003,
            -0.0716,
            0.4844,
            0.6412,
            -0.2816
          ],
          "after": [
            0.0053,
            0.1117,
            -0.0339,
            -0.0422,
            0.0228,
            -0.0133,
            0.0943,
            0.0939,
            -0.0783,
            -0.0634,
            0.0063,
            0.0015,
            0.113,
            0.0317,
            -0.0611,
            0.2205
          ]
        },
        "position_0": {
          "grad": [
            -0.0707,
            0.101,
            -0.0336,
            0.1718,
            0.1243,
            -0.1032,
            0.2326,
            0.041,
            -0.2804,
            -0.0697,
            0.0518,
            0.114,
            0.3605,
            -0.186,
            -0.2166,
            0.2049
          ],
          "after": [
            0.0067,
            -0.1163,
            -0.2526,
            0.0741,
            0.0117,
            -0.0354,
            -0.1311,
            0.0786,
            0.111,
            -0.0332,
            0.0146,
            0.1009,
            0.0187,
            0.0075,
            -0.1129,
            0.0678
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0003,
            0.0005,
            0.0012,
            -0.0008,
            0.0002,
            0.0005,
            -0.0004,
            -0.0008,
            0.0026,
            0.0003,
            0.0004,
            0.0006,
            0.0008,
            -0.0013,
            0.0005
          ],
          "after": [
            -0.0404,
            0.0082,
            -0.0668,
            -0.036,
            0.0169,
            0.1378,
            0.0443,
            -0.1696,
            0.0197,
            -0.005,
            -0.1012,
            -0.0693,
            0.1019,
            -0.0642,
            0.033,
            0.0694
          ]
        }
      }
    },
    {
      "step": 226,
      "word": "allison",
      "loss": 2.4084,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0828,
            0.1958,
            -0.1029,
            -0.0641,
            0.0275,
            0.044,
            -0.0363,
            0.1588,
            -0.1513,
            -0.0051,
            -0.132,
            0.1761,
            -0.0018,
            0.0298,
            0.0874,
            -0.1868
          ],
          "after": [
            -0.0136,
            0.0178,
            0.1614,
            -0.0174,
            0.0113,
            -0.0341,
            -0.075,
            -0.1364,
            -0.0306,
            0.0592,
            0.1355,
            -0.0117,
            -0.0588,
            -0.0181,
            -0.0389,
            -0.1488
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0399,
            0.1038,
            -0.083,
            0.0148,
            -0.0046,
            -0.1619,
            0.2443,
            -0.0157,
            -0.2178,
            0.0615,
            0.0263,
            -0.0101,
            0.159,
            -0.0761,
            -0.2148,
            0.1974
          ],
          "after": [
            0.0054,
            0.1127,
            -0.034,
            -0.0417,
            0.0234,
            -0.0137,
            0.095,
            0.0945,
            -0.0786,
            -0.0631,
            0.0069,
            0.0015,
            0.1133,
            0.0308,
            -0.0615,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            0.0963,
            -0.0734,
            0.0864,
            -0.175,
            -0.0011,
            0.0517,
            -0.2854,
            -0.0943,
            0.2183,
            0.046,
            0.2366,
            0.0737,
            -0.2374,
            0.0395,
            0.0155,
            -0.2029
          ],
          "after": [
            0.006,
            -0.1164,
            -0.2533,
            0.0748,
            0.0117,
            -0.0359,
            -0.1305,
            0.0791,
            0.1103,
            -0.0332,
            0.014,
            0.1001,
            0.0191,
            0.0076,
            -0.1134,
            0.0683
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0016,
            -0.0033,
            0.0014,
            -0.0015,
            0.0007,
            0.0003,
            -0.0022,
            0.0004,
            0.001,
            -0.0022,
            -0.0041,
            0.001,
            0.0022,
            0.005,
            0.0019
          ],
          "after": [
            -0.0401,
            0.0077,
            -0.0665,
            -0.0358,
            0.016,
            0.1379,
            0.044,
            -0.1687,
            0.02,
            -0.0064,
            -0.1009,
            -0.0687,
            0.1017,
            -0.0646,
            0.0332,
            0.0694
          ]
        }
      }
    },
    {
      "step": 227,
      "word": "cherie",
      "loss": 2.3738,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0138,
            0.0173,
            0.1621,
            -0.0171,
            0.0116,
            -0.0345,
            -0.0747,
            -0.1362,
            -0.0298,
            0.0592,
            0.1361,
            -0.0115,
            -0.0594,
            -0.0189,
            -0.0388,
            -0.149
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1301,
            -0.1469,
            0.0495,
            0.1495,
            -0.6702,
            0.2941,
            -0.4919,
            -0.0101,
            0.3522,
            0.1132,
            0.1719,
            0.0482,
            -0.1192,
            0.0825,
            0.7313,
            -0.3399
          ],
          "after": [
            0.0052,
            0.1138,
            -0.0341,
            -0.0418,
            0.0249,
            -0.0145,
            0.0961,
            0.095,
            -0.0793,
            -0.0631,
            0.0067,
            0.0014,
            0.1137,
            0.0299,
            -0.0627,
            0.222
          ]
        },
        "position_0": {
          "grad": [
            -0.0277,
            -0.0117,
            -0.0035,
            0.0946,
            -0.1518,
            -0.041,
            0.0973,
            0.0574,
            -0.0447,
            0.0312,
            0.0184,
            -0.0591,
            0.0654,
            0.0414,
            -0.0243,
            0.0292
          ],
          "after": [
            0.0056,
            -0.1164,
            -0.2539,
            0.0751,
            0.0121,
            -0.0362,
            -0.1302,
            0.0792,
            0.1099,
            -0.0334,
            0.0135,
            0.0997,
            0.0194,
            0.0076,
            -0.1137,
            0.0687
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0005,
            -0.0009,
            0.0021,
            -0.0043,
            0.0017,
            -0.0006,
            0.0011,
            -0.0014,
            0.0002,
            0.0005,
            -0.0006,
            0.0012,
            -0.0003,
            0.0042,
            0.0007
          ],
          "after": [
            -0.0397,
            0.0072,
            -0.066,
            -0.0361,
            0.016,
            0.1375,
            0.0438,
            -0.1683,
            0.0206,
            -0.0076,
            -0.1008,
            -0.0681,
            0.1013,
            -0.0649,
            0.0327,
            0.0693
          ]
        }
      }
    },
    {
      "step": 228,
      "word": "dewayne",
      "loss": 2.5966,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0225,
            -0.0376,
            0.0574,
            -0.0859,
            0.0789,
            0.0669,
            0.017,
            0.0259,
            -0.0464,
            0.0721,
            -0.0625,
            0.1923,
            0.0226,
            0.0115,
            -0.0438,
            0.0799
          ],
          "after": [
            -0.014,
            0.017,
            0.1625,
            -0.0167,
            0.0116,
            -0.0351,
            -0.0745,
            -0.1361,
            -0.029,
            0.0589,
            0.1368,
            -0.0117,
            -0.0601,
            -0.0197,
            -0.0386,
            -0.1494
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.36,
            -0.3377,
            0.1499,
            -0.1617,
            -0.0807,
            0.3073,
            -0.7227,
            -0.203,
            0.3748,
            -0.0027,
            0.1969,
            -0.2419,
            -0.2772,
            0.2603,
            0.2452,
            -0.4049
          ],
          "after": [
            0.004,
            0.1154,
            -0.0345,
            -0.0413,
            0.0263,
            -0.0157,
            0.0978,
            0.0964,
            -0.0803,
            -0.0631,
            0.0059,
            0.0019,
            0.1145,
            0.0285,
            -0.0639,
            0.2235
          ]
        },
        "position_0": {
          "grad": [
            -0.086,
            0.0502,
            -0.0204,
            -0.0416,
            0.1385,
            -0.0256,
            0.0687,
            -0.0245,
            -0.0012,
            -0.0256,
            -0.1262,
            0.0396,
            0.0321,
            0.0649,
            0.0591,
            0.1552
          ],
          "after": [
            0.0056,
            -0.1166,
            -0.2543,
            0.0755,
            0.012,
            -0.0363,
            -0.1301,
            0.0795,
            0.1095,
            -0.0335,
            0.0135,
            0.0991,
            0.0196,
            0.0074,
            -0.1141,
            0.0687
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0019,
            0.0024,
            -0.0028,
            0.0021,
            -0.0036,
            -0.0035,
            -0.0004,
            0.0006,
            0.0003,
            0.0014,
            -0.0039,
            0.0018,
            -0.0002,
            -0.0032,
            -0.0009
          ],
          "after": [
            -0.0395,
            0.0071,
            -0.0663,
            -0.0357,
            0.0156,
            0.1382,
            0.0441,
            -0.1677,
            0.021,
            -0.0087,
            -0.1012,
            -0.0668,
            0.1007,
            -0.065,
            0.0328,
            0.0694
          ]
        }
      }
    },
    {
      "step": 229,
      "word": "kol",
      "loss": 3.052,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0141,
            0.0168,
            0.1629,
            -0.0163,
            0.0116,
            -0.0357,
            -0.0743,
            -0.136,
            -0.0283,
            0.0587,
            0.1374,
            -0.0119,
            -0.0606,
            -0.0204,
            -0.0384,
            -0.1497
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0283,
            0.1216,
            -0.1616,
            0.0634,
            -0.2259,
            -0.2026,
            0.328,
            0.0569,
            -0.363,
            0.1127,
            -0.057,
            -0.1067,
            0.3133,
            -0.0302,
            -0.1929,
            0.2977
          ],
          "after": [
            0.0031,
            0.1165,
            -0.0345,
            -0.0411,
            0.0279,
            -0.0163,
            0.0988,
            0.0973,
            -0.0808,
            -0.0634,
            0.0054,
            0.0025,
            0.1147,
            0.0274,
            -0.0648,
            0.2243
          ]
        },
        "position_0": {
          "grad": [
            -0.1552,
            -0.0373,
            -0.1492,
            0.0581,
            -0.1426,
            -0.0244,
            0.1011,
            0.0423,
            0.0598,
            -0.0891,
            -0.0517,
            -0.1528,
            -0.0363,
            0.0918,
            -0.0067,
            -0.1779
          ],
          "after": [
            0.0063,
            -0.1166,
            -0.2539,
            0.0757,
            0.0123,
            -0.0363,
            -0.1301,
            0.0795,
            0.109,
            -0.033,
            0.0136,
            0.0991,
            0.0199,
            0.0069,
            -0.1144,
            0.069
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.0023,
            -0.0011,
            0.0022,
            -0.0015,
            0.0004,
            0.0021,
            -0.0013,
            -0.0008,
            0.0009,
            0.0011,
            -0.0003,
            0.001,
            0.0005,
            -0.0004,
            0.0027
          ],
          "after": [
            -0.0397,
            0.0066,
            -0.0662,
            -0.0358,
            0.0155,
            0.1386,
            0.0441,
            -0.167,
            0.0215,
            -0.0099,
            -0.1019,
            -0.0656,
            0.1,
            -0.0653,
            0.0329,
            0.0689
          ]
        }
      }
    },
    {
      "step": 230,
      "word": "ayarie",
      "loss": 1.8312,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1463,
            -0.1157,
            -0.0814,
            -0.1608,
            -0.1566,
            0.2774,
            0.0129,
            -0.0944,
            0.0542,
            0.0595,
            -0.1197,
            0.1423,
            -0.1004,
            0.0411,
            0.1467,
            0.2172
          ],
          "after": [
            -0.0146,
            0.017,
            0.1634,
            -0.0156,
            0.012,
            -0.0373,
            -0.0741,
            -0.1357,
            -0.0278,
            0.0583,
            0.1383,
            -0.0124,
            -0.0608,
            -0.0211,
            -0.0387,
            -0.1504
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0823,
            -0.1074,
            -0.1052,
            0.0721,
            -0.6171,
            -0.0186,
            -0.159,
            0.0471,
            0.051,
            0.0963,
            0.0058,
            -0.0839,
            0.1722,
            0.0374,
            0.3543,
            -0.0451
          ],
          "after": [
            0.0021,
            0.1176,
            -0.0343,
            -0.0412,
            0.0301,
            -0.0169,
            0.0999,
            0.0979,
            -0.0812,
            -0.064,
            0.0049,
            0.0033,
            0.1146,
            0.0264,
            -0.0659,
            0.225
          ]
        },
        "position_0": {
          "grad": [
            0.1393,
            -0.1057,
            0.0873,
            -0.2417,
            -0.0415,
            0.0263,
            -0.3649,
            -0.1183,
            0.2712,
            0.0638,
            0.2492,
            0.0495,
            -0.2963,
            0.0601,
            0.0187,
            -0.2538
          ],
          "after": [
            0.0062,
            -0.1164,
            -0.2541,
            0.0764,
            0.0128,
            -0.0364,
            -0.1296,
            0.08,
            0.1081,
            -0.033,
            0.0129,
            0.099,
            0.0206,
            0.0063,
            -0.1148,
            0.0698
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0006,
            0.0003,
            0.0004,
            -0.0018,
            -0.0004,
            -0.0017,
            0.0008,
            0.0005,
            -0.0005,
            0.0001,
            0.0004,
            -0.0004,
            -0.0001,
            0.0012,
            0.0004
          ],
          "after": [
            -0.0397,
            0.0063,
            -0.0662,
            -0.0361,
            0.0158,
            0.139,
            0.0443,
            -0.1665,
            0.0219,
            -0.0108,
            -0.1026,
            -0.0647,
            0.0995,
            -0.0655,
            0.0328,
            0.0683
          ]
        }
      }
    },
    {
      "step": 231,
      "word": "cathalina",
      "loss": 2.6893,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1601,
            0.0332,
            0.0919,
            0.0107,
            0.018,
            0.1579,
            0.0868,
            0.127,
            -0.1412,
            -0.0885,
            -0.1545,
            -0.023,
            0.0802,
            -0.0411,
            0.0018,
            0.0759
          ],
          "after": [
            -0.0146,
            0.017,
            0.1636,
            -0.015,
            0.0122,
            -0.0393,
            -0.0742,
            -0.1358,
            -0.027,
            0.0582,
            0.1396,
            -0.0127,
            -0.0612,
            -0.0215,
            -0.039,
            -0.1511
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1363,
            0.2044,
            -0.0599,
            0.0819,
            0.126,
            -0.1779,
            0.3487,
            0.0194,
            -0.2155,
            0.0171,
            0.0244,
            0.0753,
            0.1064,
            -0.1539,
            -0.2661,
            0.2925
          ],
          "after": [
            0.0016,
            0.1182,
            -0.034,
            -0.0415,
            0.0318,
            -0.0171,
            0.1005,
            0.0983,
            -0.0814,
            -0.0645,
            0.0044,
            0.0038,
            0.1144,
            0.0259,
            -0.0666,
            0.2252
          ]
        },
        "position_0": {
          "grad": [
            -0.0251,
            0.0172,
            -0.0194,
            0.0683,
            -0.0798,
            -0.0344,
            0.0896,
            0.0364,
            -0.0537,
            0.044,
            0.0058,
            -0.0001,
            0.0344,
            -0.0061,
            -0.0348,
            0.0245
          ],
          "after": [
            0.0063,
            -0.1162,
            -0.2541,
            0.0769,
            0.0134,
            -0.0364,
            -0.1292,
            0.0803,
            0.1075,
            -0.0332,
            0.0124,
            0.0989,
            0.0211,
            0.0058,
            -0.115,
            0.0704
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0023,
            0.0003,
            -0.0005,
            0.0002,
            -0.001,
            -0.0007,
            -0.0017,
            -0.0005,
            -0.0005,
            0.0009,
            0.0004,
            -0.0022,
            0.0008,
            0.0016,
            -0.0003,
            -0.0024
          ],
          "after": [
            -0.0403,
            0.006,
            -0.0661,
            -0.0364,
            0.0161,
            0.1396,
            0.0447,
            -0.166,
            0.0223,
            -0.0118,
            -0.1033,
            -0.0634,
            0.0989,
            -0.0661,
            0.0328,
            0.0684
          ]
        }
      }
    },
    {
      "step": 232,
      "word": "aidyn",
      "loss": 2.4074,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0675,
            -0.3827,
            0.0169,
            -0.1252,
            -0.0232,
            0.0553,
            -0.311,
            0.0274,
            0.0548,
            -0.0458,
            0.1866,
            -0.2244,
            -0.1418,
            0.1052,
            0.1214,
            -0.2111
          ],
          "after": [
            -0.0144,
            0.0182,
            0.1637,
            -0.0141,
            0.0125,
            -0.0412,
            -0.0736,
            -0.1359,
            -0.0265,
            0.0584,
            0.14,
            -0.0125,
            -0.0611,
            -0.0223,
            -0.0397,
            -0.1513
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0546,
            0.0826,
            -0.0712,
            0.0544,
            0.1036,
            -0.1481,
            0.2771,
            0.0438,
            -0.2111,
            0.1068,
            -0.0066,
            0.0125,
            0.138,
            -0.1068,
            -0.2323,
            0.1404
          ],
          "after": [
            0.0014,
            0.1186,
            -0.0336,
            -0.042,
            0.033,
            -0.017,
            0.1007,
            0.0984,
            -0.0812,
            -0.0652,
            0.0041,
            0.0042,
            0.1139,
            0.0257,
            -0.0669,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            0.153,
            -0.1255,
            0.1032,
            -0.2335,
            -0.0443,
            0.0327,
            -0.3954,
            -0.1292,
            0.2952,
            0.051,
            0.2914,
            0.0381,
            -0.3129,
            0.0781,
            0.0452,
            -0.2672
          ],
          "after": [
            0.0058,
            -0.1156,
            -0.2545,
            0.0778,
            0.0141,
            -0.0365,
            -0.1283,
            0.081,
            0.1064,
            -0.0336,
            0.0111,
            0.0987,
            0.0221,
            0.0051,
            -0.1153,
            0.0714
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0004,
            -0.0003,
            0.0006,
            0.0012,
            -0.0001,
            0.0013,
            0.0008,
            0.0001,
            0.0004,
            -0.0003,
            0.001,
            0.0003,
            -0.0006,
            -0.0009,
            0.0007
          ],
          "after": [
            -0.0409,
            0.0057,
            -0.0659,
            -0.0368,
            0.0163,
            0.1401,
            0.0449,
            -0.1657,
            0.0226,
            -0.0127,
            -0.1038,
            -0.0626,
            0.0984,
            -0.0665,
            0.0329,
            0.0683
          ]
        }
      }
    },
    {
      "step": 233,
      "word": "ansen",
      "loss": 2.4141,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.024,
            -0.0169,
            -0.4047,
            0.1542,
            -0.6291,
            0.0893,
            -0.0667,
            0.057,
            0.1065,
            -0.2476,
            -0.3028,
            -0.1946,
            0.0874,
            0.2392,
            0.4224,
            0.2252
          ],
          "after": [
            -0.0143,
            0.0193,
            0.1649,
            -0.0138,
            0.014,
            -0.0433,
            -0.073,
            -0.1363,
            -0.0264,
            0.0595,
            0.1413,
            -0.012,
            -0.0613,
            -0.0236,
            -0.0416,
            -0.1519
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3461,
            0.1805,
            0.0499,
            0.0249,
            0.0195,
            -0.1089,
            0.0607,
            0.0646,
            -0.1335,
            -0.2754,
            -0.1852,
            -0.1701,
            0.2593,
            -0.1761,
            0.2278,
            0.0669
          ],
          "after": [
            0.002,
            0.1185,
            -0.0333,
            -0.0424,
            0.0341,
            -0.0168,
            0.1007,
            0.0982,
            -0.0809,
            -0.0651,
            0.0044,
            0.0049,
            0.1132,
            0.0258,
            -0.0674,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            0.161,
            -0.1604,
            0.1168,
            -0.2001,
            -0.0998,
            0.0341,
            -0.4167,
            -0.0965,
            0.3253,
            0.0071,
            0.2524,
            -0.0153,
            -0.3093,
            0.1214,
            0.0886,
            -0.2767
          ],
          "after": [
            0.0046,
            -0.1147,
            -0.2555,
            0.079,
            0.015,
            -0.0367,
            -0.1268,
            0.082,
            0.1049,
            -0.0341,
            0.0093,
            0.0985,
            0.0234,
            0.0042,
            -0.1158,
            0.0728
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0031,
            0.0017,
            0.001,
            0.0006,
            -0.0001,
            -0.0012,
            0.0005,
            0.0004,
            -0.001,
            -0.003,
            -0.0024,
            -0.0019,
            0.0024,
            -0.0016,
            0.0028,
            -0.0002
          ],
          "after": [
            -0.0406,
            0.005,
            -0.0661,
            -0.0373,
            0.0164,
            0.1408,
            0.045,
            -0.1657,
            0.0232,
            -0.0128,
            -0.1034,
            -0.0615,
            0.0976,
            -0.0663,
            0.0326,
            0.0683
          ]
        }
      }
    },
    {
      "step": 234,
      "word": "zayleigh",
      "loss": 2.6317,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0809,
            -0.0389,
            -0.0198,
            -0.085,
            0.0076,
            0.0418,
            -0.005,
            -0.005,
            -0.0824,
            0.0656,
            -0.0886,
            0.0506,
            0.1091,
            0.1073,
            -0.0299,
            0.0628
          ],
          "after": [
            -0.0144,
            0.0203,
            0.166,
            -0.0133,
            0.0153,
            -0.0452,
            -0.0724,
            -0.1366,
            -0.026,
            0.0602,
            0.1427,
            -0.0116,
            -0.0618,
            -0.0251,
            -0.0432,
            -0.1526
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0811,
            -0.1332,
            0.0916,
            0.0022,
            0.185,
            0.024,
            -0.1165,
            0.0411,
            0.0428,
            0.0085,
            -0.0996,
            0.0994,
            -0.0663,
            0.0499,
            -0.1404,
            -0.0305
          ],
          "after": [
            0.0028,
            0.1187,
            -0.0333,
            -0.0428,
            0.0347,
            -0.0166,
            0.1009,
            0.0979,
            -0.0807,
            -0.065,
            0.0051,
            0.0053,
            0.1126,
            0.0259,
            -0.0677,
            0.2248
          ]
        },
        "position_0": {
          "grad": [
            -0.0415,
            0.1208,
            -0.0443,
            0.1786,
            0.1044,
            -0.0387,
            0.2776,
            -0.0021,
            -0.0666,
            0.0431,
            -0.0123,
            0.112,
            0.064,
            -0.0813,
            -0.0335,
            0.1299
          ],
          "after": [
            0.0039,
            -0.1142,
            -0.2561,
            0.0796,
            0.0154,
            -0.0367,
            -0.1261,
            0.0829,
            0.1038,
            -0.0347,
            0.0078,
            0.0981,
            0.0244,
            0.0036,
            -0.1161,
            0.0737
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0006,
            0.0004,
            0.0004,
            -0.0004,
            0.0,
            -0.0006,
            0.0004,
            0.0006,
            -0.0,
            -0.0003,
            -0.0006,
            -0.0005,
            0.0004,
            -0.0,
            -0.0005
          ],
          "after": [
            -0.0405,
            0.0046,
            -0.0663,
            -0.0379,
            0.0165,
            0.1414,
            0.0451,
            -0.1657,
            0.0235,
            -0.0128,
            -0.1029,
            -0.0604,
            0.0969,
            -0.0663,
            0.0322,
            0.0684
          ]
        }
      }
    },
    {
      "step": 235,
      "word": "adley",
      "loss": 2.0702,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2159,
            0.0903,
            -0.0374,
            0.104,
            0.1267,
            -0.0902,
            0.2533,
            -0.0184,
            -0.1217,
            -0.1099,
            -0.2284,
            0.0147,
            0.202,
            0.0693,
            0.0796,
            0.3385
          ],
          "after": [
            -0.014,
            0.0209,
            0.167,
            -0.0132,
            0.0161,
            -0.0464,
            -0.0725,
            -0.1367,
            -0.0254,
            0.0613,
            0.1445,
            -0.0113,
            -0.0626,
            -0.0266,
            -0.0448,
            -0.1537
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1155,
            -0.2313,
            0.0991,
            -0.1009,
            0.2968,
            -0.0211,
            -0.1718,
            0.1267,
            0.0396,
            -0.0555,
            -0.1028,
            0.0403,
            -0.1257,
            -0.1295,
            0.1043,
            -0.2619
          ],
          "after": [
            0.0037,
            0.1194,
            -0.0335,
            -0.0428,
            0.0348,
            -0.0164,
            0.1013,
            0.097,
            -0.0805,
            -0.0647,
            0.006,
            0.0055,
            0.1123,
            0.0262,
            -0.068,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            0.1368,
            -0.0994,
            0.0826,
            -0.1705,
            0.0334,
            0.0415,
            -0.3331,
            -0.1197,
            0.2406,
            0.048,
            0.305,
            0.0879,
            -0.2363,
            0.0638,
            0.011,
            -0.2064
          ],
          "after": [
            0.0027,
            -0.1136,
            -0.257,
            0.0805,
            0.0157,
            -0.0369,
            -0.1249,
            0.084,
            0.1024,
            -0.0355,
            0.0057,
            0.0974,
            0.0257,
            0.003,
            -0.1164,
            0.0748
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0,
            0.0002,
            -0.0001,
            0.0008,
            -0.0006,
            0.0003,
            0.0006,
            -0.0002,
            -0.0003,
            -0.0004,
            -0.0008,
            0.0007,
            -0.0003,
            -0.0003,
            -0.0007
          ],
          "after": [
            -0.0406,
            0.0042,
            -0.0666,
            -0.0384,
            0.0165,
            0.1421,
            0.0452,
            -0.1659,
            0.0238,
            -0.0127,
            -0.1024,
            -0.0593,
            0.0963,
            -0.0662,
            0.032,
            0.0686
          ]
        }
      }
    },
    {
      "step": 236,
      "word": "novah",
      "loss": 2.7857,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1505,
            0.0468,
            0.2523,
            0.0984,
            0.0567,
            0.032,
            0.1519,
            0.0969,
            0.0306,
            0.0312,
            -0.0578,
            -0.0763,
            0.2927,
            0.0139,
            -0.1882,
            0.2158
          ],
          "after": [
            -0.0133,
            0.0213,
            0.1672,
            -0.0133,
            0.0167,
            -0.0476,
            -0.0729,
            -0.1372,
            -0.0249,
            0.062,
            0.1462,
            -0.0108,
            -0.0641,
            -0.0279,
            -0.0455,
            -0.1551
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0881,
            0.0884,
            -0.1017,
            0.0915,
            0.0816,
            -0.2042,
            0.3792,
            0.0947,
            -0.2794,
            0.0638,
            -0.05,
            0.0675,
            0.1621,
            -0.1269,
            -0.306,
            0.2437
          ],
          "after": [
            0.0047,
            0.1197,
            -0.0335,
            -0.0431,
            0.0348,
            -0.0159,
            0.1012,
            0.0958,
            -0.0801,
            -0.0647,
            0.0069,
            0.0056,
            0.1118,
            0.0268,
            -0.068,
            0.225
          ]
        },
        "position_0": {
          "grad": [
            -0.054,
            0.042,
            0.0102,
            0.0624,
            -0.3214,
            0.0528,
            0.0206,
            -0.0148,
            0.093,
            -0.1708,
            -0.2403,
            -0.1654,
            0.0173,
            0.175,
            0.231,
            0.0724
          ],
          "after": [
            0.0018,
            -0.1131,
            -0.2578,
            0.0811,
            0.0168,
            -0.0373,
            -0.1239,
            0.0851,
            0.101,
            -0.0352,
            0.0046,
            0.0974,
            0.0267,
            0.0018,
            -0.1172,
            0.0757
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0008,
            0.0011,
            -0.0009,
            0.0012,
            -0.0011,
            0.0002,
            -0.0011,
            -0.0006,
            0.002,
            0.0026,
            -0.0003,
            -0.0002,
            -0.0014,
            -0.0011,
            -0.0022
          ],
          "after": [
            -0.0406,
            0.0038,
            -0.0671,
            -0.0385,
            0.0163,
            0.143,
            0.0453,
            -0.1658,
            0.0242,
            -0.0132,
            -0.1028,
            -0.0583,
            0.0958,
            -0.0657,
            0.032,
            0.0693
          ]
        }
      }
    },
    {
      "step": 237,
      "word": "skyley",
      "loss": 2.4446,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0127,
            0.0216,
            0.1674,
            -0.0135,
            0.0172,
            -0.0486,
            -0.0732,
            -0.1376,
            -0.0245,
            0.0626,
            0.1476,
            -0.0105,
            -0.0653,
            -0.029,
            -0.0461,
            -0.1563
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0139,
            -0.2397,
            0.1223,
            -0.034,
            0.1192,
            -0.0065,
            -0.1682,
            0.0894,
            0.0447,
            0.0517,
            -0.145,
            0.051,
            -0.0567,
            0.0866,
            -0.0588,
            -0.0935
          ],
          "after": [
            0.0056,
            0.1205,
            -0.0337,
            -0.0433,
            0.0345,
            -0.0155,
            0.1013,
            0.0944,
            -0.0797,
            -0.0648,
            0.0083,
            0.0055,
            0.1115,
            0.0271,
            -0.0679,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            -0.0352,
            0.1372,
            0.0232,
            0.1781,
            -0.1304,
            0.0907,
            0.123,
            0.0849,
            -0.1162,
            -0.0046,
            -0.1038,
            0.0377,
            0.1817,
            0.0176,
            0.1353,
            0.0863
          ],
          "after": [
            0.0013,
            -0.1131,
            -0.2586,
            0.0813,
            0.0182,
            -0.038,
            -0.1233,
            0.0857,
            0.1,
            -0.0349,
            0.004,
            0.0972,
            0.0273,
            0.0008,
            -0.1183,
            0.0762
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            -0.0003,
            0.0001,
            0.0004,
            0.0004,
            0.0004,
            -0.0007,
            -0.0,
            -0.0,
            0.0003,
            0.0002,
            -0.0003,
            -0.0001,
            0.0004,
            -0.0008,
            -0.0006
          ],
          "after": [
            -0.0409,
            0.0034,
            -0.0676,
            -0.0387,
            0.0161,
            0.1436,
            0.0454,
            -0.1657,
            0.0246,
            -0.0136,
            -0.1032,
            -0.0574,
            0.0953,
            -0.0654,
            0.0322,
            0.07
          ]
        }
      }
    },
    {
      "step": 238,
      "word": "khamil",
      "loss": 2.6772,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1467,
            0.0846,
            0.1867,
            0.1122,
            0.2011,
            -0.0255,
            0.2336,
            0.1449,
            0.0749,
            -0.0577,
            0.0965,
            0.1438,
            -0.1323,
            -0.1431,
            -0.0554,
            0.0711
          ],
          "after": [
            -0.0118,
            0.0216,
            0.167,
            -0.0139,
            0.0172,
            -0.0493,
            -0.0739,
            -0.1384,
            -0.0244,
            0.0634,
            0.1486,
            -0.0105,
            -0.0661,
            -0.0295,
            -0.0464,
            -0.1575
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2149,
            0.2851,
            -0.1814,
            0.0612,
            0.1525,
            -0.3403,
            0.5668,
            0.0876,
            -0.4393,
            0.0115,
            -0.0768,
            0.0223,
            0.2638,
            -0.1928,
            -0.4119,
            0.5231
          ],
          "after": [
            0.0069,
            0.1206,
            -0.0335,
            -0.0436,
            0.0341,
            -0.0146,
            0.1008,
            0.0928,
            -0.0789,
            -0.065,
            0.0097,
            0.0053,
            0.1108,
            0.0277,
            -0.0673,
            0.2243
          ]
        },
        "position_0": {
          "grad": [
            -0.1094,
            0.1026,
            -0.1174,
            0.0649,
            0.0517,
            -0.0309,
            0.2373,
            0.0094,
            -0.109,
            -0.0385,
            -0.0284,
            0.0146,
            0.0802,
            -0.0274,
            -0.1226,
            0.0333
          ],
          "after": [
            0.0013,
            -0.1135,
            -0.2587,
            0.0812,
            0.0192,
            -0.0385,
            -0.1231,
            0.0861,
            0.0994,
            -0.0345,
            0.0035,
            0.097,
            0.0277,
            0.0001,
            -0.1189,
            0.0767
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0025,
            0.004,
            0.0017,
            0.0028,
            -0.0046,
            -0.0023,
            0.0043,
            -0.0004,
            -0.0014,
            -0.001,
            0.001,
            0.0011,
            0.0019,
            -0.0017,
            0.0013,
            0.0048
          ],
          "after": [
            -0.0405,
            0.0023,
            -0.0685,
            -0.0396,
            0.0166,
            0.1447,
            0.045,
            -0.1654,
            0.0254,
            -0.0138,
            -0.1039,
            -0.0569,
            0.0947,
            -0.0647,
            0.0321,
            0.0695
          ]
        }
      }
    },
    {
      "step": 239,
      "word": "brion",
      "loss": 2.1879,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.011,
            0.0216,
            0.1666,
            -0.0142,
            0.0172,
            -0.05,
            -0.0745,
            -0.1392,
            -0.0243,
            0.0641,
            0.1494,
            -0.0105,
            -0.0667,
            -0.0299,
            -0.0467,
            -0.1585
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0689,
            0.1083,
            -0.0924,
            0.0486,
            -0.0117,
            -0.2325,
            0.3158,
            0.0402,
            -0.2953,
            0.0929,
            -0.0459,
            -0.0216,
            0.2616,
            -0.0672,
            -0.292,
            0.2103
          ],
          "after": [
            0.0082,
            0.1205,
            -0.0331,
            -0.044,
            0.0338,
            -0.0135,
            0.1,
            0.0913,
            -0.0778,
            -0.0653,
            0.0111,
            0.0053,
            0.1098,
            0.0284,
            -0.0665,
            0.2233
          ]
        },
        "position_0": {
          "grad": [
            -0.0121,
            0.0498,
            0.1542,
            0.0626,
            0.0156,
            -0.0207,
            0.2237,
            0.0438,
            -0.2311,
            -0.1208,
            -0.0554,
            -0.0106,
            0.2618,
            -0.1275,
            -0.0638,
            0.4198
          ],
          "after": [
            0.0013,
            -0.1139,
            -0.2595,
            0.0811,
            0.0201,
            -0.0388,
            -0.1233,
            0.0863,
            0.0993,
            -0.0335,
            0.0032,
            0.0969,
            0.0275,
            -0.0002,
            -0.1192,
            0.0763
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0,
            0.0002,
            0.0008,
            0.0021,
            -0.0006,
            0.0018,
            0.0011,
            -0.0002,
            -0.001,
            -0.0008,
            0.0028,
            -0.0006,
            -0.0016,
            0.0006,
            0.0014
          ],
          "after": [
            -0.0398,
            0.0014,
            -0.0693,
            -0.0406,
            0.0168,
            0.1457,
            0.0444,
            -0.1656,
            0.0261,
            -0.0136,
            -0.1043,
            -0.057,
            0.0942,
            -0.0636,
            0.0319,
            0.0688
          ]
        }
      }
    },
    {
      "step": 240,
      "word": "sheppard",
      "loss": 3.6063,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1411,
            -0.1964,
            -0.0051,
            0.093,
            0.1962,
            0.1219,
            0.0228,
            -0.0088,
            0.13,
            -0.2014,
            -0.1123,
            0.2419,
            -0.2094,
            -0.1668,
            0.0868,
            0.3836
          ],
          "after": [
            -0.01,
            0.0222,
            0.1664,
            -0.0147,
            0.0168,
            -0.051,
            -0.0751,
            -0.1397,
            -0.0246,
            0.0654,
            0.1504,
            -0.011,
            -0.0667,
            -0.0297,
            -0.0472,
            -0.16
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0796,
            0.1184,
            0.1035,
            0.1128,
            0.068,
            -0.0378,
            0.0859,
            -0.0036,
            -0.0141,
            0.0664,
            0.139,
            0.2372,
            -0.1427,
            -0.1472,
            -0.0292,
            -0.0438
          ],
          "after": [
            0.0095,
            0.1201,
            -0.0331,
            -0.0448,
            0.0334,
            -0.0124,
            0.0992,
            0.09,
            -0.0769,
            -0.0659,
            0.0117,
            0.0046,
            0.1092,
            0.0293,
            -0.0658,
            0.2225
          ]
        },
        "position_0": {
          "grad": [
            -0.0358,
            0.1361,
            0.0203,
            0.1698,
            -0.0723,
            0.0704,
            0.1378,
            0.0532,
            -0.1274,
            -0.0037,
            -0.0665,
            0.0568,
            0.1508,
            0.0146,
            0.1015,
            0.1073
          ],
          "after": [
            0.0015,
            -0.1147,
            -0.2603,
            0.0806,
            0.021,
            -0.0394,
            -0.1237,
            0.0863,
            0.0995,
            -0.0326,
            0.0032,
            0.0967,
            0.0272,
            -0.0004,
            -0.1198,
            0.0757
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0016,
            -0.0001,
            0.0005,
            -0.0008,
            -0.0025,
            0.0027,
            0.0007,
            -0.002,
            0.0025,
            0.0024,
            0.0015,
            0.0005,
            -0.002,
            -0.0026,
            -0.0003
          ],
          "after": [
            -0.0394,
            0.0003,
            -0.07,
            -0.0416,
            0.017,
            0.1472,
            0.0435,
            -0.1659,
            0.0273,
            -0.0142,
            -0.1054,
            -0.0574,
            0.0937,
            -0.0621,
            0.0322,
            0.0683
          ]
        }
      }
    },
    {
      "step": 241,
      "word": "keishawn",
      "loss": 2.948,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1608,
            -0.0574,
            -0.0974,
            0.3487,
            0.1815,
            -0.0938,
            0.2831,
            0.1026,
            -0.1683,
            -0.2154,
            -0.1492,
            0.1569,
            0.1069,
            -0.2602,
            -0.2254,
            0.4587
          ],
          "after": [
            -0.0088,
            0.0229,
            0.1664,
            -0.0161,
            0.0161,
            -0.0515,
            -0.0761,
            -0.1406,
            -0.0243,
            0.0673,
            0.1517,
            -0.0118,
            -0.067,
            -0.0288,
            -0.0469,
            -0.1621
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1021,
            -0.105,
            0.0691,
            -0.0232,
            0.3304,
            0.0861,
            -0.2158,
            -0.0837,
            0.2548,
            -0.0112,
            0.1419,
            0.1687,
            -0.2244,
            -0.0073,
            -0.1726,
            -0.137
          ],
          "after": [
            0.0103,
            0.1201,
            -0.0331,
            -0.0453,
            0.0325,
            -0.0117,
            0.0988,
            0.0893,
            -0.0764,
            -0.0663,
            0.0118,
            0.0037,
            0.109,
            0.0301,
            -0.0649,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0869,
            0.0641,
            -0.1009,
            0.03,
            0.0125,
            -0.0338,
            0.1396,
            0.0034,
            -0.0556,
            -0.0165,
            -0.0364,
            -0.0027,
            0.0218,
            -0.0189,
            -0.079,
            -0.0125
          ],
          "after": [
            0.002,
            -0.1156,
            -0.2605,
            0.0801,
            0.0217,
            -0.0397,
            -0.1243,
            0.0863,
            0.0997,
            -0.0318,
            0.0033,
            0.0964,
            0.0269,
            -0.0006,
            -0.1201,
            0.0753
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0009,
            0.0013,
            -0.003,
            -0.0001,
            0.0012,
            -0.0015,
            -0.0029,
            0.0022,
            0.0038,
            0.0031,
            0.0026,
            -0.0014,
            0.0001,
            -0.0009,
            -0.0037
          ],
          "after": [
            -0.0389,
            -0.0008,
            -0.0709,
            -0.0417,
            0.0172,
            0.1482,
            0.043,
            -0.1654,
            0.0276,
            -0.0155,
            -0.1072,
            -0.0583,
            0.0935,
            -0.0608,
            0.0326,
            0.0686
          ]
        }
      }
    },
    {
      "step": 242,
      "word": "vinny",
      "loss": 2.6831,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0078,
            0.0235,
            0.1664,
            -0.0172,
            0.0155,
            -0.052,
            -0.0771,
            -0.1413,
            -0.0241,
            0.0689,
            0.1528,
            -0.0125,
            -0.0672,
            -0.028,
            -0.0467,
            -0.1639
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1075,
            0.0491,
            -0.1039,
            0.0707,
            0.087,
            -0.1874,
            0.3577,
            0.0621,
            -0.2205,
            0.1169,
            -0.0384,
            0.0605,
            0.1539,
            -0.143,
            -0.2393,
            0.2157
          ],
          "after": [
            0.0113,
            0.1199,
            -0.033,
            -0.046,
            0.0317,
            -0.0108,
            0.0981,
            0.0884,
            -0.0757,
            -0.0669,
            0.0119,
            0.0027,
            0.1086,
            0.0311,
            -0.064,
            0.2214
          ]
        },
        "position_0": {
          "grad": [
            -0.174,
            0.0509,
            0.0238,
            0.1334,
            0.0711,
            0.1053,
            0.0391,
            0.0376,
            -0.0934,
            -0.0907,
            -0.1717,
            -0.1547,
            0.107,
            -0.0385,
            -0.0077,
            0.2127
          ],
          "after": [
            0.003,
            -0.1165,
            -0.2607,
            0.0794,
            0.0222,
            -0.0404,
            -0.1248,
            0.0861,
            0.1,
            -0.0306,
            0.0038,
            0.0967,
            0.0264,
            -0.0006,
            -0.1203,
            0.0745
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0001,
            0.0006,
            -0.0004,
            0.002,
            -0.0,
            -0.0006,
            0.0,
            0.0005,
            0.0008,
            0.001,
            0.0013,
            -0.0012,
            -0.001,
            -0.001,
            0.0001
          ],
          "after": [
            -0.0384,
            -0.0018,
            -0.0719,
            -0.0417,
            0.0171,
            0.149,
            0.0426,
            -0.1649,
            0.0278,
            -0.0169,
            -0.1091,
            -0.0593,
            0.0935,
            -0.0595,
            0.0331,
            0.0689
          ]
        }
      }
    },
    {
      "step": 243,
      "word": "kaceson",
      "loss": 2.4346,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1086,
            0.0738,
            0.0869,
            0.162,
            -0.0344,
            -0.0243,
            0.1836,
            0.0916,
            -0.1378,
            0.082,
            0.0576,
            0.1056,
            0.0447,
            -0.0502,
            -0.1356,
            0.0177
          ],
          "after": [
            -0.0066,
            0.0237,
            0.1662,
            -0.0186,
            0.015,
            -0.0522,
            -0.0782,
            -0.1422,
            -0.0235,
            0.07,
            0.1536,
            -0.0134,
            -0.0675,
            -0.0272,
            -0.0461,
            -0.1655
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2722,
            -0.1186,
            0.1315,
            -0.1027,
            0.0958,
            -0.1342,
            -0.0384,
            0.013,
            0.0446,
            -0.1397,
            -0.0244,
            0.1105,
            -0.1763,
            -0.1721,
            0.1651,
            -0.1069
          ],
          "after": [
            0.0128,
            0.12,
            -0.0331,
            -0.0463,
            0.0308,
            -0.0098,
            0.0975,
            0.0876,
            -0.0752,
            -0.0671,
            0.0122,
            0.0016,
            0.1086,
            0.0323,
            -0.0633,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            -0.0925,
            0.0905,
            -0.1194,
            0.023,
            0.0592,
            -0.0433,
            0.1718,
            -0.0139,
            -0.0832,
            0.0037,
            -0.0224,
            0.0372,
            0.0357,
            -0.0438,
            -0.1154,
            -0.0027
          ],
          "after": [
            0.0043,
            -0.1175,
            -0.2604,
            0.0788,
            0.0223,
            -0.0409,
            -0.1256,
            0.086,
            0.1005,
            -0.0296,
            0.0043,
            0.0968,
            0.0259,
            -0.0005,
            -0.1201,
            0.0739
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0002,
            0.0002,
            -0.0,
            -0.0013,
            -0.0018,
            -0.0011,
            -0.001,
            0.0001,
            0.0005,
            -0.0002,
            -0.0025,
            0.0005,
            0.0,
            0.0023,
            -0.0008
          ],
          "after": [
            -0.0375,
            -0.0027,
            -0.0728,
            -0.0416,
            0.0172,
            0.1502,
            0.0424,
            -0.1642,
            0.0279,
            -0.0182,
            -0.1107,
            -0.0597,
            0.0935,
            -0.0584,
            0.0331,
            0.0693
          ]
        }
      }
    },
    {
      "step": 244,
      "word": "blaze",
      "loss": 2.4831,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2266,
            0.0939,
            0.1428,
            0.2322,
            0.2512,
            -0.1669,
            0.3548,
            -0.019,
            -0.0209,
            -0.0627,
            0.0455,
            0.2439,
            0.0628,
            -0.2072,
            -0.0383,
            0.1294
          ],
          "after": [
            -0.0051,
            0.0237,
            0.1657,
            -0.0203,
            0.0141,
            -0.0518,
            -0.0798,
            -0.1429,
            -0.0229,
            0.0711,
            0.1541,
            -0.0146,
            -0.0679,
            -0.0259,
            -0.0454,
            -0.1671
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2163,
            -0.1094,
            0.2205,
            0.032,
            -0.2305,
            -0.0454,
            -0.2528,
            -0.1403,
            -0.0181,
            -0.055,
            -0.0046,
            -0.1005,
            -0.0253,
            0.1683,
            0.1467,
            0.0467
          ],
          "after": [
            0.0135,
            0.1203,
            -0.0337,
            -0.0466,
            0.0305,
            -0.0088,
            0.0973,
            0.0875,
            -0.0747,
            -0.0671,
            0.0124,
            0.001,
            0.1086,
            0.033,
            -0.0629,
            0.2205
          ]
        },
        "position_0": {
          "grad": [
            -0.0028,
            0.0814,
            0.1274,
            0.0366,
            0.1052,
            -0.0215,
            0.2195,
            0.0094,
            -0.2383,
            -0.0726,
            -0.0207,
            0.0808,
            0.2628,
            -0.1468,
            -0.1182,
            0.4152
          ],
          "after": [
            0.0055,
            -0.1186,
            -0.2608,
            0.0781,
            0.0222,
            -0.0412,
            -0.1266,
            0.0859,
            0.1013,
            -0.0284,
            0.0048,
            0.0967,
            0.0251,
            0.0001,
            -0.1197,
            0.0726
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0032,
            0.0008,
            0.0051,
            0.0015,
            -0.0001,
            -0.0003,
            -0.0022,
            -0.0031,
            -0.0005,
            0.0029,
            0.003,
            -0.0004,
            -0.0011,
            0.0017,
            -0.0029,
            -0.002
          ],
          "after": [
            -0.0376,
            -0.0036,
            -0.0749,
            -0.042,
            0.0173,
            0.1513,
            0.0426,
            -0.1628,
            0.0282,
            -0.0199,
            -0.1128,
            -0.0599,
            0.0936,
            -0.0579,
            0.0337,
            0.07
          ]
        }
      }
    },
    {
      "step": 245,
      "word": "cara",
      "loss": 2.1493,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.7178,
            -0.1628,
            -0.5493,
            -0.3203,
            -0.2888,
            -0.0047,
            -0.1109,
            -0.1972,
            0.1283,
            0.211,
            0.1706,
            -0.6826,
            -0.0004,
            0.1524,
            0.3298,
            -0.2484
          ],
          "after": [
            -0.0055,
            0.0241,
            0.1666,
            -0.021,
            0.014,
            -0.0513,
            -0.081,
            -0.1429,
            -0.0228,
            0.0713,
            0.1541,
            -0.0142,
            -0.0683,
            -0.0252,
            -0.0459,
            -0.168
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0451,
            0.0571,
            -0.1436,
            0.0686,
            -0.1112,
            -0.091,
            0.1655,
            0.0149,
            -0.2145,
            0.1001,
            -0.0138,
            -0.0816,
            0.1879,
            -0.0389,
            -0.1364,
            0.1363
          ],
          "after": [
            0.014,
            0.1204,
            -0.0339,
            -0.0471,
            0.0303,
            -0.0079,
            0.0969,
            0.0874,
            -0.074,
            -0.0674,
            0.0127,
            0.0006,
            0.1083,
            0.0336,
            -0.0625,
            0.2199
          ]
        },
        "position_0": {
          "grad": [
            -0.043,
            0.0436,
            -0.0495,
            0.0944,
            -0.1814,
            -0.0889,
            0.1914,
            0.0754,
            -0.1137,
            0.0668,
            -0.0091,
            -0.0302,
            0.1027,
            -0.0071,
            -0.0915,
            0.0509
          ],
          "after": [
            0.0066,
            -0.1197,
            -0.2608,
            0.0774,
            0.0226,
            -0.041,
            -0.1277,
            0.0855,
            0.1022,
            -0.0277,
            0.0053,
            0.0967,
            0.0242,
            0.0006,
            -0.1191,
            0.0714
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0002,
            0.0002,
            -0.0005,
            0.0006,
            -0.0001,
            -0.0003,
            -0.0,
            0.0001,
            0.0003,
            0.0006,
            0.0004,
            -0.0007,
            -0.0006,
            0.0002,
            -0.0006
          ],
          "after": [
            -0.0375,
            -0.0043,
            -0.0767,
            -0.0421,
            0.0172,
            0.1522,
            0.0427,
            -0.1616,
            0.0283,
            -0.0215,
            -0.1149,
            -0.0602,
            0.0939,
            -0.0573,
            0.0341,
            0.0707
          ]
        }
      }
    },
    {
      "step": 246,
      "word": "abiola",
      "loss": 2.4154,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1553,
            0.2067,
            0.3167,
            -0.1482,
            0.1549,
            -0.1256,
            0.2312,
            -0.0658,
            -0.2023,
            0.087,
            0.0835,
            -0.0313,
            0.3565,
            -0.1318,
            -0.3151,
            0.2357
          ],
          "after": [
            -0.0061,
            0.0239,
            0.1666,
            -0.0212,
            0.0135,
            -0.0505,
            -0.0825,
            -0.1426,
            -0.0221,
            0.0711,
            0.1538,
            -0.0138,
            -0.0694,
            -0.0243,
            -0.0453,
            -0.1691
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0316,
            0.0988,
            0.0024,
            0.0524,
            0.0874,
            -0.1682,
            0.1696,
            -0.0256,
            -0.1785,
            0.0202,
            0.0522,
            0.0113,
            0.0882,
            -0.0768,
            -0.2364,
            0.1407
          ],
          "after": [
            0.0146,
            0.1203,
            -0.0341,
            -0.0477,
            0.03,
            -0.0068,
            0.0964,
            0.0875,
            -0.0732,
            -0.0677,
            0.0127,
            0.0002,
            0.1079,
            0.0343,
            -0.0618,
            0.2192
          ]
        },
        "position_0": {
          "grad": [
            0.0901,
            -0.0277,
            0.0449,
            -0.1518,
            0.0954,
            0.0036,
            -0.2147,
            -0.1177,
            0.1275,
            0.0791,
            0.2819,
            0.131,
            -0.1718,
            -0.0176,
            -0.0577,
            -0.1227
          ],
          "after": [
            0.0072,
            -0.1206,
            -0.261,
            0.0771,
            0.0226,
            -0.0409,
            -0.1283,
            0.0856,
            0.1027,
            -0.0275,
            0.0049,
            0.0962,
            0.0238,
            0.0011,
            -0.1185,
            0.0706
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0014,
            -0.0011,
            -0.0009,
            -0.0005,
            0.0015,
            -0.0033,
            0.0004,
            0.0024,
            0.0002,
            -0.0019,
            0.0007,
            -0.0027,
            0.0016,
            0.0024,
            -0.0003
          ],
          "after": [
            -0.0374,
            -0.0047,
            -0.0779,
            -0.0421,
            0.0173,
            0.1526,
            0.0432,
            -0.1607,
            0.0278,
            -0.0229,
            -0.116,
            -0.0605,
            0.0945,
            -0.0572,
            0.0341,
            0.0715
          ]
        }
      }
    },
    {
      "step": 247,
      "word": "geriel",
      "loss": 2.4752,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0067,
            0.0237,
            0.1666,
            -0.0213,
            0.0131,
            -0.0497,
            -0.0837,
            -0.1423,
            -0.0215,
            0.071,
            0.1535,
            -0.0134,
            -0.0704,
            -0.0235,
            -0.0448,
            -0.1701
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2001,
            -0.1581,
            0.4107,
            0.2234,
            -0.049,
            -0.0848,
            -0.3866,
            0.0356,
            0.4068,
            -0.2836,
            0.0187,
            0.2364,
            -0.2769,
            0.2282,
            0.1126,
            -0.154
          ],
          "after": [
            0.0145,
            0.1206,
            -0.0351,
            -0.0489,
            0.0299,
            -0.0057,
            0.0964,
            0.0873,
            -0.073,
            -0.0671,
            0.0126,
            -0.0006,
            0.108,
            0.0344,
            -0.0613,
            0.2188
          ]
        },
        "position_0": {
          "grad": [
            -0.0137,
            0.0127,
            -0.0308,
            0.115,
            -0.1709,
            -0.1108,
            0.0157,
            0.0009,
            -0.0887,
            0.0734,
            -0.0711,
            -0.1905,
            0.0939,
            0.0598,
            -0.0801,
            0.0543
          ],
          "after": [
            0.0077,
            -0.1213,
            -0.2611,
            0.0766,
            0.0232,
            -0.0404,
            -0.1289,
            0.0858,
            0.1034,
            -0.0278,
            0.0047,
            0.0964,
            0.0232,
            0.0013,
            -0.1177,
            0.0699
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0001,
            -0.0002,
            -0.0003,
            0.0,
            -0.0001,
            0.0001,
            -0.0001,
            0.0001,
            0.0005,
            -0.0001,
            -0.0007,
            0.0004,
            0.0003,
            -0.0002,
            -0.0003
          ],
          "after": [
            -0.0373,
            -0.0049,
            -0.0789,
            -0.0419,
            0.0174,
            0.153,
            0.0437,
            -0.1599,
            0.0273,
            -0.0242,
            -0.1169,
            -0.0607,
            0.095,
            -0.0573,
            0.0341,
            0.0721
          ]
        }
      }
    },
    {
      "step": 248,
      "word": "szymon",
      "loss": 2.6361,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0072,
            0.0235,
            0.1666,
            -0.0215,
            0.0128,
            -0.0491,
            -0.0848,
            -0.1421,
            -0.021,
            0.0709,
            0.1533,
            -0.0131,
            -0.0713,
            -0.0228,
            -0.0444,
            -0.171
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0808,
            0.019,
            -0.133,
            0.027,
            0.0638,
            -0.1476,
            0.2137,
            0.0604,
            -0.2,
            0.0889,
            -0.0282,
            0.0268,
            0.1483,
            -0.1036,
            -0.2175,
            0.166
          ],
          "after": [
            0.0147,
            0.1207,
            -0.0357,
            -0.05,
            0.0296,
            -0.0046,
            0.0962,
            0.0869,
            -0.0726,
            -0.0669,
            0.0127,
            -0.0014,
            0.1078,
            0.0348,
            -0.0607,
            0.2182
          ]
        },
        "position_0": {
          "grad": [
            -0.011,
            0.1222,
            0.0095,
            0.1478,
            -0.1141,
            0.0985,
            0.0678,
            0.0493,
            -0.0825,
            0.0104,
            -0.0828,
            0.0529,
            0.1419,
            0.0382,
            0.1405,
            0.0569
          ],
          "after": [
            0.0082,
            -0.1224,
            -0.2612,
            0.0758,
            0.024,
            -0.0403,
            -0.1295,
            0.0856,
            0.104,
            -0.0281,
            0.0048,
            0.0965,
            0.0225,
            0.0014,
            -0.1174,
            0.0691
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0001,
            0.0005,
            0.001,
            0.0,
            0.0008,
            0.0003,
            -0.0002,
            -0.0006,
            0.0022,
            0.0009,
            0.0005,
            -0.0001,
            -0.0003,
            -0.0005,
            0.0005
          ],
          "after": [
            -0.0373,
            -0.0051,
            -0.0799,
            -0.042,
            0.0174,
            0.1532,
            0.044,
            -0.1592,
            0.0271,
            -0.0258,
            -0.118,
            -0.061,
            0.0955,
            -0.0572,
            0.0342,
            0.0726
          ]
        }
      }
    },
    {
      "step": 249,
      "word": "kailana",
      "loss": 2.0449,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0934,
            -0.0835,
            0.0507,
            -0.1696,
            0.0692,
            0.0405,
            -0.1683,
            -0.0654,
            0.0186,
            -0.0449,
            0.1395,
            -0.011,
            -0.2659,
            -0.0431,
            -0.032,
            -0.1327
          ],
          "after": [
            -0.0074,
            0.0236,
            0.1665,
            -0.0212,
            0.0124,
            -0.0487,
            -0.0854,
            -0.1418,
            -0.0206,
            0.0709,
            0.1527,
            -0.0128,
            -0.0713,
            -0.022,
            -0.044,
            -0.1715
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0784,
            0.1072,
            -0.0451,
            0.0438,
            0.0292,
            -0.1538,
            0.2317,
            -0.0034,
            -0.1738,
            0.0386,
            0.0241,
            -0.0006,
            0.0791,
            -0.1115,
            -0.1329,
            0.1756
          ],
          "after": [
            0.015,
            0.1207,
            -0.0361,
            -0.0511,
            0.0294,
            -0.0034,
            0.0957,
            0.0866,
            -0.0721,
            -0.0669,
            0.0126,
            -0.0021,
            0.1076,
            0.0353,
            -0.06,
            0.2174
          ]
        },
        "position_0": {
          "grad": [
            -0.0721,
            0.0593,
            -0.1342,
            -0.0367,
            0.0404,
            -0.0458,
            0.1057,
            -0.0191,
            -0.0326,
            0.0443,
            -0.0191,
            0.0426,
            -0.0276,
            -0.0533,
            -0.1123,
            -0.0729
          ],
          "after": [
            0.009,
            -0.1234,
            -0.2607,
            0.0753,
            0.0246,
            -0.0401,
            -0.1301,
            0.0856,
            0.1047,
            -0.0285,
            0.005,
            0.0964,
            0.022,
            0.0016,
            -0.1169,
            0.0686
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0023,
            -0.0015,
            0.0011,
            -0.0022,
            0.0026,
            -0.0047,
            0.0013,
            -0.0002,
            -0.0015,
            -0.0017,
            -0.0002,
            -0.0022,
            0.0019,
            0.003,
            -0.0
          ],
          "after": [
            -0.0376,
            -0.0049,
            -0.0804,
            -0.0425,
            0.0178,
            0.1527,
            0.0449,
            -0.1589,
            0.0269,
            -0.0269,
            -0.1184,
            -0.0611,
            0.0962,
            -0.0577,
            0.0337,
            0.073
          ]
        }
      }
    },
    {
      "step": 250,
      "word": "aracelis",
      "loss": 2.2801,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1198,
            0.0365,
            0.0741,
            -0.0574,
            0.0248,
            0.0208,
            0.2231,
            -0.0534,
            -0.0458,
            -0.0281,
            0.0579,
            0.0733,
            -0.0496,
            -0.2333,
            0.0213,
            0.0271
          ],
          "after": [
            -0.0073,
            0.0236,
            0.1662,
            -0.0208,
            0.0119,
            -0.0485,
            -0.0863,
            -0.1412,
            -0.0202,
            0.0711,
            0.152,
            -0.0127,
            -0.0713,
            -0.0207,
            -0.0437,
            -0.1719
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.124,
            -0.0962,
            0.1843,
            0.0043,
            -0.0001,
            -0.1024,
            -0.0969,
            -0.0885,
            0.0806,
            -0.0818,
            -0.0471,
            0.0825,
            -0.0952,
            0.0705,
            -0.0996,
            -0.0553
          ],
          "after": [
            0.0156,
            0.1208,
            -0.0368,
            -0.0521,
            0.0292,
            -0.0022,
            0.0955,
            0.0867,
            -0.0717,
            -0.0666,
            0.0128,
            -0.0029,
            0.1075,
            0.0355,
            -0.0593,
            0.2169
          ]
        },
        "position_0": {
          "grad": [
            0.0782,
            -0.0432,
            0.0316,
            -0.1734,
            0.0407,
            -0.0011,
            -0.199,
            -0.0804,
            0.1363,
            0.0815,
            0.1995,
            0.0981,
            -0.1867,
            -0.021,
            -0.0313,
            -0.1451
          ],
          "after": [
            0.0093,
            -0.1242,
            -0.2604,
            0.0752,
            0.0249,
            -0.0399,
            -0.1304,
            0.0859,
            0.105,
            -0.0293,
            0.0046,
            0.096,
            0.0218,
            0.0019,
            -0.1163,
            0.0684
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0005,
            0.001,
            0.0006,
            -0.0004,
            -0.0006,
            0.0002,
            -0.0005,
            -0.0007,
            0.0005,
            0.0009,
            -0.0004,
            0.0004,
            -0.0004,
            -0.0009,
            -0.0008
          ],
          "after": [
            -0.0379,
            -0.0047,
            -0.0811,
            -0.043,
            0.0182,
            0.1524,
            0.0457,
            -0.1586,
            0.0269,
            -0.0278,
            -0.119,
            -0.0612,
            0.0968,
            -0.058,
            0.0335,
            0.0735
          ]
        }
      }
    },
    {
      "step": 251,
      "word": "ameli",
      "loss": 1.9998,
      "learning_rate": 0.0023,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0775,
            0.0553,
            0.1165,
            0.167,
            0.0889,
            -0.0397,
            0.3296,
            0.2278,
            0.066,
            -0.0381,
            0.0417,
            0.0161,
            -0.1605,
            -0.1492,
            -0.0372,
            0.0484
          ],
          "after": [
            -0.0071,
            0.0234,
            0.1656,
            -0.0208,
            0.0114,
            -0.0481,
            -0.0877,
            -0.1416,
            -0.02,
            0.0714,
            0.1513,
            -0.0127,
            -0.0708,
            -0.0192,
            -0.0433,
            -0.1724
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0422,
            0.0529,
            0.2085,
            0.0531,
            0.0141,
            0.2304,
            -0.0504,
            -0.0924,
            0.2531,
            -0.0238,
            0.2657,
            0.1491,
            -0.3978,
            -0.1247,
            0.2151,
            -0.1815
          ],
          "after": [
            0.0159,
            0.1208,
            -0.0378,
            -0.053,
            0.029,
            -0.0015,
            0.0953,
            0.0872,
            -0.0717,
            -0.0662,
            0.012,
            -0.0039,
            0.1081,
            0.0361,
            -0.0589,
            0.2167
          ]
        },
        "position_0": {
          "grad": [
            0.1158,
            -0.1086,
            0.0755,
            -0.2224,
            0.005,
            0.0018,
            -0.3496,
            -0.1058,
            0.2545,
            0.0788,
            0.2871,
            0.0764,
            -0.2777,
            0.0195,
            0.0075,
            -0.2405
          ],
          "after": [
            0.0091,
            -0.1245,
            -0.2605,
            0.0756,
            0.0252,
            -0.0397,
            -0.13,
            0.0866,
            0.1048,
            -0.0305,
            0.0035,
            0.0954,
            0.0221,
            0.0021,
            -0.1159,
            0.0687
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0001,
            0.0011,
            -0.0005,
            0.0013,
            0.0015,
            0.0003,
            -0.0007,
            0.0004,
            -0.0003,
            0.0008,
            0.0005,
            -0.0019,
            -0.0006,
            -0.0002,
            -0.0012
          ],
          "after": [
            -0.0381,
            -0.0046,
            -0.0819,
            -0.0433,
            0.0184,
            0.1518,
            0.0463,
            -0.1581,
            0.0269,
            -0.0286,
            -0.1198,
            -0.0614,
            0.0976,
            -0.0581,
            0.0333,
            0.0742
          ]
        }
      }
    },
    {
      "step": 252,
      "word": "randell",
      "loss": 2.4342,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.03,
            0.0564,
            -0.1339,
            0.015,
            -0.2335,
            -0.0515,
            -0.0538,
            -0.0752,
            0.0582,
            -0.0646,
            -0.0576,
            -0.103,
            0.0569,
            0.0936,
            0.0719,
            0.0083
          ],
          "after": [
            -0.007,
            0.0231,
            0.1655,
            -0.021,
            0.0114,
            -0.0476,
            -0.0888,
            -0.1416,
            -0.02,
            0.0718,
            0.1509,
            -0.0124,
            -0.0706,
            -0.0182,
            -0.0432,
            -0.1728
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1404,
            -0.1118,
            0.0397,
            -0.0438,
            -0.0924,
            -0.0092,
            -0.0933,
            -0.0832,
            0.0718,
            -0.0497,
            -0.0297,
            0.1413,
            -0.0787,
            0.027,
            0.0775,
            0.1557
          ],
          "after": [
            0.0166,
            0.121,
            -0.0388,
            -0.0537,
            0.029,
            -0.0009,
            0.0952,
            0.0881,
            -0.0718,
            -0.0658,
            0.0114,
            -0.0052,
            0.1087,
            0.0364,
            -0.0587,
            0.2162
          ]
        },
        "position_0": {
          "grad": [
            0.0268,
            -0.0234,
            0.0266,
            -0.1107,
            0.0084,
            0.0945,
            0.0315,
            -0.061,
            0.1299,
            -0.0329,
            -0.0904,
            -0.0079,
            -0.1599,
            -0.0621,
            0.1146,
            0.0594
          ],
          "after": [
            0.0088,
            -0.1247,
            -0.2607,
            0.0762,
            0.0255,
            -0.04,
            -0.1298,
            0.0874,
            0.1043,
            -0.0312,
            0.0028,
            0.095,
            0.0227,
            0.0024,
            -0.1158,
            0.0688
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0007,
            0.0002,
            -0.0002,
            -0.0001,
            0.0005,
            -0.0006,
            0.0002,
            0.0005,
            -0.0,
            -0.0001,
            0.0009,
            -0.0006,
            0.0002,
            -0.0001,
            0.0008
          ],
          "after": [
            -0.0381,
            -0.0043,
            -0.0827,
            -0.0435,
            0.0185,
            0.1511,
            0.0469,
            -0.1577,
            0.0266,
            -0.0293,
            -0.1204,
            -0.0617,
            0.0983,
            -0.0583,
            0.0332,
            0.0746
          ]
        }
      }
    },
    {
      "step": 253,
      "word": "daja",
      "loss": 2.8174,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.4118,
            0.1862,
            -0.0572,
            0.3715,
            -0.2898,
            -0.0611,
            0.3646,
            0.246,
            -0.6205,
            0.1936,
            -0.1321,
            -0.4363,
            0.7467,
            0.0547,
            -0.1011,
            -0.1931
          ],
          "after": [
            -0.0077,
            0.0222,
            0.1656,
            -0.0219,
            0.012,
            -0.0468,
            -0.0904,
            -0.1424,
            -0.0183,
            0.0715,
            0.1509,
            -0.0113,
            -0.072,
            -0.0175,
            -0.0428,
            -0.1728
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0029,
            0.0835,
            -0.1069,
            0.0833,
            -0.0001,
            -0.136,
            0.2123,
            0.0359,
            -0.2164,
            0.0872,
            -0.0328,
            -0.0401,
            0.1792,
            -0.0209,
            -0.2391,
            0.1354
          ],
          "after": [
            0.0172,
            0.1211,
            -0.0394,
            -0.0546,
            0.029,
            -0.0002,
            0.0949,
            0.0886,
            -0.0716,
            -0.0657,
            0.011,
            -0.0061,
            0.1089,
            0.0368,
            -0.0582,
            0.2157
          ]
        },
        "position_0": {
          "grad": [
            -0.1153,
            0.1308,
            -0.0379,
            -0.0562,
            0.2233,
            -0.0556,
            0.1613,
            -0.0337,
            -0.0404,
            -0.0253,
            -0.2094,
            0.0651,
            0.1012,
            0.0994,
            0.0807,
            0.279
          ],
          "after": [
            0.009,
            -0.1253,
            -0.2607,
            0.0768,
            0.025,
            -0.04,
            -0.1298,
            0.0882,
            0.104,
            -0.0318,
            0.0027,
            0.0944,
            0.023,
            0.0024,
            -0.116,
            0.0684
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            -0.0,
            0.0039,
            -0.0029,
            0.0006,
            -0.0073,
            -0.0009,
            -0.0045,
            -0.0011,
            0.0002,
            0.0023,
            -0.0051,
            0.0029,
            0.0003,
            -0.0021,
            -0.0058
          ],
          "after": [
            -0.0386,
            -0.0041,
            -0.0843,
            -0.0429,
            0.0185,
            0.1522,
            0.0475,
            -0.1562,
            0.0267,
            -0.0299,
            -0.1216,
            -0.0609,
            0.0985,
            -0.0585,
            0.0335,
            0.0761
          ]
        }
      }
    },
    {
      "step": 254,
      "word": "brailey",
      "loss": 2.0995,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0672,
            -0.1686,
            0.0592,
            -0.1305,
            0.0795,
            0.0192,
            -0.1558,
            0.0131,
            0.0387,
            -0.0056,
            0.1235,
            -0.0574,
            -0.1562,
            -0.0259,
            -0.0007,
            -0.1175
          ],
          "after": [
            -0.0082,
            0.022,
            0.1654,
            -0.0224,
            0.0124,
            -0.0463,
            -0.0915,
            -0.1432,
            -0.017,
            0.0713,
            0.1506,
            -0.0103,
            -0.0729,
            -0.0168,
            -0.0425,
            -0.1726
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0581,
            -0.2021,
            -0.0145,
            -0.1179,
            -0.203,
            0.0929,
            -0.2506,
            0.0383,
            0.1113,
            0.166,
            -0.0681,
            -0.1122,
            0.0626,
            0.1672,
            0.1495,
            -0.2382
          ],
          "after": [
            0.0175,
            0.1215,
            -0.0399,
            -0.0549,
            0.0293,
            0.0002,
            0.095,
            0.0889,
            -0.0716,
            -0.0661,
            0.0109,
            -0.0066,
            0.109,
            0.0368,
            -0.058,
            0.2156
          ]
        },
        "position_0": {
          "grad": [
            0.0102,
            0.0474,
            0.0754,
            -0.0019,
            0.0776,
            -0.007,
            0.1357,
            0.0009,
            -0.157,
            -0.0407,
            -0.0187,
            0.0635,
            0.1591,
            -0.1125,
            -0.087,
            0.2851
          ],
          "after": [
            0.0092,
            -0.1259,
            -0.261,
            0.0773,
            0.0244,
            -0.0399,
            -0.1301,
            0.0889,
            0.1041,
            -0.032,
            0.0027,
            0.0937,
            0.023,
            0.0027,
            -0.1159,
            0.0676
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0037,
            -0.0011,
            0.0004,
            -0.0038,
            0.0003,
            -0.0039,
            0.0024,
            -0.0,
            -0.0022,
            -0.0034,
            0.0015,
            -0.0014,
            0.0015,
            0.006,
            0.0006
          ],
          "after": [
            -0.0386,
            -0.0031,
            -0.0854,
            -0.0425,
            0.0192,
            0.1531,
            0.0485,
            -0.1556,
            0.0268,
            -0.0299,
            -0.1216,
            -0.0606,
            0.0989,
            -0.0591,
            0.0327,
            0.0772
          ]
        }
      }
    },
    {
      "step": 255,
      "word": "vianca",
      "loss": 2.5772,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2462,
            0.1272,
            -0.2266,
            -0.0173,
            -0.3545,
            0.0055,
            -0.0636,
            -0.1299,
            0.0332,
            0.144,
            0.027,
            -0.2255,
            0.1124,
            0.2275,
            0.0367,
            -0.1744
          ],
          "after": [
            -0.0092,
            0.0215,
            0.1659,
            -0.0228,
            0.0134,
            -0.0459,
            -0.0923,
            -0.1434,
            -0.016,
            0.0705,
            0.1502,
            -0.009,
            -0.0738,
            -0.0169,
            -0.0423,
            -0.1721
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0272,
            0.0674,
            -0.0936,
            0.0646,
            0.1135,
            -0.1345,
            0.255,
            0.0298,
            -0.2208,
            0.0318,
            -0.0336,
            -0.0007,
            0.1307,
            -0.1089,
            -0.2197,
            0.1825
          ],
          "after": [
            0.0179,
            0.1217,
            -0.0401,
            -0.0554,
            0.0293,
            0.0008,
            0.0947,
            0.089,
            -0.0713,
            -0.0665,
            0.0109,
            -0.0071,
            0.1089,
            0.037,
            -0.0576,
            0.2152
          ]
        },
        "position_0": {
          "grad": [
            -0.1391,
            0.114,
            -0.0307,
            0.0677,
            0.1552,
            0.0822,
            0.0482,
            -0.0174,
            -0.115,
            -0.0107,
            -0.1021,
            -0.0304,
            0.0964,
            -0.0887,
            -0.1052,
            0.2048
          ],
          "after": [
            0.0099,
            -0.1268,
            -0.2612,
            0.0776,
            0.0234,
            -0.0402,
            -0.1304,
            0.0896,
            0.1043,
            -0.0321,
            0.003,
            0.0932,
            0.0228,
            0.0033,
            -0.1155,
            0.0665
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0033,
            -0.0014,
            -0.0002,
            0.001,
            -0.0046,
            -0.0002,
            -0.0046,
            0.0029,
            0.0023,
            0.0012,
            0.0017,
            0.0055,
            -0.0051,
            -0.0025,
            0.0046,
            -0.0001
          ],
          "after": [
            -0.0377,
            -0.002,
            -0.0863,
            -0.0425,
            0.0205,
            0.1538,
            0.05,
            -0.1558,
            0.0262,
            -0.0301,
            -0.1221,
            -0.0614,
            0.1,
            -0.0589,
            0.0312,
            0.0783
          ]
        }
      }
    },
    {
      "step": 256,
      "word": "jep",
      "loss": 3.245,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.01,
            0.021,
            0.1663,
            -0.0232,
            0.0143,
            -0.0455,
            -0.093,
            -0.1436,
            -0.0151,
            0.0699,
            0.1499,
            -0.0079,
            -0.0747,
            -0.0169,
            -0.0422,
            -0.1717
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.5357,
            -0.2677,
            0.3272,
            -0.0509,
            0.2126,
            0.2574,
            -0.805,
            -0.2202,
            0.7504,
            0.1201,
            0.4119,
            0.0606,
            -0.5739,
            0.0863,
            0.0564,
            -0.492
          ],
          "after": [
            0.0169,
            0.1224,
            -0.041,
            -0.0557,
            0.0291,
            0.0009,
            0.0954,
            0.09,
            -0.072,
            -0.0672,
            0.0096,
            -0.0076,
            0.1097,
            0.0369,
            -0.0573,
            0.2156
          ]
        },
        "position_0": {
          "grad": [
            -0.108,
            0.1644,
            0.0259,
            0.5278,
            0.1024,
            -0.1646,
            0.4786,
            0.1562,
            -0.5618,
            -0.2545,
            -0.0442,
            0.0345,
            0.7479,
            -0.1962,
            -0.2946,
            0.4313
          ],
          "after": [
            0.0109,
            -0.128,
            -0.2614,
            0.0768,
            0.0222,
            -0.0398,
            -0.1314,
            0.0895,
            0.1055,
            -0.031,
            0.0034,
            0.0927,
            0.0215,
            0.0044,
            -0.1144,
            0.0649
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.003,
            0.0046,
            -0.0018,
            0.004,
            -0.0031,
            -0.0062,
            -0.001,
            0.0031,
            -0.0003,
            0.0024,
            -0.005,
            0.001,
            -0.0008,
            -0.0026,
            -0.0014
          ],
          "after": [
            -0.0372,
            -0.0005,
            -0.0881,
            -0.042,
            0.021,
            0.1551,
            0.052,
            -0.1557,
            0.0249,
            -0.0303,
            -0.1232,
            -0.0611,
            0.1008,
            -0.0585,
            0.0305,
            0.0794
          ]
        }
      }
    },
    {
      "step": 257,
      "word": "olina",
      "loss": 2.3288,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3911,
            0.0326,
            -0.1499,
            -0.0675,
            -0.0623,
            0.0077,
            0.018,
            -0.0335,
            -0.1142,
            0.1329,
            0.0895,
            -0.3597,
            0.1892,
            0.151,
            -0.0258,
            -0.2363
          ],
          "after": [
            -0.0115,
            0.0205,
            0.167,
            -0.0233,
            0.0151,
            -0.0453,
            -0.0936,
            -0.1436,
            -0.0141,
            0.0689,
            0.1493,
            -0.0062,
            -0.0758,
            -0.0174,
            -0.042,
            -0.1709
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0187,
            0.043,
            -0.1001,
            0.0291,
            -0.0002,
            -0.1538,
            0.2186,
            -0.0149,
            -0.1785,
            0.0545,
            0.0014,
            -0.0519,
            0.1522,
            -0.0872,
            -0.1917,
            0.163
          ],
          "after": [
            0.0162,
            0.1229,
            -0.0415,
            -0.056,
            0.0288,
            0.0012,
            0.0957,
            0.091,
            -0.0724,
            -0.068,
            0.0085,
            -0.0079,
            0.1101,
            0.0371,
            -0.0568,
            0.2158
          ]
        },
        "position_0": {
          "grad": [
            0.123,
            -0.2328,
            0.0787,
            -0.1421,
            -0.1195,
            -0.0719,
            -0.3205,
            -0.0258,
            0.0994,
            0.1228,
            -0.051,
            -0.0377,
            -0.1487,
            0.0226,
            0.0752,
            -0.275
          ],
          "after": [
            0.0112,
            -0.1284,
            -0.2619,
            0.0763,
            0.0216,
            -0.0391,
            -0.1317,
            0.0896,
            0.1064,
            -0.0306,
            0.0038,
            0.0923,
            0.0206,
            0.0053,
            -0.1137,
            0.064
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0024,
            -0.0002,
            0.0033,
            0.0005,
            -0.0,
            0.0008,
            -0.0057,
            -0.0022,
            -0.0019,
            -0.0023,
            0.0001,
            -0.0016,
            -0.0013,
            0.0023,
            -0.0013,
            -0.0032
          ],
          "after": [
            -0.0373,
            0.0008,
            -0.0904,
            -0.0417,
            0.0214,
            0.1561,
            0.0543,
            -0.1551,
            0.0243,
            -0.0299,
            -0.1242,
            -0.0605,
            0.1017,
            -0.0588,
            0.03,
            0.081
          ]
        }
      }
    },
    {
      "step": 258,
      "word": "braya",
      "loss": 2.0006,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5231,
            -0.0894,
            -0.2108,
            -0.1362,
            -0.1855,
            0.1344,
            -0.0526,
            -0.0292,
            -0.1104,
            0.2235,
            0.012,
            -0.2922,
            0.1952,
            0.2853,
            0.0009,
            -0.1689
          ],
          "after": [
            -0.0137,
            0.0203,
            0.1681,
            -0.0231,
            0.0162,
            -0.0456,
            -0.094,
            -0.1435,
            -0.0129,
            0.0673,
            0.1488,
            -0.0043,
            -0.0771,
            -0.0187,
            -0.0418,
            -0.1699
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0255,
            0.0489,
            -0.074,
            0.0621,
            -0.0602,
            -0.1771,
            0.1816,
            -0.0029,
            -0.2288,
            0.0934,
            -0.0299,
            -0.0484,
            0.1974,
            -0.0532,
            -0.1614,
            0.1235
          ],
          "after": [
            0.0156,
            0.1233,
            -0.0418,
            -0.0565,
            0.0287,
            0.0018,
            0.0958,
            0.0918,
            -0.0724,
            -0.0689,
            0.0076,
            -0.0081,
            0.1102,
            0.0374,
            -0.0562,
            0.2157
          ]
        },
        "position_0": {
          "grad": [
            0.0299,
            0.0289,
            0.1012,
            -0.0023,
            0.0709,
            -0.0185,
            0.1451,
            -0.0003,
            -0.1748,
            -0.0646,
            -0.0248,
            0.045,
            0.1937,
            -0.1228,
            -0.0776,
            0.3546
          ],
          "after": [
            0.0114,
            -0.1288,
            -0.2629,
            0.076,
            0.0209,
            -0.0385,
            -0.1322,
            0.0896,
            0.1074,
            -0.03,
            0.0042,
            0.0919,
            0.0195,
            0.0064,
            -0.1129,
            0.0626
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0027,
            0.0016,
            0.0026,
            0.0005,
            -0.001,
            -0.0,
            -0.0042,
            -0.0017,
            0.0003,
            -0.0053,
            -0.0015,
            -0.0006,
            -0.0006,
            0.0047,
            -0.0008,
            -0.0019
          ],
          "after": [
            -0.0381,
            0.0016,
            -0.0929,
            -0.0416,
            0.0218,
            0.1569,
            0.0568,
            -0.1542,
            0.0237,
            -0.0283,
            -0.1246,
            -0.06,
            0.1026,
            -0.0602,
            0.0298,
            0.0827
          ]
        }
      }
    },
    {
      "step": 259,
      "word": "darleny",
      "loss": 1.9885,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0346,
            -0.0878,
            0.0002,
            -0.1463,
            0.1333,
            0.0374,
            0.015,
            -0.1401,
            0.1517,
            -0.0483,
            0.0943,
            0.0397,
            -0.1386,
            -0.144,
            0.0214,
            0.0647
          ],
          "after": [
            -0.0156,
            0.0205,
            0.1691,
            -0.0225,
            0.0169,
            -0.0461,
            -0.0944,
            -0.143,
            -0.0123,
            0.066,
            0.1481,
            -0.0027,
            -0.078,
            -0.0193,
            -0.0417,
            -0.1692
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0603,
            -0.1952,
            0.1038,
            -0.0028,
            0.0369,
            0.0781,
            -0.2325,
            0.0546,
            0.104,
            0.0389,
            -0.1017,
            0.0005,
            -0.049,
            0.1467,
            -0.0243,
            -0.2138
          ],
          "after": [
            0.0149,
            0.1239,
            -0.0422,
            -0.0569,
            0.0286,
            0.0022,
            0.0961,
            0.0923,
            -0.0725,
            -0.0697,
            0.0072,
            -0.0082,
            0.1103,
            0.0373,
            -0.0556,
            0.2159
          ]
        },
        "position_0": {
          "grad": [
            -0.0592,
            0.0391,
            -0.0246,
            -0.0621,
            0.0957,
            -0.0348,
            0.048,
            -0.0098,
            0.0198,
            -0.0095,
            -0.1407,
            0.0151,
            -0.0011,
            0.0586,
            0.075,
            0.1132
          ],
          "after": [
            0.0118,
            -0.1292,
            -0.2635,
            0.0758,
            0.02,
            -0.0378,
            -0.1328,
            0.0897,
            0.1082,
            -0.0294,
            0.005,
            0.0915,
            0.0186,
            0.0072,
            -0.1124,
            0.0613
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.001,
            0.0,
            0.0001,
            0.0003,
            0.0016,
            -0.0012,
            0.0011,
            0.0008,
            -0.0002,
            -0.0008,
            0.0013,
            -0.0012,
            0.0003,
            0.0001,
            0.0006
          ],
          "after": [
            -0.0386,
            0.0025,
            -0.095,
            -0.0415,
            0.0222,
            0.1573,
            0.0591,
            -0.1537,
            0.0229,
            -0.027,
            -0.1247,
            -0.0597,
            0.1035,
            -0.0615,
            0.0296,
            0.0841
          ]
        }
      }
    },
    {
      "step": 260,
      "word": "baylin",
      "loss": 1.8359,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.143,
            -0.1474,
            -0.0395,
            -0.1059,
            -0.1243,
            0.0394,
            -0.1,
            -0.021,
            0.0003,
            0.0483,
            -0.0678,
            -0.0215,
            0.0697,
            0.1677,
            0.0498,
            0.01
          ],
          "after": [
            -0.0176,
            0.021,
            0.17,
            -0.0218,
            0.0177,
            -0.0466,
            -0.0946,
            -0.1425,
            -0.0118,
            0.0648,
            0.1477,
            -0.0013,
            -0.0788,
            -0.0203,
            -0.0418,
            -0.1686
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0728,
            0.1044,
            -0.0664,
            0.0812,
            0.0359,
            -0.1674,
            0.2619,
            -0.0186,
            -0.2247,
            0.0679,
            0.0117,
            0.0045,
            0.1435,
            -0.11,
            -0.2059,
            0.2171
          ],
          "after": [
            0.0145,
            0.1243,
            -0.0425,
            -0.0575,
            0.0284,
            0.0028,
            0.0961,
            0.0928,
            -0.0724,
            -0.0707,
            0.0069,
            -0.0084,
            0.1102,
            0.0374,
            -0.0549,
            0.2158
          ]
        },
        "position_0": {
          "grad": [
            0.0178,
            0.0148,
            0.0777,
            -0.0149,
            0.0678,
            -0.0131,
            0.1006,
            -0.0053,
            -0.1321,
            -0.0458,
            -0.0134,
            0.0506,
            0.1483,
            -0.1056,
            -0.064,
            0.2809
          ],
          "after": [
            0.0121,
            -0.1296,
            -0.2645,
            0.0757,
            0.019,
            -0.0372,
            -0.1334,
            0.0898,
            0.1092,
            -0.0287,
            0.0056,
            0.091,
            0.0176,
            0.0082,
            -0.1118,
            0.0596
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0013,
            -0.0016,
            -0.0022,
            0.002,
            0.0013,
            -0.0008,
            0.0019,
            0.0038,
            -0.0013,
            -0.0005,
            0.0022,
            -0.0039,
            -0.002,
            0.0006,
            -0.0001
          ],
          "after": [
            -0.0388,
            0.0036,
            -0.0965,
            -0.0409,
            0.0222,
            0.1573,
            0.0611,
            -0.1537,
            0.0213,
            -0.0255,
            -0.1246,
            -0.0599,
            0.105,
            -0.0621,
            0.0293,
            0.0852
          ]
        }
      }
    },
    {
      "step": 261,
      "word": "jaxi",
      "loss": 2.5208,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.04,
            0.1012,
            -0.0215,
            0.8392,
            0.1097,
            -0.2514,
            0.178,
            0.0185,
            -0.6524,
            -0.122,
            -0.226,
            -0.0472,
            0.7586,
            -0.3371,
            -0.3135,
            0.3951
          ],
          "after": [
            -0.0192,
            0.0212,
            0.1708,
            -0.023,
            0.0182,
            -0.046,
            -0.095,
            -0.1422,
            -0.0098,
            0.0642,
            0.1481,
            -0.0,
            -0.0809,
            -0.0202,
            -0.0409,
            -0.1688
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0834,
            0.0569,
            -0.1266,
            0.0179,
            0.0242,
            -0.1133,
            0.22,
            0.0257,
            -0.2125,
            0.0836,
            -0.0663,
            -0.0476,
            0.1639,
            -0.0499,
            -0.1719,
            0.1466
          ],
          "after": [
            0.0144,
            0.1245,
            -0.0425,
            -0.0581,
            0.0282,
            0.0034,
            0.0958,
            0.0931,
            -0.072,
            -0.0717,
            0.0068,
            -0.0083,
            0.1098,
            0.0377,
            -0.0541,
            0.2155
          ]
        },
        "position_0": {
          "grad": [
            -0.0811,
            0.1659,
            -0.0569,
            0.3343,
            0.146,
            -0.112,
            0.3869,
            0.0716,
            -0.4299,
            -0.1021,
            0.0056,
            0.1424,
            0.5516,
            -0.207,
            -0.2791,
            0.353
          ],
          "after": [
            0.0126,
            -0.1305,
            -0.265,
            0.0749,
            0.0177,
            -0.0362,
            -0.1345,
            0.0896,
            0.1107,
            -0.0276,
            0.0062,
            0.0901,
            0.016,
            0.0097,
            -0.1106,
            0.0577
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            0.0004,
            0.006,
            -0.0067,
            0.0033,
            -0.0097,
            0.0009,
            -0.0069,
            -0.0004,
            -0.001,
            0.0047,
            -0.0065,
            0.0028,
            -0.0006,
            -0.0026,
            -0.009
          ],
          "after": [
            -0.0392,
            0.0043,
            -0.0989,
            -0.0389,
            0.0216,
            0.159,
            0.0627,
            -0.1522,
            0.02,
            -0.024,
            -0.1258,
            -0.059,
            0.1057,
            -0.0624,
            0.0295,
            0.0877
          ]
        }
      }
    },
    {
      "step": 262,
      "word": "kolyn",
      "loss": 2.3076,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0205,
            0.0213,
            0.1715,
            -0.024,
            0.0186,
            -0.0455,
            -0.0954,
            -0.1418,
            -0.0082,
            0.0637,
            0.1484,
            0.0011,
            -0.0827,
            -0.0201,
            -0.0402,
            -0.169
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0765,
            0.0859,
            -0.0987,
            0.0996,
            -0.0276,
            -0.1856,
            0.3185,
            0.0235,
            -0.2463,
            0.0849,
            -0.0252,
            -0.0119,
            0.1684,
            -0.1298,
            -0.1781,
            0.2486
          ],
          "after": [
            0.0145,
            0.1245,
            -0.0422,
            -0.0589,
            0.0281,
            0.0043,
            0.0953,
            0.0932,
            -0.0713,
            -0.0728,
            0.0068,
            -0.0083,
            0.1093,
            0.0382,
            -0.0532,
            0.2148
          ]
        },
        "position_0": {
          "grad": [
            -0.0909,
            -0.0039,
            -0.1051,
            0.0174,
            -0.0301,
            -0.0106,
            0.0737,
            -0.0062,
            0.0343,
            -0.0461,
            0.0017,
            -0.0853,
            -0.0242,
            0.0559,
            -0.0338,
            -0.1171
          ],
          "after": [
            0.0134,
            -0.1312,
            -0.265,
            0.0742,
            0.0167,
            -0.0353,
            -0.1355,
            0.0894,
            0.112,
            -0.0265,
            0.0067,
            0.0897,
            0.0146,
            0.0108,
            -0.1095,
            0.0562
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0035,
            -0.002,
            0.0039,
            -0.0004,
            0.0001,
            0.0055,
            -0.0002,
            -0.002,
            0.0008,
            -0.0004,
            0.0018,
            -0.0001,
            -0.002,
            0.002,
            0.0057
          ],
          "after": [
            -0.0391,
            0.0043,
            -0.1005,
            -0.0381,
            0.0212,
            0.1605,
            0.0634,
            -0.1509,
            0.0195,
            -0.023,
            -0.1267,
            -0.0585,
            0.1064,
            -0.0622,
            0.0293,
            0.0888
          ]
        }
      }
    },
    {
      "step": 263,
      "word": "whitten",
      "loss": 2.9967,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0217,
            0.0214,
            0.1721,
            -0.0249,
            0.0189,
            -0.0451,
            -0.0958,
            -0.1416,
            -0.0067,
            0.0633,
            0.1486,
            0.002,
            -0.0843,
            -0.0201,
            -0.0396,
            -0.1691
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0939,
            -0.1791,
            -0.1113,
            -0.1186,
            -0.2643,
            0.0315,
            -0.1119,
            0.0366,
            0.0283,
            0.0948,
            -0.0426,
            -0.0865,
            0.0059,
            0.0847,
            0.1933,
            -0.0109
          ],
          "after": [
            0.0143,
            0.1248,
            -0.0418,
            -0.0592,
            0.0284,
            0.005,
            0.0949,
            0.0932,
            -0.0708,
            -0.0741,
            0.0069,
            -0.008,
            0.1088,
            0.0384,
            -0.0526,
            0.2143
          ]
        },
        "position_0": {
          "grad": [
            0.0505,
            0.1895,
            -0.1205,
            0.2119,
            -0.0005,
            -0.0431,
            0.3176,
            -0.0128,
            -0.1731,
            -0.0476,
            -0.0676,
            -0.0569,
            0.2261,
            0.0014,
            -0.1196,
            0.1933
          ],
          "after": [
            0.0139,
            -0.1324,
            -0.2644,
            0.0732,
            0.0159,
            -0.0343,
            -0.1369,
            0.0894,
            0.1133,
            -0.0252,
            0.0072,
            0.0894,
            0.013,
            0.0117,
            -0.1083,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0018,
            0.0046,
            -0.0003,
            0.0063,
            0.0007,
            0.0031,
            0.0148,
            -0.0018,
            -0.0049,
            0.003,
            0.0025,
            -0.0059,
            0.0042,
            -0.0015,
            -0.0033,
            0.0013
          ],
          "after": [
            -0.0387,
            0.0034,
            -0.1019,
            -0.0387,
            0.0208,
            0.1612,
            0.0624,
            -0.1494,
            0.0203,
            -0.0228,
            -0.1281,
            -0.0571,
            0.1062,
            -0.0616,
            0.0297,
            0.0894
          ]
        }
      }
    },
    {
      "step": 264,
      "word": "guled",
      "loss": 3.0512,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0227,
            0.0215,
            0.1726,
            -0.0257,
            0.0192,
            -0.0448,
            -0.096,
            -0.1414,
            -0.0055,
            0.0629,
            0.1488,
            0.0028,
            -0.0856,
            -0.02,
            -0.039,
            -0.1693
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0708,
            -0.206,
            0.0865,
            -0.1789,
            0.2444,
            0.0899,
            -0.3038,
            0.1204,
            0.1663,
            -0.0891,
            -0.0691,
            0.0409,
            -0.1902,
            -0.0383,
            0.2512,
            -0.3362
          ],
          "after": [
            0.0143,
            0.1255,
            -0.0416,
            -0.0589,
            0.0283,
            0.0055,
            0.095,
            0.0926,
            -0.0706,
            -0.0749,
            0.0072,
            -0.0079,
            0.1087,
            0.0387,
            -0.0525,
            0.2144
          ]
        },
        "position_0": {
          "grad": [
            -0.0117,
            0.0433,
            -0.0182,
            0.1927,
            -0.0929,
            -0.0832,
            0.0443,
            -0.0551,
            -0.1287,
            0.0705,
            0.0043,
            -0.1503,
            0.1264,
            0.0854,
            -0.1019,
            0.1395
          ],
          "after": [
            0.0144,
            -0.1335,
            -0.2639,
            0.0719,
            0.0155,
            -0.0331,
            -0.1382,
            0.0895,
            0.1147,
            -0.0246,
            0.0077,
            0.0897,
            0.0115,
            0.0122,
            -0.1069,
            0.0531
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0038,
            -0.0021,
            -0.001,
            -0.0039,
            -0.0034,
            0.0006,
            -0.0034,
            0.0003,
            0.0013,
            0.0025,
            0.0018,
            -0.0007,
            -0.0009,
            -0.0004,
            0.0034,
            -0.0005
          ],
          "after": [
            -0.0374,
            0.003,
            -0.1028,
            -0.0385,
            0.0209,
            0.1617,
            0.0619,
            -0.1481,
            0.0206,
            -0.0231,
            -0.1297,
            -0.0558,
            0.1063,
            -0.0611,
            0.0295,
            0.0901
          ]
        }
      }
    },
    {
      "step": 265,
      "word": "akilan",
      "loss": 1.9959,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3226,
            0.2173,
            0.0958,
            -0.0026,
            0.1434,
            -0.1885,
            0.1673,
            -0.0501,
            0.1621,
            -0.1776,
            -0.1212,
            0.227,
            -0.1398,
            -0.1153,
            -0.2799,
            -0.0435
          ],
          "after": [
            -0.0229,
            0.021,
            0.1728,
            -0.0263,
            0.0192,
            -0.0437,
            -0.0966,
            -0.141,
            -0.0049,
            0.0633,
            0.1494,
            0.003,
            -0.0864,
            -0.0197,
            -0.0378,
            -0.1693
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0592,
            0.103,
            -0.0527,
            0.0388,
            0.0446,
            -0.1735,
            0.2406,
            -0.0186,
            -0.1578,
            0.0441,
            0.005,
            -0.0222,
            0.1006,
            -0.1049,
            -0.1657,
            0.19
          ],
          "after": [
            0.0145,
            0.1259,
            -0.0413,
            -0.0587,
            0.0281,
            0.0062,
            0.0947,
            0.0922,
            -0.0702,
            -0.0757,
            0.0075,
            -0.0078,
            0.1084,
            0.0391,
            -0.0521,
            0.2141
          ]
        },
        "position_0": {
          "grad": [
            0.0856,
            -0.1208,
            0.1088,
            -0.2662,
            0.0149,
            0.0347,
            -0.367,
            -0.0849,
            0.2771,
            0.0784,
            0.2834,
            0.0866,
            -0.3245,
            0.0092,
            0.0218,
            -0.3376
          ],
          "after": [
            0.0145,
            -0.1341,
            -0.2639,
            0.0714,
            0.015,
            -0.0323,
            -0.1386,
            0.09,
            0.1154,
            -0.0243,
            0.0074,
            0.0897,
            0.0108,
            0.0126,
            -0.1059,
            0.0523
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0012,
            -0.0005,
            0.0004,
            -0.0014,
            0.0017,
            -0.0019,
            0.0006,
            0.0006,
            -0.0001,
            0.0003,
            0.0014,
            -0.0011,
            -0.0001,
            0.0012,
            -0.001
          ],
          "after": [
            -0.0362,
            0.0029,
            -0.1034,
            -0.0384,
            0.0213,
            0.1618,
            0.0616,
            -0.1472,
            0.0208,
            -0.0235,
            -0.1312,
            -0.0549,
            0.1065,
            -0.0606,
            0.0291,
            0.0908
          ]
        }
      }
    },
    {
      "step": 266,
      "word": "normani",
      "loss": 2.5075,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1472,
            0.0663,
            -0.0055,
            0.0402,
            -0.0159,
            -0.0406,
            0.0216,
            -0.0611,
            0.0567,
            -0.1313,
            -0.0638,
            0.144,
            -0.0853,
            -0.07,
            -0.0013,
            0.124
          ],
          "after": [
            -0.0227,
            0.0203,
            0.173,
            -0.0269,
            0.0192,
            -0.0426,
            -0.0971,
            -0.1405,
            -0.0045,
            0.064,
            0.15,
            0.0029,
            -0.087,
            -0.0192,
            -0.0367,
            -0.1696
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.091,
            0.0773,
            -0.0806,
            0.0635,
            0.0575,
            -0.124,
            0.2535,
            0.0282,
            -0.1571,
            0.0725,
            -0.0243,
            0.0357,
            0.1089,
            -0.1002,
            -0.1805,
            0.1678
          ],
          "after": [
            0.0148,
            0.1261,
            -0.0409,
            -0.0588,
            0.0279,
            0.0069,
            0.0943,
            0.0918,
            -0.0697,
            -0.0766,
            0.0078,
            -0.0077,
            0.108,
            0.0398,
            -0.0516,
            0.2136
          ]
        },
        "position_0": {
          "grad": [
            -0.0106,
            0.0044,
            0.0257,
            -0.0294,
            -0.2788,
            0.0401,
            -0.0974,
            -0.0205,
            0.1616,
            -0.1107,
            -0.1742,
            -0.1379,
            -0.0963,
            0.124,
            0.2047,
            -0.063
          ],
          "after": [
            0.0145,
            -0.1346,
            -0.264,
            0.071,
            0.0155,
            -0.0318,
            -0.1389,
            0.0905,
            0.1157,
            -0.0236,
            0.0076,
            0.0901,
            0.0103,
            0.0126,
            -0.1055,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0022,
            0.0008,
            0.0001,
            0.0013,
            -0.0011,
            0.0011,
            0.0004,
            -0.0018,
            -0.0005,
            0.0007,
            -0.0,
            -0.0014,
            0.0006,
            0.0018,
            -0.0015,
            0.0001
          ],
          "after": [
            -0.0357,
            0.0026,
            -0.104,
            -0.0386,
            0.0218,
            0.1617,
            0.0614,
            -0.146,
            0.021,
            -0.0239,
            -0.1324,
            -0.0539,
            0.1066,
            -0.0606,
            0.0291,
            0.0914
          ]
        }
      }
    },
    {
      "step": 267,
      "word": "eliann",
      "loss": 2.0904,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0609,
            0.0824,
            -0.1388,
            0.0267,
            -0.1376,
            -0.0856,
            0.0386,
            -0.0729,
            0.0767,
            -0.0334,
            0.0112,
            0.0231,
            -0.087,
            -0.059,
            0.095,
            0.0053
          ],
          "after": [
            -0.0225,
            0.0195,
            0.1735,
            -0.0275,
            0.0195,
            -0.0414,
            -0.0977,
            -0.1398,
            -0.0043,
            0.0647,
            0.1506,
            0.0028,
            -0.0873,
            -0.0186,
            -0.0361,
            -0.1698
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2477,
            0.4205,
            0.5457,
            -0.048,
            0.1946,
            -0.1561,
            0.3619,
            -0.126,
            -0.0359,
            -0.1315,
            0.0708,
            0.0035,
            0.0118,
            -0.066,
            -0.2422,
            -0.0483
          ],
          "after": [
            0.0157,
            0.1255,
            -0.0417,
            -0.0587,
            0.0274,
            0.0079,
            0.0935,
            0.0919,
            -0.0692,
            -0.0769,
            0.0079,
            -0.0077,
            0.1077,
            0.0405,
            -0.0509,
            0.2133
          ]
        },
        "position_0": {
          "grad": [
            0.0271,
            -0.3048,
            0.1268,
            -0.1538,
            -0.133,
            0.0486,
            -0.3822,
            -0.06,
            0.3066,
            0.1195,
            0.012,
            -0.1179,
            -0.301,
            0.0165,
            0.1302,
            -0.4683
          ],
          "after": [
            0.0145,
            -0.1342,
            -0.2647,
            0.0709,
            0.0163,
            -0.0315,
            -0.1385,
            0.0911,
            0.1154,
            -0.0236,
            0.0077,
            0.0907,
            0.0103,
            0.0125,
            -0.1055,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.002,
            -0.001,
            -0.0002,
            -0.0042,
            -0.0004,
            -0.0055,
            0.003,
            0.0015,
            -0.0031,
            -0.0029,
            0.0032,
            -0.0027,
            0.0005,
            0.0059,
            0.0005
          ],
          "after": [
            -0.0349,
            0.0028,
            -0.1043,
            -0.0387,
            0.0228,
            0.1616,
            0.0617,
            -0.1457,
            0.0208,
            -0.0235,
            -0.1327,
            -0.0536,
            0.1071,
            -0.0607,
            0.0281,
            0.0918
          ]
        }
      }
    },
    {
      "step": 268,
      "word": "harlea",
      "loss": 1.9909,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1496,
            -0.0646,
            -0.0214,
            -0.3153,
            0.068,
            0.0524,
            -0.0497,
            -0.1605,
            0.2315,
            0.055,
            0.1987,
            -0.0718,
            -0.1524,
            -0.1148,
            0.0393,
            -0.1018
          ],
          "after": [
            -0.0226,
            0.019,
            0.174,
            -0.0273,
            0.0196,
            -0.0405,
            -0.098,
            -0.1387,
            -0.0048,
            0.0652,
            0.1504,
            0.0028,
            -0.0872,
            -0.0178,
            -0.0356,
            -0.1698
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0806,
            -0.2631,
            0.1081,
            -0.0416,
            -0.0124,
            0.1625,
            -0.3337,
            0.0637,
            0.1641,
            0.0129,
            -0.1151,
            -0.0328,
            -0.0931,
            0.2131,
            0.068,
            -0.2736
          ],
          "after": [
            0.0163,
            0.1254,
            -0.0425,
            -0.0585,
            0.027,
            0.0084,
            0.0932,
            0.0918,
            -0.069,
            -0.0773,
            0.0083,
            -0.0076,
            0.1076,
            0.0406,
            -0.0504,
            0.2135
          ]
        },
        "position_0": {
          "grad": [
            0.0516,
            -0.0916,
            0.0863,
            -0.0345,
            -0.132,
            0.0729,
            -0.0823,
            0.0573,
            0.1821,
            0.0698,
            -0.1133,
            -0.2063,
            0.0811,
            0.1236,
            0.0879,
            -0.0146
          ],
          "after": [
            0.0143,
            -0.1335,
            -0.2656,
            0.071,
            0.0173,
            -0.0316,
            -0.1381,
            0.0914,
            0.1149,
            -0.0239,
            0.0081,
            0.0919,
            0.0102,
            0.012,
            -0.1057,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0011,
            -0.003,
            -0.0004,
            -0.0029,
            0.0041,
            -0.0023,
            0.0013,
            0.002,
            0.0004,
            -0.0025,
            0.0032,
            -0.0029,
            0.0025,
            0.0041,
            0.0011
          ],
          "after": [
            -0.0341,
            0.0031,
            -0.1039,
            -0.0387,
            0.0242,
            0.1609,
            0.0623,
            -0.1457,
            0.0202,
            -0.0233,
            -0.1324,
            -0.0539,
            0.1079,
            -0.0615,
            0.0266,
            0.092
          ]
        }
      }
    },
    {
      "step": 269,
      "word": "edeline",
      "loss": 2.1299,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0227,
            0.0186,
            0.1744,
            -0.0272,
            0.0197,
            -0.0398,
            -0.0983,
            -0.1378,
            -0.0051,
            0.0655,
            0.1503,
            0.0029,
            -0.0872,
            -0.0171,
            -0.0353,
            -0.1698
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0917,
            0.1503,
            0.5512,
            -0.2418,
            -0.2174,
            0.467,
            -0.4833,
            -0.2274,
            0.3048,
            -0.2302,
            0.226,
            -0.1907,
            -0.2903,
            0.335,
            0.4269,
            -0.4816
          ],
          "after": [
            0.0165,
            0.1251,
            -0.0443,
            -0.0575,
            0.027,
            0.0081,
            0.0935,
            0.0926,
            -0.0692,
            -0.0769,
            0.0079,
            -0.007,
            0.1079,
            0.0399,
            -0.0504,
            0.2143
          ]
        },
        "position_0": {
          "grad": [
            0.0153,
            -0.2507,
            0.0999,
            -0.1099,
            -0.0911,
            0.0446,
            -0.3031,
            -0.0616,
            0.2456,
            0.1037,
            0.0241,
            -0.0894,
            -0.2429,
            0.0111,
            0.1106,
            -0.3728
          ],
          "after": [
            0.014,
            -0.1323,
            -0.2668,
            0.0713,
            0.0185,
            -0.0318,
            -0.1373,
            0.092,
            0.114,
            -0.0247,
            0.0083,
            0.0932,
            0.0105,
            0.0116,
            -0.1062,
            0.0532
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0022,
            0.0,
            -0.0009,
            -0.002,
            0.0019,
            -0.0043,
            -0.0009,
            -0.0011,
            -0.0006,
            0.0008,
            -0.0015,
            -0.0022,
            0.0012,
            0.0025,
            -0.0012
          ],
          "after": [
            -0.0333,
            0.0039,
            -0.1036,
            -0.0386,
            0.0257,
            0.1599,
            0.0632,
            -0.1455,
            0.02,
            -0.023,
            -0.1323,
            -0.0538,
            0.109,
            -0.0624,
            0.025,
            0.0924
          ]
        }
      }
    },
    {
      "step": 270,
      "word": "domonic",
      "loss": 2.7925,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0228,
            0.0182,
            0.1748,
            -0.0271,
            0.0197,
            -0.0392,
            -0.0986,
            -0.137,
            -0.0054,
            0.0659,
            0.1502,
            0.0029,
            -0.0872,
            -0.0165,
            -0.035,
            -0.1698
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0733,
            0.0796,
            -0.0517,
            0.1101,
            0.1174,
            -0.1342,
            0.2555,
            0.0172,
            -0.1989,
            0.0365,
            -0.0229,
            0.0459,
            0.1132,
            -0.0962,
            -0.253,
            0.2084
          ],
          "after": [
            0.0169,
            0.1247,
            -0.0457,
            -0.0571,
            0.0268,
            0.008,
            0.0934,
            0.0933,
            -0.0691,
            -0.0767,
            0.0076,
            -0.0067,
            0.108,
            0.0396,
            -0.0502,
            0.2147
          ]
        },
        "position_0": {
          "grad": [
            -0.0768,
            0.0312,
            -0.0092,
            -0.0571,
            0.1188,
            -0.0003,
            0.0173,
            -0.0357,
            0.0336,
            -0.0237,
            -0.0881,
            0.0237,
            -0.0072,
            0.0891,
            0.0894,
            0.1116
          ],
          "after": [
            0.0141,
            -0.1313,
            -0.2678,
            0.0716,
            0.0191,
            -0.0321,
            -0.1366,
            0.0925,
            0.1132,
            -0.0252,
            0.0088,
            0.0943,
            0.0107,
            0.0109,
            -0.1068,
            0.0538
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            0.0023,
            0.0017,
            0.0013,
            0.0016,
            0.0017,
            0.0031,
            -0.0018,
            -0.0021,
            -0.0016,
            -0.0005,
            -0.0018,
            0.0046,
            0.0023,
            -0.0041,
            0.0029
          ],
          "after": [
            -0.0328,
            0.004,
            -0.1037,
            -0.0387,
            0.0267,
            0.1588,
            0.0636,
            -0.1449,
            0.0203,
            -0.0224,
            -0.1321,
            -0.0535,
            0.1092,
            -0.0638,
            0.0242,
            0.0922
          ]
        }
      }
    },
    {
      "step": 271,
      "word": "devonne",
      "loss": 2.4111,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0228,
            0.0179,
            0.1751,
            -0.027,
            0.0198,
            -0.0387,
            -0.0988,
            -0.1363,
            -0.0057,
            0.0661,
            0.1501,
            0.0029,
            -0.0872,
            -0.016,
            -0.0347,
            -0.1698
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3809,
            -0.3101,
            0.119,
            -0.2456,
            -0.2001,
            0.3807,
            -0.793,
            -0.1341,
            0.3986,
            -0.0317,
            0.1354,
            -0.2023,
            -0.215,
            0.4,
            0.3469,
            -0.4228
          ],
          "after": [
            0.0164,
            0.1249,
            -0.0471,
            -0.0559,
            0.0269,
            0.0073,
            0.0942,
            0.0944,
            -0.0695,
            -0.0765,
            0.007,
            -0.0058,
            0.1084,
            0.0385,
            -0.0504,
            0.2157
          ]
        },
        "position_0": {
          "grad": [
            -0.0626,
            0.013,
            -0.0002,
            -0.0705,
            0.0863,
            -0.0112,
            -0.0116,
            -0.0303,
            0.0568,
            -0.0363,
            -0.1017,
            -0.0025,
            -0.025,
            0.0966,
            0.1027,
            0.0862
          ],
          "after": [
            0.0145,
            -0.1305,
            -0.2687,
            0.072,
            0.0194,
            -0.0322,
            -0.136,
            0.0932,
            0.1124,
            -0.0255,
            0.0094,
            0.0951,
            0.011,
            0.0101,
            -0.1076,
            0.0541
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0032,
            -0.0027,
            0.0016,
            -0.0021,
            0.0004,
            -0.003,
            -0.0064,
            -0.002,
            0.0015,
            0.001,
            0.0008,
            -0.0053,
            0.0013,
            0.0031,
            -0.0025,
            -0.0007
          ],
          "after": [
            -0.033,
            0.0047,
            -0.1041,
            -0.0384,
            0.0275,
            0.1584,
            0.0647,
            -0.144,
            0.0201,
            -0.0221,
            -0.1321,
            -0.0524,
            0.1092,
            -0.0656,
            0.024,
            0.0921
          ]
        }
      }
    },
    {
      "step": 272,
      "word": "seran",
      "loss": 2.023,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0072,
            0.0731,
            -0.2346,
            0.0163,
            -0.2352,
            -0.1226,
            0.0035,
            -0.0664,
            0.1018,
            0.0138,
            0.0583,
            -0.0552,
            -0.1099,
            -0.0352,
            0.1958,
            -0.0931
          ],
          "after": [
            -0.0229,
            0.0174,
            0.1759,
            -0.0269,
            0.0203,
            -0.0377,
            -0.099,
            -0.1355,
            -0.0062,
            0.0663,
            0.1498,
            0.0031,
            -0.0869,
            -0.0154,
            -0.035,
            -0.1696
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0226,
            0.4084,
            0.3055,
            0.1581,
            0.3358,
            -0.0973,
            -0.1566,
            -0.1102,
            0.239,
            0.0557,
            -0.0291,
            0.0216,
            0.0733,
            0.0837,
            -0.3747,
            0.0213
          ],
          "after": [
            0.016,
            0.1244,
            -0.0488,
            -0.0555,
            0.0265,
            0.0069,
            0.095,
            0.0958,
            -0.0702,
            -0.0764,
            0.0065,
            -0.0052,
            0.1086,
            0.0373,
            -0.0501,
            0.2165
          ]
        },
        "position_0": {
          "grad": [
            0.036,
            0.0411,
            0.0178,
            -0.0092,
            -0.2858,
            0.0666,
            -0.1314,
            0.068,
            0.0705,
            0.0479,
            -0.1138,
            -0.0593,
            0.0034,
            0.0482,
            0.2195,
            -0.1523
          ],
          "after": [
            0.0146,
            -0.1299,
            -0.2695,
            0.0724,
            0.0204,
            -0.0326,
            -0.1353,
            0.0934,
            0.1116,
            -0.0259,
            0.0103,
            0.0961,
            0.0112,
            0.0092,
            -0.1089,
            0.0546
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0009,
            -0.0007,
            -0.0002,
            -0.0011,
            0.001,
            -0.002,
            0.0004,
            0.0004,
            -0.0016,
            -0.0014,
            -0.0003,
            -0.0007,
            0.0015,
            0.0016,
            0.0006
          ],
          "after": [
            -0.0333,
            0.0054,
            -0.1043,
            -0.0381,
            0.0283,
            0.1579,
            0.0657,
            -0.1433,
            0.0199,
            -0.0215,
            -0.1318,
            -0.0515,
            0.1093,
            -0.0676,
            0.0236,
            0.092
          ]
        }
      }
    },
    {
      "step": 273,
      "word": "matviy",
      "loss": 2.4499,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0171,
            -0.165,
            0.167,
            0.1861,
            0.0146,
            0.068,
            0.2005,
            0.0087,
            -0.2567,
            -0.1311,
            -0.1169,
            -0.3121,
            0.2731,
            0.0368,
            -0.0465,
            0.1678
          ],
          "after": [
            -0.0228,
            0.0175,
            0.1762,
            -0.0273,
            0.0208,
            -0.0372,
            -0.0996,
            -0.1348,
            -0.006,
            0.0669,
            0.1499,
            0.0038,
            -0.0873,
            -0.0151,
            -0.0352,
            -0.1698
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0756,
            0.0949,
            -0.097,
            0.1033,
            0.0579,
            -0.1346,
            0.2901,
            0.0193,
            -0.2396,
            0.0762,
            -0.0412,
            -0.0172,
            0.1832,
            -0.0915,
            -0.2433,
            0.1883
          ],
          "after": [
            0.0158,
            0.1237,
            -0.0501,
            -0.0554,
            0.0261,
            0.0068,
            0.0953,
            0.0969,
            -0.0704,
            -0.0766,
            0.0062,
            -0.0046,
            0.1086,
            0.0366,
            -0.0496,
            0.2169
          ]
        },
        "position_0": {
          "grad": [
            -0.0142,
            0.0422,
            0.0012,
            0.0424,
            0.1068,
            0.0466,
            0.1575,
            0.1299,
            0.0727,
            0.0034,
            -0.0345,
            0.0417,
            -0.1449,
            -0.0244,
            0.0467,
            0.0852
          ],
          "after": [
            0.0148,
            -0.1296,
            -0.2701,
            0.0727,
            0.021,
            -0.0331,
            -0.1349,
            0.0931,
            0.1108,
            -0.0263,
            0.0111,
            0.0968,
            0.0116,
            0.0085,
            -0.11,
            0.0549
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            0.0078,
            0.0036,
            0.0044,
            0.0032,
            0.0002,
            0.0056,
            -0.0054,
            0.0021,
            0.0032,
            0.0038,
            -0.0034,
            0.0069,
            0.0017,
            -0.0089,
            -0.0025
          ],
          "after": [
            -0.0339,
            0.0046,
            -0.1052,
            -0.0387,
            0.0285,
            0.1574,
            0.0661,
            -0.1417,
            0.0193,
            -0.0216,
            -0.1324,
            -0.0501,
            0.1084,
            -0.0696,
            0.0245,
            0.0923
          ]
        }
      }
    },
    {
      "step": 274,
      "word": "pearse",
      "loss": 2.8237,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0028,
            -0.0463,
            0.0292,
            -0.0583,
            0.1518,
            0.1515,
            0.0622,
            -0.1255,
            0.129,
            -0.0314,
            0.1137,
            0.1402,
            -0.1829,
            -0.1658,
            -0.0209,
            0.1687
          ],
          "after": [
            -0.0228,
            0.0177,
            0.1764,
            -0.0275,
            0.0208,
            -0.0374,
            -0.1002,
            -0.1338,
            -0.0061,
            0.0676,
            0.1497,
            0.0041,
            -0.0872,
            -0.0143,
            -0.0353,
            -0.1703
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0735,
            0.0575,
            0.2672,
            -0.0115,
            -0.2315,
            0.4425,
            -0.6597,
            -0.1367,
            0.4959,
            0.1091,
            0.0607,
            -0.2435,
            0.0865,
            0.4113,
            0.2544,
            -0.2174
          ],
          "after": [
            0.0155,
            0.1231,
            -0.0517,
            -0.0553,
            0.0261,
            0.006,
            0.0963,
            0.0984,
            -0.0713,
            -0.077,
            0.0058,
            -0.0035,
            0.1084,
            0.0351,
            -0.0494,
            0.2175
          ]
        },
        "position_0": {
          "grad": [
            0.0428,
            0.183,
            -0.0987,
            0.0681,
            0.0,
            -0.063,
            0.2907,
            0.0564,
            -0.2785,
            0.0059,
            -0.1574,
            0.0311,
            0.0002,
            0.0913,
            -0.0366,
            0.2519
          ],
          "after": [
            0.0147,
            -0.1298,
            -0.2703,
            0.0728,
            0.0215,
            -0.0333,
            -0.1351,
            0.0926,
            0.1106,
            -0.0267,
            0.0122,
            0.0972,
            0.012,
            0.0076,
            -0.1109,
            0.0548
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.003,
            0.0012,
            0.0015,
            -0.0015,
            0.0022,
            -0.001,
            -0.001,
            0.0014,
            0.0024,
            -0.0002,
            -0.0003,
            0.0015,
            0.0008,
            0.0003,
            0.0008
          ],
          "after": [
            -0.034,
            0.0035,
            -0.1063,
            -0.0395,
            0.0289,
            0.1566,
            0.0665,
            -0.1401,
            0.0184,
            -0.0223,
            -0.1329,
            -0.0489,
            0.1074,
            -0.0716,
            0.0252,
            0.0924
          ]
        }
      }
    },
    {
      "step": 275,
      "word": "braeley",
      "loss": 2.0241,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.033,
            -0.1913,
            0.0698,
            -0.0948,
            -0.0136,
            -0.0435,
            -0.1636,
            0.0031,
            0.1187,
            0.0355,
            0.0475,
            -0.061,
            -0.1596,
            -0.0461,
            0.0925,
            -0.2894
          ],
          "after": [
            -0.0228,
            0.0184,
            0.1763,
            -0.0274,
            0.0209,
            -0.0373,
            -0.1004,
            -0.133,
            -0.0065,
            0.068,
            0.1494,
            0.0045,
            -0.0868,
            -0.0136,
            -0.0356,
            -0.1701
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.045,
            -0.2908,
            -0.1857,
            -0.2606,
            -0.3196,
            0.2988,
            -0.3517,
            0.1972,
            0.314,
            -0.216,
            -0.2809,
            -0.1083,
            0.0133,
            0.198,
            0.7144,
            -0.262
          ],
          "after": [
            0.0153,
            0.123,
            -0.0528,
            -0.0544,
            0.0266,
            0.0048,
            0.0975,
            0.0988,
            -0.0723,
            -0.0768,
            0.0064,
            -0.0022,
            0.1082,
            0.0335,
            -0.0501,
            0.2185
          ]
        },
        "position_0": {
          "grad": [
            0.0189,
            0.0232,
            0.052,
            -0.0525,
            0.0787,
            0.0073,
            0.083,
            -0.0182,
            -0.1035,
            -0.0108,
            0.0308,
            0.0861,
            0.0943,
            -0.0979,
            -0.0781,
            0.2324
          ],
          "after": [
            0.0146,
            -0.13,
            -0.2707,
            0.0729,
            0.0217,
            -0.0335,
            -0.1353,
            0.0923,
            0.1107,
            -0.027,
            0.0131,
            0.0974,
            0.0121,
            0.0072,
            -0.1115,
            0.0543
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.004,
            -0.0028,
            -0.0017,
            -0.0011,
            -0.0027,
            0.0012,
            -0.0021,
            0.0027,
            -0.0005,
            -0.0058,
            -0.006,
            0.0017,
            -0.0005,
            0.002,
            0.0079,
            0.0022
          ],
          "after": [
            -0.0333,
            0.003,
            -0.1068,
            -0.04,
            0.0297,
            0.1557,
            0.067,
            -0.1393,
            0.0177,
            -0.0217,
            -0.132,
            -0.0482,
            0.1066,
            -0.0737,
            0.0247,
            0.0922
          ]
        }
      }
    },
    {
      "step": 276,
      "word": "aivah",
      "loss": 2.4548,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2098,
            -0.4069,
            0.4269,
            -0.1199,
            0.2894,
            0.1511,
            -0.2933,
            0.1485,
            0.1334,
            0.0343,
            0.1954,
            -0.186,
            -0.0031,
            0.1384,
            -0.2326,
            -0.1101
          ],
          "after": [
            -0.0223,
            0.0202,
            0.1753,
            -0.0271,
            0.0203,
            -0.0379,
            -0.1,
            -0.1328,
            -0.0071,
            0.0682,
            0.1485,
            0.0053,
            -0.0865,
            -0.0133,
            -0.0352,
            -0.1698
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0204,
            0.0337,
            -0.036,
            0.071,
            0.1098,
            -0.1157,
            0.1973,
            -0.0018,
            -0.1482,
            0.0446,
            -0.0216,
            0.0097,
            0.0764,
            -0.0664,
            -0.2315,
            0.1269
          ],
          "after": [
            0.0152,
            0.1229,
            -0.0536,
            -0.0539,
            0.0269,
            0.004,
            0.0983,
            0.0992,
            -0.0731,
            -0.0767,
            0.0069,
            -0.0012,
            0.1079,
            0.0322,
            -0.0505,
            0.2191
          ]
        },
        "position_0": {
          "grad": [
            0.1016,
            -0.1302,
            0.1125,
            -0.2824,
            0.0219,
            0.0167,
            -0.3968,
            -0.1031,
            0.2939,
            0.0812,
            0.3245,
            0.0763,
            -0.3412,
            0.0158,
            0.0225,
            -0.3456
          ],
          "after": [
            0.0141,
            -0.1299,
            -0.2714,
            0.0737,
            0.0218,
            -0.0338,
            -0.1349,
            0.0924,
            0.1102,
            -0.0276,
            0.013,
            0.0973,
            0.0128,
            0.0068,
            -0.1121,
            0.0544
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            0.0008,
            0.0007,
            -0.0027,
            0.0011,
            -0.0012,
            -0.0011,
            -0.001,
            0.0009,
            0.003,
            0.0036,
            0.0004,
            -0.0017,
            -0.002,
            0.0008,
            -0.0032
          ],
          "after": [
            -0.0324,
            0.0024,
            -0.1074,
            -0.0398,
            0.0302,
            0.1552,
            0.0676,
            -0.1384,
            0.0169,
            -0.0217,
            -0.132,
            -0.0476,
            0.1062,
            -0.075,
            0.0242,
            0.0925
          ]
        }
      }
    },
    {
      "step": 277,
      "word": "zyair",
      "loss": 2.8633,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0487,
            -0.2728,
            0.0883,
            -0.1597,
            0.0724,
            0.0357,
            -0.2368,
            0.0457,
            0.074,
            -0.0348,
            0.1935,
            -0.1425,
            -0.2382,
            0.0189,
            0.0275,
            -0.1983
          ],
          "after": [
            -0.0218,
            0.0223,
            0.1742,
            -0.0265,
            0.0197,
            -0.0385,
            -0.0991,
            -0.1328,
            -0.0078,
            0.0686,
            0.1472,
            0.0062,
            -0.0858,
            -0.0131,
            -0.035,
            -0.1691
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.083,
            0.0925,
            -0.1169,
            0.0776,
            0.062,
            -0.1718,
            0.3264,
            0.0106,
            -0.2709,
            0.0617,
            -0.0526,
            -0.0354,
            0.1945,
            -0.1214,
            -0.28,
            0.1892
          ],
          "after": [
            0.0153,
            0.1227,
            -0.054,
            -0.0537,
            0.027,
            0.0036,
            0.0987,
            0.0995,
            -0.0734,
            -0.0769,
            0.0075,
            -0.0002,
            0.1073,
            0.0314,
            -0.0504,
            0.2193
          ]
        },
        "position_0": {
          "grad": [
            0.0108,
            0.0936,
            -0.0566,
            0.2118,
            0.0481,
            -0.0532,
            0.2665,
            -0.0013,
            0.0087,
            0.0523,
            -0.0474,
            0.052,
            0.0192,
            -0.0436,
            0.0155,
            0.1051
          ],
          "after": [
            0.0137,
            -0.13,
            -0.2719,
            0.0738,
            0.0218,
            -0.0337,
            -0.135,
            0.0925,
            0.1097,
            -0.0284,
            0.013,
            0.097,
            0.0133,
            0.0066,
            -0.1126,
            0.0543
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            0.0017,
            -0.0033,
            0.0014,
            -0.0012,
            0.002,
            0.0022,
            0.0032,
            -0.0008,
            -0.0065,
            -0.0037,
            0.0011,
            0.0003,
            -0.0013,
            0.0023,
            0.0021
          ],
          "after": [
            -0.0314,
            0.0017,
            -0.1072,
            -0.04,
            0.0308,
            0.1544,
            0.0679,
            -0.1383,
            0.0165,
            -0.0206,
            -0.1312,
            -0.0473,
            0.1058,
            -0.0758,
            0.0235,
            0.0924
          ]
        }
      }
    },
    {
      "step": 278,
      "word": "kamry",
      "loss": 2.1752,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0714,
            0.0426,
            0.1551,
            0.0854,
            0.1984,
            -0.1067,
            0.259,
            0.1713,
            0.1131,
            -0.0264,
            0.1106,
            0.0482,
            -0.1668,
            -0.1059,
            -0.0563,
            -0.006
          ],
          "after": [
            -0.0212,
            0.0241,
            0.1729,
            -0.0262,
            0.0188,
            -0.0386,
            -0.0989,
            -0.1333,
            -0.0087,
            0.0689,
            0.1458,
            0.0068,
            -0.0848,
            -0.0127,
            -0.0346,
            -0.1686
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0828,
            0.1688,
            -0.1405,
            0.1141,
            -0.0314,
            -0.1907,
            0.3834,
            0.0408,
            -0.3045,
            0.1054,
            -0.0577,
            -0.0306,
            0.2293,
            -0.1162,
            -0.2305,
            0.2771
          ],
          "after": [
            0.0156,
            0.1222,
            -0.0542,
            -0.0538,
            0.0271,
            0.0035,
            0.0986,
            0.0995,
            -0.0733,
            -0.0773,
            0.0082,
            0.0007,
            0.1065,
            0.0309,
            -0.0501,
            0.2191
          ]
        },
        "position_0": {
          "grad": [
            -0.0813,
            0.0537,
            -0.1457,
            -0.0192,
            0.0286,
            -0.0324,
            0.1283,
            -0.013,
            -0.0184,
            0.0256,
            0.0054,
            0.0042,
            -0.0287,
            -0.001,
            -0.0917,
            -0.1005
          ],
          "after": [
            0.0136,
            -0.1303,
            -0.2716,
            0.074,
            0.0217,
            -0.0336,
            -0.1353,
            0.0927,
            0.1094,
            -0.0292,
            0.013,
            0.0968,
            0.0138,
            0.0064,
            -0.1128,
            0.0544
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            0.0016,
            -0.0014,
            0.0013,
            0.0014,
            0.0003,
            0.0021,
            -0.0007,
            -0.0008,
            0.0029,
            0.0004,
            0.0006,
            0.0028,
            -0.0002,
            -0.0039,
            0.0008
          ],
          "after": [
            -0.0309,
            0.0007,
            -0.1068,
            -0.0404,
            0.0311,
            0.1536,
            0.0679,
            -0.138,
            0.0163,
            -0.0202,
            -0.1306,
            -0.0471,
            0.105,
            -0.0765,
            0.0234,
            0.0922
          ]
        }
      }
    },
    {
      "step": 279,
      "word": "karmah",
      "loss": 2.0927,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.186,
            -0.0384,
            0.2104,
            -0.0568,
            0.2175,
            0.094,
            0.1798,
            -0.1064,
            0.2109,
            0.003,
            0.0201,
            0.1156,
            0.015,
            -0.2,
            -0.0815,
            0.3392
          ],
          "after": [
            -0.0203,
            0.0257,
            0.1713,
            -0.0258,
            0.0176,
            -0.0391,
            -0.0991,
            -0.1335,
            -0.01,
            0.0692,
            0.1445,
            0.0072,
            -0.0841,
            -0.0117,
            -0.034,
            -0.1687
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1164,
            0.1707,
            -0.1207,
            0.0955,
            0.0365,
            -0.1813,
            0.3464,
            0.0542,
            -0.2596,
            0.0699,
            -0.0574,
            0.0118,
            0.1688,
            -0.1195,
            -0.2149,
            0.2719
          ],
          "after": [
            0.0161,
            0.1215,
            -0.054,
            -0.0542,
            0.0272,
            0.0038,
            0.0982,
            0.0994,
            -0.0729,
            -0.0778,
            0.009,
            0.0014,
            0.1055,
            0.0307,
            -0.0496,
            0.2186
          ]
        },
        "position_0": {
          "grad": [
            -0.0735,
            0.0622,
            -0.1384,
            -0.0239,
            0.044,
            -0.0308,
            0.1251,
            -0.0119,
            -0.0291,
            0.0373,
            0.0014,
            0.0346,
            -0.0278,
            -0.016,
            -0.1004,
            -0.0813
          ],
          "after": [
            0.0139,
            -0.1307,
            -0.2708,
            0.0742,
            0.0214,
            -0.0333,
            -0.1357,
            0.0928,
            0.1091,
            -0.03,
            0.013,
            0.0965,
            0.0143,
            0.0063,
            -0.1127,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0024,
            0.002,
            0.0014,
            0.0032,
            0.0005,
            0.0022,
            0.0022,
            -0.0014,
            0.0004,
            0.0011,
            0.0007,
            0.0027,
            -0.0,
            -0.0004,
            -0.0051,
            0.001
          ],
          "after": [
            -0.031,
            -0.0004,
            -0.1067,
            -0.0414,
            0.0313,
            0.1526,
            0.0677,
            -0.1375,
            0.016,
            -0.02,
            -0.1303,
            -0.0474,
            0.1044,
            -0.077,
            0.0239,
            0.0918
          ]
        }
      }
    },
    {
      "step": 280,
      "word": "saleen",
      "loss": 1.9992,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0826,
            0.0664,
            -0.1553,
            -0.188,
            -0.0951,
            -0.0108,
            -0.2799,
            0.121,
            0.0303,
            -0.0108,
            -0.0953,
            0.1251,
            -0.1501,
            0.0326,
            0.1828,
            -0.3417
          ],
          "after": [
            -0.0194,
            0.0268,
            0.1703,
            -0.0251,
            0.0168,
            -0.0395,
            -0.0987,
            -0.1339,
            -0.0111,
            0.0695,
            0.1437,
            0.0072,
            -0.0831,
            -0.011,
            -0.0341,
            -0.1682
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0474,
            -0.2529,
            -0.0414,
            -0.2229,
            -0.0494,
            0.4341,
            -0.4169,
            0.0547,
            0.3691,
            -0.3781,
            -0.3175,
            0.2325,
            -0.3712,
            0.2955,
            0.7552,
            -0.3488
          ],
          "after": [
            0.0167,
            0.1213,
            -0.0539,
            -0.0539,
            0.0273,
            0.0033,
            0.0983,
            0.099,
            -0.073,
            -0.0772,
            0.0106,
            0.0014,
            0.1053,
            0.03,
            -0.0501,
            0.2186
          ]
        },
        "position_0": {
          "grad": [
            0.0158,
            0.0487,
            -0.0063,
            0.0081,
            -0.1881,
            0.0467,
            -0.0935,
            0.0553,
            0.0329,
            0.0566,
            -0.0908,
            -0.0064,
            0.0267,
            0.0133,
            0.144,
            -0.1114
          ],
          "after": [
            0.014,
            -0.1312,
            -0.2701,
            0.0744,
            0.0218,
            -0.0333,
            -0.1359,
            0.0928,
            0.1089,
            -0.031,
            0.0133,
            0.0962,
            0.0146,
            0.0061,
            -0.1129,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0011,
            -0.0005,
            -0.0011,
            -0.0008,
            0.0008,
            -0.0021,
            -0.0007,
            0.0001,
            -0.0019,
            -0.0023,
            0.0005,
            -0.0017,
            0.0023,
            0.0024,
            -0.0014
          ],
          "after": [
            -0.0311,
            -0.0012,
            -0.1065,
            -0.0419,
            0.0315,
            0.1516,
            0.0677,
            -0.137,
            0.0158,
            -0.0195,
            -0.1295,
            -0.0477,
            0.1041,
            -0.0779,
            0.0241,
            0.0918
          ]
        }
      }
    },
    {
      "step": 281,
      "word": "pooja",
      "loss": 3.1113,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            0.314,
            0.0231,
            -0.1214,
            -0.1009,
            -0.0218,
            -0.0112,
            0.0157,
            0.0112,
            -0.0962,
            0.1478,
            0.1015,
            -0.3157,
            0.159,
            0.1236,
            -0.036,
            -0.2281
          ],
          "after": [
            -0.0193,
            0.0278,
            0.1698,
            -0.0243,
            0.0161,
            -0.0397,
            -0.0984,
            -0.1344,
            -0.0119,
            0.0693,
            0.1427,
            0.0078,
            -0.0826,
            -0.0108,
            -0.034,
            -0.1674
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.049,
            0.0563,
            -0.0932,
            0.0221,
            0.0244,
            -0.1534,
            0.2228,
            0.0527,
            -0.1782,
            0.043,
            -0.0191,
            0.0168,
            0.0942,
            -0.0998,
            -0.1456,
            0.1016
          ],
          "after": [
            0.0173,
            0.121,
            -0.0535,
            -0.0537,
            0.0274,
            0.0032,
            0.0981,
            0.0985,
            -0.0728,
            -0.0768,
            0.012,
            0.0014,
            0.1049,
            0.0296,
            -0.0503,
            0.2185
          ]
        },
        "position_0": {
          "grad": [
            0.0204,
            0.2705,
            -0.1212,
            0.1694,
            0.076,
            -0.0659,
            0.4539,
            0.0321,
            -0.4245,
            -0.0016,
            -0.1084,
            0.0829,
            0.1106,
            0.1066,
            -0.0743,
            0.4161
          ],
          "after": [
            0.014,
            -0.1323,
            -0.269,
            0.0742,
            0.0219,
            -0.033,
            -0.1367,
            0.0926,
            0.1094,
            -0.0318,
            0.0137,
            0.0958,
            0.0148,
            0.0057,
            -0.113,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0003,
            0.0016,
            0.0016,
            -0.0003,
            0.0014,
            -0.0013,
            -0.0011,
            -0.0001,
            0.0014,
            0.0018,
            0.0001,
            -0.0005,
            0.0006,
            -0.0024,
            -0.0008
          ],
          "after": [
            -0.0315,
            -0.0018,
            -0.1067,
            -0.0427,
            0.0318,
            0.1505,
            0.0678,
            -0.1363,
            0.0156,
            -0.0194,
            -0.1292,
            -0.048,
            0.1039,
            -0.0789,
            0.0246,
            0.0919
          ]
        }
      }
    },
    {
      "step": 282,
      "word": "damaria",
      "loss": 2.0396,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1657,
            0.089,
            0.2824,
            -0.0568,
            0.5236,
            0.1167,
            0.3487,
            -0.0206,
            0.1306,
            -0.04,
            0.0709,
            0.4141,
            -0.3317,
            -0.3865,
            -0.1829,
            0.2174
          ],
          "after": [
            -0.0188,
            0.0283,
            0.1686,
            -0.0234,
            0.0145,
            -0.0404,
            -0.0989,
            -0.1347,
            -0.0128,
            0.0692,
            0.1417,
            0.0076,
            -0.0816,
            -0.0096,
            -0.0335,
            -0.1671
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0511,
            0.1325,
            -0.0779,
            0.1074,
            0.0798,
            -0.1511,
            0.2734,
            0.0126,
            -0.2335,
            0.0517,
            -0.0353,
            -0.0049,
            0.1472,
            -0.085,
            -0.2804,
            0.176
          ],
          "after": [
            0.0179,
            0.1206,
            -0.0531,
            -0.0539,
            0.0274,
            0.0033,
            0.0977,
            0.098,
            -0.0724,
            -0.0766,
            0.0134,
            0.0014,
            0.1044,
            0.0294,
            -0.0501,
            0.2181
          ]
        },
        "position_0": {
          "grad": [
            -0.069,
            0.0797,
            -0.0608,
            -0.0971,
            0.1542,
            -0.0324,
            0.0728,
            -0.0379,
            -0.0208,
            0.0261,
            -0.1035,
            0.1164,
            -0.0078,
            0.0243,
            0.0198,
            0.1214
          ],
          "after": [
            0.0144,
            -0.1335,
            -0.2678,
            0.0742,
            0.0215,
            -0.0326,
            -0.1375,
            0.0926,
            0.1099,
            -0.0327,
            0.0144,
            0.095,
            0.0149,
            0.0052,
            -0.1131,
            0.0542
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0024,
            -0.0036,
            -0.0012,
            0.0018,
            -0.0055,
            -0.0029,
            -0.0055,
            0.0013,
            -0.0004,
            -0.0009,
            0.0003,
            -0.0007,
            -0.0019,
            0.0004,
            0.005,
            -0.0015
          ],
          "after": [
            -0.0324,
            -0.0017,
            -0.1066,
            -0.0438,
            0.0329,
            0.1501,
            0.0685,
            -0.1359,
            0.0155,
            -0.0191,
            -0.1291,
            -0.0481,
            0.1041,
            -0.0798,
            0.0243,
            0.0922
          ]
        }
      }
    },
    {
      "step": 283,
      "word": "jahmal",
      "loss": 2.3264,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2548,
            0.1048,
            0.148,
            0.1323,
            0.0354,
            0.1528,
            0.013,
            0.2761,
            0.0637,
            0.0001,
            -0.2571,
            0.2007,
            0.1381,
            0.0537,
            0.0772,
            0.1063
          ],
          "after": [
            -0.0179,
            0.0285,
            0.1673,
            -0.023,
            0.0131,
            -0.0415,
            -0.0993,
            -0.1359,
            -0.0138,
            0.0691,
            0.1416,
            0.007,
            -0.0809,
            -0.0087,
            -0.0332,
            -0.167
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1689,
            0.2318,
            -0.1592,
            0.1235,
            0.0897,
            -0.2284,
            0.4295,
            0.0519,
            -0.3168,
            0.0356,
            -0.0569,
            0.0633,
            0.184,
            -0.1467,
            -0.3153,
            0.3644
          ],
          "after": [
            0.0188,
            0.1198,
            -0.0524,
            -0.0544,
            0.0272,
            0.0037,
            0.0969,
            0.0974,
            -0.0717,
            -0.0766,
            0.0147,
            0.0012,
            0.1037,
            0.0295,
            -0.0497,
            0.2173
          ]
        },
        "position_0": {
          "grad": [
            -0.033,
            0.0936,
            -0.0404,
            0.1812,
            0.0967,
            -0.0679,
            0.2335,
            0.061,
            -0.2704,
            -0.0314,
            0.0129,
            0.1159,
            0.3341,
            -0.1495,
            -0.1944,
            0.2047
          ],
          "after": [
            0.0148,
            -0.1348,
            -0.2666,
            0.0739,
            0.0209,
            -0.0319,
            -0.1386,
            0.0923,
            0.1107,
            -0.0332,
            0.015,
            0.094,
            0.0145,
            0.0053,
            -0.1127,
            0.0535
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0026,
            0.0019,
            0.0002,
            0.0032,
            -0.0002,
            0.0004,
            0.0024,
            -0.001,
            0.0002,
            0.0011,
            0.0003,
            0.0003,
            0.0007,
            0.0014,
            -0.0049,
            0.0028
          ],
          "after": [
            -0.0336,
            -0.0019,
            -0.1066,
            -0.0452,
            0.0339,
            0.1497,
            0.0689,
            -0.1354,
            0.0154,
            -0.019,
            -0.129,
            -0.0483,
            0.1041,
            -0.0809,
            0.0247,
            0.092
          ]
        }
      }
    },
    {
      "step": 284,
      "word": "konstantin",
      "loss": 2.6396,
      "learning_rate": 0.0022,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1198,
            0.0325,
            -0.0386,
            0.0769,
            -0.0105,
            -0.0166,
            0.0146,
            -0.0396,
            0.0452,
            -0.1269,
            -0.1034,
            0.1281,
            -0.0682,
            -0.0462,
            0.0231,
            0.1513
          ],
          "after": [
            -0.017,
            0.0286,
            0.1663,
            -0.0228,
            0.012,
            -0.0425,
            -0.0996,
            -0.1367,
            -0.0147,
            0.0695,
            0.1418,
            0.0062,
            -0.0803,
            -0.0078,
            -0.033,
            -0.1672
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1102,
            0.076,
            -0.0196,
            0.0625,
            0.1027,
            -0.1261,
            0.2251,
            0.0171,
            -0.1028,
            0.0206,
            -0.0035,
            0.0499,
            0.0497,
            -0.1082,
            -0.159,
            0.1432
          ],
          "after": [
            0.0198,
            0.119,
            -0.0518,
            -0.055,
            0.0268,
            0.0043,
            0.0961,
            0.0968,
            -0.071,
            -0.0766,
            0.0158,
            0.001,
            0.103,
            0.0299,
            -0.0491,
            0.2164
          ]
        },
        "position_0": {
          "grad": [
            -0.0497,
            0.0242,
            -0.0713,
            0.01,
            0.0083,
            -0.0083,
            0.0738,
            -0.0037,
            -0.0116,
            -0.0107,
            0.0024,
            0.003,
            -0.0123,
            0.0117,
            -0.0425,
            -0.0398
          ],
          "after": [
            0.0153,
            -0.136,
            -0.2653,
            0.0735,
            0.0204,
            -0.0313,
            -0.1396,
            0.0921,
            0.1114,
            -0.0337,
            0.0154,
            0.0931,
            0.0141,
            0.0053,
            -0.1122,
            0.0529
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0042,
            -0.0046,
            -0.0015,
            -0.0028,
            -0.0031,
            0.0034,
            0.0039,
            -0.0017,
            0.0046,
            0.0001,
            0.0048,
            -0.0032,
            -0.0015,
            0.0037,
            0.0032
          ],
          "after": [
            -0.0348,
            -0.0014,
            -0.1056,
            -0.0462,
            0.0351,
            0.1499,
            0.0688,
            -0.1358,
            0.0157,
            -0.0198,
            -0.1289,
            -0.0492,
            0.1046,
            -0.0815,
            0.0246,
            0.0913
          ]
        }
      }
    },
    {
      "step": 285,
      "word": "quency",
      "loss": 3.2746,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0161,
            0.0286,
            0.1655,
            -0.0226,
            0.011,
            -0.0432,
            -0.0999,
            -0.1375,
            -0.0155,
            0.0698,
            0.1419,
            0.0056,
            -0.0797,
            -0.007,
            -0.0329,
            -0.1674
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2733,
            0.0744,
            0.037,
            0.2047,
            0.1145,
            0.3102,
            0.3351,
            -0.0047,
            -0.0503,
            -0.1751,
            -0.0177,
            0.2473,
            -0.2409,
            -0.2165,
            0.0837,
            0.1468
          ],
          "after": [
            0.0213,
            0.1182,
            -0.0514,
            -0.0561,
            0.0264,
            0.0043,
            0.095,
            0.0963,
            -0.0703,
            -0.0761,
            0.0168,
            0.0001,
            0.1028,
            0.0306,
            -0.0487,
            0.2154
          ]
        },
        "position_0": {
          "grad": [
            -0.0707,
            0.3038,
            -0.2286,
            0.4168,
            0.0886,
            -0.1519,
            0.3829,
            -0.0363,
            -0.462,
            0.0079,
            -0.0804,
            -0.252,
            0.2737,
            0.0941,
            -0.2465,
            0.4314
          ],
          "after": [
            0.016,
            -0.1378,
            -0.2633,
            0.0724,
            0.0197,
            -0.0302,
            -0.141,
            0.0921,
            0.1128,
            -0.0341,
            0.016,
            0.0932,
            0.0134,
            0.0051,
            -0.1112,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0038,
            -0.0012,
            -0.0038,
            0.0054,
            0.0007,
            0.0043,
            0.0086,
            0.0027,
            -0.0041,
            -0.0015,
            -0.0027,
            0.0018,
            -0.0006,
            -0.0046,
            0.0012,
            -0.0003
          ],
          "after": [
            -0.0349,
            -0.0008,
            -0.1041,
            -0.0479,
            0.0361,
            0.1493,
            0.0679,
            -0.1366,
            0.0169,
            -0.0202,
            -0.1283,
            -0.0502,
            0.1051,
            -0.0809,
            0.0244,
            0.0908
          ]
        }
      }
    },
    {
      "step": 286,
      "word": "keona",
      "loss": 2.1466,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3352,
            0.0208,
            -0.1107,
            -0.0925,
            -0.0287,
            -0.0147,
            0.0019,
            -0.0101,
            -0.0813,
            0.1663,
            0.111,
            -0.3359,
            0.1794,
            0.1361,
            -0.028,
            -0.2412
          ],
          "after": [
            -0.0161,
            0.0286,
            0.165,
            -0.0223,
            0.0102,
            -0.0439,
            -0.1002,
            -0.138,
            -0.0159,
            0.0695,
            0.1417,
            0.0057,
            -0.0796,
            -0.0068,
            -0.0327,
            -0.1671
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2617,
            -0.242,
            0.2128,
            -0.1221,
            0.3501,
            0.2467,
            -0.5345,
            -0.1277,
            0.5154,
            0.0602,
            0.2674,
            0.204,
            -0.3382,
            0.0843,
            -0.0969,
            -0.4123
          ],
          "after": [
            0.022,
            0.1179,
            -0.0514,
            -0.0567,
            0.0254,
            0.0039,
            0.0946,
            0.0964,
            -0.0703,
            -0.0759,
            0.0169,
            -0.0011,
            0.1031,
            0.0311,
            -0.0482,
            0.2151
          ]
        },
        "position_0": {
          "grad": [
            -0.0836,
            -0.0217,
            -0.1083,
            -0.0569,
            -0.0472,
            -0.0421,
            0.0059,
            -0.003,
            0.078,
            -0.025,
            -0.0065,
            -0.0855,
            -0.0714,
            0.04,
            -0.0514,
            -0.1684
          ],
          "after": [
            0.017,
            -0.1393,
            -0.2612,
            0.0716,
            0.0192,
            -0.0291,
            -0.1422,
            0.0921,
            0.1139,
            -0.0343,
            0.0165,
            0.0935,
            0.0129,
            0.0047,
            -0.1102,
            0.051
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0021,
            -0.0,
            0.0007,
            -0.0005,
            0.0017,
            0.0033,
            -0.0068,
            -0.0039,
            -0.0001,
            -0.0027,
            -0.0001,
            0.0017,
            -0.0045,
            0.0027,
            0.0,
            -0.0026
          ],
          "after": [
            -0.0355,
            -0.0003,
            -0.1029,
            -0.0493,
            0.0366,
            0.1483,
            0.0679,
            -0.1365,
            0.0179,
            -0.0201,
            -0.1278,
            -0.0514,
            0.1062,
            -0.081,
            0.0242,
            0.0907
          ]
        }
      }
    },
    {
      "step": 287,
      "word": "macee",
      "loss": 2.3375,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0253,
            -0.0061,
            0.0366,
            0.212,
            -0.2614,
            -0.0417,
            0.1401,
            0.1665,
            -0.128,
            0.0602,
            0.013,
            -0.0504,
            0.1008,
            0.0674,
            -0.1048,
            -0.0245
          ],
          "after": [
            -0.016,
            0.0286,
            0.1645,
            -0.0225,
            0.01,
            -0.0442,
            -0.1007,
            -0.1391,
            -0.016,
            0.069,
            0.1416,
            0.0059,
            -0.0796,
            -0.0067,
            -0.0322,
            -0.1668
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2141,
            -0.1742,
            0.0743,
            -0.2209,
            -0.1154,
            0.3524,
            -0.3833,
            -0.0931,
            0.3687,
            -0.5428,
            -0.2872,
            0.3114,
            -0.4585,
            0.2675,
            0.8363,
            -0.3137
          ],
          "after": [
            0.023,
            0.118,
            -0.0516,
            -0.0565,
            0.0248,
            0.003,
            0.0947,
            0.0969,
            -0.0708,
            -0.0744,
            0.0177,
            -0.0029,
            0.1041,
            0.0309,
            -0.0488,
            0.2154
          ]
        },
        "position_0": {
          "grad": [
            -0.0282,
            0.0025,
            0.0174,
            0.0097,
            0.0659,
            0.0363,
            0.0903,
            0.176,
            0.1502,
            -0.0279,
            -0.075,
            0.0184,
            -0.217,
            -0.0256,
            0.0944,
            0.0346
          ],
          "after": [
            0.018,
            -0.1405,
            -0.2595,
            0.0709,
            0.0186,
            -0.0282,
            -0.1433,
            0.0913,
            0.1145,
            -0.0344,
            0.0172,
            0.0937,
            0.0128,
            0.0045,
            -0.1096,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0011,
            0.0024,
            -0.005,
            -0.0019,
            -0.0034,
            -0.0035,
            -0.0049,
            0.0019,
            -0.0031,
            -0.0022,
            0.0018,
            -0.0039,
            0.0036,
            0.0038,
            -0.0056
          ],
          "after": [
            -0.0357,
            0.0004,
            -0.1024,
            -0.0496,
            0.0374,
            0.148,
            0.0681,
            -0.1356,
            0.0184,
            -0.0194,
            -0.1269,
            -0.0526,
            0.1076,
            -0.0819,
            0.0235,
            0.0916
          ]
        }
      }
    },
    {
      "step": 288,
      "word": "nohely",
      "loss": 2.6292,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0159,
            0.0286,
            0.1641,
            -0.0226,
            0.0099,
            -0.0445,
            -0.1011,
            -0.1399,
            -0.0161,
            0.0686,
            0.1414,
            0.006,
            -0.0797,
            -0.0066,
            -0.0318,
            -0.1666
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1492,
            -0.0738,
            0.1003,
            -0.14,
            0.0485,
            -0.0624,
            -0.1133,
            0.0137,
            0.0045,
            -0.1103,
            0.0516,
            0.0317,
            -0.0455,
            -0.1581,
            0.3142,
            -0.1571
          ],
          "after": [
            0.0243,
            0.1182,
            -0.0519,
            -0.056,
            0.0241,
            0.0024,
            0.0948,
            0.0972,
            -0.0712,
            -0.0728,
            0.0183,
            -0.0045,
            0.105,
            0.0311,
            -0.0496,
            0.2158
          ]
        },
        "position_0": {
          "grad": [
            -0.0066,
            -0.0011,
            0.0264,
            0.0142,
            -0.3884,
            0.0277,
            -0.1171,
            -0.0189,
            0.1947,
            -0.1636,
            -0.2204,
            -0.2342,
            -0.0652,
            0.2068,
            0.2766,
            -0.0517
          ],
          "after": [
            0.0188,
            -0.1416,
            -0.2581,
            0.0702,
            0.0192,
            -0.0277,
            -0.1441,
            0.0908,
            0.1147,
            -0.0336,
            0.0183,
            0.0946,
            0.0129,
            0.0036,
            -0.1098,
            0.0499
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0001,
            0.0005,
            -0.0006,
            0.0003,
            -0.0008,
            0.0004,
            -0.0,
            -0.0005,
            -0.0008,
            0.0003,
            -0.0001,
            0.0005,
            -0.001,
            0.0007,
            -0.0003
          ],
          "after": [
            -0.0357,
            0.0009,
            -0.1021,
            -0.0497,
            0.038,
            0.1479,
            0.0683,
            -0.1348,
            0.0189,
            -0.0187,
            -0.1262,
            -0.0537,
            0.1088,
            -0.0825,
            0.0229,
            0.0924
          ]
        }
      }
    },
    {
      "step": 289,
      "word": "slayde",
      "loss": 2.3597,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0882,
            -0.102,
            -0.0436,
            -0.0909,
            -0.0522,
            0.0854,
            -0.0368,
            -0.0086,
            -0.0213,
            0.094,
            -0.0569,
            0.0734,
            0.0223,
            0.1065,
            0.0285,
            0.0456
          ],
          "after": [
            -0.016,
            0.0289,
            0.1638,
            -0.0225,
            0.0098,
            -0.0451,
            -0.1014,
            -0.1407,
            -0.0161,
            0.0679,
            0.1414,
            0.006,
            -0.0798,
            -0.0069,
            -0.0316,
            -0.1665
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0328,
            -0.221,
            -0.1004,
            -0.1905,
            -0.4539,
            0.1588,
            -0.293,
            -0.0533,
            0.174,
            0.0888,
            0.0485,
            -0.1126,
            0.032,
            0.1405,
            0.4163,
            -0.1011
          ],
          "after": [
            0.0252,
            0.1188,
            -0.052,
            -0.055,
            0.0243,
            0.0016,
            0.0952,
            0.0977,
            -0.0717,
            -0.0717,
            0.0187,
            -0.0055,
            0.1057,
            0.0309,
            -0.0507,
            0.2163
          ]
        },
        "position_0": {
          "grad": [
            0.0183,
            0.0703,
            0.006,
            0.0523,
            -0.1496,
            0.075,
            -0.0518,
            0.0354,
            0.0002,
            0.0568,
            -0.0468,
            0.0313,
            0.0463,
            0.0225,
            0.1377,
            -0.0683
          ],
          "after": [
            0.0194,
            -0.1427,
            -0.257,
            0.0696,
            0.0201,
            -0.0275,
            -0.1448,
            0.0902,
            0.1149,
            -0.0333,
            0.0194,
            0.0952,
            0.0128,
            0.0028,
            -0.1103,
            0.0496
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0026,
            -0.0024,
            -0.0011,
            -0.0005,
            0.0013,
            0.0011,
            0.0012,
            0.0016,
            -0.0013,
            0.003,
            0.0002,
            0.0015,
            -0.001,
            -0.0029,
            0.0019,
            -0.0002
          ],
          "after": [
            -0.0352,
            0.0018,
            -0.1016,
            -0.0497,
            0.0383,
            0.1476,
            0.0684,
            -0.1344,
            0.0196,
            -0.0187,
            -0.1256,
            -0.0548,
            0.11,
            -0.0823,
            0.0221,
            0.0931
          ]
        }
      }
    },
    {
      "step": 290,
      "word": "fayre",
      "loss": 2.0802,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1836,
            -0.1977,
            -0.0696,
            -0.1424,
            -0.2153,
            0.0904,
            -0.1664,
            0.003,
            0.0032,
            0.0699,
            -0.0881,
            -0.0352,
            0.0839,
            0.2459,
            0.1143,
            -0.001
          ],
          "after": [
            -0.0165,
            0.0297,
            0.1638,
            -0.0221,
            0.0102,
            -0.046,
            -0.1013,
            -0.1413,
            -0.0161,
            0.0671,
            0.1417,
            0.0061,
            -0.08,
            -0.0077,
            -0.0317,
            -0.1663
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0615,
            -0.3354,
            0.1817,
            -0.0162,
            0.0575,
            0.1554,
            -0.5194,
            0.003,
            0.2088,
            -0.0951,
            0.0259,
            0.0124,
            -0.2277,
            0.2084,
            0.176,
            -0.1907
          ],
          "after": [
            0.0259,
            0.1198,
            -0.0524,
            -0.0542,
            0.0244,
            0.0007,
            0.0961,
            0.0981,
            -0.0725,
            -0.0705,
            0.0189,
            -0.0065,
            0.1066,
            0.0304,
            -0.0518,
            0.217
          ]
        },
        "position_0": {
          "grad": [
            0.0056,
            0.2191,
            -0.1292,
            0.173,
            0.1435,
            -0.2213,
            0.1775,
            -0.0851,
            -0.3509,
            -0.0531,
            0.0785,
            0.0898,
            0.2605,
            -0.0254,
            -0.1048,
            0.246
          ],
          "after": [
            0.0199,
            -0.1442,
            -0.2555,
            0.0687,
            0.0205,
            -0.0264,
            -0.1456,
            0.0901,
            0.1156,
            -0.0327,
            0.0201,
            0.0955,
            0.0124,
            0.0022,
            -0.1105,
            0.049
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0019,
            0.0,
            -0.0021,
            0.0021,
            0.002,
            -0.0032,
            -0.0,
            0.0028,
            -0.0004,
            -0.0001,
            0.001,
            -0.0026,
            0.0018,
            0.0008,
            0.0005
          ],
          "after": [
            -0.035,
            0.0028,
            -0.1012,
            -0.0494,
            0.0382,
            0.1471,
            0.0687,
            -0.134,
            0.0196,
            -0.0185,
            -0.1251,
            -0.0559,
            0.1113,
            -0.0825,
            0.0213,
            0.0937
          ]
        }
      }
    },
    {
      "step": 291,
      "word": "moustapha",
      "loss": 3.2169,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0832,
            -0.0039,
            0.03,
            0.2723,
            0.2467,
            -0.0479,
            0.3489,
            0.0261,
            -0.3879,
            -0.1503,
            -0.0772,
            0.1019,
            -0.028,
            -0.1598,
            -0.1683,
            0.464
          ],
          "after": [
            -0.0167,
            0.0304,
            0.1637,
            -0.0224,
            0.0101,
            -0.0465,
            -0.1019,
            -0.1419,
            -0.0153,
            0.0669,
            0.1422,
            0.0059,
            -0.0802,
            -0.008,
            -0.0313,
            -0.1671
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0549,
            0.0544,
            -0.0534,
            0.0502,
            0.0713,
            -0.118,
            0.1506,
            0.0215,
            -0.1241,
            0.0161,
            -0.0078,
            0.0216,
            0.0665,
            -0.074,
            -0.1521,
            0.1049
          ],
          "after": [
            0.0266,
            0.1206,
            -0.0527,
            -0.0536,
            0.0244,
            0.0001,
            0.0967,
            0.0984,
            -0.0729,
            -0.0695,
            0.0191,
            -0.0074,
            0.1074,
            0.0301,
            -0.0526,
            0.2175
          ]
        },
        "position_0": {
          "grad": [
            -0.0349,
            0.039,
            -0.0043,
            0.0326,
            0.1151,
            0.0533,
            0.1226,
            0.0883,
            0.0366,
            0.001,
            0.0032,
            0.086,
            -0.108,
            -0.0427,
            0.0235,
            0.0789
          ],
          "after": [
            0.0205,
            -0.1456,
            -0.2542,
            0.0678,
            0.0205,
            -0.0258,
            -0.1464,
            0.0896,
            0.1162,
            -0.0322,
            0.0207,
            0.0955,
            0.0122,
            0.0018,
            -0.1107,
            0.0483
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0012,
            0.0028,
            -0.0063,
            -0.0007,
            -0.0081,
            0.002,
            -0.0001,
            -0.0039,
            0.0045,
            0.0059,
            0.0044,
            0.0009,
            -0.0016,
            -0.0033,
            -0.005
          ],
          "after": [
            -0.0349,
            0.0035,
            -0.1013,
            -0.0481,
            0.0383,
            0.1478,
            0.0688,
            -0.1337,
            0.0205,
            -0.0192,
            -0.1259,
            -0.0576,
            0.1123,
            -0.0824,
            0.0211,
            0.0949
          ]
        }
      }
    },
    {
      "step": 292,
      "word": "janneth",
      "loss": 2.3665,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.056,
            0.0548,
            -0.2329,
            0.0388,
            -0.3857,
            -0.0349,
            -0.1271,
            -0.0629,
            0.1204,
            -0.1203,
            -0.1291,
            -0.211,
            0.0974,
            0.2437,
            0.1954,
            0.0261
          ],
          "after": [
            -0.017,
            0.0308,
            0.1641,
            -0.0227,
            0.0107,
            -0.0468,
            -0.1022,
            -0.1422,
            -0.0148,
            0.0672,
            0.1429,
            0.0062,
            -0.0805,
            -0.0089,
            -0.0315,
            -0.1678
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0761,
            -0.0305,
            0.0428,
            0.0536,
            -0.1143,
            0.0732,
            -0.2143,
            -0.0751,
            -0.0683,
            -0.1193,
            -0.0602,
            -0.1353,
            0.0768,
            0.2044,
            -0.0631,
            -0.0173
          ],
          "after": [
            0.027,
            0.1214,
            -0.053,
            -0.0532,
            0.0245,
            -0.0005,
            0.0974,
            0.0989,
            -0.0733,
            -0.0684,
            0.0195,
            -0.0078,
            0.1078,
            0.0295,
            -0.0533,
            0.2179
          ]
        },
        "position_0": {
          "grad": [
            -0.0172,
            0.0103,
            -0.0145,
            0.1235,
            0.0071,
            -0.0541,
            0.122,
            0.063,
            -0.1663,
            -0.0582,
            -0.0029,
            0.0346,
            0.2261,
            -0.1062,
            -0.0876,
            0.1135
          ],
          "after": [
            0.0211,
            -0.1469,
            -0.2531,
            0.0669,
            0.0205,
            -0.025,
            -0.1473,
            0.0889,
            0.117,
            -0.0316,
            0.0212,
            0.0953,
            0.0116,
            0.0019,
            -0.1106,
            0.0475
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0015,
            0.002,
            0.0026,
            0.002,
            0.0026,
            -0.001,
            0.0004,
            0.0008,
            -0.0021,
            0.0001,
            0.0002,
            0.0003,
            -0.0004,
            -0.0028,
            0.0013
          ],
          "after": [
            -0.0348,
            0.0038,
            -0.1019,
            -0.0474,
            0.038,
            0.1481,
            0.069,
            -0.1335,
            0.021,
            -0.0194,
            -0.1266,
            -0.059,
            0.1132,
            -0.0822,
            0.0212,
            0.0957
          ]
        }
      }
    },
    {
      "step": 293,
      "word": "kadence",
      "loss": 2.0339,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1315,
            0.0844,
            0.0426,
            -0.0648,
            0.1943,
            -0.0696,
            0.0722,
            -0.0688,
            -0.0203,
            -0.0223,
            -0.0802,
            0.0907,
            0.0449,
            0.0463,
            -0.0086,
            0.1411
          ],
          "after": [
            -0.017,
            0.031,
            0.1644,
            -0.0228,
            0.0109,
            -0.0468,
            -0.1025,
            -0.1423,
            -0.0143,
            0.0675,
            0.1438,
            0.0063,
            -0.0809,
            -0.0097,
            -0.0317,
            -0.1686
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0392,
            -0.4931,
            0.1171,
            -0.2499,
            -0.2695,
            0.1726,
            -0.5623,
            -0.0707,
            0.2995,
            0.0088,
            0.0625,
            -0.1252,
            -0.2529,
            0.1214,
            0.5669,
            -0.4778
          ],
          "after": [
            0.0273,
            0.1228,
            -0.0535,
            -0.0522,
            0.0251,
            -0.0013,
            0.0985,
            0.0997,
            -0.0739,
            -0.0675,
            0.0196,
            -0.0078,
            0.1086,
            0.0287,
            -0.0544,
            0.2189
          ]
        },
        "position_0": {
          "grad": [
            -0.0601,
            0.0114,
            -0.0974,
            -0.0461,
            0.0414,
            -0.0167,
            0.0514,
            -0.0272,
            0.0238,
            0.0274,
            0.0339,
            0.0244,
            -0.0572,
            -0.021,
            -0.072,
            -0.105
          ],
          "after": [
            0.0218,
            -0.1479,
            -0.2517,
            0.0661,
            0.0204,
            -0.0242,
            -0.1482,
            0.0884,
            0.1176,
            -0.0311,
            0.0215,
            0.0952,
            0.0113,
            0.002,
            -0.1104,
            0.0471
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0021,
            -0.0043,
            0.001,
            -0.004,
            0.0023,
            -0.0033,
            -0.0019,
            -0.0013,
            0.0014,
            0.0007,
            0.0013,
            0.0016,
            -0.0023,
            -0.0016,
            0.0,
            -0.0041
          ],
          "after": [
            -0.0343,
            0.0048,
            -0.1025,
            -0.0462,
            0.0375,
            0.1488,
            0.0693,
            -0.1331,
            0.0212,
            -0.0197,
            -0.1274,
            -0.0604,
            0.1142,
            -0.0817,
            0.0213,
            0.0971
          ]
        }
      }
    },
    {
      "step": 294,
      "word": "mylie",
      "loss": 2.5204,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.017,
            0.0311,
            0.1646,
            -0.0229,
            0.011,
            -0.0468,
            -0.1029,
            -0.1423,
            -0.0139,
            0.0678,
            0.1446,
            0.0063,
            -0.0812,
            -0.0105,
            -0.0318,
            -0.1693
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0451,
            -0.064,
            -0.0526,
            0.2762,
            -0.4785,
            -0.0797,
            -0.1767,
            0.0764,
            -0.0648,
            -0.1524,
            -0.1038,
            0.0678,
            0.1362,
            0.1128,
            0.2192,
            0.1262
          ],
          "after": [
            0.0275,
            0.1242,
            -0.0538,
            -0.0521,
            0.0263,
            -0.0018,
            0.0996,
            0.1,
            -0.0743,
            -0.0663,
            0.02,
            -0.008,
            0.1091,
            0.0278,
            -0.0556,
            0.2196
          ]
        },
        "position_0": {
          "grad": [
            -0.0259,
            -0.0341,
            0.0547,
            0.0383,
            0.0251,
            0.0697,
            0.078,
            0.1926,
            0.1896,
            -0.0621,
            -0.0563,
            -0.0251,
            -0.2422,
            0.0035,
            0.1462,
            0.0081
          ],
          "after": [
            0.0226,
            -0.1488,
            -0.2508,
            0.0654,
            0.0202,
            -0.0238,
            -0.149,
            0.0873,
            0.1178,
            -0.0304,
            0.022,
            0.0951,
            0.0113,
            0.002,
            -0.1106,
            0.0466
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0029,
            0.0027,
            0.006,
            -0.0024,
            -0.0046,
            -0.0026,
            0.0033,
            -0.0042,
            -0.0023,
            -0.0023,
            -0.0006,
            0.005,
            -0.0019,
            -0.001,
            0.0029
          ],
          "after": [
            -0.0338,
            0.0061,
            -0.1036,
            -0.0462,
            0.0373,
            0.1501,
            0.0699,
            -0.1333,
            0.0222,
            -0.0195,
            -0.1276,
            -0.0616,
            0.1143,
            -0.0809,
            0.0215,
            0.0977
          ]
        }
      }
    },
    {
      "step": 295,
      "word": "laniakea",
      "loss": 2.7319,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3342,
            0.1805,
            -0.0133,
            -0.0533,
            -0.3635,
            0.0255,
            0.0215,
            -0.042,
            0.177,
            -0.0563,
            -0.1691,
            -0.017,
            0.0135,
            0.0583,
            0.0424,
            0.0935
          ],
          "after": [
            -0.0163,
            0.0307,
            0.1649,
            -0.0229,
            0.0118,
            -0.0469,
            -0.1032,
            -0.1422,
            -0.014,
            0.0682,
            0.1457,
            0.0064,
            -0.0815,
            -0.0113,
            -0.0321,
            -0.17
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3219,
            -0.2173,
            -0.2235,
            -0.0166,
            -0.3578,
            0.2646,
            -0.2204,
            -0.0516,
            -0.0246,
            0.0617,
            0.073,
            -0.3271,
            0.1869,
            0.1866,
            0.1278,
            -0.2468
          ],
          "after": [
            0.0268,
            0.1257,
            -0.0536,
            -0.052,
            0.0278,
            -0.0027,
            0.1008,
            0.1005,
            -0.0747,
            -0.0654,
            0.0201,
            -0.0074,
            0.1092,
            0.0266,
            -0.0567,
            0.2205
          ]
        },
        "position_0": {
          "grad": [
            -0.0379,
            0.0471,
            -0.0419,
            -0.1411,
            -0.0296,
            0.0591,
            -0.1654,
            0.0961,
            0.0585,
            -0.0055,
            -0.1429,
            0.1048,
            -0.1814,
            0.0345,
            0.1407,
            -0.2274
          ],
          "after": [
            0.0234,
            -0.1496,
            -0.2498,
            0.0651,
            0.0201,
            -0.0238,
            -0.1495,
            0.0859,
            0.1178,
            -0.0298,
            0.0227,
            0.0947,
            0.0117,
            0.002,
            -0.1111,
            0.0466
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0049,
            -0.0047,
            -0.006,
            0.0026,
            -0.0086,
            0.0038,
            -0.0103,
            0.0028,
            0.0013,
            -0.0017,
            -0.0033,
            -0.0003,
            -0.0032,
            0.0046,
            0.0069,
            -0.002
          ],
          "after": [
            -0.0343,
            0.0079,
            -0.1034,
            -0.0465,
            0.0385,
            0.1506,
            0.0713,
            -0.134,
            0.0228,
            -0.0191,
            -0.1272,
            -0.0625,
            0.1149,
            -0.0811,
            0.0209,
            0.0986
          ]
        }
      }
    },
    {
      "step": 296,
      "word": "arman",
      "loss": 1.9958,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0396,
            0.0753,
            -0.4234,
            0.0191,
            -0.3874,
            0.0858,
            0.2013,
            -0.1838,
            0.1221,
            -0.1238,
            0.0871,
            -0.1184,
            -0.2215,
            -0.2224,
            0.451,
            0.1082
          ],
          "after": [
            -0.0159,
            0.0301,
            0.166,
            -0.0229,
            0.0132,
            -0.0474,
            -0.1038,
            -0.1415,
            -0.0144,
            0.069,
            0.1464,
            0.0067,
            -0.0813,
            -0.0113,
            -0.0335,
            -0.1709
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0336,
            0.0891,
            -0.1355,
            0.0628,
            -0.0135,
            -0.1607,
            0.2486,
            0.0123,
            -0.2166,
            0.082,
            -0.0509,
            -0.0447,
            0.2016,
            -0.0545,
            -0.2225,
            0.1575
          ],
          "after": [
            0.0264,
            0.1269,
            -0.0532,
            -0.0521,
            0.0292,
            -0.0032,
            0.1015,
            0.1009,
            -0.0747,
            -0.0649,
            0.0204,
            -0.0067,
            0.109,
            0.0258,
            -0.0575,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            0.0953,
            -0.1066,
            0.2021,
            -0.2742,
            0.0017,
            0.1199,
            -0.4389,
            -0.0469,
            0.2751,
            0.044,
            0.2554,
            0.1032,
            -0.3829,
            0.0443,
            0.0135,
            -0.398
          ],
          "after": [
            0.0236,
            -0.15,
            -0.2498,
            0.0654,
            0.0201,
            -0.0242,
            -0.1492,
            0.085,
            0.1174,
            -0.0295,
            0.0227,
            0.0941,
            0.0126,
            0.0018,
            -0.1115,
            0.0473
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0009,
            -0.0008,
            0.0002,
            -0.0018,
            -0.003,
            0.0051,
            0.0,
            0.0009,
            -0.0022,
            -0.0037,
            -0.0012,
            0.0041,
            -0.0006,
            -0.0001,
            0.0015
          ],
          "after": [
            -0.0349,
            0.0093,
            -0.103,
            -0.0468,
            0.0397,
            0.1515,
            0.072,
            -0.1347,
            0.0231,
            -0.0184,
            -0.1262,
            -0.0631,
            0.1148,
            -0.0812,
            0.0204,
            0.0991
          ]
        }
      }
    },
    {
      "step": 297,
      "word": "anabia",
      "loss": 2.3344,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.022,
            0.3306,
            0.0977,
            -0.1611,
            -0.2983,
            -0.0614,
            0.1362,
            -0.0498,
            -0.1144,
            -0.1669,
            -0.1817,
            -0.1402,
            0.3187,
            -0.0301,
            0.1164,
            0.2534
          ],
          "after": [
            -0.0154,
            0.0288,
            0.1668,
            -0.0226,
            0.0148,
            -0.0475,
            -0.1046,
            -0.1408,
            -0.0144,
            0.0703,
            0.1475,
            0.0072,
            -0.0818,
            -0.0113,
            -0.035,
            -0.1721
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0539,
            0.0567,
            -0.0074,
            0.0163,
            0.076,
            -0.1611,
            0.1599,
            -0.0034,
            -0.1333,
            0.0761,
            0.0107,
            0.0103,
            0.1008,
            -0.0526,
            -0.1698,
            0.083
          ],
          "after": [
            0.0261,
            0.1277,
            -0.0529,
            -0.0522,
            0.0302,
            -0.0033,
            0.102,
            0.1012,
            -0.0746,
            -0.0647,
            0.0206,
            -0.0062,
            0.1087,
            0.0251,
            -0.0579,
            0.2214
          ]
        },
        "position_0": {
          "grad": [
            0.0787,
            -0.1039,
            0.1818,
            -0.251,
            0.0032,
            0.0853,
            -0.4163,
            -0.0357,
            0.2624,
            0.0341,
            0.2128,
            0.0942,
            -0.3461,
            0.0384,
            0.0017,
            -0.3557
          ],
          "after": [
            0.0235,
            -0.1501,
            -0.2505,
            0.0662,
            0.02,
            -0.025,
            -0.1484,
            0.0843,
            0.1167,
            -0.0294,
            0.0221,
            0.0933,
            0.0138,
            0.0015,
            -0.1119,
            0.0483
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            0.0,
            0.0004,
            0.0013,
            -0.0038,
            0.0005,
            -0.0032,
            -0.0006,
            0.0003,
            0.0011,
            0.0016,
            0.0003,
            -0.0017,
            0.0006,
            0.001,
            -0.0012
          ],
          "after": [
            -0.0355,
            0.0105,
            -0.1028,
            -0.0473,
            0.0412,
            0.1522,
            0.0729,
            -0.1351,
            0.0233,
            -0.0179,
            -0.1256,
            -0.0637,
            0.115,
            -0.0814,
            0.0198,
            0.0998
          ]
        }
      }
    },
    {
      "step": 298,
      "word": "lowen",
      "loss": 2.5396,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.015,
            0.0276,
            0.1674,
            -0.0223,
            0.0163,
            -0.0476,
            -0.1053,
            -0.1402,
            -0.0145,
            0.0714,
            0.1485,
            0.0077,
            -0.0821,
            -0.0113,
            -0.0363,
            -0.1731
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1173,
            -0.1132,
            -0.1019,
            -0.2244,
            -0.0506,
            0.1259,
            -0.3098,
            0.0226,
            0.1309,
            -0.1897,
            -0.0635,
            0.0383,
            0.0318,
            -0.0537,
            0.416,
            -0.1375
          ],
          "after": [
            0.0262,
            0.1287,
            -0.0524,
            -0.0517,
            0.0312,
            -0.0037,
            0.1028,
            0.1014,
            -0.0747,
            -0.064,
            0.021,
            -0.0059,
            0.1084,
            0.0247,
            -0.0588,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            -0.0888,
            0.1177,
            -0.0746,
            -0.1172,
            -0.0149,
            0.1254,
            -0.1483,
            0.1134,
            0.0155,
            -0.0569,
            -0.1328,
            0.1193,
            -0.1331,
            0.1075,
            0.2114,
            -0.1861
          ],
          "after": [
            0.0238,
            -0.1505,
            -0.2508,
            0.0671,
            0.02,
            -0.0261,
            -0.1475,
            0.0833,
            0.116,
            -0.0291,
            0.022,
            0.0922,
            0.0151,
            0.0009,
            -0.1128,
            0.0495
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0062,
            0.0063,
            -0.0075,
            -0.0034,
            -0.0074,
            0.0049,
            0.0031,
            -0.0018,
            -0.0035,
            0.0044,
            -0.007,
            0.0162,
            -0.0024,
            -0.0099,
            -0.007
          ],
          "after": [
            -0.0362,
            0.0124,
            -0.1037,
            -0.0466,
            0.043,
            0.1537,
            0.0732,
            -0.136,
            0.0239,
            -0.017,
            -0.1259,
            -0.0631,
            0.1133,
            -0.0811,
            0.0204,
            0.1013
          ]
        }
      }
    },
    {
      "step": 299,
      "word": "josalee",
      "loss": 2.3427,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1348,
            0.1352,
            -0.0532,
            -0.1015,
            0.1182,
            0.0258,
            -0.1321,
            0.1514,
            0.0024,
            -0.071,
            -0.1353,
            0.2685,
            -0.1342,
            -0.0423,
            0.0671,
            -0.1805
          ],
          "after": [
            -0.0144,
            0.0263,
            0.1681,
            -0.0219,
            0.0173,
            -0.0478,
            -0.1057,
            -0.1401,
            -0.0145,
            0.0725,
            0.1497,
            0.0075,
            -0.0822,
            -0.0112,
            -0.0375,
            -0.1736
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3911,
            -0.4017,
            -0.3337,
            -0.2149,
            -0.7241,
            0.5425,
            -0.4547,
            0.0504,
            0.2082,
            0.0814,
            -0.159,
            -0.3587,
            0.0789,
            0.5292,
            0.7484,
            -0.512
          ],
          "after": [
            0.0254,
            0.1301,
            -0.0513,
            -0.0507,
            0.033,
            -0.0048,
            0.1038,
            0.1013,
            -0.075,
            -0.0636,
            0.0217,
            -0.0047,
            0.108,
            0.0234,
            -0.0602,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            -0.0174,
            0.0019,
            -0.007,
            0.0984,
            0.0788,
            -0.0568,
            0.121,
            0.0262,
            -0.1777,
            -0.0282,
            0.0563,
            0.0808,
            0.2472,
            -0.1467,
            -0.142,
            0.1142
          ],
          "after": [
            0.0241,
            -0.1508,
            -0.251,
            0.0676,
            0.0198,
            -0.0268,
            -0.1469,
            0.0823,
            0.1157,
            -0.0286,
            0.0217,
            0.0911,
            0.0158,
            0.0009,
            -0.1132,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0029,
            -0.0012,
            -0.0025,
            -0.0,
            -0.0053,
            0.0012,
            -0.0017,
            0.0016,
            -0.0024,
            -0.0048,
            -0.0087,
            -0.0067,
            0.004,
            0.0037,
            0.0094,
            -0.0034
          ],
          "after": [
            -0.0362,
            0.0142,
            -0.104,
            -0.046,
            0.0452,
            0.1548,
            0.0736,
            -0.137,
            0.0249,
            -0.0154,
            -0.1247,
            -0.0616,
            0.1114,
            -0.0815,
            0.0199,
            0.1031
          ]
        }
      }
    },
    {
      "step": 300,
      "word": "dmani",
      "loss": 2.3386,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1319,
            0.052,
            -0.2434,
            0.137,
            -0.3927,
            -0.0032,
            -0.0864,
            -0.096,
            0.1118,
            0.0147,
            -0.1012,
            -0.1332,
            0.0501,
            0.2282,
            0.0693,
            0.0303
          ],
          "after": [
            -0.0142,
            0.025,
            0.1692,
            -0.0218,
            0.0188,
            -0.0479,
            -0.1058,
            -0.1398,
            -0.0148,
            0.0735,
            0.151,
            0.0077,
            -0.0824,
            -0.0117,
            -0.0388,
            -0.1741
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0819,
            0.1185,
            -0.2033,
            0.0637,
            0.0706,
            -0.1848,
            0.3534,
            0.0665,
            -0.3171,
            0.0466,
            -0.0967,
            -0.0855,
            0.2445,
            -0.1104,
            -0.2503,
            0.2248
          ],
          "after": [
            0.0249,
            0.1312,
            -0.05,
            -0.05,
            0.0344,
            -0.0054,
            0.1043,
            0.101,
            -0.0748,
            -0.0634,
            0.0227,
            -0.0036,
            0.1073,
            0.0224,
            -0.0612,
            0.2237
          ]
        },
        "position_0": {
          "grad": [
            -0.0674,
            0.0306,
            -0.025,
            -0.0973,
            0.1495,
            -0.0155,
            0.0264,
            -0.0673,
            0.0536,
            -0.0137,
            -0.103,
            0.0372,
            0.0002,
            0.0857,
            0.1014,
            0.1395
          ],
          "after": [
            0.0247,
            -0.1512,
            -0.2511,
            0.0683,
            0.0192,
            -0.0274,
            -0.1464,
            0.0817,
            0.1154,
            -0.0282,
            0.0218,
            0.09,
            0.0164,
            0.0006,
            -0.1137,
            0.0509
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0028,
            0.0056,
            -0.0006,
            0.0081,
            0.0035,
            0.004,
            0.0115,
            0.0013,
            -0.0083,
            -0.0092,
            -0.0039,
            -0.0081,
            0.0061,
            -0.0017,
            -0.0012,
            0.0001
          ],
          "after": [
            -0.0368,
            0.0149,
            -0.1042,
            -0.0466,
            0.0466,
            0.1553,
            0.073,
            -0.1382,
            0.0271,
            -0.0129,
            -0.1231,
            -0.0594,
            0.1092,
            -0.0816,
            0.0197,
            0.1046
          ]
        }
      }
    },
    {
      "step": 301,
      "word": "lindell",
      "loss": 2.6055,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.014,
            0.0239,
            0.1702,
            -0.0217,
            0.0201,
            -0.0481,
            -0.1059,
            -0.1395,
            -0.015,
            0.0743,
            0.1521,
            0.0078,
            -0.0825,
            -0.0121,
            -0.0399,
            -0.1745
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1628,
            -0.0319,
            0.0475,
            0.0091,
            -0.0271,
            -0.0941,
            0.0282,
            -0.1175,
            0.0196,
            -0.0339,
            0.0161,
            0.1234,
            -0.0324,
            -0.0341,
            -0.0449,
            0.2156
          ],
          "after": [
            0.0248,
            0.1321,
            -0.049,
            -0.0494,
            0.0357,
            -0.0058,
            0.1048,
            0.1012,
            -0.0748,
            -0.0632,
            0.0234,
            -0.0029,
            0.1067,
            0.0217,
            -0.062,
            0.224
          ]
        },
        "position_0": {
          "grad": [
            -0.055,
            0.0735,
            -0.0586,
            -0.0798,
            -0.031,
            0.0896,
            -0.1295,
            0.1121,
            0.0372,
            -0.0399,
            -0.1326,
            0.0894,
            -0.132,
            0.0635,
            0.1638,
            -0.1679
          ],
          "after": [
            0.0254,
            -0.1517,
            -0.2509,
            0.069,
            0.0187,
            -0.0282,
            -0.1458,
            0.0808,
            0.1151,
            -0.0276,
            0.0221,
            0.0888,
            0.0171,
            0.0001,
            -0.1146,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            -0.0,
            -0.0001,
            0.0004,
            -0.0003,
            -0.0002,
            0.0037,
            -0.0005,
            0.0018,
            0.0026,
            0.0006,
            0.0027,
            0.001,
            0.0005,
            -0.0021,
            0.0028
          ],
          "after": [
            -0.0371,
            0.0154,
            -0.1044,
            -0.0472,
            0.0479,
            0.1557,
            0.0721,
            -0.1391,
            0.0287,
            -0.0112,
            -0.1219,
            -0.0579,
            0.1071,
            -0.0817,
            0.0196,
            0.1054
          ]
        }
      }
    },
    {
      "step": 302,
      "word": "marceline",
      "loss": 2.2093,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0387,
            -0.0581,
            -0.0042,
            -0.1274,
            0.1272,
            0.0358,
            0.0263,
            -0.1133,
            0.1084,
            -0.0315,
            0.0683,
            0.0389,
            -0.0941,
            -0.1239,
            -0.0078,
            0.0504
          ],
          "after": [
            -0.0139,
            0.0232,
            0.171,
            -0.0214,
            0.021,
            -0.0483,
            -0.106,
            -0.1389,
            -0.0155,
            0.075,
            0.1528,
            0.0078,
            -0.0824,
            -0.0122,
            -0.0408,
            -0.175
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.201,
            -0.123,
            0.08,
            -0.0274,
            -0.2221,
            0.0896,
            -0.3544,
            -0.1293,
            -0.0015,
            -0.0563,
            0.0229,
            -0.2158,
            0.0063,
            0.2833,
            0.0575,
            -0.3193
          ],
          "after": [
            0.0243,
            0.1331,
            -0.0483,
            -0.0489,
            0.0371,
            -0.0063,
            0.1055,
            0.1019,
            -0.0747,
            -0.0628,
            0.024,
            -0.0018,
            0.1062,
            0.0205,
            -0.0627,
            0.2246
          ]
        },
        "position_0": {
          "grad": [
            -0.0232,
            0.0004,
            0.001,
            -0.0085,
            0.0904,
            0.043,
            0.0665,
            0.0966,
            0.078,
            0.0086,
            -0.0131,
            0.0621,
            -0.1424,
            -0.0516,
            0.025,
            0.0217
          ],
          "after": [
            0.0261,
            -0.1521,
            -0.2508,
            0.0696,
            0.0181,
            -0.0291,
            -0.1454,
            0.0797,
            0.1147,
            -0.0272,
            0.0225,
            0.0876,
            0.018,
            -0.0001,
            -0.1154,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.005,
            0.0015,
            -0.0012,
            0.0033,
            -0.003,
            0.0014,
            -0.0002,
            -0.0007,
            -0.0008,
            0.0027,
            0.0003,
            -0.0007,
            0.0008,
            0.0022,
            -0.0032,
            -0.0002
          ],
          "after": [
            -0.0382,
            0.0157,
            -0.1043,
            -0.0481,
            0.0493,
            0.1558,
            0.0713,
            -0.1397,
            0.0302,
            -0.0101,
            -0.1208,
            -0.0565,
            0.1053,
            -0.0823,
            0.02,
            0.1062
          ]
        }
      }
    },
    {
      "step": 303,
      "word": "quenton",
      "loss": 2.7001,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0138,
            0.0225,
            0.1717,
            -0.0211,
            0.0217,
            -0.0485,
            -0.1061,
            -0.1383,
            -0.0159,
            0.0757,
            0.1535,
            0.0078,
            -0.0824,
            -0.0122,
            -0.0416,
            -0.1753
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2805,
            0.0808,
            0.0802,
            0.1919,
            0.084,
            0.2263,
            0.2974,
            -0.0078,
            -0.0022,
            -0.1048,
            0.0038,
            0.2131,
            -0.1983,
            -0.1729,
            0.0315,
            0.0714
          ],
          "after": [
            0.0244,
            0.1339,
            -0.0479,
            -0.0489,
            0.0382,
            -0.007,
            0.1058,
            0.1025,
            -0.0746,
            -0.0622,
            0.0245,
            -0.0014,
            0.1061,
            0.0199,
            -0.0634,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            -0.0531,
            0.2401,
            -0.2348,
            0.3027,
            0.1569,
            -0.1734,
            0.3228,
            -0.0997,
            -0.3495,
            0.0446,
            0.0479,
            -0.1866,
            0.2163,
            0.0561,
            -0.2482,
            0.3379
          ],
          "after": [
            0.0269,
            -0.1531,
            -0.2499,
            0.0696,
            0.0172,
            -0.0292,
            -0.1456,
            0.0791,
            0.1149,
            -0.027,
            0.0227,
            0.0871,
            0.0183,
            -0.0005,
            -0.1155,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0033,
            -0.001,
            -0.0011,
            0.004,
            -0.0002,
            0.0039,
            0.0052,
            0.0014,
            -0.0018,
            -0.0001,
            -0.0008,
            0.0009,
            -0.001,
            -0.0025,
            0.0004,
            -0.0001
          ],
          "after": [
            -0.0386,
            0.0161,
            -0.104,
            -0.0495,
            0.0506,
            0.1554,
            0.0703,
            -0.1404,
            0.0318,
            -0.0091,
            -0.1198,
            -0.0555,
            0.1039,
            -0.0822,
            0.0202,
            0.1068
          ]
        }
      }
    },
    {
      "step": 304,
      "word": "maevyn",
      "loss": 2.4746,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0414,
            -0.2978,
            0.0436,
            0.0401,
            -0.1201,
            0.1521,
            -0.2989,
            -0.0041,
            0.1871,
            0.1044,
            0.0988,
            -0.0155,
            -0.2851,
            0.0263,
            0.1224,
            -0.3258
          ],
          "after": [
            -0.0136,
            0.0227,
            0.1722,
            -0.0209,
            0.0226,
            -0.0493,
            -0.1057,
            -0.1379,
            -0.0167,
            0.0759,
            0.1537,
            0.0079,
            -0.0818,
            -0.0123,
            -0.0425,
            -0.1751
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2254,
            0.1648,
            -0.2335,
            0.1641,
            0.0729,
            0.2843,
            0.4223,
            0.2298,
            -0.0492,
            -0.1717,
            -0.2066,
            0.2717,
            -0.0006,
            -0.1466,
            0.1644,
            0.4416
          ],
          "after": [
            0.0251,
            0.1342,
            -0.0471,
            -0.0494,
            0.039,
            -0.0081,
            0.1057,
            0.1022,
            -0.0745,
            -0.0613,
            0.0255,
            -0.0016,
            0.106,
            0.0196,
            -0.0641,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            -0.0299,
            -0.01,
            0.0075,
            -0.0131,
            0.1228,
            0.0578,
            0.0773,
            0.1329,
            0.1214,
            0.0019,
            -0.0116,
            0.0631,
            -0.1885,
            -0.0564,
            0.0406,
            0.0239
          ],
          "after": [
            0.0277,
            -0.154,
            -0.2491,
            0.0695,
            0.0161,
            -0.0294,
            -0.1458,
            0.0781,
            0.1149,
            -0.0269,
            0.0228,
            0.0866,
            0.0189,
            -0.0006,
            -0.1157,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            -0.0006,
            -0.0009,
            0.0072,
            0.0012,
            0.0065,
            0.0041,
            0.006,
            0.0008,
            0.0038,
            0.0002,
            0.0045,
            0.0007,
            -0.0028,
            -0.0055,
            0.0088
          ],
          "after": [
            -0.0391,
            0.0165,
            -0.1036,
            -0.0515,
            0.0515,
            0.1542,
            0.069,
            -0.1421,
            0.033,
            -0.0088,
            -0.119,
            -0.0552,
            0.1026,
            -0.0817,
            0.021,
            0.1062
          ]
        }
      }
    },
    {
      "step": 305,
      "word": "ashaz",
      "loss": 2.8817,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.251,
            0.3474,
            0.362,
            0.5842,
            0.1541,
            -0.0678,
            0.5084,
            0.0184,
            -0.0841,
            -0.189,
            -0.202,
            0.4573,
            0.2866,
            -0.1533,
            -0.1847,
            0.5942
          ],
          "after": [
            -0.013,
            0.0221,
            0.1718,
            -0.022,
            0.023,
            -0.0497,
            -0.1062,
            -0.1376,
            -0.0172,
            0.0768,
            0.1545,
            0.0071,
            -0.0818,
            -0.012,
            -0.0429,
            -0.1759
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.028,
            0.0524,
            -0.0944,
            0.071,
            0.1134,
            -0.1326,
            0.2074,
            0.0093,
            -0.1609,
            0.0265,
            -0.0122,
            0.0222,
            0.0825,
            -0.0704,
            -0.28,
            0.1538
          ],
          "after": [
            0.0257,
            0.1344,
            -0.0462,
            -0.05,
            0.0396,
            -0.0088,
            0.1054,
            0.1018,
            -0.0742,
            -0.0606,
            0.0264,
            -0.0019,
            0.1058,
            0.0195,
            -0.0644,
            0.2245
          ]
        },
        "position_0": {
          "grad": [
            0.0845,
            -0.0846,
            0.1669,
            -0.1738,
            -0.0629,
            0.0757,
            -0.3923,
            -0.0471,
            0.2539,
            0.009,
            0.2442,
            0.0643,
            -0.3321,
            0.0704,
            0.0477,
            -0.3178
          ],
          "after": [
            0.028,
            -0.1545,
            -0.249,
            0.0698,
            0.0153,
            -0.03,
            -0.1454,
            0.0774,
            0.1144,
            -0.0268,
            0.0224,
            0.0859,
            0.0199,
            -0.001,
            -0.1159,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            -0.0001,
            0.0022,
            -0.0039,
            -0.0005,
            -0.0029,
            -0.0004,
            -0.0022,
            -0.0013,
            0.0037,
            0.0052,
            -0.0012,
            -0.0015,
            -0.0028,
            0.0038,
            -0.0062
          ],
          "after": [
            -0.0393,
            0.0168,
            -0.1037,
            -0.0527,
            0.0523,
            0.1536,
            0.068,
            -0.1431,
            0.0343,
            -0.0091,
            -0.1191,
            -0.0547,
            0.1016,
            -0.0807,
            0.0213,
            0.1064
          ]
        }
      }
    },
    {
      "step": 306,
      "word": "elad",
      "loss": 2.6358,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1015,
            0.0897,
            0.1243,
            -0.0225,
            0.2068,
            -0.1196,
            0.0989,
            -0.0951,
            -0.0051,
            -0.0512,
            -0.0487,
            0.0348,
            0.0647,
            0.1083,
            -0.0275,
            0.1658
          ],
          "after": [
            -0.0122,
            0.0213,
            0.1712,
            -0.0228,
            0.0231,
            -0.0496,
            -0.1069,
            -0.137,
            -0.0175,
            0.0776,
            0.1553,
            0.0063,
            -0.082,
            -0.012,
            -0.0431,
            -0.1768
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2993,
            0.8277,
            0.5338,
            -0.0318,
            0.1495,
            -0.2982,
            0.6709,
            -0.0635,
            -0.2311,
            -0.1908,
            0.0116,
            -0.1197,
            0.1388,
            -0.0259,
            -0.2894,
            0.0045
          ],
          "after": [
            0.0268,
            0.1333,
            -0.0465,
            -0.0504,
            0.0398,
            -0.009,
            0.1045,
            0.1017,
            -0.0736,
            -0.0595,
            0.0271,
            -0.0019,
            0.1054,
            0.0195,
            -0.0644,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            -0.037,
            -0.3455,
            0.1579,
            -0.1046,
            -0.1898,
            0.1232,
            -0.4492,
            -0.0454,
            0.3313,
            0.1163,
            -0.0717,
            -0.1073,
            -0.3454,
            0.0659,
            0.1724,
            -0.551
          ],
          "after": [
            0.0285,
            -0.154,
            -0.2495,
            0.0703,
            0.0151,
            -0.0309,
            -0.1444,
            0.077,
            0.1136,
            -0.0274,
            0.0221,
            0.0856,
            0.0213,
            -0.0015,
            -0.1166,
            0.0535
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0026,
            0.0022,
            0.0009,
            0.002,
            0.0021,
            0.0006,
            0.0019,
            -0.0001,
            -0.0016,
            0.0035,
            0.0023,
            -0.0002,
            0.0008,
            -0.0005,
            -0.0036,
            -0.0024
          ],
          "after": [
            -0.04,
            0.0168,
            -0.1039,
            -0.0539,
            0.0527,
            0.153,
            0.0669,
            -0.1439,
            0.0357,
            -0.0099,
            -0.1196,
            -0.0544,
            0.1007,
            -0.0797,
            0.0219,
            0.107
          ]
        }
      }
    },
    {
      "step": 307,
      "word": "kamia",
      "loss": 1.7917,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.187,
            0.074,
            0.0645,
            -0.1066,
            0.2254,
            -0.1075,
            0.2051,
            0.1663,
            0.0664,
            0.1275,
            0.234,
            -0.1737,
            -0.0832,
            -0.0506,
            -0.1159,
            -0.293
          ],
          "after": [
            -0.012,
            0.0204,
            0.1705,
            -0.0233,
            0.0227,
            -0.049,
            -0.1078,
            -0.137,
            -0.018,
            0.0779,
            0.1554,
            0.006,
            -0.082,
            -0.0119,
            -0.0429,
            -0.1772
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0628,
            0.1003,
            -0.13,
            0.043,
            0.0155,
            -0.13,
            0.27,
            0.0221,
            -0.2618,
            0.0835,
            -0.0504,
            -0.0936,
            0.1893,
            -0.0491,
            -0.2087,
            0.2005
          ],
          "after": [
            0.0279,
            0.1321,
            -0.0465,
            -0.0509,
            0.0399,
            -0.0089,
            0.1035,
            0.1016,
            -0.0729,
            -0.0588,
            0.0279,
            -0.0016,
            0.1048,
            0.0195,
            -0.0642,
            0.2236
          ]
        },
        "position_0": {
          "grad": [
            -0.0565,
            0.0058,
            -0.1339,
            -0.0487,
            0.0261,
            -0.0489,
            0.0791,
            -0.0423,
            0.0048,
            0.0407,
            0.0249,
            -0.0167,
            -0.0149,
            -0.03,
            -0.1233,
            -0.1175
          ],
          "after": [
            0.0291,
            -0.1536,
            -0.2495,
            0.0708,
            0.0149,
            -0.0315,
            -0.1437,
            0.0769,
            0.1128,
            -0.028,
            0.0219,
            0.0855,
            0.0225,
            -0.0018,
            -0.1168,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            -0.0022,
            0.0048,
            0.0035,
            -0.0027,
            -0.0032,
            -0.0089,
            -0.0006,
            0.0007,
            -0.0032,
            0.0019,
            0.0053,
            -0.0043,
            -0.0012,
            0.0008,
            -0.0028
          ],
          "after": [
            -0.0408,
            0.0171,
            -0.1048,
            -0.0555,
            0.0534,
            0.1529,
            0.0668,
            -0.1445,
            0.0367,
            -0.0101,
            -0.1203,
            -0.0548,
            0.1004,
            -0.0787,
            0.0223,
            0.1078
          ]
        }
      }
    },
    {
      "step": 308,
      "word": "agathe",
      "loss": 2.5948,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1616,
            -0.0359,
            -0.0441,
            0.3068,
            -0.0033,
            -0.3096,
            0.2443,
            -0.0895,
            -0.3445,
            0.1236,
            -0.1141,
            -0.3243,
            0.5244,
            0.1109,
            -0.3505,
            0.4632
          ],
          "after": [
            -0.0121,
            0.0197,
            0.1701,
            -0.0243,
            0.0224,
            -0.0475,
            -0.109,
            -0.1367,
            -0.0176,
            0.0778,
            0.1557,
            0.0063,
            -0.0829,
            -0.012,
            -0.0419,
            -0.1782
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1608,
            -0.0943,
            -0.0653,
            -0.1731,
            -0.5245,
            0.1792,
            -0.3417,
            -0.0567,
            0.1269,
            0.1997,
            0.1346,
            -0.3121,
            0.2193,
            0.2284,
            0.3102,
            -0.3201
          ],
          "after": [
            0.0284,
            0.1313,
            -0.0463,
            -0.0508,
            0.0408,
            -0.0091,
            0.1029,
            0.1017,
            -0.0724,
            -0.0587,
            0.0282,
            -0.0007,
            0.1039,
            0.0191,
            -0.0643,
            0.2235
          ]
        },
        "position_0": {
          "grad": [
            0.064,
            -0.0643,
            0.1334,
            -0.1407,
            -0.0024,
            0.0715,
            -0.3197,
            -0.0666,
            0.1797,
            0.0053,
            0.2455,
            0.0706,
            -0.2298,
            0.0494,
            0.002,
            -0.2271
          ],
          "after": [
            0.0293,
            -0.153,
            -0.2499,
            0.0715,
            0.0148,
            -0.0323,
            -0.1427,
            0.077,
            0.1118,
            -0.0286,
            0.0211,
            0.0851,
            0.0238,
            -0.0023,
            -0.117,
            0.056
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            0.0004,
            -0.0019,
            0.0011,
            0.0018,
            0.0002,
            0.0021,
            -0.0001,
            -0.0017,
            -0.0014,
            -0.0013,
            0.0006,
            -0.0003,
            -0.0005,
            -0.0004,
            0.001
          ],
          "after": [
            -0.0417,
            0.0173,
            -0.1053,
            -0.0569,
            0.0538,
            0.1528,
            0.0665,
            -0.1451,
            0.0379,
            -0.01,
            -0.1207,
            -0.0551,
            0.1003,
            -0.0777,
            0.0227,
            0.1083
          ]
        }
      }
    },
    {
      "step": 309,
      "word": "halaina",
      "loss": 2.021,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.116,
            -0.1141,
            0.0536,
            -0.3118,
            0.1624,
            0.0843,
            -0.4185,
            0.1159,
            -0.0096,
            0.047,
            0.0247,
            0.1224,
            -0.2621,
            0.0718,
            -0.0005,
            -0.4974
          ],
          "after": [
            -0.012,
            0.0195,
            0.1696,
            -0.0246,
            0.0218,
            -0.0465,
            -0.1093,
            -0.1369,
            -0.0173,
            0.0775,
            0.1559,
            0.0064,
            -0.0832,
            -0.0124,
            -0.0411,
            -0.1782
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.066,
            0.1032,
            -0.0909,
            0.0683,
            0.0485,
            -0.1276,
            0.2622,
            -0.0125,
            -0.2037,
            0.0357,
            -0.0056,
            -0.0006,
            0.1138,
            -0.0991,
            -0.2161,
            0.2134
          ],
          "after": [
            0.0291,
            0.1305,
            -0.046,
            -0.051,
            0.0415,
            -0.0091,
            0.1022,
            0.1018,
            -0.0717,
            -0.0587,
            0.0284,
            0.0001,
            0.103,
            0.019,
            -0.0642,
            0.2232
          ]
        },
        "position_0": {
          "grad": [
            0.0314,
            -0.0464,
            0.0574,
            -0.0146,
            -0.1164,
            0.0716,
            -0.0411,
            0.07,
            0.1275,
            0.0558,
            -0.1295,
            -0.1511,
            0.1035,
            0.0982,
            0.0607,
            0.0303
          ],
          "after": [
            0.0294,
            -0.1524,
            -0.2505,
            0.0722,
            0.0149,
            -0.0333,
            -0.1417,
            0.0768,
            0.1108,
            -0.0293,
            0.0207,
            0.0853,
            0.0248,
            -0.003,
            -0.1174,
            0.0571
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0043,
            -0.0056,
            -0.0022,
            0.0017,
            -0.0034,
            0.004,
            -0.0107,
            0.0003,
            -0.0006,
            -0.005,
            -0.0048,
            -0.0049,
            -0.0029,
            0.0063,
            0.004,
            -0.0003
          ],
          "after": [
            -0.0431,
            0.0183,
            -0.1054,
            -0.0583,
            0.0546,
            0.1522,
            0.0672,
            -0.1456,
            0.039,
            -0.0093,
            -0.1203,
            -0.0549,
            0.1004,
            -0.078,
            0.0227,
            0.1089
          ]
        }
      }
    },
    {
      "step": 310,
      "word": "yannis",
      "loss": 2.3682,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0688,
            0.0196,
            -0.2073,
            0.0163,
            -0.4087,
            -0.0877,
            -0.1475,
            -0.0834,
            0.1397,
            -0.1466,
            -0.1382,
            -0.2112,
            0.0811,
            0.2159,
            0.1783,
            -0.0064
          ],
          "after": [
            -0.012,
            0.0192,
            0.1696,
            -0.0248,
            0.022,
            -0.0453,
            -0.1093,
            -0.1368,
            -0.0173,
            0.0777,
            0.1565,
            0.0068,
            -0.0836,
            -0.0132,
            -0.0408,
            -0.1783
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0713,
            0.0262,
            -0.055,
            0.0201,
            0.1048,
            -0.1018,
            0.2324,
            0.0219,
            -0.0999,
            0.0878,
            0.0053,
            0.0569,
            0.0504,
            -0.1184,
            -0.1559,
            0.124
          ],
          "after": [
            0.0298,
            0.1297,
            -0.0457,
            -0.0511,
            0.042,
            -0.0089,
            0.1014,
            0.1019,
            -0.071,
            -0.059,
            0.0286,
            0.0006,
            0.1021,
            0.0191,
            -0.0639,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.0318,
            -0.1163,
            0.0843,
            -0.1504,
            -0.1743,
            0.1272,
            -0.1315,
            0.044,
            0.1138,
            0.005,
            -0.1749,
            -0.0346,
            -0.1045,
            0.1456,
            0.1804,
            -0.0722
          ],
          "after": [
            0.0293,
            -0.1516,
            -0.2513,
            0.073,
            0.0155,
            -0.0346,
            -0.1407,
            0.0765,
            0.1098,
            -0.03,
            0.0208,
            0.0855,
            0.0258,
            -0.0041,
            -0.1181,
            0.0581
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            0.0005,
            -0.0001,
            -0.0016,
            0.0016,
            0.0006,
            -0.0015,
            0.0003,
            0.0026,
            -0.0016,
            -0.0004,
            0.0028,
            -0.0025,
            0.001,
            -0.0014,
            0.0008
          ],
          "after": [
            -0.0446,
            0.019,
            -0.1054,
            -0.0593,
            0.055,
            0.1516,
            0.0678,
            -0.146,
            0.0395,
            -0.0085,
            -0.1199,
            -0.055,
            0.1008,
            -0.0785,
            0.0228,
            0.1092
          ]
        }
      }
    },
    {
      "step": 311,
      "word": "robert",
      "loss": 3.1298,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.012,
            0.0189,
            0.1696,
            -0.0251,
            0.0222,
            -0.0443,
            -0.1092,
            -0.1366,
            -0.0173,
            0.0779,
            0.157,
            0.0072,
            -0.084,
            -0.0139,
            -0.0406,
            -0.1783
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1651,
            0.0224,
            -0.1024,
            -0.0759,
            0.1913,
            0.0921,
            0.1931,
            0.0339,
            0.0889,
            -0.2025,
            -0.0217,
            0.0365,
            -0.1568,
            -0.313,
            0.1339,
            0.0156
          ],
          "after": [
            0.0307,
            0.129,
            -0.0452,
            -0.0511,
            0.0421,
            -0.0089,
            0.1005,
            0.1018,
            -0.0705,
            -0.0586,
            0.0288,
            0.001,
            0.1016,
            0.0198,
            -0.0638,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            0.0091,
            -0.0151,
            0.044,
            -0.0789,
            0.0493,
            0.1344,
            0.0453,
            -0.0584,
            0.124,
            -0.0677,
            -0.0624,
            -0.0221,
            -0.1412,
            -0.0548,
            0.1175,
            0.1024
          ],
          "after": [
            0.0292,
            -0.1509,
            -0.2521,
            0.0739,
            0.0159,
            -0.0362,
            -0.14,
            0.0764,
            0.1087,
            -0.0302,
            0.0211,
            0.0858,
            0.0269,
            -0.0048,
            -0.119,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0021,
            0.0018,
            -0.0025,
            0.0021,
            -0.002,
            0.0068,
            0.0016,
            -0.0005,
            -0.0032,
            0.001,
            -0.0027,
            0.0047,
            -0.0063,
            -0.0062,
            -0.0031
          ],
          "after": [
            -0.0458,
            0.02,
            -0.1057,
            -0.0599,
            0.0551,
            0.1514,
            0.0678,
            -0.1467,
            0.04,
            -0.0074,
            -0.1197,
            -0.0547,
            0.1007,
            -0.0778,
            0.0235,
            0.1099
          ]
        }
      }
    },
    {
      "step": 312,
      "word": "keelyn",
      "loss": 2.0649,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.012,
            0.0187,
            0.1696,
            -0.0252,
            0.0224,
            -0.0434,
            -0.1092,
            -0.1366,
            -0.0173,
            0.0781,
            0.1574,
            0.0075,
            -0.0843,
            -0.0145,
            -0.0404,
            -0.1783
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0766,
            0.0995,
            0.0572,
            -0.0314,
            0.3001,
            0.4218,
            -0.2769,
            -0.0945,
            0.4934,
            -0.0554,
            0.1833,
            0.6296,
            -0.6102,
            0.0213,
            0.2918,
            -0.2359
          ],
          "after": [
            0.0313,
            0.1283,
            -0.0449,
            -0.0509,
            0.0417,
            -0.0096,
            0.1001,
            0.1021,
            -0.0707,
            -0.0582,
            0.0284,
            -0.0,
            0.1021,
            0.0203,
            -0.064,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0614,
            -0.0209,
            -0.1,
            -0.0148,
            -0.025,
            -0.0472,
            0.0514,
            -0.014,
            0.0378,
            -0.0163,
            0.0004,
            -0.0872,
            -0.0018,
            0.0144,
            -0.0681,
            -0.1092
          ],
          "after": [
            0.0294,
            -0.1502,
            -0.2525,
            0.0747,
            0.0163,
            -0.0374,
            -0.1394,
            0.0764,
            0.1077,
            -0.0303,
            0.0213,
            0.0862,
            0.0278,
            -0.0055,
            -0.1196,
            0.0596
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0014,
            -0.0034,
            -0.0006,
            0.0005,
            0.0044,
            0.0006,
            -0.0008,
            0.0012,
            0.0007,
            0.0004,
            0.0083,
            -0.0052,
            -0.0011,
            0.0031,
            0.0001
          ],
          "after": [
            -0.0465,
            0.0206,
            -0.1055,
            -0.0603,
            0.0551,
            0.1506,
            0.0678,
            -0.1471,
            0.0402,
            -0.0065,
            -0.1196,
            -0.0555,
            0.1011,
            -0.0771,
            0.0238,
            0.1105
          ]
        }
      }
    },
    {
      "step": 313,
      "word": "lannie",
      "loss": 1.9391,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.08,
            0.0046,
            -0.2121,
            -0.016,
            -0.4261,
            -0.0893,
            -0.1865,
            -0.0713,
            0.1607,
            -0.1409,
            -0.1363,
            -0.213,
            0.0579,
            0.206,
            0.1797,
            -0.0293
          ],
          "after": [
            -0.0122,
            0.0186,
            0.1701,
            -0.0254,
            0.0233,
            -0.0424,
            -0.1089,
            -0.1362,
            -0.0177,
            0.0787,
            0.1581,
            0.0081,
            -0.0847,
            -0.0156,
            -0.0407,
            -0.1783
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0792,
            -0.0361,
            -0.1575,
            0.0733,
            -0.6478,
            0.0338,
            -0.1177,
            0.0903,
            0.083,
            0.0462,
            0.0286,
            -0.0627,
            0.1594,
            0.0609,
            0.4133,
            -0.0443
          ],
          "after": [
            0.0317,
            0.1277,
            -0.0444,
            -0.051,
            0.0423,
            -0.0101,
            0.0998,
            0.1019,
            -0.0709,
            -0.058,
            0.0281,
            -0.0008,
            0.1023,
            0.0207,
            -0.0647,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            -0.0424,
            0.0684,
            -0.1095,
            -0.1307,
            -0.0545,
            0.0641,
            -0.1448,
            0.1144,
            0.0319,
            -0.0265,
            -0.1711,
            0.1016,
            -0.1361,
            0.0545,
            0.1661,
            -0.1993
          ],
          "after": [
            0.0298,
            -0.1499,
            -0.2524,
            0.0756,
            0.0168,
            -0.0387,
            -0.1387,
            0.076,
            0.1068,
            -0.0303,
            0.0219,
            0.0863,
            0.0288,
            -0.0062,
            -0.1205,
            0.0606
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0013,
            0.0001,
            0.0021,
            -0.0051,
            0.0018,
            -0.0059,
            0.0029,
            0.0025,
            0.0001,
            -0.0006,
            0.0024,
            -0.0022,
            0.0012,
            0.0025,
            0.0018
          ],
          "after": [
            -0.0471,
            0.0213,
            -0.1052,
            -0.0608,
            0.0558,
            0.1497,
            0.0682,
            -0.148,
            0.0399,
            -0.0057,
            -0.1194,
            -0.0565,
            0.1017,
            -0.0766,
            0.0238,
            0.1107
          ]
        }
      }
    },
    {
      "step": 314,
      "word": "arzaan",
      "loss": 2.3897,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0057,
            -0.1011,
            0.298,
            0.0663,
            -0.0508,
            0.1768,
            -0.203,
            -0.1925,
            0.3558,
            -0.3721,
            0.2023,
            0.2027,
            -0.5331,
            -0.0939,
            0.1451,
            -0.0311
          ],
          "after": [
            -0.0123,
            0.0187,
            0.1699,
            -0.0256,
            0.0241,
            -0.0421,
            -0.1082,
            -0.1354,
            -0.0188,
            0.0804,
            0.1582,
            0.0083,
            -0.084,
            -0.0162,
            -0.0413,
            -0.1782
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0344,
            0.0555,
            -0.0835,
            0.0381,
            0.0176,
            -0.135,
            0.1826,
            -0.0126,
            -0.1437,
            0.069,
            -0.007,
            -0.0077,
            0.1357,
            -0.0643,
            -0.2155,
            0.1011
          ],
          "after": [
            0.0321,
            0.1271,
            -0.0438,
            -0.0512,
            0.0428,
            -0.0104,
            0.0993,
            0.1019,
            -0.071,
            -0.058,
            0.0277,
            -0.0014,
            0.1023,
            0.0211,
            -0.065,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0747,
            -0.0894,
            0.0965,
            -0.1899,
            0.0017,
            0.0654,
            -0.2932,
            -0.0745,
            0.1607,
            0.0608,
            0.2577,
            0.1285,
            -0.2436,
            -0.024,
            -0.0321,
            -0.2581
          ],
          "after": [
            0.0297,
            -0.1493,
            -0.2527,
            0.0768,
            0.0172,
            -0.04,
            -0.1376,
            0.0759,
            0.1058,
            -0.0306,
            0.0218,
            0.0861,
            0.03,
            -0.0068,
            -0.1212,
            0.0618
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            -0.0006,
            -0.0025,
            0.0011,
            -0.0051,
            0.0002,
            0.0024,
            0.0017,
            0.0026,
            -0.0035,
            -0.0065,
            -0.0012,
            0.0039,
            0.0027,
            0.0022,
            0.0049
          ],
          "after": [
            -0.0476,
            0.022,
            -0.1047,
            -0.0615,
            0.057,
            0.1489,
            0.0684,
            -0.149,
            0.0392,
            -0.0046,
            -0.1183,
            -0.0572,
            0.1018,
            -0.0767,
            0.0235,
            0.1103
          ]
        }
      }
    },
    {
      "step": 315,
      "word": "lochlann",
      "loss": 2.713,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1544,
            0.0449,
            -0.0192,
            0.0906,
            -0.0488,
            -0.0283,
            -0.0099,
            -0.0205,
            0.0898,
            -0.1973,
            -0.1353,
            0.1769,
            -0.1191,
            -0.0376,
            0.0523,
            0.1582
          ],
          "after": [
            -0.0121,
            0.0186,
            0.1697,
            -0.026,
            0.0249,
            -0.0418,
            -0.1076,
            -0.1346,
            -0.02,
            0.0824,
            0.1586,
            0.0082,
            -0.0833,
            -0.0166,
            -0.0419,
            -0.1784
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0954,
            0.2172,
            -0.1804,
            0.1191,
            0.0975,
            -0.2229,
            0.4514,
            0.0202,
            -0.3278,
            0.0145,
            -0.016,
            0.0428,
            0.1721,
            -0.219,
            -0.3073,
            0.3957
          ],
          "after": [
            0.0326,
            0.1263,
            -0.0429,
            -0.0516,
            0.0431,
            -0.0103,
            0.0985,
            0.1018,
            -0.0706,
            -0.058,
            0.0275,
            -0.002,
            0.102,
            0.0218,
            -0.0649,
            0.2217
          ]
        },
        "position_0": {
          "grad": [
            -0.0568,
            0.0877,
            -0.0904,
            -0.0602,
            0.0054,
            0.0668,
            -0.0488,
            0.0671,
            -0.0296,
            -0.0217,
            -0.0912,
            0.1177,
            -0.0596,
            0.0407,
            0.1164,
            -0.0798
          ],
          "after": [
            0.0299,
            -0.149,
            -0.2526,
            0.0779,
            0.0176,
            -0.0414,
            -0.1367,
            0.0756,
            0.105,
            -0.0307,
            0.0219,
            0.0855,
            0.0311,
            -0.0074,
            -0.122,
            0.0629
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0059,
            0.0001,
            -0.0035,
            0.0021,
            -0.0033,
            0.0016,
            0.003,
            0.0022,
            -0.0016,
            0.0018,
            -0.0,
            -0.0057,
            0.0061,
            0.0027,
            -0.003,
            0.002
          ],
          "after": [
            -0.049,
            0.0225,
            -0.1036,
            -0.0623,
            0.0585,
            0.1481,
            0.0683,
            -0.1503,
            0.0389,
            -0.0039,
            -0.1174,
            -0.057,
            0.1012,
            -0.0772,
            0.0236,
            0.1097
          ]
        }
      }
    },
    {
      "step": 316,
      "word": "manami",
      "loss": 2.0711,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2275,
            0.1276,
            0.1542,
            0.0975,
            -0.0325,
            -0.0277,
            0.0959,
            0.0808,
            0.2153,
            -0.3185,
            -0.1137,
            0.103,
            -0.1472,
            -0.0407,
            0.0036,
            0.188
          ],
          "after": [
            -0.0115,
            0.0183,
            0.1692,
            -0.0265,
            0.0256,
            -0.0414,
            -0.1073,
            -0.1341,
            -0.0214,
            0.085,
            0.1593,
            0.0078,
            -0.0824,
            -0.0169,
            -0.0425,
            -0.1789
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1119,
            0.0779,
            -0.098,
            0.0644,
            0.1678,
            -0.146,
            0.2599,
            0.0241,
            -0.2155,
            0.0659,
            -0.0477,
            0.0502,
            0.1147,
            -0.1124,
            -0.2645,
            0.2062
          ],
          "after": [
            0.0333,
            0.1254,
            -0.042,
            -0.0522,
            0.0431,
            -0.0101,
            0.0976,
            0.1016,
            -0.0701,
            -0.0582,
            0.0275,
            -0.0026,
            0.1016,
            0.0227,
            -0.0646,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            -0.0381,
            0.0235,
            -0.0223,
            0.0317,
            0.135,
            0.021,
            0.1352,
            0.1449,
            0.0628,
            0.0078,
            -0.0384,
            0.0928,
            -0.1398,
            -0.099,
            -0.0022,
            0.087
          ],
          "after": [
            0.0303,
            -0.1489,
            -0.2524,
            0.0788,
            0.0175,
            -0.0427,
            -0.1361,
            0.0748,
            0.1042,
            -0.0309,
            0.0221,
            0.0847,
            0.0323,
            -0.0076,
            -0.1228,
            0.0637
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0007,
            0.0004,
            -0.0016,
            0.0004,
            0.0028,
            -0.0053,
            -0.0002,
            -0.0001,
            -0.006,
            -0.0021,
            -0.0011,
            -0.0036,
            0.0004,
            0.004,
            0.0002
          ],
          "after": [
            -0.05,
            0.0231,
            -0.1028,
            -0.0628,
            0.0597,
            0.147,
            0.0687,
            -0.1513,
            0.0387,
            -0.0026,
            -0.1163,
            -0.0568,
            0.1011,
            -0.0777,
            0.0233,
            0.1092
          ]
        }
      }
    },
    {
      "step": 317,
      "word": "jeylin",
      "loss": 1.952,
      "learning_rate": 0.0021,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.011,
            0.018,
            0.1688,
            -0.0269,
            0.0263,
            -0.0411,
            -0.107,
            -0.1338,
            -0.0227,
            0.0872,
            0.1598,
            0.0075,
            -0.0816,
            -0.0172,
            -0.043,
            -0.1793
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2298,
            -0.0542,
            0.2278,
            -0.0036,
            0.2588,
            0.0113,
            -0.2945,
            -0.1426,
            0.3257,
            0.1565,
            0.2804,
            0.156,
            -0.298,
            -0.0786,
            -0.1476,
            -0.1258
          ],
          "after": [
            0.0334,
            0.1248,
            -0.0417,
            -0.0527,
            0.0428,
            -0.0098,
            0.0971,
            0.102,
            -0.07,
            -0.0587,
            0.0266,
            -0.0034,
            0.1017,
            0.0236,
            -0.0642,
            0.2203
          ]
        },
        "position_0": {
          "grad": [
            -0.0067,
            0.0473,
            0.0347,
            0.1952,
            0.0188,
            -0.1164,
            0.2002,
            0.068,
            -0.2303,
            -0.1133,
            -0.0192,
            -0.0501,
            0.3613,
            -0.1179,
            -0.122,
            0.191
          ],
          "after": [
            0.0306,
            -0.1489,
            -0.2524,
            0.0791,
            0.0174,
            -0.0433,
            -0.1358,
            0.0738,
            0.1039,
            -0.0304,
            0.0223,
            0.0842,
            0.0327,
            -0.0074,
            -0.1231,
            0.0642
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0002,
            0.0005,
            -0.0004,
            0.0006,
            0.0007,
            -0.0007,
            -0.0001,
            0.0021,
            0.0017,
            0.0011,
            0.0036,
            -0.0024,
            -0.001,
            0.0,
            -0.0
          ],
          "after": [
            -0.0506,
            0.0237,
            -0.1021,
            -0.0632,
            0.0607,
            0.1459,
            0.069,
            -0.1521,
            0.0381,
            -0.0017,
            -0.1155,
            -0.057,
            0.1013,
            -0.078,
            0.0231,
            0.1087
          ]
        }
      }
    },
    {
      "step": 318,
      "word": "adonia",
      "loss": 2.0395,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.007,
            0.1532,
            0.0109,
            -0.1988,
            0.068,
            -0.0999,
            0.0918,
            -0.1232,
            -0.0703,
            0.1167,
            0.037,
            -0.1578,
            0.1925,
            0.1316,
            0.0258,
            -0.0557
          ],
          "after": [
            -0.0106,
            0.0173,
            0.1684,
            -0.0269,
            0.0267,
            -0.0405,
            -0.1069,
            -0.1331,
            -0.0236,
            0.0887,
            0.1602,
            0.0076,
            -0.0813,
            -0.0177,
            -0.0435,
            -0.1795
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.019,
            0.0497,
            -0.0657,
            0.0282,
            0.1184,
            -0.1359,
            0.2115,
            0.0144,
            -0.1728,
            0.0517,
            -0.0032,
            -0.0187,
            0.11,
            -0.1016,
            -0.2229,
            0.1023
          ],
          "after": [
            0.0335,
            0.1242,
            -0.0412,
            -0.0532,
            0.0423,
            -0.0094,
            0.0965,
            0.1022,
            -0.0697,
            -0.0593,
            0.0259,
            -0.0041,
            0.1016,
            0.0245,
            -0.0636,
            0.2198
          ]
        },
        "position_0": {
          "grad": [
            0.0698,
            -0.1012,
            0.0938,
            -0.1734,
            0.014,
            0.0543,
            -0.2909,
            -0.0829,
            0.1716,
            0.0468,
            0.2779,
            0.1117,
            -0.2291,
            -0.0205,
            -0.0337,
            -0.2444
          ],
          "after": [
            0.0306,
            -0.1486,
            -0.2527,
            0.0798,
            0.0172,
            -0.044,
            -0.1352,
            0.0733,
            0.1033,
            -0.0303,
            0.0218,
            0.0834,
            0.0334,
            -0.0071,
            -0.1233,
            0.0649
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0005,
            -0.0005,
            0.0008,
            -0.0042,
            -0.001,
            -0.0037,
            -0.0002,
            0.0006,
            -0.0011,
            -0.0007,
            0.0006,
            -0.0017,
            0.0006,
            0.0049,
            0.0001
          ],
          "after": [
            -0.051,
            0.0241,
            -0.1015,
            -0.0636,
            0.062,
            0.1452,
            0.0696,
            -0.1528,
            0.0374,
            -0.0007,
            -0.1148,
            -0.0573,
            0.1016,
            -0.0783,
            0.0223,
            0.1083
          ]
        }
      }
    },
    {
      "step": 319,
      "word": "mianna",
      "loss": 2.0342,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2632,
            0.1042,
            -0.2405,
            -0.0378,
            -0.3607,
            0.017,
            -0.0916,
            -0.0745,
            0.0936,
            0.206,
            0.0147,
            -0.2305,
            0.082,
            0.2284,
            0.0235,
            -0.1923
          ],
          "after": [
            -0.0107,
            0.0165,
            0.1686,
            -0.0268,
            0.0276,
            -0.04,
            -0.1067,
            -0.1322,
            -0.0245,
            0.0893,
            0.1605,
            0.0081,
            -0.0812,
            -0.0187,
            -0.044,
            -0.1794
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0704,
            0.0304,
            -0.1019,
            0.016,
            0.0852,
            -0.121,
            0.2123,
            0.02,
            -0.1627,
            0.0742,
            -0.0269,
            0.002,
            0.1104,
            -0.1119,
            -0.1679,
            0.1306
          ],
          "after": [
            0.0338,
            0.1236,
            -0.0407,
            -0.0536,
            0.0418,
            -0.0089,
            0.0958,
            0.1024,
            -0.0693,
            -0.06,
            0.0254,
            -0.0047,
            0.1014,
            0.0255,
            -0.0629,
            0.2191
          ]
        },
        "position_0": {
          "grad": [
            -0.0401,
            0.006,
            -0.0207,
            0.0195,
            0.1178,
            0.0163,
            0.1276,
            0.1492,
            0.0745,
            -0.007,
            -0.0427,
            0.0648,
            -0.1339,
            -0.0837,
            0.0052,
            0.0794
          ],
          "after": [
            0.0307,
            -0.1484,
            -0.2529,
            0.0803,
            0.0168,
            -0.0447,
            -0.1349,
            0.0723,
            0.1028,
            -0.0301,
            0.0215,
            0.0825,
            0.0342,
            -0.0066,
            -0.1235,
            0.0654
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0003,
            0.0009,
            0.0028,
            -0.009,
            0.0015,
            -0.0108,
            0.0007,
            0.0009,
            -0.0013,
            0.0018,
            0.0025,
            -0.0058,
            0.0013,
            0.0067,
            -0.0008
          ],
          "after": [
            -0.0514,
            0.0244,
            -0.1011,
            -0.0643,
            0.0642,
            0.1444,
            0.071,
            -0.1535,
            0.0368,
            0.0003,
            -0.1144,
            -0.0578,
            0.1025,
            -0.0788,
            0.021,
            0.108
          ]
        }
      }
    },
    {
      "step": 320,
      "word": "dastan",
      "loss": 1.9093,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1377,
            0.2623,
            0.1232,
            0.2465,
            -0.1416,
            -0.0157,
            0.0434,
            0.0063,
            -0.0751,
            -0.0334,
            -0.1166,
            0.2429,
            0.0494,
            -0.0855,
            0.0555,
            0.0353
          ],
          "after": [
            -0.0106,
            0.0152,
            0.1685,
            -0.0272,
            0.0287,
            -0.0396,
            -0.1066,
            -0.1315,
            -0.0252,
            0.09,
            0.1611,
            0.008,
            -0.0812,
            -0.0194,
            -0.0445,
            -0.1794
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0356,
            0.0926,
            -0.1087,
            0.0711,
            0.0547,
            -0.1202,
            0.2361,
            0.0125,
            -0.1846,
            0.0435,
            0.0109,
            -0.0209,
            0.1329,
            -0.1093,
            -0.1892,
            0.1171
          ],
          "after": [
            0.0341,
            0.123,
            -0.04,
            -0.0542,
            0.0413,
            -0.0083,
            0.0949,
            0.1025,
            -0.0687,
            -0.0607,
            0.0249,
            -0.0051,
            0.101,
            0.0266,
            -0.0621,
            0.2184
          ]
        },
        "position_0": {
          "grad": [
            -0.0596,
            0.0718,
            -0.051,
            -0.0283,
            0.115,
            -0.0762,
            0.1122,
            -0.0345,
            -0.0362,
            -0.0431,
            -0.1532,
            0.0103,
            0.0842,
            0.0701,
            0.051,
            0.2024
          ],
          "after": [
            0.0311,
            -0.1484,
            -0.2529,
            0.0808,
            0.0161,
            -0.045,
            -0.1348,
            0.0716,
            0.1023,
            -0.0298,
            0.0216,
            0.0818,
            0.0348,
            -0.0064,
            -0.1238,
            0.0655
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0025,
            -0.005,
            0.0008,
            -0.0036,
            -0.0011,
            -0.0027,
            -0.0024,
            -0.0007,
            0.0005,
            0.0024,
            0.001,
            0.0027,
            -0.0028,
            0.0019,
            -0.0024,
            0.0002
          ],
          "after": [
            -0.0522,
            0.0255,
            -0.1009,
            -0.0644,
            0.0661,
            0.144,
            0.0723,
            -0.154,
            0.0361,
            0.0008,
            -0.1142,
            -0.0586,
            0.1035,
            -0.0795,
            0.0202,
            0.1078
          ]
        }
      }
    },
    {
      "step": 321,
      "word": "nyla",
      "loss": 2.6368,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5713,
            -0.0356,
            -0.4502,
            -0.1336,
            -0.3695,
            0.0582,
            -0.0865,
            0.0661,
            -0.0969,
            0.2916,
            0.0921,
            -0.5815,
            0.2918,
            0.2816,
            0.1933,
            -0.2407
          ],
          "after": [
            -0.0115,
            0.0141,
            0.1694,
            -0.0273,
            0.0302,
            -0.0394,
            -0.1064,
            -0.1312,
            -0.0255,
            0.0897,
            0.1613,
            0.009,
            -0.0817,
            -0.0206,
            -0.0455,
            -0.179
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0406,
            0.0667,
            -0.1498,
            0.081,
            -0.0761,
            -0.1697,
            0.3033,
            -0.012,
            -0.2559,
            0.1197,
            0.0007,
            -0.0557,
            0.2279,
            -0.1137,
            -0.2097,
            0.2293
          ],
          "after": [
            0.0344,
            0.1223,
            -0.0392,
            -0.0549,
            0.0409,
            -0.0075,
            0.0939,
            0.1026,
            -0.0678,
            -0.0616,
            0.0245,
            -0.0054,
            0.1003,
            0.0277,
            -0.0612,
            0.2174
          ]
        },
        "position_0": {
          "grad": [
            -0.0566,
            0.0642,
            -0.0593,
            0.026,
            -0.5751,
            0.055,
            -0.0485,
            0.0647,
            0.2112,
            -0.2786,
            -0.3942,
            -0.2399,
            -0.0636,
            0.247,
            0.3969,
            0.0326
          ],
          "after": [
            0.0317,
            -0.1486,
            -0.2527,
            0.0812,
            0.017,
            -0.0455,
            -0.1346,
            0.0708,
            0.1016,
            -0.0283,
            0.0226,
            0.0818,
            0.0354,
            -0.0071,
            -0.1249,
            0.0656
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0011,
            0.0002,
            -0.0004,
            0.0002,
            -0.0007,
            0.0005,
            0.0002,
            -0.0003,
            0.0004,
            -0.0004,
            -0.0004,
            0.0008,
            -0.0005,
            0.0002,
            -0.0002
          ],
          "after": [
            -0.0527,
            0.0265,
            -0.1008,
            -0.0645,
            0.0678,
            0.1438,
            0.0734,
            -0.1544,
            0.0356,
            0.0011,
            -0.114,
            -0.0592,
            0.1043,
            -0.0801,
            0.0194,
            0.1076
          ]
        }
      }
    },
    {
      "step": 322,
      "word": "evander",
      "loss": 2.5404,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0813,
            0.0645,
            -0.1615,
            0.0596,
            -0.2051,
            0.0388,
            -0.023,
            -0.0576,
            0.0536,
            0.0407,
            -0.0668,
            -0.0433,
            0.0213,
            0.099,
            0.0143,
            0.0215
          ],
          "after": [
            -0.0125,
            0.013,
            0.1705,
            -0.0275,
            0.0318,
            -0.0394,
            -0.1061,
            -0.1306,
            -0.026,
            0.0894,
            0.1617,
            0.0099,
            -0.0821,
            -0.0219,
            -0.0463,
            -0.1787
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1835,
            0.1563,
            0.4676,
            -0.2161,
            -0.225,
            0.1302,
            -0.1105,
            -0.0668,
            0.3516,
            -0.0388,
            0.0447,
            -0.0253,
            -0.1522,
            0.2206,
            0.3456,
            -0.3393
          ],
          "after": [
            0.0351,
            0.1215,
            -0.0393,
            -0.0549,
            0.041,
            -0.0071,
            0.0932,
            0.103,
            -0.0676,
            -0.0622,
            0.024,
            -0.0056,
            0.0999,
            0.0282,
            -0.0608,
            0.2171
          ]
        },
        "position_0": {
          "grad": [
            -0.0351,
            -0.1728,
            0.0381,
            -0.0178,
            -0.0497,
            0.0744,
            -0.1606,
            -0.0703,
            0.097,
            0.0957,
            0.009,
            -0.0026,
            -0.1093,
            -0.0107,
            0.0412,
            -0.2096
          ],
          "after": [
            0.0323,
            -0.1483,
            -0.2526,
            0.0815,
            0.0178,
            -0.0462,
            -0.1342,
            0.0703,
            0.1008,
            -0.0274,
            0.0234,
            0.0819,
            0.036,
            -0.0076,
            -0.126,
            0.0659
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0033,
            -0.0008,
            -0.0018,
            -0.0007,
            -0.0009,
            -0.0007,
            0.0039,
            0.0021,
            0.0007,
            0.0009,
            0.0001,
            0.0012,
            0.0008,
            -0.0031,
            0.0024,
            0.0017
          ],
          "after": [
            -0.0526,
            0.0275,
            -0.1004,
            -0.0645,
            0.0693,
            0.1438,
            0.074,
            -0.1552,
            0.035,
            0.0013,
            -0.1138,
            -0.0599,
            0.1049,
            -0.08,
            0.0185,
            0.1072
          ]
        }
      }
    },
    {
      "step": 323,
      "word": "yassir",
      "loss": 2.7253,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0411,
            0.2161,
            0.1283,
            0.184,
            -0.1266,
            0.0246,
            0.0825,
            0.0465,
            -0.1452,
            0.1364,
            -0.032,
            0.093,
            0.1847,
            -0.0294,
            0.0397,
            -0.0379
          ],
          "after": [
            -0.0132,
            0.0116,
            0.1711,
            -0.028,
            0.0334,
            -0.0395,
            -0.106,
            -0.1304,
            -0.026,
            0.0887,
            0.1621,
            0.0105,
            -0.0828,
            -0.0229,
            -0.0472,
            -0.1783
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0508,
            0.0212,
            -0.0496,
            0.0164,
            0.1053,
            -0.1207,
            0.1911,
            -0.0156,
            -0.1351,
            0.0739,
            0.0097,
            0.0651,
            0.0429,
            -0.1221,
            -0.2013,
            0.1222
          ],
          "after": [
            0.0358,
            0.1208,
            -0.0394,
            -0.055,
            0.0409,
            -0.0065,
            0.0924,
            0.1034,
            -0.0672,
            -0.063,
            0.0236,
            -0.0059,
            0.0996,
            0.0289,
            -0.0603,
            0.2167
          ]
        },
        "position_0": {
          "grad": [
            0.033,
            -0.0763,
            0.0468,
            -0.1109,
            -0.0836,
            0.1165,
            -0.0561,
            0.0193,
            0.0228,
            0.0361,
            -0.1462,
            0.0214,
            -0.0228,
            0.1067,
            0.1012,
            0.0234
          ],
          "after": [
            0.0327,
            -0.1478,
            -0.2528,
            0.0821,
            0.0188,
            -0.0472,
            -0.1338,
            0.0699,
            0.1001,
            -0.0269,
            0.0245,
            0.0819,
            0.0366,
            -0.0084,
            -0.1272,
            0.0662
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0038,
            0.0029,
            0.001,
            -0.0023,
            -0.0011,
            -0.0004,
            -0.0023,
            0.0021,
            0.0003,
            -0.0009,
            0.0014,
            0.0021,
            0.0018,
            -0.0033,
            0.0001
          ],
          "after": [
            -0.0526,
            0.0278,
            -0.1005,
            -0.0646,
            0.0709,
            0.1438,
            0.0746,
            -0.1554,
            0.0341,
            0.0015,
            -0.1136,
            -0.0606,
            0.1052,
            -0.0803,
            0.0181,
            0.1069
          ]
        }
      }
    },
    {
      "step": 324,
      "word": "edelweiss",
      "loss": 3.2309,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0138,
            0.0104,
            0.1717,
            -0.0285,
            0.0347,
            -0.0395,
            -0.106,
            -0.1301,
            -0.026,
            0.0881,
            0.1624,
            0.011,
            -0.0834,
            -0.0238,
            -0.0479,
            -0.1781
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1074,
            0.0844,
            0.4425,
            -0.2372,
            -0.2706,
            0.4054,
            -0.4756,
            -0.0973,
            0.6001,
            -0.0688,
            0.1293,
            0.0708,
            -0.3182,
            0.3117,
            0.5559,
            -0.4369
          ],
          "after": [
            0.0366,
            0.12,
            -0.0402,
            -0.0544,
            0.0411,
            -0.0066,
            0.0921,
            0.1041,
            -0.0676,
            -0.0634,
            0.0228,
            -0.0063,
            0.0997,
            0.0289,
            -0.0604,
            0.2169
          ]
        },
        "position_0": {
          "grad": [
            -0.0369,
            -0.1314,
            0.0242,
            -0.0085,
            -0.0165,
            0.0627,
            -0.1158,
            -0.0624,
            0.0678,
            0.0873,
            0.0162,
            0.0158,
            -0.0813,
            -0.023,
            0.0233,
            -0.1602
          ],
          "after": [
            0.0332,
            -0.1471,
            -0.2529,
            0.0825,
            0.0196,
            -0.0483,
            -0.1333,
            0.0697,
            0.0994,
            -0.0268,
            0.0253,
            0.0819,
            0.0373,
            -0.0089,
            -0.1282,
            0.0667
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0002,
            -0.0006,
            0.0007,
            -0.0009,
            0.0001,
            0.0018,
            -0.0002,
            -0.0008,
            0.0002,
            0.0015,
            0.0002,
            -0.0003,
            -0.0017,
            0.0004,
            0.0017
          ],
          "after": [
            -0.0525,
            0.0281,
            -0.1005,
            -0.0648,
            0.0723,
            0.1439,
            0.0749,
            -0.1556,
            0.0335,
            0.0016,
            -0.1136,
            -0.0612,
            0.1055,
            -0.0802,
            0.0177,
            0.1063
          ]
        }
      }
    },
    {
      "step": 325,
      "word": "dashiell",
      "loss": 2.5112,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0569,
            0.1948,
            0.1175,
            0.1357,
            -0.0476,
            0.0345,
            0.0817,
            0.027,
            -0.1306,
            0.1258,
            -0.0203,
            0.1118,
            0.1448,
            -0.0561,
            -0.0075,
            -0.0182
          ],
          "after": [
            -0.0143,
            0.0088,
            0.1719,
            -0.0291,
            0.036,
            -0.0397,
            -0.1061,
            -0.13,
            -0.0257,
            0.0873,
            0.1628,
            0.0112,
            -0.0842,
            -0.0244,
            -0.0485,
            -0.1778
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.005,
            0.0512,
            -0.1345,
            0.1236,
            -0.403,
            -0.0396,
            0.0152,
            0.0445,
            0.0025,
            -0.0515,
            0.0145,
            0.0103,
            0.0907,
            -0.0268,
            0.2331,
            0.1024
          ],
          "after": [
            0.0373,
            0.1193,
            -0.0406,
            -0.0542,
            0.0419,
            -0.0067,
            0.0919,
            0.1045,
            -0.0679,
            -0.0637,
            0.0221,
            -0.0066,
            0.0998,
            0.029,
            -0.0607,
            0.2169
          ]
        },
        "position_0": {
          "grad": [
            -0.0469,
            0.063,
            -0.0404,
            -0.0106,
            0.1222,
            -0.0568,
            0.1021,
            -0.0386,
            -0.0372,
            -0.0195,
            -0.1054,
            0.0199,
            0.0684,
            0.0427,
            0.0234,
            0.1644
          ],
          "after": [
            0.0338,
            -0.1466,
            -0.253,
            0.0829,
            0.02,
            -0.049,
            -0.133,
            0.0697,
            0.0989,
            -0.0266,
            0.0263,
            0.0818,
            0.0377,
            -0.0096,
            -0.1292,
            0.0668
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0031,
            -0.0015,
            -0.0026,
            -0.0003,
            -0.0053,
            -0.0013,
            -0.0015,
            -0.0007,
            0.0013,
            0.0034,
            0.0013,
            -0.0001,
            -0.0006,
            0.002,
            0.0011,
            -0.0009
          ],
          "after": [
            -0.053,
            0.0285,
            -0.1001,
            -0.0649,
            0.0742,
            0.1441,
            0.0753,
            -0.1556,
            0.0328,
            0.0012,
            -0.1138,
            -0.0617,
            0.1058,
            -0.0805,
            0.0173,
            0.106
          ]
        }
      }
    },
    {
      "step": 326,
      "word": "haaken",
      "loss": 2.1276,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2551,
            0.1205,
            0.4527,
            -0.0913,
            0.1539,
            0.0536,
            -0.1056,
            0.0824,
            0.1386,
            -0.1842,
            0.4224,
            0.2255,
            -0.3849,
            -0.1908,
            -0.106,
            -0.338
          ],
          "after": [
            -0.0141,
            0.0072,
            0.1712,
            -0.0295,
            0.0367,
            -0.0401,
            -0.106,
            -0.1302,
            -0.0258,
            0.0871,
            0.162,
            0.011,
            -0.0842,
            -0.0245,
            -0.0487,
            -0.177
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1092,
            -0.3085,
            0.0199,
            0.0475,
            -0.1174,
            0.2369,
            -0.369,
            -0.0729,
            0.1543,
            -0.0687,
            0.0207,
            -0.057,
            -0.0147,
            0.1707,
            0.0431,
            -0.2354
          ],
          "after": [
            0.0376,
            0.1192,
            -0.041,
            -0.0542,
            0.0428,
            -0.007,
            0.0921,
            0.1051,
            -0.0684,
            -0.0637,
            0.0215,
            -0.0068,
            0.0998,
            0.0287,
            -0.0611,
            0.2173
          ]
        },
        "position_0": {
          "grad": [
            0.0259,
            -0.0203,
            0.0458,
            0.0449,
            -0.0823,
            0.0566,
            0.0381,
            0.0684,
            0.0695,
            0.0577,
            -0.1528,
            -0.1696,
            0.2042,
            0.0979,
            0.0256,
            0.1206
          ],
          "after": [
            0.0343,
            -0.1461,
            -0.2531,
            0.0832,
            0.0206,
            -0.0499,
            -0.1328,
            0.0695,
            0.0983,
            -0.0267,
            0.0274,
            0.0822,
            0.0377,
            -0.0104,
            -0.13,
            0.0668
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0004,
            -0.0021,
            0.004,
            -0.0021,
            0.0074,
            -0.0014,
            0.0022,
            0.0023,
            0.0012,
            -0.0003,
            0.0054,
            -0.0034,
            0.0011,
            0.0002,
            0.0033
          ],
          "after": [
            -0.0533,
            0.029,
            -0.0994,
            -0.0655,
            0.076,
            0.1433,
            0.0758,
            -0.156,
            0.0317,
            0.0007,
            -0.1139,
            -0.0628,
            0.1064,
            -0.0809,
            0.0169,
            0.1053
          ]
        }
      }
    },
    {
      "step": 327,
      "word": "adair",
      "loss": 2.225,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.22,
            -0.1742,
            0.1953,
            -0.2299,
            0.2935,
            -0.1066,
            -0.0639,
            -0.0687,
            -0.0204,
            -0.0672,
            0.0914,
            -0.1669,
            -0.0863,
            0.0437,
            0.0299,
            -0.0487
          ],
          "after": [
            -0.0136,
            0.0063,
            0.1702,
            -0.0294,
            0.0369,
            -0.04,
            -0.1057,
            -0.1301,
            -0.0258,
            0.0871,
            0.161,
            0.0112,
            -0.084,
            -0.0246,
            -0.049,
            -0.1763
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0008,
            0.1032,
            -0.0977,
            0.0499,
            0.1506,
            -0.1818,
            0.2944,
            -0.0147,
            -0.2678,
            0.069,
            -0.0158,
            -0.0464,
            0.1676,
            -0.1311,
            -0.3433,
            0.1566
          ],
          "after": [
            0.0379,
            0.1189,
            -0.0412,
            -0.0544,
            0.0433,
            -0.0071,
            0.0919,
            0.1058,
            -0.0684,
            -0.064,
            0.021,
            -0.0069,
            0.0996,
            0.0287,
            -0.061,
            0.2174
          ]
        },
        "position_0": {
          "grad": [
            0.0746,
            -0.0791,
            0.0872,
            -0.1768,
            0.0434,
            0.0295,
            -0.2889,
            -0.0967,
            0.1361,
            0.0713,
            0.3205,
            0.1567,
            -0.2118,
            -0.0628,
            -0.0862,
            -0.2349
          ],
          "after": [
            0.0343,
            -0.1455,
            -0.2536,
            0.0838,
            0.021,
            -0.0507,
            -0.1322,
            0.0697,
            0.0976,
            -0.0272,
            0.0277,
            0.0821,
            0.0381,
            -0.011,
            -0.1306,
            0.0671
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0002,
            -0.0014,
            -0.001,
            0.0009,
            0.0003,
            0.0015,
            0.0011,
            0.0003,
            -0.0021,
            -0.0019,
            -0.0006,
            0.0004,
            -0.0008,
            0.0016,
            0.0011
          ],
          "after": [
            -0.0534,
            0.0293,
            -0.0986,
            -0.0659,
            0.0774,
            0.1427,
            0.076,
            -0.1565,
            0.0307,
            0.0006,
            -0.1137,
            -0.0637,
            0.1068,
            -0.0811,
            0.0164,
            0.1046
          ]
        }
      }
    },
    {
      "step": 328,
      "word": "amran",
      "loss": 1.8682,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1057,
            0.2045,
            -0.0572,
            0.1105,
            0.0057,
            -0.1207,
            0.382,
            0.2131,
            0.0962,
            0.0017,
            0.0917,
            0.052,
            -0.2547,
            -0.2196,
            0.0803,
            -0.031
          ],
          "after": [
            -0.013,
            0.0051,
            0.1694,
            -0.0295,
            0.0371,
            -0.0395,
            -0.1063,
            -0.1307,
            -0.0261,
            0.0871,
            0.16,
            0.0112,
            -0.0835,
            -0.0242,
            -0.0494,
            -0.1756
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0243,
            0.0368,
            -0.1225,
            0.0474,
            0.0338,
            -0.1572,
            0.2053,
            -0.0001,
            -0.2106,
            0.097,
            -0.0338,
            -0.0204,
            0.1681,
            -0.0886,
            -0.2327,
            0.1457
          ],
          "after": [
            0.0382,
            0.1186,
            -0.0411,
            -0.0546,
            0.0436,
            -0.0069,
            0.0916,
            0.1063,
            -0.0683,
            -0.0644,
            0.0206,
            -0.0069,
            0.0991,
            0.0289,
            -0.0607,
            0.2172
          ]
        },
        "position_0": {
          "grad": [
            0.0692,
            -0.0812,
            0.0825,
            -0.1508,
            0.0243,
            0.0345,
            -0.2638,
            -0.0957,
            0.1236,
            0.0693,
            0.3031,
            0.1582,
            -0.2006,
            -0.0445,
            -0.0745,
            -0.2265
          ],
          "after": [
            0.034,
            -0.1448,
            -0.2543,
            0.0846,
            0.0212,
            -0.0516,
            -0.1314,
            0.0701,
            0.0968,
            -0.0278,
            0.0272,
            0.0815,
            0.0387,
            -0.0113,
            -0.1308,
            0.0677
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.001,
            -0.0009,
            0.0004,
            0.0013,
            -0.0003,
            0.0011,
            0.0017,
            0.0003,
            -0.0023,
            -0.0028,
            0.0006,
            0.0012,
            0.0001,
            -0.0,
            0.0024
          ],
          "after": [
            -0.0533,
            0.0298,
            -0.0978,
            -0.0663,
            0.0785,
            0.1421,
            0.0762,
            -0.1573,
            0.0299,
            0.0008,
            -0.1131,
            -0.0645,
            0.1071,
            -0.0813,
            0.0159,
            0.1036
          ]
        }
      }
    },
    {
      "step": 329,
      "word": "amaziah",
      "loss": 2.1825,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.5198,
            0.159,
            0.4782,
            0.4037,
            0.7029,
            -0.0981,
            0.712,
            0.2157,
            -0.1092,
            -0.0602,
            -0.0169,
            0.437,
            0.1031,
            -0.4687,
            -0.3136,
            0.4988
          ],
          "after": [
            -0.0116,
            0.0036,
            0.1679,
            -0.0303,
            0.0361,
            -0.0387,
            -0.1079,
            -0.1319,
            -0.026,
            0.0873,
            0.1592,
            0.0105,
            -0.0831,
            -0.0228,
            -0.049,
            -0.1758
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0766,
            0.0447,
            -0.1041,
            0.0289,
            0.129,
            -0.1499,
            0.1947,
            0.0023,
            -0.2026,
            0.0538,
            -0.0264,
            0.0026,
            0.117,
            -0.1054,
            -0.249,
            0.148
          ],
          "after": [
            0.0386,
            0.1183,
            -0.0409,
            -0.0549,
            0.0438,
            -0.0065,
            0.0911,
            0.1067,
            -0.0679,
            -0.0649,
            0.0204,
            -0.0069,
            0.0986,
            0.0292,
            -0.0602,
            0.2169
          ]
        },
        "position_0": {
          "grad": [
            0.0425,
            -0.0229,
            0.0509,
            -0.109,
            0.0961,
            0.027,
            -0.1593,
            -0.1045,
            0.0472,
            0.0879,
            0.2563,
            0.1794,
            -0.1254,
            -0.0616,
            -0.1153,
            -0.1265
          ],
          "after": [
            0.0336,
            -0.1441,
            -0.2551,
            0.0855,
            0.0212,
            -0.0524,
            -0.1304,
            0.071,
            0.096,
            -0.0288,
            0.0263,
            0.0805,
            0.0394,
            -0.0113,
            -0.1308,
            0.0684
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            0.001,
            0.0031,
            0.0021,
            -0.0029,
            -0.0012,
            -0.0024,
            -0.0024,
            -0.0002,
            0.0045,
            0.0043,
            0.0023,
            -0.0011,
            0.0006,
            -0.0023,
            -0.0018
          ],
          "after": [
            -0.0535,
            0.0301,
            -0.0976,
            -0.0669,
            0.0798,
            0.1418,
            0.0765,
            -0.1575,
            0.0292,
            0.0004,
            -0.1133,
            -0.0654,
            0.1075,
            -0.0816,
            0.0158,
            0.1031
          ]
        }
      }
    },
    {
      "step": 330,
      "word": "kennedie",
      "loss": 2.3488,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0104,
            0.0024,
            0.1666,
            -0.031,
            0.0352,
            -0.038,
            -0.1092,
            -0.1329,
            -0.026,
            0.0874,
            0.1584,
            0.0099,
            -0.0829,
            -0.0216,
            -0.0486,
            -0.176
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.531,
            -0.2569,
            0.1239,
            0.0042,
            -0.4072,
            0.4318,
            -0.9849,
            -0.0204,
            0.556,
            -0.1258,
            0.1154,
            -0.1506,
            -0.1599,
            0.4205,
            0.369,
            -0.5916
          ],
          "after": [
            0.0379,
            0.1185,
            -0.0409,
            -0.0551,
            0.0445,
            -0.0068,
            0.0917,
            0.1072,
            -0.0682,
            -0.065,
            0.0199,
            -0.0066,
            0.0983,
            0.0287,
            -0.0601,
            0.2175
          ]
        },
        "position_0": {
          "grad": [
            -0.0472,
            0.0391,
            -0.0865,
            0.0666,
            -0.0249,
            -0.0738,
            0.1602,
            0.0076,
            -0.0499,
            -0.05,
            -0.0492,
            -0.0978,
            0.0672,
            0.0083,
            -0.0635,
            0.0147
          ],
          "after": [
            0.0335,
            -0.1436,
            -0.2554,
            0.0862,
            0.0212,
            -0.0528,
            -0.1298,
            0.0716,
            0.0954,
            -0.0294,
            0.0256,
            0.08,
            0.0399,
            -0.0114,
            -0.1306,
            0.069
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0029,
            -0.0008,
            -0.0005,
            0.0013,
            -0.0035,
            0.0002,
            -0.0053,
            0.0005,
            0.0015,
            0.001,
            0.0004,
            -0.001,
            -0.0005,
            0.0014,
            0.0002,
            -0.0013
          ],
          "after": [
            -0.0541,
            0.0304,
            -0.0973,
            -0.0675,
            0.0813,
            0.1415,
            0.0772,
            -0.1578,
            0.0283,
            -0.0001,
            -0.1135,
            -0.0661,
            0.1078,
            -0.082,
            0.0156,
            0.1027
          ]
        }
      }
    },
    {
      "step": 331,
      "word": "renata",
      "loss": 2.1582,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0451,
            -0.1532,
            0.2049,
            -0.0228,
            0.0894,
            0.0244,
            0.0986,
            -0.0252,
            -0.2756,
            0.0873,
            0.1001,
            -0.3292,
            0.2579,
            -0.0082,
            -0.1653,
            -0.022
          ],
          "after": [
            -0.0094,
            0.0017,
            0.1652,
            -0.0316,
            0.0344,
            -0.0375,
            -0.1105,
            -0.1336,
            -0.0253,
            0.0873,
            0.1576,
            0.0099,
            -0.0831,
            -0.0206,
            -0.0479,
            -0.1761
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0759,
            -0.1242,
            0.2899,
            -0.09,
            0.5121,
            0.0653,
            -0.4023,
            0.0057,
            0.395,
            0.0184,
            0.1913,
            0.2727,
            -0.4097,
            0.0441,
            -0.0487,
            -0.1415
          ],
          "after": [
            0.0371,
            0.1188,
            -0.0414,
            -0.0551,
            0.0444,
            -0.0072,
            0.0925,
            0.1076,
            -0.0689,
            -0.0651,
            0.0189,
            -0.0069,
            0.0987,
            0.0282,
            -0.06,
            0.2181
          ]
        },
        "position_0": {
          "grad": [
            0.0345,
            -0.0254,
            0.0024,
            -0.0754,
            -0.046,
            0.0778,
            0.0804,
            -0.0007,
            0.1356,
            -0.1129,
            -0.1568,
            -0.0743,
            -0.1378,
            -0.0425,
            0.1358,
            0.1076
          ],
          "after": [
            0.0332,
            -0.1431,
            -0.2557,
            0.0869,
            0.0214,
            -0.0534,
            -0.1294,
            0.0722,
            0.0947,
            -0.0294,
            0.0253,
            0.0797,
            0.0406,
            -0.0113,
            -0.1308,
            0.0693
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0009,
            0.0023,
            -0.0035,
            0.0004,
            -0.002,
            -0.0036,
            -0.0015,
            -0.0006,
            0.0023,
            0.0029,
            0.0007,
            -0.0028,
            -0.001,
            0.0004,
            -0.0027
          ],
          "after": [
            -0.0545,
            0.0308,
            -0.0975,
            -0.0677,
            0.0825,
            0.1415,
            0.0781,
            -0.1578,
            0.0277,
            -0.0008,
            -0.1141,
            -0.0668,
            0.1084,
            -0.0823,
            0.0155,
            0.1028
          ]
        }
      }
    },
    {
      "step": 332,
      "word": "kelsie",
      "loss": 2.349,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0086,
            0.0011,
            0.1639,
            -0.0321,
            0.0337,
            -0.0371,
            -0.1117,
            -0.1343,
            -0.0248,
            0.0872,
            0.1568,
            0.0099,
            -0.0833,
            -0.0197,
            -0.0472,
            -0.1762
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3267,
            -0.2813,
            0.131,
            -0.0153,
            -0.3316,
            0.3133,
            -0.8069,
            -0.0917,
            0.6942,
            -0.0033,
            0.2676,
            0.1834,
            -0.2996,
            0.2248,
            0.4516,
            -0.495
          ],
          "after": [
            0.0358,
            0.1195,
            -0.0421,
            -0.055,
            0.0447,
            -0.0079,
            0.0938,
            0.1082,
            -0.0703,
            -0.0652,
            0.0173,
            -0.0076,
            0.0995,
            0.0274,
            -0.0604,
            0.2192
          ]
        },
        "position_0": {
          "grad": [
            -0.0598,
            0.0798,
            -0.1037,
            0.1075,
            -0.0044,
            -0.0925,
            0.2302,
            -0.0099,
            -0.1009,
            -0.0547,
            -0.0585,
            -0.122,
            0.1486,
            0.0164,
            -0.1014,
            0.0549
          ],
          "after": [
            0.0332,
            -0.1429,
            -0.2556,
            0.0872,
            0.0215,
            -0.0536,
            -0.1294,
            0.0727,
            0.0943,
            -0.0291,
            0.0252,
            0.0799,
            0.0409,
            -0.0113,
            -0.1307,
            0.0695
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0034,
            0.0055,
            0.0069,
            -0.0072,
            0.0003,
            -0.0065,
            -0.0015,
            0.0027,
            0.0006,
            0.0012,
            -0.0009,
            0.0033,
            0.0024,
            -0.0023,
            0.0027
          ],
          "after": [
            -0.0549,
            0.0307,
            -0.0984,
            -0.0686,
            0.0843,
            0.1415,
            0.0793,
            -0.1575,
            0.0267,
            -0.0015,
            -0.1147,
            -0.0672,
            0.1085,
            -0.0829,
            0.0156,
            0.1026
          ]
        }
      }
    },
    {
      "step": 333,
      "word": "jaeleen",
      "loss": 1.9315,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0769,
            -0.2229,
            0.0924,
            0.0136,
            -0.0828,
            0.1493,
            -0.2404,
            -0.0077,
            0.1642,
            0.1203,
            0.0742,
            0.0521,
            -0.2738,
            -0.0568,
            0.0522,
            -0.3101
          ],
          "after": [
            -0.0077,
            0.0012,
            0.1627,
            -0.0325,
            0.0332,
            -0.0373,
            -0.1122,
            -0.1348,
            -0.0247,
            0.0868,
            0.156,
            0.0099,
            -0.083,
            -0.0188,
            -0.0468,
            -0.1759
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0318,
            -0.1333,
            -0.1774,
            -0.0398,
            -0.5257,
            0.8592,
            -0.3921,
            0.1938,
            0.6654,
            -0.375,
            -0.3386,
            0.1356,
            -0.2663,
            0.4801,
            1.1193,
            -0.2385
          ],
          "after": [
            0.0348,
            0.1204,
            -0.0424,
            -0.0548,
            0.0457,
            -0.0097,
            0.0953,
            0.108,
            -0.0721,
            -0.0644,
            0.017,
            -0.0084,
            0.1006,
            0.0259,
            -0.0618,
            0.2205
          ]
        },
        "position_0": {
          "grad": [
            0.0045,
            0.1414,
            0.0283,
            0.2056,
            0.0506,
            -0.1282,
            0.2896,
            0.0349,
            -0.3162,
            -0.0614,
            -0.081,
            -0.0298,
            0.3987,
            -0.1418,
            -0.1583,
            0.2426
          ],
          "after": [
            0.0332,
            -0.1432,
            -0.2556,
            0.0871,
            0.0215,
            -0.0533,
            -0.1298,
            0.073,
            0.0944,
            -0.0286,
            0.0254,
            0.0801,
            0.0406,
            -0.0109,
            -0.1302,
            0.0693
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.002,
            -0.0006,
            -0.0037,
            0.0013,
            -0.003,
            0.0048,
            -0.0001,
            0.0029,
            0.0015,
            -0.0015,
            -0.0033,
            0.0034,
            -0.0029,
            0.0013,
            0.0051,
            0.0031
          ],
          "after": [
            -0.0549,
            0.0307,
            -0.0987,
            -0.0696,
            0.0862,
            0.1409,
            0.0804,
            -0.1578,
            0.0255,
            -0.0019,
            -0.1148,
            -0.068,
            0.109,
            -0.0836,
            0.0152,
            0.1019
          ]
        }
      }
    },
    {
      "step": 334,
      "word": "kymbrie",
      "loss": 2.793,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.007,
            0.0012,
            0.1616,
            -0.0329,
            0.0327,
            -0.0374,
            -0.1127,
            -0.1353,
            -0.0246,
            0.0864,
            0.1553,
            0.0098,
            -0.0827,
            -0.0181,
            -0.0465,
            -0.1755
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1719,
            -0.0353,
            -0.2018,
            0.1879,
            -0.4606,
            -0.1343,
            0.0187,
            0.0782,
            -0.2147,
            0.0395,
            -0.0298,
            -0.1439,
            0.2684,
            0.0146,
            0.1469,
            0.0429
          ],
          "after": [
            0.0336,
            0.1211,
            -0.0422,
            -0.0552,
            0.0471,
            -0.011,
            0.0966,
            0.1076,
            -0.0735,
            -0.0638,
            0.0167,
            -0.0088,
            0.1011,
            0.0246,
            -0.0631,
            0.2216
          ]
        },
        "position_0": {
          "grad": [
            -0.0546,
            0.1074,
            -0.0856,
            0.1227,
            0.0652,
            -0.056,
            0.2865,
            -0.04,
            -0.1628,
            -0.0174,
            -0.0072,
            -0.0505,
            0.1769,
            -0.0005,
            -0.1125,
            0.1084
          ],
          "after": [
            0.0335,
            -0.1437,
            -0.2553,
            0.0868,
            0.0214,
            -0.0528,
            -0.1306,
            0.0734,
            0.0948,
            -0.0282,
            0.0255,
            0.0804,
            0.04,
            -0.0105,
            -0.1296,
            0.069
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            -0.0015,
            0.0,
            0.0056,
            -0.0025,
            -0.0044,
            0.0018,
            0.0011,
            -0.0054,
            0.0039,
            -0.0001,
            -0.0017,
            0.0066,
            0.0005,
            -0.0019,
            0.0048
          ],
          "after": [
            -0.0551,
            0.0309,
            -0.0989,
            -0.0711,
            0.0881,
            0.1409,
            0.0811,
            -0.1582,
            0.0255,
            -0.0027,
            -0.1149,
            -0.0685,
            0.1086,
            -0.0843,
            0.015,
            0.1008
          ]
        }
      }
    },
    {
      "step": 335,
      "word": "kosisochukwu",
      "loss": 4.0971,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0064,
            0.0012,
            0.1607,
            -0.0333,
            0.0324,
            -0.0376,
            -0.1131,
            -0.1357,
            -0.0245,
            0.0861,
            0.1547,
            0.0098,
            -0.0825,
            -0.0174,
            -0.0462,
            -0.1752
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0985,
            0.0724,
            -0.0363,
            0.0539,
            0.1922,
            -0.2105,
            0.2459,
            0.0221,
            -0.1796,
            0.0157,
            0.0032,
            0.1098,
            0.0278,
            -0.1572,
            -0.2412,
            0.2607
          ],
          "after": [
            0.0327,
            0.1217,
            -0.0421,
            -0.0556,
            0.048,
            -0.0118,
            0.0975,
            0.1071,
            -0.0744,
            -0.0633,
            0.0165,
            -0.0094,
            0.1015,
            0.0238,
            -0.064,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0443,
            0.069,
            -0.0542,
            0.0681,
            0.0736,
            -0.0353,
            0.1597,
            -0.0341,
            -0.0927,
            -0.0051,
            0.0135,
            -0.0055,
            0.0973,
            -0.0111,
            -0.0892,
            0.0681
          ],
          "after": [
            0.0339,
            -0.1443,
            -0.2549,
            0.0864,
            0.021,
            -0.0522,
            -0.1315,
            0.0739,
            0.0953,
            -0.0277,
            0.0255,
            0.0807,
            0.0394,
            -0.0101,
            -0.1288,
            0.0686
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0084,
            0.0059,
            0.0056,
            0.0,
            0.0065,
            -0.0023,
            0.0058,
            0.0011,
            -0.0003,
            -0.006,
            -0.0012,
            0.0003,
            0.0033,
            -0.0033,
            -0.0006,
            0.0081
          ],
          "after": [
            -0.054,
            0.0302,
            -0.0999,
            -0.0723,
            0.089,
            0.1412,
            0.0813,
            -0.1587,
            0.0256,
            -0.0027,
            -0.1148,
            -0.0689,
            0.108,
            -0.0843,
            0.0149,
            0.0989
          ]
        }
      }
    },
    {
      "step": 336,
      "word": "calypso",
      "loss": 3.183,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0893,
            0.1738,
            -0.0877,
            -0.0688,
            0.097,
            0.0056,
            -0.0568,
            0.0646,
            -0.1339,
            0.0163,
            -0.0095,
            0.1902,
            0.0026,
            0.0031,
            0.0447,
            -0.1965
          ],
          "after": [
            -0.0057,
            0.0008,
            0.1601,
            -0.0334,
            0.0319,
            -0.0377,
            -0.1133,
            -0.1362,
            -0.0241,
            0.0858,
            0.1542,
            0.0094,
            -0.0823,
            -0.0169,
            -0.0461,
            -0.1747
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0015,
            0.0887,
            -0.1585,
            0.0613,
            0.0199,
            -0.2107,
            0.2981,
            0.0223,
            -0.2991,
            0.0726,
            0.0062,
            0.0074,
            0.1561,
            -0.1695,
            -0.2545,
            0.2276
          ],
          "after": [
            0.032,
            0.122,
            -0.0416,
            -0.0562,
            0.0488,
            -0.0122,
            0.098,
            0.1066,
            -0.0749,
            -0.0631,
            0.0164,
            -0.0099,
            0.1016,
            0.0233,
            -0.0645,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0089,
            0.0481,
            0.0044,
            0.1465,
            -0.0846,
            -0.0372,
            0.1741,
            0.0413,
            -0.1223,
            0.0118,
            -0.0094,
            -0.0638,
            0.1375,
            0.03,
            -0.0411,
            0.1076
          ],
          "after": [
            0.0342,
            -0.1449,
            -0.2545,
            0.0857,
            0.021,
            -0.0516,
            -0.1325,
            0.0742,
            0.0959,
            -0.0274,
            0.0256,
            0.0811,
            0.0387,
            -0.0099,
            -0.128,
            0.0681
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0027,
            -0.0011,
            -0.0002,
            0.0042,
            0.0034,
            -0.0025,
            0.0136,
            0.0017,
            -0.0089,
            0.0126,
            0.0071,
            0.0028,
            0.0059,
            -0.0121,
            -0.0057,
            0.0042
          ],
          "after": [
            -0.0527,
            0.0298,
            -0.1008,
            -0.0739,
            0.0893,
            0.1418,
            0.0805,
            -0.1595,
            0.027,
            -0.0041,
            -0.1156,
            -0.0696,
            0.1069,
            -0.0827,
            0.0154,
            0.0968
          ]
        }
      }
    },
    {
      "step": 337,
      "word": "phinley",
      "loss": 2.6987,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0052,
            0.0005,
            0.1596,
            -0.0335,
            0.0315,
            -0.0378,
            -0.1135,
            -0.1367,
            -0.0238,
            0.0855,
            0.1538,
            0.0091,
            -0.0821,
            -0.0164,
            -0.046,
            -0.1742
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0432,
            -0.2357,
            -0.0787,
            -0.0953,
            -0.1256,
            0.0735,
            -0.1289,
            0.065,
            0.0928,
            0.1109,
            -0.0586,
            -0.114,
            0.0494,
            0.0816,
            0.103,
            -0.1607
          ],
          "after": [
            0.0313,
            0.1226,
            -0.0411,
            -0.0564,
            0.0496,
            -0.0127,
            0.0985,
            0.1059,
            -0.0754,
            -0.0632,
            0.0163,
            -0.0101,
            0.1016,
            0.0229,
            -0.065,
            0.2226
          ]
        },
        "position_0": {
          "grad": [
            0.0821,
            0.1699,
            -0.0407,
            0.193,
            -0.0312,
            -0.1094,
            0.3703,
            0.0514,
            -0.2762,
            -0.0538,
            -0.1415,
            -0.1138,
            0.1169,
            0.1536,
            0.0166,
            0.3033
          ],
          "after": [
            0.0342,
            -0.1459,
            -0.254,
            0.0847,
            0.021,
            -0.0507,
            -0.1338,
            0.0742,
            0.0969,
            -0.0269,
            0.026,
            0.0818,
            0.0379,
            -0.0102,
            -0.1274,
            0.0672
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0013,
            0.0001,
            0.0,
            -0.0008,
            0.0011,
            -0.0006,
            -0.0008,
            -0.0009,
            0.0005,
            0.0003,
            0.0002,
            -0.001,
            0.0009,
            0.0006,
            -0.0008
          ],
          "after": [
            -0.0515,
            0.0296,
            -0.1015,
            -0.0753,
            0.0897,
            0.1421,
            0.0798,
            -0.16,
            0.0284,
            -0.0053,
            -0.1164,
            -0.0703,
            0.1061,
            -0.0815,
            0.0158,
            0.0951
          ]
        }
      }
    },
    {
      "step": 338,
      "word": "beasley",
      "loss": 2.4272,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0515,
            0.1402,
            0.07,
            0.0813,
            -0.0763,
            0.0222,
            0.0396,
            0.0599,
            -0.0711,
            0.0518,
            -0.0525,
            0.061,
            0.1275,
            -0.0482,
            0.0506,
            -0.0327
          ],
          "after": [
            -0.0046,
            -0.0001,
            0.159,
            -0.0338,
            0.0313,
            -0.038,
            -0.1138,
            -0.1373,
            -0.0234,
            0.0852,
            0.1535,
            0.0087,
            -0.0822,
            -0.0159,
            -0.046,
            -0.1737
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1437,
            -0.3958,
            0.1482,
            -0.2471,
            0.1937,
            0.4174,
            -0.563,
            0.0121,
            0.7291,
            0.038,
            0.105,
            0.1222,
            -0.532,
            0.0533,
            0.3601,
            -0.4762
          ],
          "after": [
            0.0304,
            0.1238,
            -0.041,
            -0.0559,
            0.05,
            -0.0136,
            0.0994,
            0.1052,
            -0.0766,
            -0.0634,
            0.0161,
            -0.0105,
            0.1023,
            0.0223,
            -0.0658,
            0.2235
          ]
        },
        "position_0": {
          "grad": [
            0.0425,
            0.0212,
            0.1021,
            -0.009,
            0.0654,
            -0.0433,
            0.0984,
            -0.0077,
            -0.1231,
            -0.0377,
            -0.0235,
            -0.0198,
            0.1794,
            -0.1016,
            -0.098,
            0.2573
          ],
          "after": [
            0.034,
            -0.1468,
            -0.254,
            0.0839,
            0.0208,
            -0.0497,
            -0.1351,
            0.0742,
            0.098,
            -0.0263,
            0.0263,
            0.0824,
            0.0369,
            -0.0101,
            -0.1267,
            0.0661
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            -0.0011,
            0.0006,
            0.0012,
            -0.0003,
            0.0059,
            -0.0051,
            -0.0019,
            0.0024,
            0.0017,
            -0.0018,
            0.0024,
            -0.0052,
            0.0025,
            0.0016,
            -0.0018
          ],
          "after": [
            -0.0503,
            0.0296,
            -0.1022,
            -0.0765,
            0.09,
            0.1417,
            0.0796,
            -0.1601,
            0.0291,
            -0.0065,
            -0.1169,
            -0.0711,
            0.1059,
            -0.0808,
            0.0159,
            0.0938
          ]
        }
      }
    },
    {
      "step": 339,
      "word": "ashtyn",
      "loss": 2.4871,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0504,
            0.1298,
            -0.038,
            0.2309,
            -0.2703,
            0.073,
            -0.0048,
            0.1441,
            -0.0112,
            -0.0016,
            -0.1788,
            0.0193,
            0.2254,
            0.1291,
            0.1985,
            0.0835
          ],
          "after": [
            -0.0042,
            -0.0009,
            0.1586,
            -0.0345,
            0.0316,
            -0.0385,
            -0.114,
            -0.1382,
            -0.023,
            0.0849,
            0.1538,
            0.0084,
            -0.0827,
            -0.0158,
            -0.0465,
            -0.1735
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0258,
            0.0592,
            -0.1249,
            0.0614,
            0.0899,
            -0.1515,
            0.2659,
            0.0119,
            -0.1869,
            0.0909,
            0.001,
            0.0153,
            0.1354,
            -0.1321,
            -0.259,
            0.185
          ],
          "after": [
            0.0297,
            0.1247,
            -0.0406,
            -0.0557,
            0.0503,
            -0.0142,
            0.0999,
            0.1046,
            -0.0774,
            -0.0637,
            0.0158,
            -0.0109,
            0.1028,
            0.0221,
            -0.0663,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            0.054,
            -0.0965,
            0.1001,
            -0.0862,
            -0.0735,
            0.0194,
            -0.2583,
            -0.0399,
            0.1553,
            -0.0115,
            0.2411,
            0.0455,
            -0.2006,
            0.031,
            0.0536,
            -0.1919
          ],
          "after": [
            0.0336,
            -0.1473,
            -0.2543,
            0.0834,
            0.0209,
            -0.049,
            -0.1358,
            0.0744,
            0.0986,
            -0.0257,
            0.0261,
            0.0828,
            0.0364,
            -0.0101,
            -0.1262,
            0.0654
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0005,
            -0.0012,
            0.0007,
            0.0005,
            -0.0002,
            0.0024,
            0.0002,
            0.0,
            0.0005,
            -0.0001,
            0.0004,
            0.0009,
            0.0,
            -0.0019,
            0.0015
          ],
          "after": [
            -0.0495,
            0.0296,
            -0.1027,
            -0.0777,
            0.0903,
            0.1414,
            0.0792,
            -0.1602,
            0.0298,
            -0.0076,
            -0.1172,
            -0.0718,
            0.1056,
            -0.0801,
            0.0163,
            0.0926
          ]
        }
      }
    },
    {
      "step": 340,
      "word": "maclyn",
      "loss": 2.1417,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0067,
            0.0062,
            0.0223,
            0.2161,
            -0.1421,
            -0.0521,
            0.1753,
            0.0931,
            -0.1549,
            0.0733,
            0.0549,
            -0.0618,
            0.126,
            0.061,
            -0.0759,
            0.0429
          ],
          "after": [
            -0.0038,
            -0.0017,
            0.1582,
            -0.0354,
            0.032,
            -0.0386,
            -0.1145,
            -0.1393,
            -0.0223,
            0.0844,
            0.1539,
            0.0082,
            -0.0833,
            -0.0158,
            -0.0468,
            -0.1733
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0266,
            0.1306,
            -0.2012,
            0.089,
            0.0235,
            -0.2072,
            0.3426,
            0.021,
            -0.3392,
            0.1203,
            -0.0056,
            -0.0241,
            0.24,
            -0.1713,
            -0.2817,
            0.3032
          ],
          "after": [
            0.0292,
            0.1253,
            -0.04,
            -0.0557,
            0.0505,
            -0.0144,
            0.1001,
            0.1041,
            -0.0778,
            -0.0643,
            0.0156,
            -0.0112,
            0.1028,
            0.0223,
            -0.0663,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            -0.0357,
            0.0457,
            -0.0145,
            0.0835,
            0.1612,
            0.0204,
            0.1989,
            0.1325,
            0.028,
            -0.0154,
            -0.0237,
            0.0745,
            -0.0738,
            -0.0595,
            0.0038,
            0.1515
          ],
          "after": [
            0.0334,
            -0.1479,
            -0.2546,
            0.0828,
            0.0205,
            -0.0484,
            -0.1367,
            0.0741,
            0.0991,
            -0.0252,
            0.026,
            0.083,
            0.0361,
            -0.01,
            -0.1257,
            0.0646
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0021,
            0.0019,
            -0.0018,
            0.0019,
            -0.0049,
            0.0039,
            -0.0026,
            0.0001,
            0.0022,
            0.003,
            0.0004,
            0.0026,
            -0.0024,
            -0.0032,
            -0.0007
          ],
          "after": [
            -0.0488,
            0.0292,
            -0.1033,
            -0.0785,
            0.0903,
            0.1417,
            0.0787,
            -0.1598,
            0.0303,
            -0.0088,
            -0.1179,
            -0.0725,
            0.1051,
            -0.0793,
            0.0169,
            0.0917
          ]
        }
      }
    },
    {
      "step": 341,
      "word": "tiger",
      "loss": 3.0812,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0035,
            -0.0023,
            0.1578,
            -0.0363,
            0.0324,
            -0.0388,
            -0.1149,
            -0.1402,
            -0.0217,
            0.084,
            0.154,
            0.008,
            -0.0838,
            -0.0158,
            -0.047,
            -0.1732
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0813,
            -0.1141,
            0.1933,
            0.0314,
            0.1346,
            -0.1622,
            0.0991,
            0.0276,
            -0.0071,
            -0.3117,
            -0.0996,
            -0.1078,
            -0.0255,
            -0.0701,
            0.0792,
            -0.1452
          ],
          "after": [
            0.0289,
            0.1259,
            -0.0398,
            -0.0558,
            0.0505,
            -0.0144,
            0.1002,
            0.1034,
            -0.078,
            -0.0641,
            0.0157,
            -0.0112,
            0.1029,
            0.0225,
            -0.0665,
            0.2244
          ]
        },
        "position_0": {
          "grad": [
            0.0072,
            -0.2149,
            0.1399,
            0.1674,
            -0.2318,
            0.0901,
            0.0744,
            0.0625,
            -0.0745,
            -0.2328,
            -0.2332,
            -0.4846,
            0.1775,
            0.1723,
            0.1965,
            0.1539
          ],
          "after": [
            0.0332,
            -0.1478,
            -0.2553,
            0.082,
            0.0208,
            -0.0483,
            -0.1376,
            0.0736,
            0.0996,
            -0.0238,
            0.0264,
            0.0844,
            0.0356,
            -0.0104,
            -0.1259,
            0.0637
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0025,
            0.0012,
            -0.0051,
            0.0046,
            -0.002,
            0.0012,
            -0.0014,
            0.0007,
            -0.0014,
            -0.002,
            -0.0044,
            0.001,
            0.0018,
            -0.0019,
            -0.0016
          ],
          "after": [
            -0.0483,
            0.0293,
            -0.104,
            -0.0786,
            0.0898,
            0.1421,
            0.0781,
            -0.1593,
            0.0307,
            -0.0096,
            -0.1182,
            -0.0726,
            0.1046,
            -0.0788,
            0.0176,
            0.091
          ]
        }
      }
    },
    {
      "step": 342,
      "word": "pheobe",
      "loss": 2.9072,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0032,
            -0.0028,
            0.1576,
            -0.037,
            0.0327,
            -0.0389,
            -0.1152,
            -0.141,
            -0.0212,
            0.0836,
            0.1541,
            0.0079,
            -0.0842,
            -0.0159,
            -0.0472,
            -0.1731
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0688,
            -0.2252,
            0.1494,
            -0.0804,
            -0.3181,
            0.5536,
            -0.5027,
            -0.0079,
            0.6076,
            0.0389,
            0.2438,
            0.0228,
            -0.4601,
            0.027,
            0.6773,
            -0.5862
          ],
          "after": [
            0.0285,
            0.1268,
            -0.0398,
            -0.0557,
            0.0509,
            -0.0151,
            0.1007,
            0.1029,
            -0.0789,
            -0.064,
            0.0151,
            -0.0112,
            0.1036,
            0.0226,
            -0.0673,
            0.2253
          ]
        },
        "position_0": {
          "grad": [
            0.0874,
            0.1905,
            -0.0741,
            0.1621,
            0.0167,
            -0.1172,
            0.3714,
            0.0171,
            -0.3124,
            -0.034,
            -0.1112,
            -0.0774,
            0.1248,
            0.1629,
            -0.0263,
            0.3294
          ],
          "after": [
            0.0326,
            -0.1482,
            -0.2556,
            0.0809,
            0.021,
            -0.0477,
            -0.1389,
            0.073,
            0.1006,
            -0.0224,
            0.0269,
            0.0857,
            0.035,
            -0.0113,
            -0.1259,
            0.0625
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0029,
            0.002,
            -0.0037,
            -0.0018,
            0.0005,
            0.0028,
            0.0036,
            0.0007,
            0.0042,
            0.0081,
            -0.0004,
            0.0005,
            -0.0061,
            -0.0042,
            -0.0057
          ],
          "after": [
            -0.0476,
            0.0297,
            -0.1049,
            -0.0782,
            0.0895,
            0.1425,
            0.0774,
            -0.1595,
            0.0309,
            -0.0108,
            -0.1195,
            -0.0726,
            0.1041,
            -0.0776,
            0.0186,
            0.0912
          ]
        }
      }
    },
    {
      "step": 343,
      "word": "kainan",
      "loss": 1.7768,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0976,
            -0.2657,
            0.0057,
            -0.0597,
            0.044,
            0.0917,
            -0.2694,
            0.0641,
            0.0866,
            -0.199,
            0.055,
            -0.0438,
            -0.3543,
            -0.0174,
            0.0432,
            -0.0863
          ],
          "after": [
            -0.0028,
            -0.0026,
            0.1573,
            -0.0375,
            0.0329,
            -0.0393,
            -0.115,
            -0.1419,
            -0.021,
            0.0839,
            0.154,
            0.0079,
            -0.084,
            -0.0159,
            -0.0475,
            -0.1729
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0656,
            0.0659,
            -0.0777,
            0.0113,
            0.051,
            -0.155,
            0.2452,
            0.0331,
            -0.1815,
            0.0735,
            -0.0101,
            -0.0117,
            0.1037,
            -0.1121,
            -0.1563,
            0.1632
          ],
          "after": [
            0.0283,
            0.1275,
            -0.0398,
            -0.0556,
            0.0512,
            -0.0155,
            0.1009,
            0.1024,
            -0.0794,
            -0.064,
            0.0147,
            -0.0113,
            0.1041,
            0.0229,
            -0.0678,
            0.2259
          ]
        },
        "position_0": {
          "grad": [
            -0.0529,
            0.0232,
            -0.1244,
            0.0081,
            0.0136,
            -0.0785,
            0.1258,
            -0.0206,
            -0.0256,
            0.0089,
            -0.0229,
            -0.0472,
            0.0358,
            -0.0362,
            -0.1132,
            -0.0639
          ],
          "after": [
            0.0324,
            -0.1486,
            -0.2554,
            0.08,
            0.0211,
            -0.0469,
            -0.1401,
            0.0727,
            0.1015,
            -0.0213,
            0.0275,
            0.087,
            0.0343,
            -0.0119,
            -0.1256,
            0.0615
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.002,
            -0.0002,
            -0.0006,
            0.0027,
            -0.0038,
            0.0057,
            -0.0039,
            0.0027,
            0.0026,
            -0.0045,
            -0.0039,
            0.0026,
            -0.0028,
            0.0027,
            0.0032,
            0.0046
          ],
          "after": [
            -0.0467,
            0.0302,
            -0.1056,
            -0.0782,
            0.0898,
            0.1421,
            0.0771,
            -0.16,
            0.0306,
            -0.0113,
            -0.1201,
            -0.0729,
            0.104,
            -0.077,
            0.0191,
            0.0908
          ]
        }
      }
    },
    {
      "step": 344,
      "word": "mekai",
      "loss": 2.5192,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0161,
            -0.4015,
            0.0959,
            -0.0745,
            0.0779,
            0.0447,
            -0.3296,
            0.038,
            0.0469,
            -0.0011,
            0.1636,
            -0.1894,
            -0.1658,
            0.1481,
            0.0196,
            -0.2411
          ],
          "after": [
            -0.0025,
            -0.0016,
            0.1569,
            -0.0378,
            0.033,
            -0.0399,
            -0.1143,
            -0.1428,
            -0.0209,
            0.0841,
            0.1535,
            0.0081,
            -0.0835,
            -0.0162,
            -0.0478,
            -0.1723
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2041,
            -0.0356,
            0.4192,
            -0.1524,
            0.406,
            0.0121,
            -0.2144,
            -0.0938,
            0.4473,
            0.0912,
            0.2495,
            0.2179,
            -0.429,
            0.0028,
            -0.1651,
            -0.1442
          ],
          "after": [
            0.0277,
            0.1282,
            -0.0404,
            -0.0552,
            0.0509,
            -0.0158,
            0.1012,
            0.1023,
            -0.0804,
            -0.0643,
            0.0136,
            -0.0117,
            0.1051,
            0.0232,
            -0.0681,
            0.2265
          ]
        },
        "position_0": {
          "grad": [
            -0.0354,
            0.0215,
            -0.0128,
            0.0544,
            0.157,
            0.0125,
            0.1485,
            0.1572,
            0.0936,
            -0.0189,
            -0.0202,
            0.0733,
            -0.136,
            -0.0647,
            0.007,
            0.1145
          ],
          "after": [
            0.0323,
            -0.1491,
            -0.2552,
            0.0791,
            0.0209,
            -0.0463,
            -0.1413,
            0.0718,
            0.1021,
            -0.0202,
            0.028,
            0.0879,
            0.034,
            -0.0122,
            -0.1255,
            0.0605
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0021,
            -0.0034,
            0.0043,
            -0.0043,
            0.0099,
            0.0005,
            0.0,
            -0.0004,
            0.0037,
            -0.0062,
            -0.0032,
            -0.0041,
            0.0011,
            0.0015,
            -0.0078,
            0.0014
          ],
          "after": [
            -0.0463,
            0.031,
            -0.1068,
            -0.0777,
            0.0889,
            0.1417,
            0.0768,
            -0.1605,
            0.0298,
            -0.0111,
            -0.1202,
            -0.0726,
            0.1038,
            -0.0766,
            0.0203,
            0.0902
          ]
        }
      }
    },
    {
      "step": 345,
      "word": "charline",
      "loss": 2.4152,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0541,
            -0.0607,
            -0.003,
            -0.0453,
            0.099,
            0.0861,
            0.0491,
            -0.0718,
            0.0815,
            -0.0154,
            0.0611,
            0.065,
            -0.1142,
            -0.1113,
            -0.015,
            0.1063
          ],
          "after": [
            -0.0023,
            -0.0005,
            0.1565,
            -0.0379,
            0.0328,
            -0.0407,
            -0.1138,
            -0.1433,
            -0.0211,
            0.0844,
            0.1529,
            0.0083,
            -0.0829,
            -0.0162,
            -0.048,
            -0.1719
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2813,
            0.1611,
            -0.2747,
            -0.0414,
            -0.3137,
            0.0541,
            0.0233,
            -0.0107,
            -0.2725,
            0.0209,
            -0.0751,
            -0.4813,
            0.3623,
            0.1207,
            -0.117,
            0.0284
          ],
          "after": [
            0.0267,
            0.1284,
            -0.0405,
            -0.0547,
            0.0511,
            -0.0162,
            0.1015,
            0.1023,
            -0.0808,
            -0.0646,
            0.013,
            -0.0112,
            0.1054,
            0.0232,
            -0.0682,
            0.2271
          ]
        },
        "position_0": {
          "grad": [
            -0.0022,
            -0.0004,
            -0.0019,
            0.073,
            -0.1111,
            -0.0237,
            0.0705,
            0.0373,
            -0.0428,
            0.0137,
            0.0074,
            -0.0565,
            0.0538,
            0.0389,
            -0.0057,
            0.0249
          ],
          "after": [
            0.0323,
            -0.1494,
            -0.2551,
            0.0782,
            0.0209,
            -0.0457,
            -0.1425,
            0.0709,
            0.1026,
            -0.0194,
            0.0284,
            0.0888,
            0.0337,
            -0.0126,
            -0.1253,
            0.0597
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0049,
            0.0023,
            -0.0024,
            0.0018,
            -0.0051,
            0.0024,
            -0.0033,
            0.001,
            -0.0014,
            -0.0005,
            -0.0008,
            -0.0089,
            0.0034,
            0.0025,
            0.0014,
            -0.0007
          ],
          "after": [
            -0.0466,
            0.0314,
            -0.1075,
            -0.0775,
            0.0887,
            0.1411,
            0.0768,
            -0.161,
            0.0294,
            -0.0108,
            -0.1202,
            -0.0715,
            0.1032,
            -0.0767,
            0.0212,
            0.0899
          ]
        }
      }
    },
    {
      "step": 346,
      "word": "angelo",
      "loss": 2.5105,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0047,
            0.0413,
            -0.3312,
            0.0909,
            -0.5726,
            0.1204,
            -0.0974,
            -0.0181,
            0.0669,
            -0.2938,
            -0.1952,
            -0.1777,
            0.0354,
            0.2095,
            0.3858,
            0.1181
          ],
          "after": [
            -0.0022,
            0.0003,
            0.1569,
            -0.0382,
            0.0336,
            -0.0418,
            -0.1132,
            -0.1437,
            -0.0213,
            0.0854,
            0.1529,
            0.0087,
            -0.0825,
            -0.0167,
            -0.0491,
            -0.1719
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0936,
            -0.0908,
            0.2043,
            0.0358,
            0.1341,
            -0.2082,
            0.1031,
            0.0323,
            0.0149,
            -0.2744,
            -0.0374,
            -0.0001,
            -0.0849,
            -0.1196,
            0.0836,
            -0.0638
          ],
          "after": [
            0.026,
            0.1288,
            -0.0409,
            -0.0543,
            0.051,
            -0.0162,
            0.1017,
            0.1021,
            -0.0813,
            -0.0642,
            0.0125,
            -0.0107,
            0.1058,
            0.0235,
            -0.0684,
            0.2276
          ]
        },
        "position_0": {
          "grad": [
            0.0632,
            -0.0876,
            0.1133,
            -0.1168,
            -0.0281,
            0.027,
            -0.2812,
            -0.0548,
            0.1639,
            -0.01,
            0.242,
            0.0774,
            -0.2086,
            0.0063,
            -0.0104,
            -0.2191
          ],
          "after": [
            0.032,
            -0.1495,
            -0.2553,
            0.0777,
            0.021,
            -0.0453,
            -0.1431,
            0.0703,
            0.1029,
            -0.0187,
            0.0282,
            0.0894,
            0.0337,
            -0.013,
            -0.1251,
            0.0592
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0018,
            0.0007,
            0.001,
            0.0005,
            -0.0002,
            0.0019,
            -0.0008,
            -0.0017,
            -0.0007,
            0.0015,
            0.0003,
            0.001,
            -0.0014,
            -0.0017,
            0.0011
          ],
          "after": [
            -0.047,
            0.0315,
            -0.1082,
            -0.0774,
            0.0885,
            0.1407,
            0.0767,
            -0.1613,
            0.0292,
            -0.0105,
            -0.1204,
            -0.0705,
            0.1027,
            -0.0765,
            0.0221,
            0.0894
          ]
        }
      }
    },
    {
      "step": 347,
      "word": "schuyler",
      "loss": 3.065,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.002,
            0.001,
            0.1572,
            -0.0385,
            0.0343,
            -0.0427,
            -0.1127,
            -0.144,
            -0.0215,
            0.0863,
            0.153,
            0.0091,
            -0.0821,
            -0.0171,
            -0.0501,
            -0.1718
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1972,
            -0.2494,
            -0.1184,
            -0.0504,
            -0.0173,
            0.0772,
            -0.1128,
            0.064,
            -0.0315,
            0.148,
            -0.0599,
            -0.152,
            0.101,
            0.0784,
            -0.0166,
            -0.1225
          ],
          "after": [
            0.025,
            0.1296,
            -0.0411,
            -0.0539,
            0.051,
            -0.0164,
            0.1019,
            0.1017,
            -0.0816,
            -0.0642,
            0.0122,
            -0.01,
            0.106,
            0.0235,
            -0.0685,
            0.2282
          ]
        },
        "position_0": {
          "grad": [
            0.001,
            0.0775,
            -0.0027,
            0.1055,
            -0.1167,
            0.0665,
            0.0101,
            0.045,
            -0.0349,
            0.0055,
            -0.0712,
            0.0219,
            0.0872,
            0.0343,
            0.1182,
            0.0358
          ],
          "after": [
            0.0317,
            -0.1497,
            -0.2556,
            0.077,
            0.0214,
            -0.0452,
            -0.1437,
            0.0697,
            0.1031,
            -0.0181,
            0.0282,
            0.0899,
            0.0336,
            -0.0134,
            -0.1252,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0034,
            0.0009,
            -0.0026,
            0.0017,
            -0.0002,
            0.0011,
            0.0019,
            0.0002,
            -0.0003,
            0.001,
            -0.0015,
            -0.0011,
            0.0015,
            0.0019,
            -0.0023,
            0.0023
          ],
          "after": [
            -0.0478,
            0.0314,
            -0.1083,
            -0.0775,
            0.0883,
            0.1401,
            0.0765,
            -0.1616,
            0.0292,
            -0.0103,
            -0.1204,
            -0.0695,
            0.1021,
            -0.0767,
            0.0231,
            0.0888
          ]
        }
      }
    },
    {
      "step": 348,
      "word": "solstice",
      "loss": 2.5475,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0019,
            0.0015,
            0.1575,
            -0.0387,
            0.0349,
            -0.0435,
            -0.1123,
            -0.1443,
            -0.0217,
            0.087,
            0.153,
            0.0094,
            -0.0817,
            -0.0175,
            -0.0509,
            -0.1717
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1031,
            -0.0479,
            -0.0755,
            -0.0922,
            -0.2069,
            0.0602,
            -0.0491,
            0.0234,
            0.0125,
            0.1908,
            0.0088,
            -0.2644,
            0.1467,
            0.058,
            -0.0617,
            -0.2577
          ],
          "after": [
            0.024,
            0.1302,
            -0.0411,
            -0.0533,
            0.0513,
            -0.0166,
            0.1021,
            0.1013,
            -0.0819,
            -0.0647,
            0.012,
            -0.0088,
            0.1059,
            0.0235,
            -0.0686,
            0.229
          ]
        },
        "position_0": {
          "grad": [
            -0.0033,
            0.0626,
            -0.0033,
            0.068,
            -0.1189,
            0.0653,
            -0.0212,
            0.0418,
            -0.0177,
            0.0148,
            -0.0612,
            0.0245,
            0.0719,
            0.0162,
            0.0984,
            0.003
          ],
          "after": [
            0.0315,
            -0.1501,
            -0.2557,
            0.0763,
            0.022,
            -0.0454,
            -0.1441,
            0.069,
            0.1033,
            -0.0176,
            0.0284,
            0.0902,
            0.0334,
            -0.0138,
            -0.1256,
            0.0585
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0019,
            0.0005,
            -0.0032,
            0.0008,
            0.0002,
            0.0035,
            -0.001,
            0.0019,
            0.0038,
            0.0009,
            -0.0017,
            0.0011,
            -0.0001,
            -0.0046,
            -0.001
          ],
          "after": [
            -0.0482,
            0.0311,
            -0.1086,
            -0.0773,
            0.0881,
            0.1396,
            0.076,
            -0.1617,
            0.0288,
            -0.0106,
            -0.1205,
            -0.0685,
            0.1014,
            -0.0767,
            0.0244,
            0.0883
          ]
        }
      }
    },
    {
      "step": 349,
      "word": "matt",
      "loss": 2.8062,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0583,
            -0.4008,
            0.1561,
            0.2303,
            -0.1752,
            0.1021,
            0.133,
            0.0336,
            -0.2691,
            -0.2324,
            -0.1702,
            -0.6025,
            0.3461,
            0.2024,
            0.0977,
            0.2162
          ],
          "after": [
            -0.0019,
            0.0029,
            0.1574,
            -0.0393,
            0.0356,
            -0.0446,
            -0.1121,
            -0.1446,
            -0.0213,
            0.0883,
            0.1534,
            0.0106,
            -0.082,
            -0.0183,
            -0.0518,
            -0.172
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0349,
            0.1582,
            -0.2248,
            0.0987,
            -0.0439,
            -0.1322,
            0.2769,
            0.007,
            -0.2697,
            0.0752,
            -0.0109,
            -0.1404,
            0.2942,
            -0.0702,
            -0.2298,
            0.1408
          ],
          "after": [
            0.0231,
            0.1306,
            -0.0407,
            -0.0531,
            0.0516,
            -0.0166,
            0.1021,
            0.1009,
            -0.0819,
            -0.0652,
            0.0118,
            -0.0076,
            0.1055,
            0.0235,
            -0.0684,
            0.2296
          ]
        },
        "position_0": {
          "grad": [
            -0.0237,
            -0.0092,
            0.0009,
            0.082,
            0.1409,
            0.0214,
            0.1509,
            0.1961,
            0.1383,
            -0.0379,
            -0.0283,
            0.0346,
            -0.2009,
            -0.0449,
            0.0734,
            0.1089
          ],
          "after": [
            0.0314,
            -0.1504,
            -0.2559,
            0.0755,
            0.0222,
            -0.0456,
            -0.1446,
            0.0677,
            0.1033,
            -0.017,
            0.0285,
            0.0903,
            0.0335,
            -0.014,
            -0.1261,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0029,
            0.0086,
            0.0003,
            0.0035,
            0.003,
            -0.0019,
            0.0073,
            -0.0054,
            0.0012,
            0.0036,
            0.0043,
            -0.0051,
            0.0085,
            0.0004,
            -0.007,
            -0.0048
          ],
          "after": [
            -0.0491,
            0.0298,
            -0.1088,
            -0.0774,
            0.0876,
            0.1394,
            0.0751,
            -0.1609,
            0.0284,
            -0.0113,
            -0.1211,
            -0.0671,
            0.1001,
            -0.0769,
            0.0262,
            0.0885
          ]
        }
      }
    },
    {
      "step": 350,
      "word": "kiyani",
      "loss": 2.138,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0421,
            0.1186,
            -0.2274,
            -0.0297,
            -0.1988,
            -0.0864,
            0.0224,
            -0.0393,
            0.0538,
            0.0177,
            -0.0054,
            -0.0196,
            -0.0377,
            -0.0167,
            0.1217,
            -0.0295
          ],
          "after": [
            -0.002,
            0.0038,
            0.1578,
            -0.0398,
            0.0366,
            -0.0452,
            -0.112,
            -0.1448,
            -0.021,
            0.0893,
            0.1538,
            0.0117,
            -0.0822,
            -0.0189,
            -0.0529,
            -0.1722
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0923,
            0.0775,
            -0.0698,
            -0.0207,
            0.1364,
            -0.1646,
            0.2832,
            0.0199,
            -0.2206,
            0.0506,
            -0.0318,
            -0.0218,
            0.1141,
            -0.1141,
            -0.2208,
            0.2019
          ],
          "after": [
            0.0226,
            0.1307,
            -0.0403,
            -0.0528,
            0.0516,
            -0.0164,
            0.1018,
            0.1005,
            -0.0817,
            -0.0658,
            0.0117,
            -0.0065,
            0.105,
            0.0238,
            -0.068,
            0.2298
          ]
        },
        "position_0": {
          "grad": [
            -0.0635,
            -0.0044,
            -0.1047,
            -0.0099,
            -0.0142,
            -0.0516,
            0.0716,
            -0.0285,
            0.0154,
            -0.0013,
            0.0102,
            -0.0544,
            0.0052,
            -0.0174,
            -0.0832,
            -0.1003
          ],
          "after": [
            0.0316,
            -0.1507,
            -0.2556,
            0.0749,
            0.0224,
            -0.0456,
            -0.1452,
            0.0667,
            0.1032,
            -0.0166,
            0.0287,
            0.0906,
            0.0336,
            -0.0142,
            -0.1263,
            0.0577
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0029,
            0.0024,
            0.0015,
            -0.0061,
            0.0036,
            0.0036,
            -0.0031,
            -0.0028,
            0.004,
            -0.0059,
            -0.0025,
            -0.001,
            -0.0015,
            0.0035,
            -0.0009,
            -0.0015
          ],
          "after": [
            -0.0494,
            0.0283,
            -0.1092,
            -0.0769,
            0.0868,
            0.1388,
            0.0745,
            -0.1598,
            0.0273,
            -0.0112,
            -0.1213,
            -0.0658,
            0.0991,
            -0.0775,
            0.0277,
            0.0889
          ]
        }
      }
    },
    {
      "step": 351,
      "word": "nochum",
      "loss": 3.3376,
      "learning_rate": 0.002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0021,
            0.0046,
            0.1581,
            -0.0402,
            0.0374,
            -0.0457,
            -0.112,
            -0.1449,
            -0.0208,
            0.0902,
            0.1542,
            0.0126,
            -0.0824,
            -0.0195,
            -0.0539,
            -0.1724
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.071,
            0.2201,
            -0.2054,
            0.0575,
            0.1647,
            -0.2389,
            0.3846,
            0.0934,
            -0.2657,
            0.0638,
            -0.0282,
            0.0893,
            0.1808,
            -0.1743,
            -0.3327,
            0.3446
          ],
          "after": [
            0.0223,
            0.1305,
            -0.0396,
            -0.0527,
            0.0515,
            -0.0159,
            0.1012,
            0.0997,
            -0.0812,
            -0.0665,
            0.0117,
            -0.0057,
            0.1043,
            0.0243,
            -0.0674,
            0.2295
          ]
        },
        "position_0": {
          "grad": [
            -0.0444,
            0.0723,
            -0.0574,
            0.0447,
            -0.2973,
            0.0521,
            -0.0135,
            0.0028,
            0.095,
            -0.1859,
            -0.2131,
            -0.0825,
            -0.0309,
            0.1454,
            0.2068,
            0.1021
          ],
          "after": [
            0.032,
            -0.1511,
            -0.2552,
            0.0742,
            0.0232,
            -0.0458,
            -0.1457,
            0.0658,
            0.103,
            -0.0154,
            0.0292,
            0.0911,
            0.0338,
            -0.0147,
            -0.1269,
            0.0574
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0034,
            0.0024,
            -0.0018,
            -0.0002,
            0.0028,
            -0.0011,
            0.0021,
            0.0007,
            -0.0006,
            0.001,
            0.0008,
            -0.0001,
            0.0012,
            -0.0007,
            -0.0027,
            0.0016
          ],
          "after": [
            -0.0501,
            0.0268,
            -0.1093,
            -0.0764,
            0.0859,
            0.1385,
            0.0739,
            -0.159,
            0.0266,
            -0.0112,
            -0.1216,
            -0.0647,
            0.0981,
            -0.0778,
            0.0293,
            0.089
          ]
        }
      }
    },
    {
      "step": 352,
      "word": "elenah",
      "loss": 2.0626,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1204,
            -0.0172,
            0.1924,
            0.0818,
            0.0379,
            0.0702,
            0.0528,
            0.0663,
            0.0777,
            0.079,
            -0.0455,
            -0.0337,
            0.1688,
            0.0234,
            -0.0908,
            0.1346
          ],
          "after": [
            -0.002,
            0.0052,
            0.158,
            -0.0408,
            0.038,
            -0.0463,
            -0.112,
            -0.1453,
            -0.0207,
            0.0907,
            0.1546,
            0.0135,
            -0.0829,
            -0.02,
            -0.0544,
            -0.1728
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3241,
            0.3,
            0.7806,
            -0.0616,
            0.3523,
            0.227,
            -0.0041,
            0.0926,
            0.4219,
            -0.0905,
            0.0285,
            0.2483,
            -0.4793,
            0.1389,
            0.1088,
            -0.4143
          ],
          "after": [
            0.0227,
            0.1299,
            -0.0402,
            -0.0525,
            0.0509,
            -0.0157,
            0.1007,
            0.0987,
            -0.0812,
            -0.0668,
            0.0117,
            -0.0056,
            0.1044,
            0.0245,
            -0.067,
            0.2298
          ]
        },
        "position_0": {
          "grad": [
            -0.0103,
            -0.2586,
            0.0812,
            -0.0572,
            -0.1445,
            0.0666,
            -0.3047,
            -0.0333,
            0.2157,
            0.0857,
            -0.0103,
            -0.0867,
            -0.2069,
            0.0166,
            0.0902,
            -0.3742
          ],
          "after": [
            0.0324,
            -0.1508,
            -0.2552,
            0.0738,
            0.0243,
            -0.0462,
            -0.1457,
            0.0652,
            0.1025,
            -0.0147,
            0.0298,
            0.0917,
            0.0342,
            -0.0153,
            -0.1277,
            0.0576
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0004,
            0.003,
            -0.0004,
            0.002,
            0.0018,
            -0.0036,
            -0.0006,
            0.0001,
            -0.003,
            -0.0012,
            0.0021,
            -0.0029,
            0.001,
            -0.0015,
            -0.0005
          ],
          "after": [
            -0.0506,
            0.0254,
            -0.1098,
            -0.076,
            0.0849,
            0.138,
            0.0736,
            -0.1582,
            0.0259,
            -0.0109,
            -0.1216,
            -0.0639,
            0.0975,
            -0.0783,
            0.0308,
            0.0891
          ]
        }
      }
    },
    {
      "step": 353,
      "word": "kumar",
      "loss": 2.3604,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0645,
            -0.1052,
            -0.0023,
            -0.033,
            0.1474,
            0.1012,
            0.1239,
            -0.1193,
            0.1411,
            -0.0616,
            0.092,
            0.1135,
            -0.1843,
            -0.1509,
            0.071,
            0.1645
          ],
          "after": [
            -0.0019,
            0.0061,
            0.1579,
            -0.0411,
            0.0383,
            -0.0473,
            -0.1122,
            -0.1452,
            -0.021,
            0.0913,
            0.1547,
            0.014,
            -0.0829,
            -0.0201,
            -0.0551,
            -0.1734
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0234,
            0.1922,
            -0.1614,
            0.0519,
            0.0578,
            -0.2757,
            0.3505,
            0.0373,
            -0.3409,
            0.0613,
            -0.044,
            -0.0343,
            0.2322,
            -0.1133,
            -0.312,
            0.2873
          ],
          "after": [
            0.0231,
            0.129,
            -0.0405,
            -0.0524,
            0.0503,
            -0.0153,
            0.1,
            0.0977,
            -0.0809,
            -0.0672,
            0.0118,
            -0.0054,
            0.1041,
            0.0249,
            -0.0663,
            0.2297
          ]
        },
        "position_0": {
          "grad": [
            -0.0814,
            0.0761,
            -0.1285,
            0.0231,
            0.1168,
            -0.0465,
            0.1926,
            -0.0916,
            -0.0901,
            0.0273,
            0.0671,
            0.0364,
            0.1041,
            -0.0344,
            -0.167,
            0.0043
          ],
          "after": [
            0.0331,
            -0.1507,
            -0.2546,
            0.0734,
            0.0249,
            -0.0464,
            -0.1459,
            0.065,
            0.1022,
            -0.0143,
            0.03,
            0.0922,
            0.0344,
            -0.0156,
            -0.128,
            0.0578
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0033,
            0.0006,
            -0.0005,
            -0.0007,
            -0.0049,
            0.0062,
            0.0027,
            -0.0024,
            -0.0043,
            -0.0028,
            -0.0028,
            0.0032,
            -0.001,
            0.0026,
            0.0034
          ],
          "after": [
            -0.0509,
            0.0239,
            -0.1103,
            -0.0755,
            0.0841,
            0.1381,
            0.0729,
            -0.1579,
            0.0257,
            -0.0102,
            -0.1214,
            -0.063,
            0.0968,
            -0.0786,
            0.0319,
            0.0889
          ]
        }
      }
    },
    {
      "step": 354,
      "word": "swayze",
      "loss": 2.6636,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0578,
            -0.1266,
            0.0191,
            -0.1006,
            0.0285,
            0.0914,
            -0.0464,
            0.0039,
            -0.0255,
            0.0471,
            -0.004,
            0.0968,
            -0.0055,
            0.0443,
            0.0244,
            0.0503
          ],
          "after": [
            -0.002,
            0.007,
            0.1578,
            -0.0413,
            0.0386,
            -0.0484,
            -0.1124,
            -0.1451,
            -0.0213,
            0.0917,
            0.1548,
            0.0143,
            -0.0829,
            -0.0202,
            -0.0557,
            -0.1739
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2057,
            -0.1955,
            0.028,
            -0.0748,
            -0.3821,
            0.1231,
            -0.2066,
            -0.0451,
            0.112,
            0.1445,
            0.0814,
            -0.2027,
            -0.02,
            0.1011,
            0.3533,
            -0.1553
          ],
          "after": [
            0.023,
            0.1286,
            -0.0408,
            -0.0522,
            0.0503,
            -0.015,
            0.0996,
            0.0971,
            -0.0807,
            -0.068,
            0.0116,
            -0.0048,
            0.1039,
            0.0251,
            -0.0661,
            0.2298
          ]
        },
        "position_0": {
          "grad": [
            -0.0025,
            0.1147,
            -0.0094,
            0.1126,
            -0.0983,
            0.0807,
            0.0184,
            0.0331,
            -0.0679,
            0.0289,
            -0.0637,
            0.0794,
            0.1321,
            0.0197,
            0.1051,
            0.0611
          ],
          "after": [
            0.0337,
            -0.1509,
            -0.2542,
            0.0728,
            0.0257,
            -0.0468,
            -0.1462,
            0.0647,
            0.102,
            -0.0141,
            0.0304,
            0.0923,
            0.0344,
            -0.016,
            -0.1285,
            0.0579
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.002,
            -0.0021,
            -0.0004,
            -0.0008,
            0.0004,
            0.0025,
            0.0003,
            -0.0015,
            0.0057,
            0.0014,
            -0.0001,
            0.0008,
            -0.0031,
            0.0033,
            0.0002
          ],
          "after": [
            -0.051,
            0.0228,
            -0.1105,
            -0.0751,
            0.0835,
            0.1382,
            0.0721,
            -0.1578,
            0.0258,
            -0.0102,
            -0.1213,
            -0.0622,
            0.096,
            -0.0784,
            0.0324,
            0.0886
          ]
        }
      }
    },
    {
      "step": 355,
      "word": "yabdiel",
      "loss": 2.5805,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0327,
            0.0795,
            0.3041,
            0.0216,
            0.1523,
            0.0163,
            0.1491,
            -0.0019,
            -0.1918,
            -0.0043,
            0.0168,
            0.1097,
            0.1501,
            -0.1655,
            -0.1487,
            0.2838
          ],
          "after": [
            -0.0021,
            0.0077,
            0.1571,
            -0.0414,
            0.0385,
            -0.0494,
            -0.1127,
            -0.145,
            -0.021,
            0.092,
            0.1549,
            0.0144,
            -0.0832,
            -0.02,
            -0.0559,
            -0.1749
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0336,
            0.0202,
            -0.0435,
            0.1536,
            -0.4068,
            -0.0637,
            -0.0741,
            0.0099,
            0.0242,
            -0.0014,
            0.053,
            0.0154,
            0.0863,
            0.0104,
            0.2313,
            0.0129
          ],
          "after": [
            0.0229,
            0.1282,
            -0.041,
            -0.0524,
            0.0508,
            -0.0147,
            0.0993,
            0.0965,
            -0.0806,
            -0.0686,
            0.0113,
            -0.0044,
            0.1037,
            0.0252,
            -0.0662,
            0.2299
          ]
        },
        "position_0": {
          "grad": [
            0.0282,
            -0.0722,
            0.0587,
            -0.1147,
            -0.019,
            0.1106,
            -0.0676,
            -0.0127,
            0.0034,
            0.063,
            -0.0621,
            0.0674,
            -0.0464,
            0.0575,
            0.0507,
            -0.0021
          ],
          "after": [
            0.034,
            -0.1509,
            -0.254,
            0.0725,
            0.0264,
            -0.0477,
            -0.1463,
            0.0645,
            0.1019,
            -0.0141,
            0.0309,
            0.0923,
            0.0344,
            -0.0165,
            -0.129,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.002,
            0.0012,
            -0.0002,
            0.0009,
            -0.003,
            -0.0023,
            -0.0016,
            -0.0024,
            -0.0004,
            0.0006,
            0.0004,
            0.0016,
            0.0004,
            0.0032,
            0.0008,
            0.001
          ],
          "after": [
            -0.0514,
            0.0217,
            -0.1106,
            -0.0748,
            0.0833,
            0.1385,
            0.0716,
            -0.1573,
            0.0259,
            -0.0103,
            -0.1213,
            -0.0617,
            0.0953,
            -0.0787,
            0.0328,
            0.0883
          ]
        }
      }
    },
    {
      "step": 356,
      "word": "araina",
      "loss": 2.0127,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1825,
            -0.3361,
            -0.0547,
            -0.3316,
            0.0166,
            0.1721,
            -0.1334,
            -0.1077,
            0.1229,
            0.0315,
            0.3177,
            -0.3011,
            -0.3093,
            -0.1803,
            0.0843,
            -0.2704
          ],
          "after": [
            -0.0024,
            0.009,
            0.1566,
            -0.0409,
            0.0384,
            -0.0509,
            -0.1128,
            -0.1446,
            -0.021,
            0.0922,
            0.1541,
            0.0149,
            -0.0829,
            -0.0194,
            -0.0562,
            -0.1753
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.016,
            0.0372,
            -0.0459,
            -0.019,
            0.0989,
            -0.1242,
            0.1603,
            -0.0267,
            -0.1049,
            0.0563,
            0.011,
            0.0083,
            0.0742,
            -0.0882,
            -0.1875,
            0.0732
          ],
          "after": [
            0.0228,
            0.1278,
            -0.041,
            -0.0525,
            0.0511,
            -0.0143,
            0.0989,
            0.0961,
            -0.0804,
            -0.0692,
            0.011,
            -0.004,
            0.1033,
            0.0254,
            -0.066,
            0.2299
          ]
        },
        "position_0": {
          "grad": [
            0.0743,
            -0.0589,
            0.1023,
            -0.1671,
            -0.0047,
            0.0151,
            -0.2766,
            -0.0552,
            0.1336,
            0.0386,
            0.2371,
            0.0967,
            -0.2256,
            -0.0729,
            -0.0615,
            -0.2406
          ],
          "after": [
            0.034,
            -0.1507,
            -0.2542,
            0.0727,
            0.027,
            -0.0484,
            -0.146,
            0.0645,
            0.1016,
            -0.0143,
            0.0308,
            0.092,
            0.0348,
            -0.0166,
            -0.1293,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0001,
            0.0005,
            -0.0003,
            -0.0025,
            0.0004,
            -0.003,
            -0.0005,
            0.0001,
            -0.0017,
            -0.0002,
            -0.001,
            -0.0009,
            0.001,
            0.0025,
            -0.0007
          ],
          "after": [
            -0.0516,
            0.0208,
            -0.1107,
            -0.0746,
            0.0834,
            0.1387,
            0.0714,
            -0.1567,
            0.026,
            -0.0102,
            -0.1213,
            -0.0611,
            0.0948,
            -0.079,
            0.0329,
            0.0881
          ]
        }
      }
    },
    {
      "step": 357,
      "word": "jacks",
      "loss": 2.6867,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0374,
            0.0528,
            0.0658,
            0.2296,
            -0.1564,
            -0.0762,
            0.167,
            0.0908,
            -0.1512,
            0.1004,
            0.0848,
            -0.0227,
            0.1055,
            0.0545,
            -0.0764,
            -0.0081
          ],
          "after": [
            -0.0027,
            0.0099,
            0.1561,
            -0.0409,
            0.0386,
            -0.0519,
            -0.1131,
            -0.1446,
            -0.0207,
            0.0921,
            0.1532,
            0.0154,
            -0.0829,
            -0.019,
            -0.0563,
            -0.1756
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0514,
            0.1887,
            -0.1944,
            -0.0056,
            0.0559,
            -0.2297,
            0.3867,
            0.0782,
            -0.3127,
            0.0167,
            -0.0394,
            0.001,
            0.1589,
            -0.1264,
            -0.1872,
            0.2847
          ],
          "after": [
            0.0228,
            0.1272,
            -0.0408,
            -0.0526,
            0.0513,
            -0.0137,
            0.0982,
            0.0954,
            -0.0799,
            -0.0698,
            0.0109,
            -0.0037,
            0.1028,
            0.0259,
            -0.0657,
            0.2295
          ]
        },
        "position_0": {
          "grad": [
            -0.0028,
            0.0755,
            0.0454,
            0.1835,
            0.1243,
            -0.1333,
            0.2099,
            -0.0036,
            -0.3,
            -0.0248,
            0.0653,
            0.0645,
            0.4291,
            -0.1683,
            -0.2023,
            0.1967
          ],
          "after": [
            0.034,
            -0.1508,
            -0.2545,
            0.0724,
            0.0273,
            -0.0486,
            -0.146,
            0.0646,
            0.1018,
            -0.0144,
            0.0305,
            0.0916,
            0.0345,
            -0.0163,
            -0.1291,
            0.0585
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0047,
            0.0043,
            -0.0023,
            0.0003,
            0.002,
            -0.0026,
            0.0062,
            0.0004,
            -0.0028,
            0.0028,
            0.0013,
            -0.0014,
            0.0034,
            -0.0001,
            -0.0031,
            0.0033
          ],
          "after": [
            -0.0524,
            0.0195,
            -0.1105,
            -0.0744,
            0.0832,
            0.1392,
            0.0707,
            -0.1563,
            0.0264,
            -0.0104,
            -0.1214,
            -0.0605,
            0.0941,
            -0.0793,
            0.0333,
            0.0875
          ]
        }
      }
    },
    {
      "step": 358,
      "word": "kierce",
      "loss": 2.175,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0029,
            0.0107,
            0.1556,
            -0.0409,
            0.0388,
            -0.0527,
            -0.1134,
            -0.1445,
            -0.0205,
            0.0921,
            0.1524,
            0.0158,
            -0.0828,
            -0.0186,
            -0.0564,
            -0.1758
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1509,
            -0.0864,
            0.1912,
            0.2168,
            -0.569,
            0.1378,
            -0.2815,
            0.0529,
            0.3201,
            0.0526,
            0.0591,
            0.1723,
            -0.1964,
            0.0589,
            0.5472,
            -0.228
          ],
          "after": [
            0.0231,
            0.1268,
            -0.0409,
            -0.0532,
            0.0522,
            -0.0133,
            0.0979,
            0.0946,
            -0.0798,
            -0.0704,
            0.0106,
            -0.0038,
            0.1027,
            0.0261,
            -0.066,
            0.2295
          ]
        },
        "position_0": {
          "grad": [
            -0.0687,
            0.0032,
            -0.1066,
            -0.0153,
            0.0012,
            -0.0556,
            0.0794,
            -0.0402,
            0.005,
            -0.0008,
            0.0216,
            -0.0414,
            0.0112,
            -0.0121,
            -0.0936,
            -0.0879
          ],
          "after": [
            0.0343,
            -0.1509,
            -0.2544,
            0.0722,
            0.0275,
            -0.0484,
            -0.1462,
            0.0648,
            0.102,
            -0.0145,
            0.0303,
            0.0913,
            0.0342,
            -0.0159,
            -0.1287,
            0.0587
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0,
            0.0008,
            0.0048,
            -0.0056,
            0.0005,
            -0.0049,
            0.0008,
            0.0019,
            0.0038,
            0.0036,
            0.0064,
            -0.0032,
            -0.0031,
            0.0009,
            -0.0011
          ],
          "after": [
            -0.053,
            0.0184,
            -0.1105,
            -0.0748,
            0.0837,
            0.1396,
            0.0705,
            -0.1561,
            0.0266,
            -0.0109,
            -0.122,
            -0.0607,
            0.0938,
            -0.0792,
            0.0336,
            0.0871
          ]
        }
      }
    },
    {
      "step": 359,
      "word": "fannie",
      "loss": 2.0568,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0519,
            -0.0017,
            -0.2037,
            -0.0828,
            -0.3221,
            -0.1137,
            -0.1509,
            -0.0889,
            0.1362,
            -0.1491,
            -0.1155,
            -0.133,
            0.0227,
            0.116,
            0.1206,
            -0.0029
          ],
          "after": [
            -0.0032,
            0.0114,
            0.1557,
            -0.0407,
            0.0394,
            -0.053,
            -0.1134,
            -0.1442,
            -0.0206,
            0.0924,
            0.1521,
            0.0164,
            -0.0828,
            -0.0186,
            -0.0568,
            -0.176
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0448,
            -0.0848,
            -0.0856,
            0.1249,
            -0.574,
            0.0191,
            -0.1626,
            0.0488,
            0.1315,
            0.0583,
            0.0305,
            -0.0142,
            0.0786,
            0.0233,
            0.3886,
            -0.1069
          ],
          "after": [
            0.0233,
            0.1266,
            -0.0409,
            -0.0541,
            0.0536,
            -0.0131,
            0.0977,
            0.0937,
            -0.0799,
            -0.0711,
            0.0103,
            -0.0038,
            0.1024,
            0.0263,
            -0.0665,
            0.2296
          ]
        },
        "position_0": {
          "grad": [
            0.0282,
            0.1786,
            -0.0359,
            0.1356,
            0.1421,
            -0.255,
            0.1241,
            -0.0995,
            -0.2691,
            -0.0322,
            0.0999,
            -0.003,
            0.2632,
            -0.0053,
            -0.1024,
            0.1647
          ],
          "after": [
            0.0345,
            -0.1514,
            -0.2542,
            0.0717,
            0.0273,
            -0.0474,
            -0.1465,
            0.0653,
            0.1026,
            -0.0144,
            0.0298,
            0.0911,
            0.0336,
            -0.0156,
            -0.1281,
            0.0586
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0008,
            0.001,
            0.0008,
            -0.0023,
            0.0011,
            -0.0046,
            0.0005,
            0.002,
            -0.0011,
            0.0,
            0.0006,
            -0.0017,
            0.0015,
            0.0016,
            0.001
          ],
          "after": [
            -0.0536,
            0.0176,
            -0.1106,
            -0.0752,
            0.0843,
            0.1398,
            0.0707,
            -0.1561,
            0.0264,
            -0.0113,
            -0.1225,
            -0.0609,
            0.0937,
            -0.0792,
            0.0336,
            0.0867
          ]
        }
      }
    },
    {
      "step": 360,
      "word": "harlowe",
      "loss": 2.4888,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0684,
            -0.0605,
            -0.0503,
            -0.1135,
            0.1936,
            0.0755,
            0.0861,
            -0.1471,
            0.1,
            -0.0321,
            0.0839,
            0.0654,
            -0.0878,
            -0.1456,
            -0.0228,
            0.1281
          ],
          "after": [
            -0.0035,
            0.0122,
            0.1558,
            -0.0403,
            0.0397,
            -0.0536,
            -0.1136,
            -0.1435,
            -0.0209,
            0.0928,
            0.1516,
            0.0168,
            -0.0827,
            -0.0182,
            -0.0571,
            -0.1764
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.284,
            -0.1564,
            -0.2911,
            -0.0903,
            -0.3967,
            0.1986,
            -0.2656,
            -0.0532,
            -0.0188,
            0.1229,
            -0.0043,
            -0.3129,
            0.277,
            0.1866,
            0.1453,
            -0.1985
          ],
          "after": [
            0.0229,
            0.1266,
            -0.0404,
            -0.0546,
            0.0552,
            -0.0131,
            0.0978,
            0.0932,
            -0.0799,
            -0.072,
            0.0101,
            -0.0033,
            0.1018,
            0.0261,
            -0.0672,
            0.2299
          ]
        },
        "position_0": {
          "grad": [
            0.0084,
            -0.0509,
            0.0615,
            0.0156,
            -0.0417,
            0.081,
            -0.0386,
            0.036,
            0.0926,
            0.0823,
            -0.0597,
            -0.098,
            0.1219,
            0.0748,
            0.0225,
            0.0579
          ],
          "after": [
            0.0345,
            -0.1517,
            -0.2543,
            0.0713,
            0.0272,
            -0.0469,
            -0.1467,
            0.0656,
            0.1029,
            -0.0147,
            0.0296,
            0.0912,
            0.0329,
            -0.0155,
            -0.1276,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0022,
            -0.0019,
            0.0008,
            -0.0012,
            0.0014,
            0.0008,
            -0.0009,
            0.0006,
            -0.0004,
            0.0011,
            0.0022,
            -0.0,
            0.0019,
            -0.0006,
            -0.0039,
            -0.0022
          ],
          "after": [
            -0.0544,
            0.0171,
            -0.1108,
            -0.0754,
            0.0847,
            0.1398,
            0.0709,
            -0.1561,
            0.0263,
            -0.0117,
            -0.1232,
            -0.0611,
            0.0934,
            -0.0792,
            0.034,
            0.0866
          ]
        }
      }
    },
    {
      "step": 361,
      "word": "jules",
      "loss": 2.4669,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0038,
            0.0128,
            0.1559,
            -0.04,
            0.0399,
            -0.054,
            -0.1137,
            -0.1428,
            -0.0212,
            0.0931,
            0.1511,
            0.0172,
            -0.0825,
            -0.0179,
            -0.0573,
            -0.1767
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.126,
            -0.2471,
            0.0574,
            -0.1897,
            0.1811,
            0.0248,
            -0.2017,
            0.1446,
            0.0843,
            -0.1183,
            -0.1338,
            0.0477,
            -0.1178,
            -0.0429,
            0.2434,
            -0.2393
          ],
          "after": [
            0.0228,
            0.1271,
            -0.0401,
            -0.0546,
            0.0564,
            -0.0131,
            0.0981,
            0.0922,
            -0.08,
            -0.0724,
            0.0102,
            -0.0029,
            0.1015,
            0.0261,
            -0.068,
            0.2305
          ]
        },
        "position_0": {
          "grad": [
            -0.0138,
            0.049,
            0.0529,
            0.1876,
            0.1061,
            -0.1104,
            0.1811,
            -0.0006,
            -0.2671,
            -0.0526,
            0.0791,
            0.0318,
            0.4141,
            -0.1302,
            -0.1804,
            0.1959
          ],
          "after": [
            0.0347,
            -0.1521,
            -0.2545,
            0.0706,
            0.0269,
            -0.046,
            -0.1471,
            0.0659,
            0.1037,
            -0.0147,
            0.0292,
            0.0912,
            0.0316,
            -0.0151,
            -0.1268,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0022,
            -0.0005,
            -0.0001,
            -0.0025,
            0.0026,
            -0.0025,
            0.0038,
            0.0035,
            -0.0022,
            -0.0019,
            -0.0015,
            -0.0012,
            0.003,
            -0.0021,
            0.0002,
            -0.0007
          ],
          "after": [
            -0.0548,
            0.0167,
            -0.1109,
            -0.0753,
            0.0848,
            0.1401,
            0.0708,
            -0.1567,
            0.0265,
            -0.0118,
            -0.1236,
            -0.0611,
            0.0928,
            -0.0789,
            0.0344,
            0.0866
          ]
        }
      }
    },
    {
      "step": 362,
      "word": "akeem",
      "loss": 2.6076,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2498,
            0.1898,
            0.0962,
            -0.1573,
            0.1273,
            -0.1529,
            0.194,
            0.0265,
            0.1385,
            -0.081,
            -0.1291,
            0.0346,
            -0.0655,
            0.0422,
            -0.2686,
            -0.2197
          ],
          "after": [
            -0.0036,
            0.0129,
            0.1557,
            -0.0395,
            0.0398,
            -0.0538,
            -0.1142,
            -0.1424,
            -0.0217,
            0.0936,
            0.1511,
            0.0174,
            -0.0823,
            -0.0178,
            -0.0568,
            -0.1767
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2198,
            -0.0439,
            -0.0649,
            0.0197,
            0.0353,
            0.4579,
            -0.2452,
            -0.0053,
            0.2616,
            -0.2527,
            -0.066,
            0.3688,
            -0.3602,
            -0.0317,
            0.576,
            -0.316
          ],
          "after": [
            0.0232,
            0.1275,
            -0.0397,
            -0.0546,
            0.0573,
            -0.0137,
            0.0986,
            0.0914,
            -0.0804,
            -0.0722,
            0.0105,
            -0.0032,
            0.1017,
            0.0261,
            -0.0691,
            0.2313
          ]
        },
        "position_0": {
          "grad": [
            0.0789,
            -0.0552,
            0.1274,
            -0.1497,
            0.0108,
            0.022,
            -0.3181,
            -0.0634,
            0.1626,
            0.0233,
            0.287,
            0.108,
            -0.2452,
            -0.0464,
            -0.044,
            -0.2554
          ],
          "after": [
            0.0344,
            -0.1523,
            -0.2551,
            0.0702,
            0.0266,
            -0.0453,
            -0.147,
            0.0664,
            0.104,
            -0.0148,
            0.0283,
            0.0909,
            0.031,
            -0.0145,
            -0.126,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0021,
            -0.0003,
            -0.0013,
            -0.0038,
            0.0028,
            -0.0033,
            -0.0012,
            0.0023,
            0.0013,
            0.0044,
            0.0003,
            -0.0039,
            -0.0021,
            0.0046,
            -0.0051
          ],
          "after": [
            -0.0549,
            0.0167,
            -0.111,
            -0.0751,
            0.0852,
            0.1401,
            0.0709,
            -0.157,
            0.0264,
            -0.0121,
            -0.1245,
            -0.0612,
            0.0928,
            -0.0784,
            0.0342,
            0.0872
          ]
        }
      }
    },
    {
      "step": 363,
      "word": "hellen",
      "loss": 1.8075,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0034,
            0.013,
            0.1557,
            -0.039,
            0.0398,
            -0.0537,
            -0.1145,
            -0.142,
            -0.0222,
            0.094,
            0.1511,
            0.0176,
            -0.0821,
            -0.0176,
            -0.0564,
            -0.1766
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1649,
            -0.3746,
            0.5282,
            -0.1092,
            0.3518,
            0.1408,
            -0.7495,
            -0.0684,
            0.5613,
            0.1338,
            0.1077,
            0.184,
            -0.3376,
            0.2328,
            -0.0636,
            -0.5039
          ],
          "after": [
            0.0232,
            0.1285,
            -0.0402,
            -0.0543,
            0.0577,
            -0.0145,
            0.0996,
            0.0909,
            -0.0813,
            -0.0723,
            0.0105,
            -0.0039,
            0.1023,
            0.0256,
            -0.0701,
            0.2326
          ]
        },
        "position_0": {
          "grad": [
            0.0163,
            -0.1126,
            0.0888,
            0.0194,
            -0.1264,
            0.0923,
            -0.1044,
            0.0859,
            0.1806,
            0.0445,
            -0.0972,
            -0.1821,
            0.1008,
            0.1355,
            0.0835,
            0.0121
          ],
          "after": [
            0.0341,
            -0.1522,
            -0.256,
            0.0699,
            0.0267,
            -0.0451,
            -0.1468,
            0.0665,
            0.104,
            -0.0151,
            0.0277,
            0.0911,
            0.0302,
            -0.0145,
            -0.1255,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0004,
            0.0025,
            -0.0015,
            0.0027,
            -0.0015,
            -0.0032,
            -0.0012,
            0.0017,
            0.0021,
            0.0015,
            0.0015,
            -0.0005,
            0.0001,
            -0.0025,
            -0.0015
          ],
          "after": [
            -0.0551,
            0.0167,
            -0.1115,
            -0.0748,
            0.0853,
            0.1402,
            0.0713,
            -0.157,
            0.026,
            -0.0125,
            -0.1254,
            -0.0614,
            0.0928,
            -0.0779,
            0.0343,
            0.0879
          ]
        }
      }
    },
    {
      "step": 364,
      "word": "atalia",
      "loss": 2.0691,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0654,
            -0.1132,
            0.0282,
            -0.0751,
            -0.0605,
            0.082,
            -0.0614,
            0.1338,
            -0.2122,
            0.0069,
            -0.1915,
            -0.2546,
            0.152,
            0.121,
            0.0693,
            -0.159
          ],
          "after": [
            -0.0034,
            0.0133,
            0.1555,
            -0.0384,
            0.0399,
            -0.0539,
            -0.1147,
            -0.1421,
            -0.0221,
            0.0944,
            0.1515,
            0.0182,
            -0.0822,
            -0.0178,
            -0.0562,
            -0.1763
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0368,
            0.1494,
            -0.0919,
            0.0095,
            0.1205,
            -0.1514,
            0.2376,
            -0.0456,
            -0.1779,
            0.0196,
            0.041,
            -0.0264,
            0.1148,
            -0.1085,
            -0.2296,
            0.1549
          ],
          "after": [
            0.0232,
            0.1291,
            -0.0405,
            -0.054,
            0.0579,
            -0.0149,
            0.1003,
            0.0907,
            -0.0818,
            -0.0725,
            0.0104,
            -0.0044,
            0.1027,
            0.0255,
            -0.0707,
            0.2336
          ]
        },
        "position_0": {
          "grad": [
            0.0824,
            -0.044,
            0.1101,
            -0.1636,
            0.0087,
            0.0199,
            -0.2773,
            -0.0588,
            0.1319,
            0.0228,
            0.2471,
            0.0982,
            -0.2121,
            -0.0441,
            -0.0597,
            -0.2136
          ],
          "after": [
            0.0335,
            -0.1519,
            -0.2571,
            0.07,
            0.0267,
            -0.045,
            -0.1462,
            0.0668,
            0.1038,
            -0.0154,
            0.0267,
            0.0911,
            0.03,
            -0.0144,
            -0.125,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0006,
            0.0007,
            0.0001,
            -0.0013,
            -0.0003,
            -0.0018,
            0.0001,
            0.0006,
            0.0001,
            0.0001,
            0.0008,
            -0.0008,
            0.0002,
            0.0008,
            -0.0006
          ],
          "after": [
            -0.0552,
            0.0168,
            -0.112,
            -0.0745,
            0.0855,
            0.1404,
            0.0718,
            -0.1571,
            0.0256,
            -0.0129,
            -0.1262,
            -0.0617,
            0.0928,
            -0.0776,
            0.0344,
            0.0885
          ]
        }
      }
    },
    {
      "step": 365,
      "word": "rosilyn",
      "loss": 2.6723,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0034,
            0.0136,
            0.1554,
            -0.0379,
            0.04,
            -0.054,
            -0.1149,
            -0.1422,
            -0.022,
            0.0947,
            0.1519,
            0.0187,
            -0.0823,
            -0.0179,
            -0.0561,
            -0.176
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0576,
            0.1226,
            -0.0749,
            0.0341,
            0.1096,
            -0.1566,
            0.2887,
            -0.0247,
            -0.1904,
            -0.0129,
            0.0231,
            0.0531,
            0.0727,
            -0.1433,
            -0.2146,
            0.2181
          ],
          "after": [
            0.0234,
            0.1294,
            -0.0406,
            -0.0539,
            0.058,
            -0.015,
            0.1006,
            0.0907,
            -0.0821,
            -0.0726,
            0.0102,
            -0.0049,
            0.1029,
            0.0256,
            -0.071,
            0.2341
          ]
        },
        "position_0": {
          "grad": [
            0.0319,
            -0.0328,
            0.0268,
            -0.0772,
            -0.0067,
            0.1089,
            0.042,
            -0.0319,
            0.1164,
            -0.0739,
            -0.0765,
            -0.0275,
            -0.1453,
            -0.0396,
            0.1111,
            0.0673
          ],
          "after": [
            0.0329,
            -0.1516,
            -0.2581,
            0.0702,
            0.0268,
            -0.0453,
            -0.1458,
            0.0671,
            0.1035,
            -0.0154,
            0.026,
            0.0911,
            0.0299,
            -0.0141,
            -0.1248,
            0.0585
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0021,
            -0.0003,
            0.0017,
            -0.0016,
            -0.0008,
            -0.0013,
            0.0005,
            -0.0009,
            -0.004,
            -0.0017,
            -0.003,
            0.0012,
            0.0009,
            0.0026,
            -0.0002
          ],
          "after": [
            -0.0555,
            0.0166,
            -0.1123,
            -0.0744,
            0.0859,
            0.1406,
            0.0722,
            -0.1572,
            0.0254,
            -0.0128,
            -0.1267,
            -0.0616,
            0.0928,
            -0.0774,
            0.0341,
            0.0891
          ]
        }
      }
    },
    {
      "step": 366,
      "word": "korie",
      "loss": 1.9552,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0034,
            0.0139,
            0.1553,
            -0.0375,
            0.04,
            -0.0541,
            -0.1151,
            -0.1422,
            -0.0219,
            0.0949,
            0.1523,
            0.0191,
            -0.0824,
            -0.0181,
            -0.056,
            -0.1758
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0124,
            -0.0668,
            0.047,
            0.2405,
            -0.4126,
            -0.0601,
            -0.1493,
            0.0756,
            0.0082,
            -0.1919,
            -0.099,
            0.0753,
            0.0155,
            0.0743,
            0.2605,
            0.0682
          ],
          "after": [
            0.0235,
            0.1298,
            -0.0407,
            -0.0545,
            0.0585,
            -0.0151,
            0.101,
            0.0903,
            -0.0824,
            -0.0722,
            0.0103,
            -0.0055,
            0.1031,
            0.0255,
            -0.0715,
            0.2345
          ]
        },
        "position_0": {
          "grad": [
            -0.09,
            -0.0059,
            -0.1194,
            -0.0341,
            0.0004,
            -0.0497,
            0.0875,
            -0.0456,
            0.0146,
            -0.0157,
            0.0276,
            -0.0537,
            0.0041,
            0.0023,
            -0.0841,
            -0.0939
          ],
          "after": [
            0.0327,
            -0.1514,
            -0.2586,
            0.0705,
            0.0268,
            -0.0453,
            -0.1456,
            0.0676,
            0.1031,
            -0.0153,
            0.0253,
            0.0912,
            0.0299,
            -0.0139,
            -0.1244,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0022,
            -0.0024,
            0.0028,
            0.0027,
            -0.0013,
            -0.0018,
            -0.008,
            0.0026,
            -0.0,
            -0.006,
            -0.0011,
            0.0024,
            -0.0046,
            -0.0011,
            0.0056,
            0.0014
          ],
          "after": [
            -0.0554,
            0.0168,
            -0.113,
            -0.0747,
            0.0863,
            0.141,
            0.0732,
            -0.1578,
            0.0252,
            -0.0121,
            -0.127,
            -0.0617,
            0.0932,
            -0.0771,
            0.0334,
            0.0894
          ]
        }
      }
    },
    {
      "step": 367,
      "word": "georgio",
      "loss": 3.063,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0033,
            0.0141,
            0.1552,
            -0.0372,
            0.0401,
            -0.0542,
            -0.1152,
            -0.1423,
            -0.0219,
            0.0951,
            0.1526,
            0.0195,
            -0.0824,
            -0.0182,
            -0.0558,
            -0.1756
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0543,
            -0.0598,
            0.2939,
            0.0306,
            0.3824,
            -0.1124,
            -0.0606,
            -0.032,
            0.2174,
            0.0269,
            0.0594,
            0.1707,
            -0.1945,
            0.0047,
            -0.29,
            -0.0793
          ],
          "after": [
            0.0235,
            0.1302,
            -0.0413,
            -0.055,
            0.0585,
            -0.015,
            0.1014,
            0.0901,
            -0.0828,
            -0.072,
            0.0102,
            -0.0064,
            0.1035,
            0.0255,
            -0.0716,
            0.2349
          ]
        },
        "position_0": {
          "grad": [
            0.0037,
            0.025,
            0.007,
            0.0829,
            -0.0447,
            -0.1063,
            -0.0321,
            -0.0495,
            -0.0679,
            0.0908,
            0.0703,
            -0.1178,
            0.0693,
            0.0774,
            -0.1041,
            0.0556
          ],
          "after": [
            0.0326,
            -0.1512,
            -0.259,
            0.0705,
            0.0269,
            -0.045,
            -0.1453,
            0.0682,
            0.1029,
            -0.0156,
            0.0246,
            0.0917,
            0.0297,
            -0.0139,
            -0.1238,
            0.0589
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0015,
            0.0002,
            0.0012,
            -0.0018,
            -0.0006,
            -0.0013,
            0.0005,
            0.0,
            0.0003,
            0.0007,
            0.001,
            -0.001,
            0.0001,
            0.0008,
            -0.0007
          ],
          "after": [
            -0.0554,
            0.0171,
            -0.1137,
            -0.0751,
            0.0869,
            0.1414,
            0.0741,
            -0.1583,
            0.025,
            -0.0116,
            -0.1273,
            -0.062,
            0.0936,
            -0.0769,
            0.0327,
            0.0898
          ]
        }
      }
    },
    {
      "step": 368,
      "word": "penn",
      "loss": 2.4816,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0033,
            0.0142,
            0.1552,
            -0.0369,
            0.0401,
            -0.0543,
            -0.1153,
            -0.1423,
            -0.0218,
            0.0953,
            0.1528,
            0.0198,
            -0.0825,
            -0.0183,
            -0.0558,
            -0.1754
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0668,
            -0.0931,
            0.4222,
            0.051,
            0.4793,
            0.2166,
            -0.3852,
            -0.097,
            0.4604,
            0.2462,
            0.0442,
            0.1684,
            -0.1416,
            0.1321,
            -0.1768,
            -0.0474
          ],
          "after": [
            0.0236,
            0.1307,
            -0.0424,
            -0.0556,
            0.0579,
            -0.0152,
            0.102,
            0.0904,
            -0.0836,
            -0.0724,
            0.0101,
            -0.0074,
            0.104,
            0.0252,
            -0.0716,
            0.2353
          ]
        },
        "position_0": {
          "grad": [
            0.1297,
            0.0991,
            -0.099,
            0.0783,
            -0.137,
            -0.1588,
            0.2871,
            0.065,
            -0.2455,
            -0.0555,
            -0.1606,
            -0.1569,
            -0.0142,
            0.2576,
            0.093,
            0.2326
          ],
          "after": [
            0.0319,
            -0.1514,
            -0.259,
            0.0704,
            0.0274,
            -0.0442,
            -0.1455,
            0.0685,
            0.1032,
            -0.0156,
            0.0244,
            0.0925,
            0.0296,
            -0.0148,
            -0.1236,
            0.0587
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            -0.0002,
            -0.0002,
            -0.0001,
            0.0023,
            -0.0006,
            0.003,
            0.001,
            0.0014,
            0.0035,
            0.0009,
            0.0015,
            0.0015,
            -0.0023,
            -0.0011,
            0.0009
          ],
          "after": [
            -0.0553,
            0.0174,
            -0.1142,
            -0.0754,
            0.0871,
            0.1418,
            0.0747,
            -0.1589,
            0.0247,
            -0.0115,
            -0.1277,
            -0.0624,
            0.0938,
            -0.0764,
            0.0322,
            0.09
          ]
        }
      }
    },
    {
      "step": 369,
      "word": "daeshawn",
      "loss": 2.8289,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2349,
            -0.0973,
            0.0243,
            0.3427,
            0.1268,
            0.0694,
            0.174,
            0.0052,
            -0.0704,
            0.0196,
            -0.0002,
            0.2181,
            -0.0535,
            -0.2929,
            -0.2019,
            0.2036
          ],
          "after": [
            -0.0028,
            0.0146,
            0.155,
            -0.0373,
            0.0399,
            -0.0547,
            -0.1157,
            -0.1424,
            -0.0216,
            0.0954,
            0.153,
            0.0197,
            -0.0824,
            -0.0176,
            -0.0552,
            -0.1756
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1793,
            0.1934,
            -0.111,
            0.1809,
            0.0377,
            0.2199,
            0.3697,
            0.1881,
            -0.0022,
            -0.2357,
            -0.147,
            0.1898,
            -0.0547,
            -0.0972,
            0.1719,
            0.3095
          ],
          "after": [
            0.024,
            0.1308,
            -0.0432,
            -0.0566,
            0.0574,
            -0.0156,
            0.1023,
            0.0898,
            -0.0844,
            -0.0721,
            0.0103,
            -0.0086,
            0.1045,
            0.0252,
            -0.0717,
            0.2353
          ]
        },
        "position_0": {
          "grad": [
            -0.0341,
            0.0437,
            -0.0333,
            -0.066,
            0.129,
            -0.0491,
            0.0466,
            -0.0568,
            -0.0101,
            0.0108,
            -0.0847,
            0.0451,
            0.0181,
            0.0228,
            0.0184,
            0.1185
          ],
          "after": [
            0.0314,
            -0.1516,
            -0.2589,
            0.0705,
            0.0275,
            -0.0433,
            -0.1457,
            0.0689,
            0.1034,
            -0.0157,
            0.0243,
            0.093,
            0.0295,
            -0.0156,
            -0.1234,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0053,
            0.0032,
            0.0062,
            0.0005,
            0.002,
            0.0012,
            -0.0001,
            -0.0007,
            0.0014,
            0.0007,
            0.0044,
            0.0077,
            -0.0038,
            -0.0042,
            -0.0032,
            0.0002
          ],
          "after": [
            -0.0545,
            0.0173,
            -0.1155,
            -0.0757,
            0.0871,
            0.142,
            0.0751,
            -0.1593,
            0.0242,
            -0.0114,
            -0.1285,
            -0.0635,
            0.0944,
            -0.0754,
            0.0321,
            0.0902
          ]
        }
      }
    },
    {
      "step": 370,
      "word": "tamari",
      "loss": 2.0917,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1483,
            0.0711,
            0.2323,
            -0.0657,
            0.4845,
            0.0751,
            0.38,
            -0.0153,
            0.1977,
            -0.0631,
            0.1386,
            0.3352,
            -0.3442,
            -0.3597,
            -0.1292,
            0.179
          ],
          "after": [
            -0.0022,
            0.0148,
            0.1545,
            -0.0375,
            0.039,
            -0.0552,
            -0.1167,
            -0.1424,
            -0.0219,
            0.0957,
            0.1528,
            0.019,
            -0.0818,
            -0.0163,
            -0.0544,
            -0.1761
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0965,
            0.1614,
            -0.0809,
            0.0689,
            0.1677,
            -0.1617,
            0.2888,
            -0.0208,
            -0.217,
            0.0352,
            -0.0178,
            0.0488,
            0.1304,
            -0.1049,
            -0.3242,
            0.2046
          ],
          "after": [
            0.0246,
            0.1306,
            -0.0437,
            -0.0575,
            0.0568,
            -0.0158,
            0.1023,
            0.0895,
            -0.0847,
            -0.072,
            0.0106,
            -0.0098,
            0.1048,
            0.0253,
            -0.0716,
            0.235
          ]
        },
        "position_0": {
          "grad": [
            -0.009,
            -0.1537,
            0.0766,
            0.0422,
            -0.0811,
            0.085,
            0.0515,
            0.0276,
            -0.0972,
            -0.0884,
            -0.1503,
            -0.2009,
            0.0962,
            0.0017,
            0.0704,
            0.1154
          ],
          "after": [
            0.0311,
            -0.1514,
            -0.259,
            0.0704,
            0.0277,
            -0.0428,
            -0.146,
            0.0692,
            0.1038,
            -0.0154,
            0.0246,
            0.094,
            0.0293,
            -0.0163,
            -0.1234,
            0.0579
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0004,
            0.0008,
            -0.0009,
            0.0004,
            -0.0004,
            -0.0003,
            -0.0004,
            0.0004,
            -0.0011,
            -0.0004,
            -0.0001,
            -0.0003,
            -0.0001,
            0.0,
            -0.0002
          ],
          "after": [
            -0.0537,
            0.0172,
            -0.1167,
            -0.0758,
            0.0871,
            0.1423,
            0.0756,
            -0.1596,
            0.0237,
            -0.0113,
            -0.1292,
            -0.0644,
            0.0949,
            -0.0746,
            0.032,
            0.0903
          ]
        }
      }
    },
    {
      "step": 371,
      "word": "zee",
      "loss": 2.9322,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            -0.0016,
            0.0149,
            0.154,
            -0.0377,
            0.0383,
            -0.0557,
            -0.1176,
            -0.1424,
            -0.0221,
            0.0959,
            0.1527,
            0.0185,
            -0.0813,
            -0.0152,
            -0.0537,
            -0.1765
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1894,
            0.151,
            0.4923,
            -0.0445,
            0.1728,
            0.8044,
            -0.632,
            -0.0569,
            1.0022,
            0.0532,
            0.1165,
            0.8111,
            -1.1393,
            0.2591,
            0.9805,
            -0.4957
          ],
          "after": [
            0.0247,
            0.1303,
            -0.0448,
            -0.0583,
            0.0561,
            -0.0169,
            0.1028,
            0.0894,
            -0.086,
            -0.072,
            0.0105,
            -0.012,
            0.1063,
            0.025,
            -0.0723,
            0.2354
          ]
        },
        "position_0": {
          "grad": [
            0.1166,
            -0.0684,
            -0.0121,
            0.2721,
            -0.1356,
            -0.1768,
            0.1676,
            0.0438,
            0.2215,
            -0.0212,
            -0.0895,
            -0.2124,
            -0.0293,
            0.1122,
            0.1598,
            -0.0255
          ],
          "after": [
            0.0303,
            -0.151,
            -0.2592,
            0.0698,
            0.0283,
            -0.0419,
            -0.1465,
            0.0692,
            0.1037,
            -0.015,
            0.025,
            0.0954,
            0.0291,
            -0.0172,
            -0.1238,
            0.0575
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0017,
            0.0003,
            -0.0013,
            -0.0053,
            0.0012,
            0.005,
            -0.0047,
            -0.0013,
            0.0038,
            0.0004,
            -0.0027,
            0.0117,
            -0.0137,
            -0.0,
            0.0124,
            -0.0015
          ],
          "after": [
            -0.0529,
            0.0171,
            -0.1176,
            -0.0754,
            0.0869,
            0.1419,
            0.0763,
            -0.1596,
            0.0227,
            -0.0112,
            -0.1295,
            -0.0663,
            0.0965,
            -0.0739,
            0.0309,
            0.0906
          ]
        }
      }
    },
    {
      "step": 372,
      "word": "rainn",
      "loss": 2.2532,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0254,
            -0.4548,
            0.0168,
            -0.0728,
            0.0197,
            0.1185,
            -0.3249,
            0.1318,
            0.0591,
            -0.0675,
            0.1583,
            -0.2605,
            -0.2434,
            0.0701,
            0.0685,
            -0.1553
          ],
          "after": [
            -0.001,
            0.016,
            0.1536,
            -0.0378,
            0.0376,
            -0.0565,
            -0.1177,
            -0.1429,
            -0.0225,
            0.0963,
            0.1521,
            0.0185,
            -0.0804,
            -0.0145,
            -0.0533,
            -0.1765
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.054,
            0.0702,
            -0.0945,
            0.0174,
            0.0781,
            -0.1216,
            0.2789,
            0.003,
            -0.1563,
            0.0683,
            -0.0001,
            0.0283,
            0.1045,
            -0.1267,
            -0.1871,
            0.1417
          ],
          "after": [
            0.0249,
            0.1298,
            -0.0457,
            -0.0589,
            0.0554,
            -0.0177,
            0.1029,
            0.0893,
            -0.0869,
            -0.0722,
            0.0104,
            -0.0139,
            0.1075,
            0.0249,
            -0.0727,
            0.2355
          ]
        },
        "position_0": {
          "grad": [
            0.0544,
            -0.048,
            0.0267,
            -0.1116,
            -0.0543,
            0.1265,
            0.0495,
            -0.0205,
            0.1749,
            -0.1099,
            -0.1501,
            -0.0532,
            -0.2163,
            -0.0664,
            0.18,
            0.0684
          ],
          "after": [
            0.0294,
            -0.1506,
            -0.2593,
            0.0695,
            0.0289,
            -0.0415,
            -0.1469,
            0.0694,
            0.1034,
            -0.0143,
            0.0257,
            0.0968,
            0.0293,
            -0.0178,
            -0.1246,
            0.0571
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0007,
            -0.0007,
            0.0015,
            0.001,
            0.003,
            0.0,
            0.003,
            0.001,
            -0.0012,
            -0.0013,
            0.0028,
            -0.0021,
            -0.0007,
            0.0008,
            0.0029
          ],
          "after": [
            -0.052,
            0.0171,
            -0.1182,
            -0.0752,
            0.0867,
            0.1412,
            0.0769,
            -0.1601,
            0.0217,
            -0.011,
            -0.1295,
            -0.0682,
            0.0981,
            -0.0731,
            0.0298,
            0.0905
          ]
        }
      }
    },
    {
      "step": 373,
      "word": "ekam",
      "loss": 2.8189,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1353,
            0.0786,
            0.25,
            0.1007,
            0.2648,
            -0.0163,
            0.2452,
            0.1722,
            0.1255,
            -0.0786,
            0.1041,
            0.1843,
            -0.1927,
            -0.096,
            -0.1004,
            0.0639
          ],
          "after": [
            -0.0003,
            0.0167,
            0.1528,
            -0.038,
            0.0366,
            -0.0571,
            -0.1183,
            -0.1438,
            -0.023,
            0.0969,
            0.1514,
            0.0181,
            -0.0793,
            -0.0136,
            -0.0526,
            -0.1767
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.4062,
            0.7771,
            0.5686,
            -0.1337,
            0.1506,
            -0.2679,
            0.6516,
            0.0218,
            -0.101,
            -0.2115,
            -0.0911,
            -0.1277,
            0.0667,
            0.11,
            -0.1396,
            0.0301
          ],
          "after": [
            0.0258,
            0.1284,
            -0.0471,
            -0.0592,
            0.0546,
            -0.0181,
            0.1026,
            0.0891,
            -0.0876,
            -0.0719,
            0.0106,
            -0.0153,
            0.1085,
            0.0247,
            -0.073,
            0.2356
          ]
        },
        "position_0": {
          "grad": [
            -0.0286,
            -0.2599,
            0.098,
            -0.0412,
            -0.0693,
            0.1019,
            -0.315,
            -0.076,
            0.1888,
            0.1933,
            0.0326,
            0.0313,
            -0.225,
            -0.0422,
            0.0674,
            -0.4276
          ],
          "after": [
            0.0288,
            -0.1495,
            -0.2598,
            0.0693,
            0.0296,
            -0.0415,
            -0.1469,
            0.0698,
            0.1028,
            -0.0144,
            0.0262,
            0.0978,
            0.0298,
            -0.0181,
            -0.1254,
            0.0574
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            0.0002,
            0.0032,
            0.0026,
            -0.0013,
            -0.0014,
            0.0015,
            -0.0004,
            -0.0026,
            0.0018,
            0.0036,
            -0.0046,
            0.0034,
            -0.0007,
            -0.0036,
            -0.0008
          ],
          "after": [
            -0.0513,
            0.0171,
            -0.1192,
            -0.0753,
            0.0866,
            0.1408,
            0.0773,
            -0.1605,
            0.0212,
            -0.0111,
            -0.13,
            -0.0693,
            0.0991,
            -0.0725,
            0.0292,
            0.0906
          ]
        }
      }
    },
    {
      "step": 374,
      "word": "siman",
      "loss": 2.0698,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0457,
            0.1106,
            -0.2602,
            -0.0136,
            -0.2304,
            -0.1257,
            -0.0305,
            -0.0385,
            0.0633,
            0.0264,
            -0.0002,
            -0.0063,
            -0.0498,
            0.011,
            0.1466,
            -0.0579
          ],
          "after": [
            0.0002,
            0.0171,
            0.1525,
            -0.0381,
            0.0361,
            -0.0572,
            -0.1187,
            -0.1444,
            -0.0237,
            0.0973,
            0.1508,
            0.0179,
            -0.0784,
            -0.0128,
            -0.0525,
            -0.1768
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0224,
            0.0119,
            -0.1361,
            -0.0044,
            0.1151,
            -0.1177,
            0.1972,
            0.01,
            -0.1851,
            0.0712,
            -0.0289,
            0.0148,
            0.1119,
            -0.0676,
            -0.2296,
            0.1589
          ],
          "after": [
            0.0267,
            0.1271,
            -0.0482,
            -0.0593,
            0.0538,
            -0.0182,
            0.1021,
            0.0889,
            -0.0881,
            -0.0718,
            0.0108,
            -0.0165,
            0.1091,
            0.0246,
            -0.073,
            0.2355
          ]
        },
        "position_0": {
          "grad": [
            0.0064,
            0.0782,
            -0.011,
            0.0436,
            -0.1655,
            0.0985,
            -0.0571,
            0.0677,
            -0.0051,
            0.0354,
            -0.1131,
            0.0676,
            0.0709,
            -0.0093,
            0.1367,
            -0.0337
          ],
          "after": [
            0.0282,
            -0.1489,
            -0.2602,
            0.0691,
            0.0305,
            -0.0418,
            -0.1467,
            0.0699,
            0.1023,
            -0.0147,
            0.0269,
            0.0985,
            0.0301,
            -0.0184,
            -0.1264,
            0.0577
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0008,
            -0.0003,
            -0.0,
            -0.0014,
            0.0011,
            -0.0016,
            0.0001,
            0.0007,
            -0.002,
            -0.0016,
            -0.0014,
            -0.0002,
            0.002,
            0.0012,
            0.0008
          ],
          "after": [
            -0.0508,
            0.0172,
            -0.1199,
            -0.0754,
            0.0867,
            0.1403,
            0.0777,
            -0.1608,
            0.0207,
            -0.0109,
            -0.1303,
            -0.0702,
            0.1,
            -0.0721,
            0.0286,
            0.0905
          ]
        }
      }
    },
    {
      "step": 375,
      "word": "mahalia",
      "loss": 2.0734,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0847,
            0.1602,
            0.1227,
            -0.0763,
            0.1387,
            0.1413,
            -0.0302,
            0.1457,
            -0.0156,
            0.1795,
            -0.1486,
            0.1131,
            0.122,
            0.0504,
            -0.0828,
            -0.1149
          ],
          "after": [
            0.0009,
            0.0171,
            0.1521,
            -0.0381,
            0.0355,
            -0.0578,
            -0.119,
            -0.1454,
            -0.0242,
            0.0971,
            0.1506,
            0.0174,
            -0.0777,
            -0.0123,
            -0.0522,
            -0.1766
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1131,
            0.2006,
            -0.1161,
            0.0862,
            0.1739,
            -0.2047,
            0.3615,
            -0.0007,
            -0.2904,
            -0.0097,
            -0.0243,
            0.0775,
            0.1064,
            -0.1308,
            -0.323,
            0.3413
          ],
          "after": [
            0.0276,
            0.1258,
            -0.0489,
            -0.0597,
            0.0529,
            -0.0181,
            0.1013,
            0.0888,
            -0.0881,
            -0.0716,
            0.0111,
            -0.0177,
            0.1096,
            0.0247,
            -0.0727,
            0.235
          ]
        },
        "position_0": {
          "grad": [
            -0.0324,
            0.0453,
            -0.0179,
            0.015,
            0.1548,
            0.0192,
            0.1496,
            0.0883,
            0.0333,
            0.0267,
            -0.0006,
            0.1269,
            -0.1126,
            -0.1047,
            -0.0325,
            0.0969
          ],
          "after": [
            0.0278,
            -0.1484,
            -0.2605,
            0.0689,
            0.031,
            -0.0422,
            -0.1468,
            0.0696,
            0.1018,
            -0.0151,
            0.0275,
            0.0988,
            0.0306,
            -0.0183,
            -0.1272,
            0.0578
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0023,
            -0.0034,
            -0.0012,
            0.0015,
            -0.0056,
            -0.0035,
            -0.0056,
            0.0007,
            -0.001,
            -0.0001,
            0.0007,
            -0.0012,
            -0.0008,
            0.0002,
            0.0056,
            -0.0029
          ],
          "after": [
            -0.0507,
            0.0177,
            -0.1204,
            -0.0756,
            0.0874,
            0.1403,
            0.0785,
            -0.1612,
            0.0205,
            -0.0107,
            -0.1306,
            -0.0707,
            0.1008,
            -0.0719,
            0.0276,
            0.0908
          ]
        }
      }
    },
    {
      "step": 376,
      "word": "ahmeer",
      "loss": 2.5387,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.008,
            0.0342,
            0.2139,
            0.0675,
            0.0001,
            0.1272,
            0.0455,
            0.0735,
            0.0895,
            0.1952,
            -0.0661,
            -0.163,
            0.3217,
            0.1231,
            -0.11,
            0.0805
          ],
          "after": [
            0.0014,
            0.017,
            0.1514,
            -0.0382,
            0.035,
            -0.0587,
            -0.1193,
            -0.1465,
            -0.0248,
            0.0964,
            0.1507,
            0.0174,
            -0.0777,
            -0.0122,
            -0.0516,
            -0.1766
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0054,
            -0.1065,
            -0.0447,
            -0.1483,
            -0.2123,
            0.3285,
            -0.1729,
            -0.043,
            0.3116,
            -0.5028,
            -0.268,
            0.0593,
            -0.2641,
            0.1853,
            0.7382,
            -0.1749
          ],
          "after": [
            0.0284,
            0.1248,
            -0.0495,
            -0.0597,
            0.0524,
            -0.0184,
            0.1009,
            0.0888,
            -0.0885,
            -0.0705,
            0.0121,
            -0.0188,
            0.1103,
            0.0245,
            -0.0731,
            0.2348
          ]
        },
        "position_0": {
          "grad": [
            0.0778,
            -0.0386,
            0.1087,
            -0.1576,
            0.0445,
            0.0209,
            -0.26,
            -0.0581,
            0.1349,
            0.0514,
            0.2456,
            0.1247,
            -0.2173,
            -0.0537,
            -0.0591,
            -0.203
          ],
          "after": [
            0.0272,
            -0.1479,
            -0.2611,
            0.069,
            0.0313,
            -0.0426,
            -0.1466,
            0.0696,
            0.1012,
            -0.0156,
            0.0275,
            0.0987,
            0.0313,
            -0.0181,
            -0.1277,
            0.0581
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0009,
            -0.0021,
            -0.0017,
            0.0021,
            -0.0019,
            0.0026,
            0.0004,
            0.0007,
            -0.0076,
            -0.0063,
            -0.0011,
            0.0006,
            0.0018,
            0.0002,
            0.0039
          ],
          "after": [
            -0.0508,
            0.0182,
            -0.1206,
            -0.0756,
            0.0877,
            0.1406,
            0.079,
            -0.1616,
            0.0201,
            -0.0098,
            -0.13,
            -0.0711,
            0.1015,
            -0.0719,
            0.0268,
            0.0906
          ]
        }
      }
    },
    {
      "step": 377,
      "word": "pantelis",
      "loss": 2.3558,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0025,
            0.1045,
            -0.1386,
            -0.0467,
            -0.1627,
            -0.055,
            -0.0448,
            -0.0977,
            0.0182,
            -0.0674,
            -0.086,
            -0.0265,
            0.0583,
            0.0484,
            0.0389,
            0.0528
          ],
          "after": [
            0.0018,
            0.0167,
            0.151,
            -0.0383,
            0.0348,
            -0.0593,
            -0.1195,
            -0.1472,
            -0.0254,
            0.096,
            0.151,
            0.0173,
            -0.0778,
            -0.0122,
            -0.0512,
            -0.1767
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0303,
            -0.1988,
            0.0637,
            -0.0436,
            0.0032,
            -0.0509,
            -0.0936,
            0.0112,
            -0.024,
            -0.0533,
            -0.0413,
            0.1204,
            -0.135,
            0.0043,
            0.0124,
            0.0887
          ],
          "after": [
            0.029,
            0.1242,
            -0.0501,
            -0.0595,
            0.052,
            -0.0186,
            0.1006,
            0.0888,
            -0.0888,
            -0.0694,
            0.013,
            -0.02,
            0.111,
            0.0243,
            -0.0735,
            0.2345
          ]
        },
        "position_0": {
          "grad": [
            0.0579,
            0.1293,
            -0.0818,
            0.041,
            0.0466,
            -0.0883,
            0.241,
            -0.014,
            -0.2278,
            0.0323,
            -0.0468,
            0.0284,
            0.0355,
            0.0609,
            -0.0449,
            0.1911
          ],
          "after": [
            0.0264,
            -0.1478,
            -0.2613,
            0.069,
            0.0314,
            -0.0426,
            -0.1467,
            0.0696,
            0.101,
            -0.0161,
            0.0275,
            0.0985,
            0.0318,
            -0.0181,
            -0.128,
            0.0582
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0008,
            0.0009,
            0.0003,
            0.0006,
            -0.0002,
            -0.0009,
            0.0003,
            0.0003,
            0.0009,
            0.0002,
            0.0004,
            -0.0001,
            -0.0001,
            -0.0011,
            0.0003
          ],
          "after": [
            -0.0509,
            0.0188,
            -0.1208,
            -0.0757,
            0.0879,
            0.1408,
            0.0795,
            -0.162,
            0.0198,
            -0.0092,
            -0.1296,
            -0.0715,
            0.1021,
            -0.0719,
            0.0261,
            0.0904
          ]
        }
      }
    },
    {
      "step": 378,
      "word": "kobie",
      "loss": 2.2033,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0022,
            0.0165,
            0.1507,
            -0.0383,
            0.0346,
            -0.0598,
            -0.1197,
            -0.1477,
            -0.0259,
            0.0956,
            0.1512,
            0.0173,
            -0.0779,
            -0.0121,
            -0.0509,
            -0.1768
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0004,
            -0.0553,
            0.0419,
            0.2313,
            -0.4098,
            -0.0812,
            -0.1437,
            0.0941,
            -0.0319,
            -0.1921,
            -0.105,
            0.0688,
            0.0423,
            0.1039,
            0.2697,
            0.0849
          ],
          "after": [
            0.0295,
            0.1238,
            -0.0506,
            -0.0599,
            0.0521,
            -0.0187,
            0.1004,
            0.0884,
            -0.089,
            -0.0681,
            0.014,
            -0.0211,
            0.1116,
            0.024,
            -0.074,
            0.2341
          ]
        },
        "position_0": {
          "grad": [
            -0.1064,
            0.0448,
            -0.1217,
            -0.0117,
            0.0793,
            -0.0465,
            0.1425,
            -0.076,
            -0.0419,
            -0.0029,
            0.0543,
            0.0059,
            0.0528,
            -0.0076,
            -0.1211,
            -0.0234
          ],
          "after": [
            0.0262,
            -0.1479,
            -0.2611,
            0.0691,
            0.0313,
            -0.0424,
            -0.147,
            0.07,
            0.1009,
            -0.0166,
            0.0275,
            0.0984,
            0.0322,
            -0.018,
            -0.128,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0106,
            0.0054,
            -0.0024,
            -0.0047,
            -0.0046,
            -0.005,
            0.0054,
            0.0022,
            -0.0079,
            0.0015,
            -0.0025,
            0.0006,
            -0.0038,
            -0.0021,
            -0.0045
          ],
          "after": [
            -0.0511,
            0.0204,
            -0.1217,
            -0.0754,
            0.0886,
            0.1415,
            0.0803,
            -0.1631,
            0.0192,
            -0.0079,
            -0.1295,
            -0.0716,
            0.1025,
            -0.0714,
            0.0258,
            0.0907
          ]
        }
      }
    },
    {
      "step": 379,
      "word": "zazil",
      "loss": 2.4233,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.035,
            0.0426,
            -0.0221,
            0.3908,
            0.1572,
            -0.1748,
            0.3996,
            -0.0322,
            -0.012,
            0.0895,
            0.0398,
            0.0755,
            0.1229,
            -0.0333,
            -0.0736,
            0.1069
          ],
          "after": [
            0.0024,
            0.0162,
            0.1504,
            -0.039,
            0.0342,
            -0.0597,
            -0.1205,
            -0.148,
            -0.0263,
            0.0951,
            0.1513,
            0.0172,
            -0.0782,
            -0.012,
            -0.0505,
            -0.177
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1529,
            0.274,
            -0.2346,
            0.0779,
            0.1914,
            -0.2918,
            0.5218,
            -0.0044,
            -0.4368,
            -0.0306,
            -0.0152,
            0.0302,
            0.2412,
            -0.1892,
            -0.4817,
            0.4112
          ],
          "after": [
            0.0302,
            0.1231,
            -0.0508,
            -0.0605,
            0.0519,
            -0.0184,
            0.0998,
            0.0881,
            -0.0887,
            -0.0669,
            0.015,
            -0.0221,
            0.1118,
            0.024,
            -0.0741,
            0.2334
          ]
        },
        "position_0": {
          "grad": [
            0.0532,
            0.0873,
            -0.0434,
            0.2319,
            0.102,
            -0.1063,
            0.2858,
            -0.045,
            -0.0233,
            0.0616,
            0.012,
            0.0208,
            0.1158,
            -0.0279,
            -0.028,
            0.1331
          ],
          "after": [
            0.0258,
            -0.1481,
            -0.2607,
            0.0687,
            0.031,
            -0.042,
            -0.1476,
            0.0704,
            0.1009,
            -0.0172,
            0.0274,
            0.0982,
            0.0323,
            -0.0179,
            -0.128,
            0.0581
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.0004,
            0.0023,
            -0.003,
            0.0019,
            -0.0044,
            -0.0004,
            -0.0036,
            -0.001,
            0.0001,
            0.0017,
            -0.002,
            0.0008,
            -0.0008,
            0.0,
            -0.0029
          ],
          "after": [
            -0.0514,
            0.0218,
            -0.1229,
            -0.0749,
            0.089,
            0.1426,
            0.0809,
            -0.1636,
            0.0188,
            -0.0068,
            -0.1295,
            -0.0715,
            0.1028,
            -0.0709,
            0.0255,
            0.0913
          ]
        }
      }
    },
    {
      "step": 380,
      "word": "emmarie",
      "loss": 2.2487,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0181,
            -0.0182,
            0.015,
            -0.0488,
            0.1822,
            0.1063,
            0.1169,
            -0.0955,
            0.0818,
            -0.0483,
            0.0117,
            0.1391,
            -0.1055,
            -0.1682,
            -0.0246,
            0.1849
          ],
          "after": [
            0.0027,
            0.016,
            0.1502,
            -0.0396,
            0.0336,
            -0.0599,
            -0.1214,
            -0.148,
            -0.0268,
            0.0948,
            0.1513,
            0.0168,
            -0.0783,
            -0.0116,
            -0.05,
            -0.1776
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0433,
            0.2764,
            0.2211,
            0.1105,
            -0.378,
            -0.0693,
            0.1248,
            0.0255,
            -0.0181,
            -0.0805,
            -0.0341,
            -0.1952,
            0.1279,
            0.1748,
            0.1626,
            -0.2019
          ],
          "after": [
            0.0308,
            0.122,
            -0.0512,
            -0.0613,
            0.0522,
            -0.018,
            0.0992,
            0.0878,
            -0.0885,
            -0.0657,
            0.0159,
            -0.0226,
            0.1118,
            0.0238,
            -0.0743,
            0.2329
          ]
        },
        "position_0": {
          "grad": [
            -0.0287,
            -0.1238,
            0.045,
            -0.0584,
            0.0264,
            0.0836,
            -0.151,
            -0.0698,
            0.0695,
            0.1671,
            0.0325,
            0.0956,
            -0.1243,
            -0.0892,
            -0.0367,
            -0.223
          ],
          "after": [
            0.0255,
            -0.148,
            -0.2606,
            0.0684,
            0.0306,
            -0.0418,
            -0.1479,
            0.0711,
            0.1008,
            -0.0184,
            0.0273,
            0.0978,
            0.0326,
            -0.0175,
            -0.1278,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0023,
            -0.002,
            -0.0034,
            0.0044,
            -0.004,
            -0.0047,
            -0.0012,
            0.0022,
            -0.0038,
            -0.0004,
            -0.0022,
            -0.004,
            0.0058,
            0.0009,
            0.001,
            0.0023
          ],
          "after": [
            -0.0519,
            0.0232,
            -0.1233,
            -0.0749,
            0.0897,
            0.144,
            0.0816,
            -0.1642,
            0.0191,
            -0.0058,
            -0.1293,
            -0.071,
            0.1025,
            -0.0705,
            0.0251,
            0.0916
          ]
        }
      }
    },
    {
      "step": 381,
      "word": "manessa",
      "loss": 2.3098,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0791,
            0.0988,
            -0.1627,
            -0.1121,
            -0.2089,
            -0.0918,
            -0.0907,
            -0.1194,
            -0.0114,
            -0.0292,
            -0.0549,
            -0.0892,
            0.0633,
            0.1017,
            0.0486,
            -0.0242
          ],
          "after": [
            0.0027,
            0.0156,
            0.1503,
            -0.0398,
            0.0334,
            -0.0597,
            -0.122,
            -0.1476,
            -0.0272,
            0.0946,
            0.1515,
            0.0166,
            -0.0784,
            -0.0114,
            -0.0497,
            -0.178
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0243,
            -0.0399,
            -0.0538,
            -0.0906,
            -0.0946,
            0.0223,
            -0.2498,
            -0.0179,
            -0.1022,
            -0.2477,
            -0.0723,
            -0.1653,
            0.0068,
            0.0689,
            0.2278,
            -0.0783
          ],
          "after": [
            0.0313,
            0.1212,
            -0.0515,
            -0.0617,
            0.0526,
            -0.0178,
            0.099,
            0.0875,
            -0.0882,
            -0.0642,
            0.0169,
            -0.0227,
            0.1118,
            0.0234,
            -0.0746,
            0.2327
          ]
        },
        "position_0": {
          "grad": [
            -0.0315,
            0.022,
            -0.0128,
            0.0294,
            0.1333,
            0.02,
            0.1228,
            0.1082,
            0.0615,
            0.0231,
            -0.0208,
            0.1096,
            -0.1307,
            -0.0968,
            -0.0013,
            0.0691
          ],
          "after": [
            0.0255,
            -0.148,
            -0.2604,
            0.0681,
            0.03,
            -0.0418,
            -0.1484,
            0.0712,
            0.1006,
            -0.0196,
            0.0272,
            0.0972,
            0.0331,
            -0.0169,
            -0.1277,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            0.0008,
            0.0004,
            0.0018,
            -0.0013,
            0.0018,
            -0.0052,
            0.0008,
            -0.0027,
            -0.0015,
            -0.0004,
            -0.0086,
            0.0015,
            0.0014,
            0.0031,
            -0.0017
          ],
          "after": [
            -0.0526,
            0.0243,
            -0.1238,
            -0.0752,
            0.0904,
            0.145,
            0.0826,
            -0.1649,
            0.0197,
            -0.0048,
            -0.1291,
            -0.0698,
            0.1021,
            -0.0704,
            0.0246,
            0.092
          ]
        }
      }
    },
    {
      "step": 382,
      "word": "nili",
      "loss": 2.5598,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0028,
            0.0152,
            0.1504,
            -0.0401,
            0.0333,
            -0.0596,
            -0.1225,
            -0.1473,
            -0.0276,
            0.0944,
            0.1517,
            0.0165,
            -0.0786,
            -0.0113,
            -0.0495,
            -0.1783
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0519,
            0.1714,
            -0.2241,
            -0.0288,
            0.1241,
            -0.1968,
            0.4694,
            -0.0374,
            -0.3404,
            0.102,
            0.0035,
            -0.0634,
            0.2465,
            -0.1351,
            -0.3864,
            0.3481
          ],
          "after": [
            0.0318,
            0.1203,
            -0.0514,
            -0.062,
            0.0528,
            -0.0173,
            0.0983,
            0.0875,
            -0.0876,
            -0.0631,
            0.0177,
            -0.0227,
            0.1116,
            0.0234,
            -0.0746,
            0.2321
          ]
        },
        "position_0": {
          "grad": [
            -0.0237,
            0.0021,
            -0.0696,
            0.0176,
            -0.5612,
            0.051,
            -0.1382,
            0.0616,
            0.2412,
            -0.3173,
            -0.3415,
            -0.2349,
            -0.1393,
            0.2307,
            0.4143,
            0.0079
          ],
          "after": [
            0.0256,
            -0.148,
            -0.2601,
            0.0679,
            0.0308,
            -0.0419,
            -0.1486,
            0.0711,
            0.1,
            -0.0193,
            0.0279,
            0.0973,
            0.0337,
            -0.0171,
            -0.1285,
            0.0585
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            0.0011,
            -0.0004,
            -0.0052,
            0.0064,
            0.0007,
            0.0013,
            -0.0021,
            -0.0001,
            0.0015,
            0.0,
            -0.0035,
            0.0011,
            0.0011,
            -0.0035,
            -0.0019
          ],
          "after": [
            -0.0533,
            0.0251,
            -0.1241,
            -0.0748,
            0.0904,
            0.1458,
            0.0833,
            -0.1652,
            0.0202,
            -0.0041,
            -0.1289,
            -0.0684,
            0.1017,
            -0.0705,
            0.0244,
            0.0926
          ]
        }
      }
    },
    {
      "step": 383,
      "word": "jenavi",
      "loss": 2.2878,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2576,
            -0.0268,
            0.1033,
            0.1039,
            0.2973,
            0.0716,
            -0.0391,
            -0.0866,
            -0.0979,
            -0.0374,
            0.0579,
            0.126,
            0.0937,
            -0.157,
            -0.2336,
            0.3784
          ],
          "after": [
            0.0033,
            0.015,
            0.1503,
            -0.0404,
            0.0327,
            -0.0598,
            -0.1229,
            -0.1468,
            -0.0277,
            0.0944,
            0.1517,
            0.0162,
            -0.0789,
            -0.0108,
            -0.0488,
            -0.1792
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1996,
            -0.1599,
            0.283,
            0.0584,
            0.301,
            -0.0258,
            -0.3069,
            -0.1342,
            0.2941,
            0.2078,
            0.2224,
            0.1275,
            -0.2296,
            -0.0617,
            -0.2835,
            -0.1678
          ],
          "after": [
            0.0319,
            0.1197,
            -0.0517,
            -0.0624,
            0.0526,
            -0.0169,
            0.098,
            0.088,
            -0.0874,
            -0.0626,
            0.0178,
            -0.023,
            0.1116,
            0.0234,
            -0.0743,
            0.2317
          ]
        },
        "position_0": {
          "grad": [
            -0.0055,
            0.0395,
            0.0331,
            0.1513,
            0.0585,
            -0.1104,
            0.1369,
            0.0212,
            -0.2215,
            -0.0488,
            0.0322,
            0.0168,
            0.3283,
            -0.1287,
            -0.1689,
            0.1411
          ],
          "after": [
            0.0256,
            -0.1481,
            -0.2599,
            0.0673,
            0.0313,
            -0.0417,
            -0.1489,
            0.0709,
            0.0999,
            -0.0188,
            0.0284,
            0.0973,
            0.0337,
            -0.0169,
            -0.1289,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            0.0006,
            0.0013,
            -0.0019,
            0.0012,
            -0.0025,
            -0.0039,
            -0.003,
            0.0005,
            0.0042,
            0.0054,
            -0.0006,
            -0.0022,
            -0.0021,
            -0.001,
            -0.0046
          ],
          "after": [
            -0.0542,
            0.0257,
            -0.1246,
            -0.0743,
            0.0903,
            0.1467,
            0.0841,
            -0.165,
            0.0206,
            -0.0039,
            -0.1293,
            -0.0672,
            0.1016,
            -0.0702,
            0.0243,
            0.0936
          ]
        }
      }
    },
    {
      "step": 384,
      "word": "cloe",
      "loss": 3.0824,
      "learning_rate": 0.0019,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0038,
            0.0148,
            0.1502,
            -0.0408,
            0.0322,
            -0.0599,
            -0.1232,
            -0.1463,
            -0.0277,
            0.0943,
            0.1516,
            0.0159,
            -0.0791,
            -0.0104,
            -0.0481,
            -0.18
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.188,
            0.3465,
            -0.5072,
            -0.0154,
            -0.179,
            -0.0641,
            0.2522,
            -0.0622,
            -0.179,
            -0.4512,
            -0.3065,
            -0.1624,
            0.1693,
            -0.1159,
            0.6687,
            0.4095
          ],
          "after": [
            0.0323,
            0.1188,
            -0.0513,
            -0.0627,
            0.0527,
            -0.0164,
            0.0976,
            0.0886,
            -0.0871,
            -0.0614,
            0.0186,
            -0.0229,
            0.1114,
            0.0237,
            -0.0746,
            0.231
          ]
        },
        "position_0": {
          "grad": [
            -0.0064,
            -0.0459,
            0.0074,
            0.1166,
            -0.2253,
            -0.0067,
            0.0304,
            0.0834,
            -0.0179,
            -0.015,
            0.0139,
            -0.1064,
            0.0473,
            0.1103,
            0.0342,
            -0.0007
          ],
          "after": [
            0.0257,
            -0.148,
            -0.2597,
            0.0666,
            0.0322,
            -0.0414,
            -0.1493,
            0.0705,
            0.0998,
            -0.0184,
            0.0287,
            0.0976,
            0.0337,
            -0.017,
            -0.1292,
            0.0582
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0041,
            0.0072,
            -0.0092,
            0.0007,
            -0.0012,
            -0.0028,
            0.0035,
            -0.0024,
            -0.0024,
            -0.0109,
            -0.008,
            -0.0036,
            0.0001,
            -0.0021,
            0.0172,
            0.0089
          ],
          "after": [
            -0.0543,
            0.0254,
            -0.1238,
            -0.0739,
            0.0903,
            0.1478,
            0.0846,
            -0.1645,
            0.0213,
            -0.0028,
            -0.1288,
            -0.0659,
            0.1014,
            -0.0698,
            0.0229,
            0.0934
          ]
        }
      }
    },
    {
      "step": 385,
      "word": "schuyler",
      "loss": 3.0343,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0042,
            0.0147,
            0.1501,
            -0.0411,
            0.0317,
            -0.06,
            -0.1235,
            -0.1459,
            -0.0278,
            0.0943,
            0.1516,
            0.0157,
            -0.0793,
            -0.0101,
            -0.0475,
            -0.1807
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.284,
            -0.304,
            -0.1868,
            -0.1029,
            -0.0777,
            0.1132,
            -0.2021,
            0.0725,
            -0.0596,
            0.1884,
            -0.0487,
            -0.2075,
            0.1305,
            0.1276,
            0.012,
            -0.1891
          ],
          "after": [
            0.0322,
            0.1184,
            -0.0508,
            -0.0627,
            0.0528,
            -0.0162,
            0.0974,
            0.0889,
            -0.0867,
            -0.0607,
            0.0195,
            -0.0225,
            0.1111,
            0.0237,
            -0.0749,
            0.2305
          ]
        },
        "position_0": {
          "grad": [
            -0.0027,
            0.0889,
            -0.0181,
            0.0892,
            -0.0806,
            0.0704,
            0.0211,
            0.0219,
            -0.0533,
            0.0244,
            -0.0638,
            0.0626,
            0.0898,
            0.0163,
            0.0999,
            0.0361
          ],
          "after": [
            0.0258,
            -0.1482,
            -0.2595,
            0.0658,
            0.0332,
            -0.0415,
            -0.1496,
            0.07,
            0.0998,
            -0.0182,
            0.0292,
            0.0977,
            0.0335,
            -0.0172,
            -0.1297,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0026,
            0.0002,
            -0.0007,
            0.0018,
            0.0021,
            -0.0,
            0.0025,
            0.0,
            -0.0015,
            0.0011,
            0.0,
            0.0,
            0.0018,
            0.0002,
            -0.004,
            0.0033
          ],
          "after": [
            -0.0548,
            0.0251,
            -0.123,
            -0.0738,
            0.0902,
            0.1488,
            0.0849,
            -0.164,
            0.0221,
            -0.0019,
            -0.1284,
            -0.0647,
            0.1012,
            -0.0694,
            0.0221,
            0.093
          ]
        }
      }
    },
    {
      "step": 386,
      "word": "meilany",
      "loss": 2.2128,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1296,
            0.127,
            -0.0014,
            0.0447,
            -0.0016,
            -0.0399,
            0.0344,
            -0.0396,
            0.0306,
            -0.1786,
            -0.0937,
            0.1782,
            -0.0665,
            -0.0707,
            -0.0198,
            0.1392
          ],
          "after": [
            0.0048,
            0.0142,
            0.1501,
            -0.0414,
            0.0314,
            -0.06,
            -0.1238,
            -0.1454,
            -0.0279,
            0.0948,
            0.1519,
            0.0152,
            -0.0793,
            -0.0096,
            -0.047,
            -0.1815
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.183,
            -0.0443,
            0.2767,
            -0.0824,
            0.2267,
            -0.0117,
            -0.173,
            -0.109,
            0.3079,
            0.121,
            0.2612,
            0.0858,
            -0.2446,
            -0.0599,
            -0.1724,
            -0.1401
          ],
          "after": [
            0.0317,
            0.1181,
            -0.0506,
            -0.0624,
            0.0526,
            -0.016,
            0.0973,
            0.0896,
            -0.0867,
            -0.0603,
            0.0195,
            -0.0223,
            0.1111,
            0.0238,
            -0.075,
            0.2303
          ]
        },
        "position_0": {
          "grad": [
            -0.0384,
            -0.0059,
            -0.0045,
            0.0107,
            0.104,
            0.0306,
            0.0831,
            0.1103,
            0.0979,
            0.0027,
            -0.0053,
            0.0833,
            -0.1554,
            -0.0728,
            0.0111,
            0.0391
          ],
          "after": [
            0.0261,
            -0.1484,
            -0.2593,
            0.0651,
            0.0338,
            -0.0416,
            -0.15,
            0.0692,
            0.0997,
            -0.0179,
            0.0296,
            0.0975,
            0.0336,
            -0.0171,
            -0.1302,
            0.0578
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0016,
            -0.0006,
            -0.002,
            -0.0013,
            0.0055,
            -0.0047,
            -0.0005,
            0.0032,
            0.0011,
            -0.0002,
            0.0052,
            -0.009,
            0.0027,
            0.0048,
            -0.003
          ],
          "after": [
            -0.0552,
            0.0251,
            -0.1223,
            -0.0735,
            0.0901,
            0.149,
            0.0854,
            -0.1635,
            0.0223,
            -0.0013,
            -0.128,
            -0.0642,
            0.1017,
            -0.0694,
            0.021,
            0.0929
          ]
        }
      }
    },
    {
      "step": 387,
      "word": "griffon",
      "loss": 3.9046,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0053,
            0.0139,
            0.15,
            -0.0417,
            0.0311,
            -0.0599,
            -0.124,
            -0.145,
            -0.028,
            0.0952,
            0.1521,
            0.0147,
            -0.0794,
            -0.0092,
            -0.0466,
            -0.1822
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1192,
            0.1906,
            -0.1764,
            0.0245,
            0.0813,
            -0.2275,
            0.3755,
            -0.0167,
            -0.3231,
            0.0597,
            -0.0465,
            0.0637,
            0.1989,
            -0.1796,
            -0.3151,
            0.2932
          ],
          "after": [
            0.0315,
            0.1176,
            -0.0503,
            -0.0623,
            0.0524,
            -0.0155,
            0.097,
            0.0902,
            -0.0864,
            -0.0601,
            0.0197,
            -0.0222,
            0.1109,
            0.0242,
            -0.0748,
            0.2298
          ]
        },
        "position_0": {
          "grad": [
            -0.0177,
            0.1076,
            -0.0183,
            0.1755,
            0.0458,
            -0.0715,
            0.1245,
            -0.0779,
            -0.1937,
            0.1189,
            0.0924,
            -0.0149,
            0.1393,
            0.036,
            -0.16,
            0.1762
          ],
          "after": [
            0.0263,
            -0.1488,
            -0.2591,
            0.0642,
            0.0342,
            -0.0415,
            -0.1505,
            0.0688,
            0.0999,
            -0.0182,
            0.0298,
            0.0975,
            0.0334,
            -0.0171,
            -0.1303,
            0.0574
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            0.0034,
            -0.0012,
            0.0011,
            0.003,
            0.0009,
            0.0059,
            0.0003,
            -0.0041,
            0.004,
            0.0014,
            0.0011,
            0.0024,
            -0.0022,
            -0.0057,
            0.0014
          ],
          "after": [
            -0.0554,
            0.0246,
            -0.1216,
            -0.0733,
            0.0898,
            0.149,
            0.0855,
            -0.1632,
            0.0231,
            -0.0011,
            -0.1278,
            -0.0639,
            0.102,
            -0.0691,
            0.0205,
            0.0926
          ]
        }
      }
    },
    {
      "step": 388,
      "word": "roselina",
      "loss": 2.3928,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0421,
            -0.0033,
            0.0069,
            -0.0426,
            -0.026,
            0.0149,
            -0.0376,
            -0.0358,
            0.0154,
            0.0554,
            0.0364,
            -0.0549,
            0.0047,
            0.0299,
            -0.0085,
            -0.0504
          ],
          "after": [
            0.0056,
            0.0136,
            0.1499,
            -0.0418,
            0.0308,
            -0.0599,
            -0.1242,
            -0.1445,
            -0.0281,
            0.0954,
            0.1521,
            0.0145,
            -0.0794,
            -0.009,
            -0.0462,
            -0.1826
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2628,
            0.2209,
            0.098,
            0.0653,
            0.0343,
            -0.0482,
            0.1259,
            -0.0103,
            -0.1496,
            -0.1642,
            -0.1109,
            -0.1307,
            0.1805,
            -0.0975,
            0.0365,
            0.1301
          ],
          "after": [
            0.0318,
            0.1169,
            -0.0501,
            -0.0624,
            0.0522,
            -0.0151,
            0.0966,
            0.0907,
            -0.086,
            -0.0596,
            0.0201,
            -0.0219,
            0.1105,
            0.0247,
            -0.0747,
            0.2293
          ]
        },
        "position_0": {
          "grad": [
            0.0313,
            -0.0203,
            0.011,
            -0.0726,
            -0.0059,
            0.0838,
            0.0439,
            -0.0329,
            0.1034,
            -0.0674,
            -0.0744,
            -0.0252,
            -0.1227,
            -0.0393,
            0.105,
            0.0633
          ],
          "after": [
            0.0264,
            -0.1491,
            -0.259,
            0.0635,
            0.0345,
            -0.0417,
            -0.151,
            0.0685,
            0.0999,
            -0.0182,
            0.0301,
            0.0975,
            0.0335,
            -0.0171,
            -0.1305,
            0.0569
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0024,
            0.001,
            -0.0,
            -0.0039,
            0.002,
            -0.0062,
            0.0004,
            -0.0006,
            -0.0015,
            -0.0038,
            -0.0095,
            0.0038,
            0.0033,
            0.0046,
            -0.0048
          ],
          "after": [
            -0.0557,
            0.024,
            -0.121,
            -0.0732,
            0.0899,
            0.1488,
            0.0859,
            -0.1629,
            0.0238,
            -0.0009,
            -0.1272,
            -0.0628,
            0.1019,
            -0.0694,
            0.0197,
            0.093
          ]
        }
      }
    },
    {
      "step": 389,
      "word": "azalaya",
      "loss": 2.2943,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0617,
            0.1305,
            0.035,
            0.0335,
            0.3132,
            -0.1078,
            0.2748,
            -0.013,
            -0.0917,
            0.3237,
            -0.0328,
            0.47,
            0.0474,
            -0.1036,
            -0.1669,
            -0.0734
          ],
          "after": [
            0.0061,
            0.0131,
            0.1498,
            -0.042,
            0.0302,
            -0.0596,
            -0.1247,
            -0.1441,
            -0.028,
            0.0947,
            0.1523,
            0.0135,
            -0.0796,
            -0.0085,
            -0.0454,
            -0.1829
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0535,
            0.1089,
            -0.1018,
            0.0064,
            0.0627,
            -0.1929,
            0.2034,
            -0.0469,
            -0.2007,
            0.0117,
            0.0201,
            0.0184,
            0.1271,
            -0.1089,
            -0.2148,
            0.1786
          ],
          "after": [
            0.0322,
            0.1161,
            -0.0498,
            -0.0624,
            0.0519,
            -0.0144,
            0.0961,
            0.0914,
            -0.0855,
            -0.0592,
            0.0204,
            -0.0217,
            0.1101,
            0.0254,
            -0.0744,
            0.2286
          ]
        },
        "position_0": {
          "grad": [
            0.0687,
            -0.0114,
            0.0859,
            -0.1451,
            0.0487,
            0.0059,
            -0.1992,
            -0.0649,
            0.0821,
            0.0555,
            0.2131,
            0.1179,
            -0.1803,
            -0.0677,
            -0.0457,
            -0.1687
          ],
          "after": [
            0.0262,
            -0.1493,
            -0.2591,
            0.0632,
            0.0347,
            -0.0418,
            -0.1511,
            0.0686,
            0.0997,
            -0.0183,
            0.0298,
            0.0971,
            0.0338,
            -0.0168,
            -0.1307,
            0.0568
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0021,
            0.0004,
            -0.0013,
            -0.0001,
            -0.0002,
            -0.0003,
            -0.0009,
            0.0001,
            -0.0002,
            -0.0013,
            -0.0003,
            -0.0015,
            -0.0006,
            0.0014,
            0.0005,
            -0.0016
          ],
          "after": [
            -0.0561,
            0.0234,
            -0.1204,
            -0.0731,
            0.09,
            0.1487,
            0.0864,
            -0.1627,
            0.0245,
            -0.0005,
            -0.1267,
            -0.0617,
            0.1018,
            -0.0697,
            0.019,
            0.0934
          ]
        }
      }
    },
    {
      "step": 390,
      "word": "chiagoziem",
      "loss": 3.3602,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.081,
            0.0068,
            0.0987,
            0.1193,
            0.1038,
            -0.057,
            0.0105,
            -0.027,
            -0.1018,
            -0.0005,
            0.001,
            0.0235,
            0.098,
            0.0321,
            -0.2147,
            0.1869
          ],
          "after": [
            0.0066,
            0.0126,
            0.1495,
            -0.0424,
            0.0294,
            -0.0591,
            -0.1253,
            -0.1437,
            -0.0277,
            0.0941,
            0.1524,
            0.0126,
            -0.0798,
            -0.0082,
            -0.0443,
            -0.1835
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1578,
            0.057,
            -0.2927,
            0.0303,
            -0.1962,
            -0.121,
            0.0459,
            0.0716,
            -0.2629,
            0.007,
            -0.006,
            -0.0582,
            0.2065,
            -0.0223,
            0.0264,
            0.056
          ],
          "after": [
            0.0322,
            0.1154,
            -0.0492,
            -0.0626,
            0.0519,
            -0.0138,
            0.0956,
            0.0917,
            -0.0848,
            -0.0589,
            0.0207,
            -0.0214,
            0.1094,
            0.026,
            -0.0742,
            0.2279
          ]
        },
        "position_0": {
          "grad": [
            -0.0156,
            0.0256,
            -0.013,
            0.0731,
            -0.0485,
            -0.0074,
            0.056,
            0.015,
            -0.0486,
            0.0332,
            0.0206,
            0.0084,
            0.0339,
            0.0143,
            -0.0337,
            0.0422
          ],
          "after": [
            0.0261,
            -0.1496,
            -0.2592,
            0.0629,
            0.035,
            -0.0419,
            -0.1513,
            0.0686,
            0.0997,
            -0.0186,
            0.0296,
            0.0969,
            0.034,
            -0.0166,
            -0.1307,
            0.0566
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0038,
            0.0017,
            -0.0034,
            0.0013,
            -0.0057,
            -0.0033,
            -0.0025,
            0.0027,
            -0.0039,
            0.0032,
            0.0035,
            -0.0003,
            0.0001,
            -0.0018,
            0.0058,
            -0.0051
          ],
          "after": [
            -0.057,
            0.0227,
            -0.1195,
            -0.0731,
            0.0907,
            0.149,
            0.087,
            -0.163,
            0.0256,
            -0.0005,
            -0.1267,
            -0.0608,
            0.1018,
            -0.0698,
            0.018,
            0.0943
          ]
        }
      }
    },
    {
      "step": 391,
      "word": "maryjean",
      "loss": 2.4323,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1031,
            0.0651,
            -0.0717,
            -0.0054,
            0.0811,
            0.0422,
            0.1061,
            -0.1381,
            0.163,
            -0.2386,
            -0.1046,
            0.2137,
            -0.1312,
            -0.165,
            0.042,
            0.3124
          ],
          "after": [
            0.0072,
            0.0121,
            0.1494,
            -0.0427,
            0.0287,
            -0.0588,
            -0.1259,
            -0.1428,
            -0.0278,
            0.0942,
            0.1528,
            0.0115,
            -0.0799,
            -0.0076,
            -0.0434,
            -0.1845
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1169,
            -0.1475,
            -0.0488,
            0.031,
            -0.3205,
            0.0969,
            -0.2131,
            -0.076,
            0.0625,
            0.134,
            0.1003,
            -0.1747,
            0.0658,
            0.0549,
            0.1329,
            -0.1394
          ],
          "after": [
            0.032,
            0.115,
            -0.0486,
            -0.0628,
            0.0523,
            -0.0133,
            0.0954,
            0.0922,
            -0.0842,
            -0.0589,
            0.0207,
            -0.0209,
            0.1088,
            0.0264,
            -0.0741,
            0.2275
          ]
        },
        "position_0": {
          "grad": [
            -0.0346,
            0.0344,
            -0.013,
            0.0285,
            0.1372,
            0.0294,
            0.1341,
            0.0778,
            0.0303,
            0.0309,
            0.0145,
            0.1241,
            -0.1015,
            -0.0902,
            -0.0184,
            0.0809
          ],
          "after": [
            0.0262,
            -0.1499,
            -0.2593,
            0.0625,
            0.0349,
            -0.0421,
            -0.1517,
            0.0683,
            0.0996,
            -0.0189,
            0.0294,
            0.0963,
            0.0344,
            -0.0161,
            -0.1307,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0037,
            -0.0043,
            0.0018,
            -0.0015,
            0.0007,
            0.0084,
            0.0001,
            -0.0039,
            0.0165,
            0.0056,
            -0.0001,
            0.0038,
            -0.0079,
            0.0007,
            0.0008
          ],
          "after": [
            -0.0576,
            0.0225,
            -0.1182,
            -0.0734,
            0.0914,
            0.1491,
            0.0868,
            -0.1632,
            0.027,
            -0.0018,
            -0.1273,
            -0.06,
            0.1014,
            -0.0689,
            0.0171,
            0.095
          ]
        }
      }
    },
    {
      "step": 392,
      "word": "julina",
      "loss": 2.1785,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1736,
            0.0545,
            -0.0579,
            -0.1537,
            -0.0518,
            -0.0378,
            -0.037,
            -0.0454,
            -0.0707,
            0.1566,
            0.1262,
            -0.1604,
            0.0825,
            0.0796,
            -0.0148,
            -0.2017
          ],
          "after": [
            0.0074,
            0.0115,
            0.1494,
            -0.0427,
            0.0281,
            -0.0584,
            -0.1264,
            -0.142,
            -0.0277,
            0.0939,
            0.1528,
            0.0108,
            -0.08,
            -0.0072,
            -0.0426,
            -0.185
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0633,
            0.1844,
            -0.1775,
            -0.0125,
            0.1036,
            -0.2231,
            0.3919,
            0.0023,
            -0.3119,
            0.0431,
            -0.0265,
            0.0062,
            0.2201,
            -0.1176,
            -0.2896,
            0.3132
          ],
          "after": [
            0.0319,
            0.1143,
            -0.0479,
            -0.0629,
            0.0525,
            -0.0126,
            0.0948,
            0.0927,
            -0.0834,
            -0.059,
            0.0207,
            -0.0205,
            0.108,
            0.0269,
            -0.0738,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            -0.0165,
            0.0521,
            0.0391,
            0.1549,
            0.1081,
            -0.0922,
            0.1568,
            -0.0016,
            -0.2366,
            -0.0435,
            0.0835,
            0.0424,
            0.3646,
            -0.1206,
            -0.1946,
            0.1618
          ],
          "after": [
            0.0263,
            -0.1503,
            -0.2595,
            0.0619,
            0.0346,
            -0.042,
            -0.1522,
            0.068,
            0.0999,
            -0.0191,
            0.0291,
            0.0957,
            0.0341,
            -0.0154,
            -0.1302,
            0.0558
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0008,
            -0.0002,
            -0.0029,
            -0.0018,
            0.0004,
            -0.0004,
            0.0002,
            -0.0018,
            -0.002,
            -0.002,
            -0.003,
            0.0017,
            0.003,
            0.0027,
            -0.0016
          ],
          "after": [
            -0.0579,
            0.0222,
            -0.1171,
            -0.0732,
            0.0921,
            0.1492,
            0.0868,
            -0.1634,
            0.0285,
            -0.0027,
            -0.1275,
            -0.059,
            0.101,
            -0.0685,
            0.0161,
            0.0957
          ]
        }
      }
    },
    {
      "step": 393,
      "word": "dameir",
      "loss": 2.1905,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0789,
            0.0233,
            0.142,
            -0.0491,
            0.2041,
            -0.0696,
            0.1572,
            0.0978,
            0.1301,
            -0.0257,
            0.0982,
            0.0847,
            -0.1782,
            -0.1017,
            -0.0815,
            -0.0557
          ],
          "after": [
            0.0078,
            0.0109,
            0.1492,
            -0.0426,
            0.0273,
            -0.0579,
            -0.127,
            -0.1416,
            -0.0279,
            0.0937,
            0.1525,
            0.0101,
            -0.0798,
            -0.0066,
            -0.0417,
            -0.1853
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0123,
            -0.038,
            0.1469,
            -0.1065,
            0.0803,
            0.0103,
            -0.0377,
            -0.0218,
            0.0655,
            -0.1506,
            0.0944,
            -0.0315,
            -0.1403,
            -0.1529,
            0.1461,
            -0.1321
          ],
          "after": [
            0.0319,
            0.1139,
            -0.0475,
            -0.0628,
            0.0526,
            -0.0121,
            0.0944,
            0.0932,
            -0.0829,
            -0.0588,
            0.0205,
            -0.02,
            0.1074,
            0.0277,
            -0.0737,
            0.2264
          ]
        },
        "position_0": {
          "grad": [
            -0.0284,
            0.0356,
            -0.0256,
            -0.1051,
            0.1506,
            -0.079,
            0.041,
            -0.0707,
            -0.0017,
            -0.0016,
            -0.0834,
            0.0015,
            0.0545,
            0.049,
            0.0167,
            0.1383
          ],
          "after": [
            0.0265,
            -0.1508,
            -0.2595,
            0.0615,
            0.0341,
            -0.0416,
            -0.1527,
            0.0681,
            0.1002,
            -0.0192,
            0.0289,
            0.0952,
            0.0338,
            -0.0149,
            -0.1299,
            0.0552
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0028,
            0.0015,
            0.0014,
            -0.0034,
            -0.0013,
            0.0025,
            0.0008,
            -0.0029,
            0.0025,
            -0.0067,
            -0.0047,
            0.0045,
            -0.0035,
            0.0014,
            0.0045,
            -0.0009
          ],
          "after": [
            -0.0578,
            0.0219,
            -0.1163,
            -0.0727,
            0.0929,
            0.149,
            0.0867,
            -0.1631,
            0.0295,
            -0.003,
            -0.1272,
            -0.0586,
            0.1009,
            -0.0683,
            0.0149,
            0.0965
          ]
        }
      }
    },
    {
      "step": 394,
      "word": "amirra",
      "loss": 2.285,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0455,
            0.1087,
            0.1221,
            -0.1142,
            0.03,
            -0.0157,
            0.2034,
            0.1433,
            0.0194,
            0.1218,
            0.2362,
            -0.0712,
            -0.1733,
            -0.0666,
            -0.0038,
            -0.3248
          ],
          "after": [
            0.008,
            0.0102,
            0.1487,
            -0.0423,
            0.0266,
            -0.0574,
            -0.1279,
            -0.1417,
            -0.0282,
            0.0932,
            0.1517,
            0.0096,
            -0.0794,
            -0.006,
            -0.0409,
            -0.1851
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0441,
            0.146,
            -0.1156,
            0.015,
            0.1131,
            -0.1917,
            0.2995,
            -0.0427,
            -0.2435,
            0.065,
            -0.0529,
            0.0235,
            0.1846,
            -0.1101,
            -0.3252,
            0.2157
          ],
          "after": [
            0.032,
            0.1133,
            -0.0469,
            -0.0627,
            0.0525,
            -0.0114,
            0.0938,
            0.0937,
            -0.0821,
            -0.0587,
            0.0205,
            -0.0197,
            0.1068,
            0.0285,
            -0.0733,
            0.2258
          ]
        },
        "position_0": {
          "grad": [
            0.089,
            -0.0527,
            0.1036,
            -0.169,
            -0.0227,
            0.0208,
            -0.265,
            -0.0494,
            0.1352,
            0.028,
            0.2151,
            0.0636,
            -0.2355,
            -0.0409,
            0.0092,
            -0.2349
          ],
          "after": [
            0.0263,
            -0.151,
            -0.2599,
            0.0616,
            0.0336,
            -0.0413,
            -0.1528,
            0.0683,
            0.1002,
            -0.0194,
            0.0284,
            0.0946,
            0.0339,
            -0.0144,
            -0.1296,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0002,
            0.0014,
            -0.0001,
            -0.0014,
            0.0019,
            0.0001,
            -0.0002,
            0.0013,
            0.0017,
            0.0033,
            0.0007,
            -0.0032,
            -0.0013,
            0.001,
            -0.0019
          ],
          "after": [
            -0.0577,
            0.0215,
            -0.1158,
            -0.0723,
            0.0937,
            0.1486,
            0.0866,
            -0.1629,
            0.0301,
            -0.0034,
            -0.1274,
            -0.0584,
            0.1011,
            -0.068,
            0.0139,
            0.0973
          ]
        }
      }
    },
    {
      "step": 395,
      "word": "aires",
      "loss": 2.4542,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0174,
            -0.5733,
            -0.0189,
            -0.2614,
            0.0107,
            0.1286,
            -0.5142,
            0.1711,
            0.179,
            -0.0662,
            0.1401,
            -0.302,
            -0.3354,
            0.1408,
            0.052,
            -0.3014
          ],
          "after": [
            0.0081,
            0.0108,
            0.1483,
            -0.0415,
            0.026,
            -0.0574,
            -0.1278,
            -0.1423,
            -0.0288,
            0.093,
            0.1506,
            0.0097,
            -0.0784,
            -0.0058,
            -0.0404,
            -0.1844
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0807,
            -0.2689,
            0.0767,
            -0.194,
            0.2197,
            0.0716,
            -0.3794,
            0.0729,
            0.1221,
            -0.1931,
            0.0504,
            -0.014,
            -0.2385,
            -0.0427,
            0.3337,
            -0.1815
          ],
          "after": [
            0.0322,
            0.1131,
            -0.0466,
            -0.0621,
            0.0522,
            -0.0108,
            0.0936,
            0.0939,
            -0.0816,
            -0.0583,
            0.0203,
            -0.0194,
            0.1065,
            0.0293,
            -0.0732,
            0.2254
          ]
        },
        "position_0": {
          "grad": [
            0.104,
            -0.099,
            0.1364,
            -0.2347,
            -0.0648,
            0.0237,
            -0.376,
            -0.053,
            0.2076,
            0.0054,
            0.2538,
            0.0181,
            -0.2997,
            -0.0225,
            0.0288,
            -0.3139
          ],
          "after": [
            0.0257,
            -0.1509,
            -0.2608,
            0.0621,
            0.0334,
            -0.0412,
            -0.1523,
            0.0687,
            0.0999,
            -0.0196,
            0.0274,
            0.0941,
            0.0345,
            -0.0139,
            -0.1294,
            0.0553
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0011,
            -0.0002,
            -0.0019,
            0.0027,
            -0.0007,
            0.0002,
            0.0005,
            0.0003,
            -0.0013,
            -0.0001,
            0.0001,
            -0.0005,
            -0.0007,
            -0.0005,
            0.0003
          ],
          "after": [
            -0.0575,
            0.0214,
            -0.1153,
            -0.0717,
            0.0941,
            0.1483,
            0.0865,
            -0.1627,
            0.0305,
            -0.0036,
            -0.1275,
            -0.0581,
            0.1013,
            -0.0677,
            0.013,
            0.0979
          ]
        }
      }
    },
    {
      "step": 396,
      "word": "harliee",
      "loss": 1.8654,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1275,
            -0.1137,
            -0.0918,
            -0.1427,
            0.057,
            0.0434,
            0.0075,
            -0.1135,
            0.1348,
            -0.069,
            0.019,
            -0.0247,
            -0.0881,
            -0.0886,
            0.0435,
            0.0823
          ],
          "after": [
            0.0079,
            0.0116,
            0.1482,
            -0.0406,
            0.0254,
            -0.0575,
            -0.1278,
            -0.1425,
            -0.0296,
            0.0929,
            0.1496,
            0.0098,
            -0.0775,
            -0.0055,
            -0.0401,
            -0.184
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2811,
            -0.0586,
            -0.4662,
            0.0439,
            -0.8721,
            0.3021,
            -0.1651,
            0.0009,
            0.1147,
            -0.0345,
            -0.0886,
            -0.2022,
            0.0976,
            0.2613,
            0.8016,
            -0.2405
          ],
          "after": [
            0.0318,
            0.1131,
            -0.0457,
            -0.0617,
            0.0529,
            -0.0108,
            0.0936,
            0.0941,
            -0.0813,
            -0.0578,
            0.0204,
            -0.0188,
            0.1061,
            0.0295,
            -0.0738,
            0.2254
          ]
        },
        "position_0": {
          "grad": [
            0.005,
            -0.0665,
            0.0601,
            -0.0058,
            -0.0614,
            0.0777,
            -0.0691,
            0.0558,
            0.1212,
            0.0634,
            -0.0751,
            -0.1027,
            0.0955,
            0.0696,
            0.0253,
            0.0342
          ],
          "after": [
            0.0251,
            -0.1507,
            -0.2617,
            0.0626,
            0.0333,
            -0.0413,
            -0.1518,
            0.0689,
            0.0994,
            -0.02,
            0.0267,
            0.0939,
            0.0348,
            -0.0137,
            -0.1294,
            0.0556
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            -0.0017,
            -0.0029,
            0.0012,
            -0.0039,
            0.0008,
            -0.0022,
            0.0001,
            0.0003,
            0.0012,
            -0.0012,
            0.001,
            -0.0018,
            0.0023,
            0.0037,
            -0.0009
          ],
          "after": [
            -0.0576,
            0.0214,
            -0.1146,
            -0.0714,
            0.0948,
            0.148,
            0.0865,
            -0.1626,
            0.0309,
            -0.0039,
            -0.1274,
            -0.058,
            0.1017,
            -0.0677,
            0.012,
            0.0986
          ]
        }
      }
    },
    {
      "step": 397,
      "word": "emiliano",
      "loss": 2.2492,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1551,
            0.1086,
            -0.0419,
            0.0657,
            -0.0871,
            -0.0375,
            -0.0097,
            -0.034,
            0.0931,
            -0.2025,
            -0.1182,
            0.1581,
            -0.0813,
            -0.0355,
            0.0544,
            0.1583
          ],
          "after": [
            0.0081,
            0.012,
            0.1482,
            -0.0399,
            0.025,
            -0.0575,
            -0.1277,
            -0.1425,
            -0.0306,
            0.0934,
            0.1491,
            0.0097,
            -0.0765,
            -0.0051,
            -0.04,
            -0.1838
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2316,
            0.4265,
            0.3199,
            -0.0254,
            0.1818,
            -0.155,
            0.4558,
            -0.0387,
            -0.1048,
            -0.0795,
            -0.0247,
            -0.0376,
            0.0415,
            0.009,
            -0.2431,
            0.0845
          ],
          "after": [
            0.032,
            0.1125,
            -0.0454,
            -0.0613,
            0.0533,
            -0.0105,
            0.0932,
            0.0944,
            -0.0809,
            -0.0573,
            0.0206,
            -0.0182,
            0.1058,
            0.0296,
            -0.0742,
            0.2253
          ]
        },
        "position_0": {
          "grad": [
            -0.0142,
            -0.147,
            0.0456,
            -0.039,
            -0.062,
            0.0599,
            -0.186,
            -0.0503,
            0.109,
            0.0858,
            -0.0188,
            -0.0036,
            -0.147,
            -0.027,
            0.0568,
            -0.2307
          ],
          "after": [
            0.0247,
            -0.1501,
            -0.2626,
            0.0631,
            0.0334,
            -0.0417,
            -0.1511,
            0.0692,
            0.0988,
            -0.0206,
            0.0262,
            0.0937,
            0.0352,
            -0.0134,
            -0.1294,
            0.0561
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0022,
            -0.0022,
            0.0001,
            0.0015,
            -0.0052,
            0.0027,
            -0.0034,
            0.0019,
            0.0012,
            -0.0,
            0.0012,
            -0.0039,
            -0.0008,
            0.0013,
            0.0016,
            -0.0014
          ],
          "after": [
            -0.058,
            0.0217,
            -0.114,
            -0.0712,
            0.096,
            0.1474,
            0.0868,
            -0.1628,
            0.0311,
            -0.0041,
            -0.1275,
            -0.0576,
            0.102,
            -0.0678,
            0.011,
            0.0993
          ]
        }
      }
    },
    {
      "step": 398,
      "word": "joey",
      "loss": 2.7946,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0083,
            0.0123,
            0.1482,
            -0.0393,
            0.0246,
            -0.0575,
            -0.1276,
            -0.1426,
            -0.0314,
            0.0939,
            0.1487,
            0.0095,
            -0.0757,
            -0.0047,
            -0.0398,
            -0.1837
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2453,
            0.4632,
            -0.2506,
            0.2532,
            -0.0327,
            0.2436,
            0.2297,
            0.0002,
            0.0259,
            -0.1553,
            -0.2146,
            0.1863,
            -0.1843,
            -0.0527,
            0.6002,
            0.339
          ],
          "after": [
            0.0325,
            0.1113,
            -0.0448,
            -0.0616,
            0.0536,
            -0.0106,
            0.0927,
            0.0946,
            -0.0806,
            -0.0566,
            0.0212,
            -0.018,
            0.1057,
            0.0299,
            -0.075,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            -0.021,
            0.0182,
            0.0532,
            0.1674,
            0.0652,
            -0.1474,
            0.1392,
            0.0136,
            -0.2683,
            -0.0891,
            0.0894,
            -0.0202,
            0.442,
            -0.1398,
            -0.2299,
            0.1543
          ],
          "after": [
            0.0245,
            -0.1497,
            -0.2636,
            0.0632,
            0.0333,
            -0.0415,
            -0.1507,
            0.0694,
            0.0987,
            -0.0208,
            0.0255,
            0.0936,
            0.035,
            -0.0127,
            -0.1289,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0025,
            0.0058,
            -0.0033,
            0.0048,
            0.0003,
            0.0033,
            -0.0023,
            -0.0018,
            0.0009,
            -0.0017,
            -0.0035,
            0.0042,
            -0.0072,
            0.0,
            0.0078,
            0.0048
          ],
          "after": [
            -0.058,
            0.0213,
            -0.113,
            -0.0717,
            0.0969,
            0.1466,
            0.0872,
            -0.1627,
            0.0311,
            -0.0042,
            -0.1272,
            -0.0576,
            0.1029,
            -0.0679,
            0.0096,
            0.0994
          ]
        }
      }
    },
    {
      "step": 399,
      "word": "arafat",
      "loss": 2.5426,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2392,
            -0.0203,
            0.1838,
            0.2335,
            0.3352,
            -0.105,
            0.478,
            -0.1634,
            -0.498,
            -0.4692,
            -0.0607,
            -0.0871,
            0.3854,
            -0.2564,
            0.0403,
            0.6318
          ],
          "after": [
            0.0089,
            0.0127,
            0.1478,
            -0.0393,
            0.0238,
            -0.0571,
            -0.1283,
            -0.1421,
            -0.0309,
            0.0953,
            0.1485,
            0.0096,
            -0.0756,
            -0.0039,
            -0.0398,
            -0.1846
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0595,
            0.1269,
            -0.0651,
            0.0366,
            0.0419,
            -0.1618,
            0.2061,
            -0.0382,
            -0.161,
            0.0478,
            -0.0085,
            0.0238,
            0.1352,
            -0.0951,
            -0.2075,
            0.1121
          ],
          "after": [
            0.0331,
            0.1102,
            -0.0442,
            -0.062,
            0.0539,
            -0.0105,
            0.0921,
            0.095,
            -0.0802,
            -0.056,
            0.0218,
            -0.0179,
            0.1055,
            0.0302,
            -0.0754,
            0.2243
          ]
        },
        "position_0": {
          "grad": [
            0.0887,
            -0.0444,
            0.093,
            -0.1549,
            -0.0119,
            0.0135,
            -0.2521,
            -0.0575,
            0.1123,
            0.0246,
            0.2184,
            0.0545,
            -0.2043,
            -0.0237,
            0.0085,
            -0.2052
          ],
          "after": [
            0.0239,
            -0.1491,
            -0.2648,
            0.0635,
            0.0333,
            -0.0413,
            -0.1501,
            0.0697,
            0.0985,
            -0.0211,
            0.0245,
            0.0934,
            0.0351,
            -0.0121,
            -0.1286,
            0.0568
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            0.0022,
            0.0033,
            0.0009,
            0.0022,
            -0.0001,
            -0.0005,
            -0.0032,
            -0.0014,
            0.0024,
            0.0037,
            0.0003,
            0.0001,
            0.0006,
            -0.0041,
            -0.0031
          ],
          "after": [
            -0.0582,
            0.0207,
            -0.1127,
            -0.0721,
            0.0974,
            0.1459,
            0.0876,
            -0.1621,
            0.0312,
            -0.0044,
            -0.1273,
            -0.0576,
            0.1037,
            -0.0681,
            0.0087,
            0.0998
          ]
        }
      }
    },
    {
      "step": 400,
      "word": "bryler",
      "loss": 2.4345,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0094,
            0.0129,
            0.1474,
            -0.0392,
            0.0231,
            -0.0568,
            -0.1289,
            -0.1417,
            -0.0306,
            0.0966,
            0.1483,
            0.0096,
            -0.0756,
            -0.0031,
            -0.0398,
            -0.1853
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0126,
            -0.2583,
            0.1375,
            -0.0107,
            0.0746,
            0.0077,
            -0.2268,
            0.0207,
            0.0157,
            0.0443,
            -0.1166,
            0.0226,
            0.0292,
            0.1267,
            -0.1354,
            -0.1344
          ],
          "after": [
            0.0336,
            0.1096,
            -0.0438,
            -0.0623,
            0.0541,
            -0.0104,
            0.0917,
            0.0952,
            -0.0799,
            -0.0556,
            0.0226,
            -0.0179,
            0.1053,
            0.0303,
            -0.0757,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            0.0529,
            0.0006,
            0.1251,
            0.0414,
            0.0301,
            0.0005,
            0.0941,
            -0.0171,
            -0.1312,
            -0.0691,
            0.0159,
            -0.0162,
            0.1723,
            -0.0918,
            -0.039,
            0.2864
          ],
          "after": [
            0.0231,
            -0.1487,
            -0.2662,
            0.0638,
            0.0332,
            -0.0412,
            -0.1496,
            0.0701,
            0.0985,
            -0.0211,
            0.0237,
            0.0932,
            0.0349,
            -0.0113,
            -0.1281,
            0.0568
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.004,
            0.0035,
            0.0022,
            -0.0034,
            -0.003,
            0.0037,
            -0.0017,
            -0.0026,
            -0.0017,
            -0.0025,
            -0.0002,
            0.0075,
            0.0016,
            -0.0017,
            0.0012
          ],
          "after": [
            -0.0584,
            0.0198,
            -0.1128,
            -0.0727,
            0.0982,
            0.1456,
            0.0877,
            -0.1614,
            0.0318,
            -0.0045,
            -0.1272,
            -0.0576,
            0.1037,
            -0.0685,
            0.008,
            0.1001
          ]
        }
      }
    },
    {
      "step": 401,
      "word": "momo",
      "loss": 2.6238,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0099,
            0.0132,
            0.1472,
            -0.0392,
            0.0225,
            -0.0566,
            -0.1294,
            -0.1414,
            -0.0302,
            0.0976,
            0.1481,
            0.0096,
            -0.0756,
            -0.0025,
            -0.0398,
            -0.186
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0473,
            0.0461,
            -0.2016,
            0.0537,
            -0.0377,
            -0.1318,
            0.2073,
            0.0314,
            -0.2893,
            0.0729,
            -0.0839,
            -0.0303,
            0.243,
            -0.0202,
            -0.2262,
            0.2036
          ],
          "after": [
            0.034,
            0.109,
            -0.0433,
            -0.0626,
            0.0542,
            -0.0102,
            0.0913,
            0.0953,
            -0.0793,
            -0.0555,
            0.0235,
            -0.0178,
            0.1048,
            0.0304,
            -0.0758,
            0.2236
          ]
        },
        "position_0": {
          "grad": [
            -0.073,
            0.0172,
            -0.0105,
            0.0501,
            0.1806,
            0.0708,
            0.1846,
            0.1434,
            0.1194,
            -0.0176,
            0.0238,
            0.1286,
            -0.1731,
            -0.0493,
            0.0321,
            0.1343
          ],
          "after": [
            0.0228,
            -0.1484,
            -0.2674,
            0.0639,
            0.0327,
            -0.0413,
            -0.1495,
            0.0699,
            0.0983,
            -0.021,
            0.0229,
            0.0928,
            0.035,
            -0.0104,
            -0.1279,
            0.0566
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            0.0016,
            -0.0029,
            0.0039,
            -0.0001,
            0.0002,
            0.0027,
            0.0002,
            -0.0038,
            0.0011,
            -0.0032,
            -0.0008,
            0.0058,
            0.0026,
            -0.0037,
            0.0039
          ],
          "after": [
            -0.0587,
            0.0188,
            -0.1125,
            -0.0737,
            0.0989,
            0.1454,
            0.0876,
            -0.1608,
            0.0327,
            -0.0046,
            -0.1267,
            -0.0576,
            0.1033,
            -0.0691,
            0.0078,
            0.0998
          ]
        }
      }
    },
    {
      "step": 402,
      "word": "sinead",
      "loss": 2.762,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2593,
            0.0297,
            0.094,
            0.0134,
            0.2111,
            -0.0692,
            0.154,
            -0.0516,
            0.0338,
            -0.1428,
            -0.1344,
            0.1244,
            0.0556,
            0.0053,
            -0.0208,
            0.3326
          ],
          "after": [
            0.0108,
            0.0133,
            0.1467,
            -0.0392,
            0.0217,
            -0.0561,
            -0.1301,
            -0.1409,
            -0.03,
            0.0989,
            0.1484,
            0.0094,
            -0.0757,
            -0.002,
            -0.0398,
            -0.187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0241,
            -0.0161,
            0.0286,
            -0.0999,
            0.0515,
            0.0053,
            -0.2631,
            -0.0404,
            -0.0962,
            -0.2767,
            -0.0276,
            -0.1265,
            -0.0419,
            0.0334,
            0.1374,
            -0.0828
          ],
          "after": [
            0.0345,
            0.1085,
            -0.0429,
            -0.0627,
            0.0543,
            -0.01,
            0.0911,
            0.0955,
            -0.0787,
            -0.0548,
            0.0243,
            -0.0175,
            0.1044,
            0.0304,
            -0.076,
            0.2233
          ]
        },
        "position_0": {
          "grad": [
            0.0131,
            0.0377,
            -0.014,
            0.0464,
            -0.2136,
            0.0689,
            -0.0889,
            0.0675,
            0.0281,
            0.0032,
            -0.1288,
            -0.0107,
            0.0342,
            0.0302,
            0.1729,
            -0.07
          ],
          "after": [
            0.0225,
            -0.1482,
            -0.2683,
            0.0639,
            0.0328,
            -0.0417,
            -0.1493,
            0.0695,
            0.0981,
            -0.021,
            0.0225,
            0.0924,
            0.035,
            -0.0098,
            -0.128,
            0.0566
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0017,
            -0.0005,
            -0.0013,
            -0.0009,
            -0.0002,
            -0.0018,
            -0.0007,
            0.0002,
            0.0011,
            0.0007,
            -0.001,
            -0.0008,
            0.0007,
            0.0009,
            -0.0009
          ],
          "after": [
            -0.059,
            0.0182,
            -0.1122,
            -0.0744,
            0.0996,
            0.1452,
            0.0876,
            -0.1602,
            0.0335,
            -0.0048,
            -0.1264,
            -0.0574,
            0.1029,
            -0.0697,
            0.0075,
            0.0998
          ]
        }
      }
    },
    {
      "step": 403,
      "word": "amera",
      "loss": 1.9475,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2399,
            0.0285,
            0.0114,
            -0.0712,
            -0.0427,
            -0.0173,
            0.1788,
            0.1629,
            -0.0709,
            0.2322,
            0.2479,
            -0.2998,
            -0.0764,
            0.0128,
            0.0439,
            -0.3701
          ],
          "after": [
            0.011,
            0.0134,
            0.1464,
            -0.0391,
            0.0211,
            -0.0556,
            -0.131,
            -0.141,
            -0.0297,
            0.0993,
            0.1479,
            0.0097,
            -0.0756,
            -0.0015,
            -0.0398,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1468,
            -0.0763,
            0.3379,
            0.0475,
            -0.0028,
            0.2404,
            -0.2348,
            -0.0176,
            0.3477,
            0.1149,
            0.3377,
            0.1211,
            -0.4154,
            -0.0892,
            0.152,
            -0.3197
          ],
          "after": [
            0.0346,
            0.1082,
            -0.0429,
            -0.0629,
            0.0544,
            -0.0101,
            0.0911,
            0.0958,
            -0.0786,
            -0.0544,
            0.0242,
            -0.0174,
            0.1046,
            0.0306,
            -0.0763,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            0.098,
            -0.073,
            0.0952,
            -0.2234,
            -0.0289,
            0.0203,
            -0.3326,
            -0.0544,
            0.1763,
            0.0392,
            0.2451,
            0.0625,
            -0.2649,
            -0.0315,
            -0.0035,
            -0.2856
          ],
          "after": [
            0.0218,
            -0.1479,
            -0.2695,
            0.0643,
            0.0329,
            -0.042,
            -0.1486,
            0.0694,
            0.0977,
            -0.0211,
            0.0216,
            0.092,
            0.0355,
            -0.0092,
            -0.1281,
            0.0569
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0019,
            0.0009,
            -0.0007,
            -0.0018,
            0.0028,
            -0.0029,
            0.0007,
            0.0018,
            -0.0012,
            0.0011,
            0.0005,
            -0.0042,
            0.0009,
            0.0034,
            -0.001
          ],
          "after": [
            -0.0591,
            0.0179,
            -0.1121,
            -0.0749,
            0.1004,
            0.1447,
            0.0878,
            -0.1598,
            0.0339,
            -0.0049,
            -0.1262,
            -0.0573,
            0.103,
            -0.0704,
            0.0069,
            0.0998
          ]
        }
      }
    },
    {
      "step": 404,
      "word": "branson",
      "loss": 2.3216,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0642,
            0.0872,
            -0.1348,
            0.0732,
            -0.2111,
            -0.0145,
            -0.0291,
            -0.0628,
            0.013,
            0.0019,
            -0.0588,
            -0.0212,
            0.0409,
            0.0942,
            0.0235,
            0.0573
          ],
          "after": [
            0.0111,
            0.0133,
            0.1463,
            -0.0391,
            0.0209,
            -0.0552,
            -0.1317,
            -0.1409,
            -0.0294,
            0.0997,
            0.1477,
            0.01,
            -0.0756,
            -0.0014,
            -0.0399,
            -0.1876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0628,
            0.0696,
            -0.074,
            0.0078,
            0.0356,
            -0.1927,
            0.2302,
            -0.0,
            -0.189,
            0.0352,
            -0.0211,
            0.0593,
            0.14,
            -0.0926,
            -0.1847,
            0.1507
          ],
          "after": [
            0.0348,
            0.1078,
            -0.0429,
            -0.063,
            0.0544,
            -0.01,
            0.091,
            0.096,
            -0.0783,
            -0.0542,
            0.0241,
            -0.0175,
            0.1046,
            0.031,
            -0.0763,
            0.2233
          ]
        },
        "position_0": {
          "grad": [
            0.0462,
            0.0111,
            0.0984,
            0.0235,
            0.0361,
            -0.0171,
            0.091,
            -0.0168,
            -0.1244,
            -0.051,
            0.0057,
            0.0058,
            0.1463,
            -0.0987,
            -0.0384,
            0.2525
          ],
          "after": [
            0.021,
            -0.1476,
            -0.2707,
            0.0646,
            0.0329,
            -0.0423,
            -0.1482,
            0.0693,
            0.0975,
            -0.021,
            0.0209,
            0.0915,
            0.0356,
            -0.0083,
            -0.1282,
            0.0568
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0039,
            0.0015,
            -0.0006,
            0.0029,
            -0.0008,
            -0.0013,
            0.005,
            0.0013,
            -0.0019,
            0.0022,
            -0.0002,
            0.0026,
            0.0023,
            -0.0016,
            0.004,
            0.0031
          ],
          "after": [
            -0.0587,
            0.0174,
            -0.1119,
            -0.0756,
            0.1011,
            0.1445,
            0.0876,
            -0.1597,
            0.0345,
            -0.0052,
            -0.1261,
            -0.0575,
            0.1029,
            -0.0707,
            0.0062,
            0.0995
          ]
        }
      }
    },
    {
      "step": 405,
      "word": "calum",
      "loss": 2.705,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0952,
            0.1343,
            -0.1796,
            -0.0937,
            -0.0082,
            0.0055,
            -0.1654,
            0.049,
            -0.1337,
            -0.0181,
            -0.0888,
            0.2284,
            -0.0196,
            0.0683,
            0.1045,
            -0.2578
          ],
          "after": [
            0.0114,
            0.0129,
            0.1466,
            -0.039,
            0.0207,
            -0.0548,
            -0.132,
            -0.141,
            -0.0289,
            0.1001,
            0.1477,
            0.0099,
            -0.0756,
            -0.0014,
            -0.0403,
            -0.1876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0111,
            0.19,
            -0.2432,
            0.0652,
            0.0244,
            -0.2201,
            0.3093,
            0.0024,
            -0.3669,
            0.034,
            -0.0431,
            -0.0289,
            0.2726,
            -0.0949,
            -0.2733,
            0.3296
          ],
          "after": [
            0.035,
            0.1073,
            -0.0426,
            -0.0634,
            0.0544,
            -0.0096,
            0.0906,
            0.0962,
            -0.0776,
            -0.054,
            0.0241,
            -0.0175,
            0.1043,
            0.0314,
            -0.0762,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0142,
            0.0148,
            -0.0128,
            0.131,
            -0.1187,
            -0.0298,
            0.0806,
            0.0472,
            -0.0672,
            0.0326,
            0.0167,
            -0.0272,
            0.0717,
            0.0422,
            -0.0429,
            0.0453
          ],
          "after": [
            0.0204,
            -0.1474,
            -0.2718,
            0.0647,
            0.0332,
            -0.0424,
            -0.1479,
            0.0691,
            0.0974,
            -0.021,
            0.0202,
            0.0913,
            0.0356,
            -0.0078,
            -0.1281,
            0.0567
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0024,
            0.0041,
            -0.001,
            0.003,
            -0.0021,
            -0.0006,
            0.004,
            -0.0012,
            0.0007,
            0.0027,
            0.0028,
            0.0005,
            0.0014,
            0.0001,
            -0.0027,
            0.0041
          ],
          "after": [
            -0.0587,
            0.0166,
            -0.1116,
            -0.0766,
            0.1019,
            0.1443,
            0.0872,
            -0.1593,
            0.035,
            -0.0056,
            -0.1262,
            -0.0577,
            0.1027,
            -0.071,
            0.0058,
            0.0988
          ]
        }
      }
    },
    {
      "step": 406,
      "word": "najee",
      "loss": 2.5386,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1565,
            0.1559,
            0.3542,
            0.2792,
            0.0508,
            -0.0952,
            0.3184,
            0.1559,
            -0.3357,
            -0.0798,
            -0.1067,
            0.0242,
            0.3879,
            -0.169,
            -0.2254,
            0.0988
          ],
          "after": [
            0.012,
            0.0123,
            0.1462,
            -0.0394,
            0.0205,
            -0.0542,
            -0.1327,
            -0.1415,
            -0.0278,
            0.1007,
            0.148,
            0.0098,
            -0.0762,
            -0.0011,
            -0.04,
            -0.1876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0654,
            -0.2912,
            -0.0546,
            -0.0304,
            -0.3103,
            0.4662,
            -0.5423,
            -0.0979,
            0.3659,
            -0.3945,
            -0.1578,
            0.0207,
            -0.3239,
            0.1374,
            0.939,
            -0.3877
          ],
          "after": [
            0.035,
            0.1072,
            -0.0422,
            -0.0635,
            0.0548,
            -0.0098,
            0.0907,
            0.0967,
            -0.0775,
            -0.0532,
            0.0245,
            -0.0176,
            0.1044,
            0.0315,
            -0.0768,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0187,
            0.0818,
            -0.071,
            0.0148,
            -0.3575,
            0.0027,
            -0.0279,
            0.0024,
            0.0953,
            -0.1863,
            -0.2468,
            -0.1047,
            -0.0468,
            0.1246,
            0.2426,
            0.0755
          ],
          "after": [
            0.0199,
            -0.1475,
            -0.2725,
            0.0646,
            0.0341,
            -0.0425,
            -0.1477,
            0.0688,
            0.0972,
            -0.0203,
            0.0202,
            0.0913,
            0.0357,
            -0.0076,
            -0.1286,
            0.0565
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0006,
            -0.0023,
            -0.002,
            -0.0033,
            0.0008,
            -0.002,
            -0.0024,
            0.0008,
            -0.0009,
            -0.002,
            0.0005,
            -0.0018,
            0.0029,
            0.0052,
            -0.0021
          ],
          "after": [
            -0.0586,
            0.016,
            -0.1111,
            -0.0772,
            0.1029,
            0.1441,
            0.087,
            -0.1587,
            0.0352,
            -0.0059,
            -0.1262,
            -0.0579,
            0.1026,
            -0.0717,
            0.0051,
            0.0985
          ]
        }
      }
    },
    {
      "step": 407,
      "word": "rylieann",
      "loss": 2.3045,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.164,
            0.1117,
            -0.0284,
            0.0876,
            -0.0816,
            -0.0386,
            0.0063,
            -0.026,
            0.0857,
            -0.204,
            -0.1204,
            0.1736,
            -0.082,
            -0.0461,
            0.0508,
            0.1734
          ],
          "after": [
            0.0127,
            0.0115,
            0.1459,
            -0.0399,
            0.0204,
            -0.0535,
            -0.1334,
            -0.1419,
            -0.027,
            0.1016,
            0.1486,
            0.0094,
            -0.0766,
            -0.0007,
            -0.04,
            -0.188
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0317,
            -0.0033,
            0.003,
            0.1885,
            -0.2221,
            -0.0682,
            -0.0262,
            0.0116,
            -0.0143,
            -0.1267,
            -0.0503,
            0.1075,
            0.0249,
            -0.0048,
            0.1191,
            0.1087
          ],
          "after": [
            0.0351,
            0.1071,
            -0.0419,
            -0.0642,
            0.0553,
            -0.01,
            0.0908,
            0.0971,
            -0.0773,
            -0.0522,
            0.025,
            -0.0178,
            0.1044,
            0.0317,
            -0.0774,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.0318,
            -0.0336,
            0.0139,
            -0.0539,
            -0.0338,
            0.093,
            0.0419,
            -0.0134,
            0.1172,
            -0.0715,
            -0.0867,
            -0.0149,
            -0.1384,
            -0.038,
            0.1243,
            0.0476
          ],
          "after": [
            0.0194,
            -0.1474,
            -0.2731,
            0.0647,
            0.0351,
            -0.0429,
            -0.1475,
            0.0687,
            0.0969,
            -0.0195,
            0.0203,
            0.0914,
            0.036,
            -0.0074,
            -0.1292,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0011,
            -0.003,
            0.0018,
            -0.0062,
            0.0017,
            -0.0016,
            0.0011,
            0.0011,
            -0.0016,
            -0.0031,
            -0.0004,
            -0.0005,
            0.002,
            0.0054,
            0.0006
          ],
          "after": [
            -0.0586,
            0.0156,
            -0.1103,
            -0.0779,
            0.1043,
            0.1437,
            0.0869,
            -0.1584,
            0.0353,
            -0.0061,
            -0.1258,
            -0.0581,
            0.1026,
            -0.0724,
            0.004,
            0.0981
          ]
        }
      }
    },
    {
      "step": 408,
      "word": "jack",
      "loss": 2.6748,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0366,
            0.0505,
            0.0763,
            0.2847,
            -0.1762,
            -0.0725,
            0.1658,
            0.1149,
            -0.1754,
            0.0766,
            0.0699,
            -0.063,
            0.1563,
            0.1217,
            -0.096,
            0.0254
          ],
          "after": [
            0.0135,
            0.0108,
            0.1455,
            -0.0409,
            0.0207,
            -0.0527,
            -0.1342,
            -0.1426,
            -0.026,
            0.1022,
            0.1489,
            0.0092,
            -0.0772,
            -0.0006,
            -0.0396,
            -0.1883
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.043,
            0.207,
            -0.2234,
            -0.0056,
            -0.0295,
            -0.1853,
            0.3889,
            0.0662,
            -0.3302,
            0.0335,
            -0.0616,
            -0.0487,
            0.2267,
            -0.0707,
            -0.1564,
            0.311
          ],
          "after": [
            0.0352,
            0.1068,
            -0.0414,
            -0.0647,
            0.0558,
            -0.0098,
            0.0906,
            0.0972,
            -0.0769,
            -0.0515,
            0.0256,
            -0.0179,
            0.1042,
            0.0319,
            -0.0778,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            -0.0284,
            0.0901,
            0.0146,
            0.2409,
            0.1205,
            -0.1507,
            0.2746,
            0.0222,
            -0.3523,
            -0.0727,
            0.0471,
            0.0574,
            0.5096,
            -0.1709,
            -0.2409,
            0.2312
          ],
          "after": [
            0.0191,
            -0.1477,
            -0.2737,
            0.0643,
            0.0356,
            -0.0428,
            -0.1478,
            0.0685,
            0.0971,
            -0.0186,
            0.0203,
            0.0913,
            0.0355,
            -0.0067,
            -0.1293,
            0.0557
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0079,
            0.0024,
            -0.0029,
            0.0017,
            -0.0003,
            0.0007,
            0.0017,
            -0.0019,
            -0.0022,
            0.0023,
            0.0019,
            -0.0028,
            0.0039,
            0.0028,
            -0.0039,
            0.0019
          ],
          "after": [
            -0.0596,
            0.0149,
            -0.1093,
            -0.0787,
            0.1056,
            0.1433,
            0.0867,
            -0.1578,
            0.0356,
            -0.0063,
            -0.1256,
            -0.0579,
            0.1023,
            -0.0734,
            0.0035,
            0.0976
          ]
        }
      }
    },
    {
      "step": 409,
      "word": "mahmud",
      "loss": 2.8862,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0234,
            0.0356,
            0.1702,
            0.1359,
            0.0639,
            0.1072,
            0.0813,
            0.0673,
            0.0256,
            0.1511,
            -0.0419,
            -0.1235,
            0.3081,
            0.1181,
            -0.1074,
            0.1488
          ],
          "after": [
            0.0141,
            0.0101,
            0.1448,
            -0.042,
            0.0208,
            -0.0524,
            -0.1351,
            -0.1434,
            -0.0252,
            0.1023,
            0.1492,
            0.0092,
            -0.0782,
            -0.0008,
            -0.0391,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1127,
            0.255,
            -0.2038,
            0.0968,
            0.1429,
            -0.2568,
            0.4041,
            0.0758,
            -0.3393,
            0.0264,
            -0.1032,
            0.0594,
            0.2346,
            -0.103,
            -0.3565,
            0.3555
          ],
          "after": [
            0.0356,
            0.1062,
            -0.0406,
            -0.0654,
            0.056,
            -0.0094,
            0.0901,
            0.097,
            -0.0761,
            -0.0509,
            0.0263,
            -0.0181,
            0.1038,
            0.0323,
            -0.0779,
            0.2217
          ]
        },
        "position_0": {
          "grad": [
            -0.0602,
            0.0651,
            -0.0329,
            0.0681,
            0.194,
            0.0478,
            0.2069,
            0.1066,
            0.0318,
            0.0149,
            -0.0092,
            0.1643,
            -0.0873,
            -0.0702,
            -0.0174,
            0.1476
          ],
          "after": [
            0.0191,
            -0.148,
            -0.274,
            0.0638,
            0.0356,
            -0.0428,
            -0.1483,
            0.068,
            0.0973,
            -0.0178,
            0.0203,
            0.0908,
            0.0352,
            -0.006,
            -0.1293,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0076,
            0.0084,
            -0.0053,
            0.0022,
            0.0092,
            -0.007,
            0.0089,
            0.0017,
            -0.0039,
            0.0059,
            -0.0006,
            0.0017,
            0.0089,
            -0.0005,
            -0.0118,
            0.006
          ],
          "after": [
            -0.0613,
            0.0136,
            -0.1078,
            -0.0796,
            0.1058,
            0.1438,
            0.086,
            -0.1575,
            0.0365,
            -0.0071,
            -0.1255,
            -0.058,
            0.1013,
            -0.0742,
            0.0038,
            0.0966
          ]
        }
      }
    },
    {
      "step": 410,
      "word": "mahrukh",
      "loss": 2.8654,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0259,
            0.0371,
            0.1563,
            0.1112,
            0.06,
            0.0923,
            0.0727,
            0.0593,
            0.0205,
            0.1364,
            -0.0403,
            -0.1053,
            0.2633,
            0.0918,
            -0.0998,
            0.1263
          ],
          "after": [
            0.0148,
            0.0094,
            0.1439,
            -0.0431,
            0.0207,
            -0.0524,
            -0.1359,
            -0.1442,
            -0.0245,
            0.1021,
            0.1496,
            0.0094,
            -0.0795,
            -0.0012,
            -0.0384,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1029,
            0.1688,
            -0.1228,
            0.0563,
            0.1042,
            -0.2188,
            0.3238,
            0.0394,
            -0.2303,
            0.0262,
            -0.067,
            0.0797,
            0.1361,
            -0.0936,
            -0.2618,
            0.2988
          ],
          "after": [
            0.036,
            0.1054,
            -0.0399,
            -0.0661,
            0.0561,
            -0.0088,
            0.0894,
            0.0966,
            -0.0753,
            -0.0504,
            0.0271,
            -0.0184,
            0.1032,
            0.0328,
            -0.0777,
            0.2207
          ]
        },
        "position_0": {
          "grad": [
            -0.0533,
            0.0527,
            -0.0324,
            0.0587,
            0.1517,
            0.0349,
            0.1767,
            0.0968,
            0.0346,
            0.01,
            -0.0126,
            0.1403,
            -0.0938,
            -0.0595,
            -0.0032,
            0.122
          ],
          "after": [
            0.0193,
            -0.1485,
            -0.2742,
            0.0633,
            0.0353,
            -0.0429,
            -0.1489,
            0.0671,
            0.0974,
            -0.0172,
            0.0204,
            0.09,
            0.0351,
            -0.0051,
            -0.1293,
            0.0543
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0037,
            0.007,
            0.0003,
            -0.001,
            0.0052,
            -0.0073,
            0.0076,
            0.0,
            -0.0006,
            0.0031,
            0.0012,
            0.0022,
            0.0049,
            -0.0034,
            -0.0065,
            0.0022
          ],
          "after": [
            -0.0632,
            0.0118,
            -0.1066,
            -0.0802,
            0.1055,
            0.1449,
            0.0848,
            -0.1573,
            0.0373,
            -0.0079,
            -0.1254,
            -0.0582,
            0.1001,
            -0.0745,
            0.0046,
            0.0955
          ]
        }
      }
    },
    {
      "step": 411,
      "word": "taylyn",
      "loss": 2.2265,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0997,
            -0.1818,
            -0.0133,
            -0.0801,
            -0.012,
            0.1066,
            -0.1052,
            0.0109,
            -0.0537,
            0.0911,
            -0.0054,
            0.0484,
            0.0301,
            0.1013,
            0.0194,
            0.0418
          ],
          "after": [
            0.0151,
            0.0092,
            0.1432,
            -0.0439,
            0.0207,
            -0.0528,
            -0.1364,
            -0.1449,
            -0.0239,
            0.1018,
            0.15,
            0.0094,
            -0.0806,
            -0.0018,
            -0.0378,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0892,
            0.1105,
            -0.0963,
            0.0616,
            0.0387,
            -0.1435,
            0.2686,
            -0.0184,
            -0.2035,
            0.0543,
            0.0044,
            0.0133,
            0.1465,
            -0.1326,
            -0.1745,
            0.1928
          ],
          "after": [
            0.0366,
            0.1046,
            -0.0391,
            -0.0669,
            0.0562,
            -0.0081,
            0.0886,
            0.0964,
            -0.0744,
            -0.0501,
            0.0277,
            -0.0187,
            0.1026,
            0.0335,
            -0.0775,
            0.2197
          ]
        },
        "position_0": {
          "grad": [
            -0.0167,
            -0.1635,
            0.0794,
            0.0728,
            -0.115,
            0.0811,
            0.0504,
            0.0271,
            -0.097,
            -0.1359,
            -0.133,
            -0.252,
            0.1109,
            0.039,
            0.1077,
            0.1266
          ],
          "after": [
            0.0196,
            -0.1484,
            -0.2747,
            0.0627,
            0.0353,
            -0.0433,
            -0.1496,
            0.0663,
            0.0977,
            -0.0162,
            0.0207,
            0.0899,
            0.0348,
            -0.0045,
            -0.1295,
            0.0534
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0003,
            0.0001,
            0.0008,
            0.001,
            -0.0001,
            0.0022,
            0.0004,
            -0.001,
            0.0005,
            0.0005,
            0.001,
            0.0005,
            -0.0021,
            -0.0005,
            0.0017
          ],
          "after": [
            -0.0647,
            0.0102,
            -0.1056,
            -0.0809,
            0.1051,
            0.1458,
            0.0837,
            -0.1572,
            0.0381,
            -0.0087,
            -0.1255,
            -0.0585,
            0.0991,
            -0.0744,
            0.0052,
            0.0944
          ]
        }
      }
    },
    {
      "step": 412,
      "word": "elias",
      "loss": 2.1934,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0654,
            0.192,
            0.0873,
            0.1348,
            -0.1198,
            0.042,
            0.0109,
            0.0585,
            -0.0686,
            0.0607,
            -0.0952,
            0.1129,
            0.1328,
            -0.0148,
            0.0328,
            -0.0568
          ],
          "after": [
            0.0155,
            0.0086,
            0.1424,
            -0.0449,
            0.0209,
            -0.0533,
            -0.1369,
            -0.1458,
            -0.0231,
            0.1013,
            0.1505,
            0.0093,
            -0.0818,
            -0.0022,
            -0.0374,
            -0.1903
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2217,
            0.54,
            0.545,
            -0.0403,
            0.0885,
            -0.1145,
            0.4826,
            -0.0457,
            0.0021,
            -0.1957,
            0.0266,
            -0.0774,
            -0.0345,
            0.0632,
            -0.1278,
            -0.0352
          ],
          "after": [
            0.0375,
            0.1033,
            -0.0391,
            -0.0675,
            0.0561,
            -0.0074,
            0.0875,
            0.0964,
            -0.0736,
            -0.0495,
            0.0282,
            -0.0188,
            0.102,
            0.0339,
            -0.0771,
            0.2188
          ]
        },
        "position_0": {
          "grad": [
            -0.0191,
            -0.2441,
            0.0714,
            -0.0525,
            -0.1313,
            0.08,
            -0.3112,
            -0.0652,
            0.1936,
            0.0974,
            -0.0205,
            -0.0466,
            -0.2136,
            -0.002,
            0.0976,
            -0.3733
          ],
          "after": [
            0.0199,
            -0.1477,
            -0.2753,
            0.0623,
            0.0356,
            -0.0439,
            -0.1497,
            0.0659,
            0.0975,
            -0.0157,
            0.021,
            0.09,
            0.0349,
            -0.004,
            -0.1299,
            0.0533
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0021,
            0.0004,
            -0.0015,
            0.0015,
            -0.0033,
            -0.0013,
            0.0019,
            -0.0006,
            -0.0019,
            0.002,
            0.0054,
            -0.0044,
            -0.0051,
            0.0019,
            -0.0023
          ],
          "after": [
            -0.0658,
            0.0091,
            -0.1047,
            -0.0813,
            0.1047,
            0.147,
            0.0828,
            -0.1574,
            0.0389,
            -0.0092,
            -0.1257,
            -0.0592,
            0.0985,
            -0.0738,
            0.0057,
            0.0937
          ]
        }
      }
    },
    {
      "step": 413,
      "word": "alula",
      "loss": 2.3673,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3275,
            0.0942,
            -0.4595,
            -0.2752,
            -0.2184,
            0.0254,
            -0.3579,
            0.1036,
            -0.2532,
            0.175,
            -0.048,
            -0.1515,
            0.0639,
            0.281,
            0.2628,
            -0.6038
          ],
          "after": [
            0.0152,
            0.0079,
            0.1426,
            -0.0451,
            0.0214,
            -0.0538,
            -0.1367,
            -0.1468,
            -0.022,
            0.1005,
            0.1511,
            0.0095,
            -0.0829,
            -0.0032,
            -0.0378,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0192,
            0.1609,
            -0.1628,
            0.0177,
            -0.0421,
            -0.2145,
            0.2368,
            -0.0585,
            -0.2456,
            0.0325,
            0.0269,
            -0.056,
            0.1995,
            -0.0879,
            -0.1843,
            0.2244
          ],
          "after": [
            0.0382,
            0.1019,
            -0.0389,
            -0.068,
            0.0561,
            -0.0065,
            0.0864,
            0.0966,
            -0.0728,
            -0.0491,
            0.0286,
            -0.0188,
            0.1014,
            0.0345,
            -0.0767,
            0.2179
          ]
        },
        "position_0": {
          "grad": [
            0.0881,
            -0.0707,
            0.1114,
            -0.2047,
            0.0239,
            0.0262,
            -0.3362,
            -0.0856,
            0.1601,
            0.0308,
            0.2848,
            0.0703,
            -0.2304,
            0.0009,
            -0.0296,
            -0.2599
          ],
          "after": [
            0.0198,
            -0.1469,
            -0.2762,
            0.0624,
            0.0357,
            -0.0445,
            -0.1493,
            0.0658,
            0.0972,
            -0.0154,
            0.0207,
            0.0899,
            0.0353,
            -0.0035,
            -0.1302,
            0.0535
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.0018,
            -0.0013,
            0.0005,
            -0.0017,
            -0.0023,
            0.0027,
            -0.0018,
            -0.0006,
            -0.0025,
            -0.0009,
            -0.0024,
            0.0034,
            0.0007,
            0.0001,
            0.0029
          ],
          "after": [
            -0.0669,
            0.0079,
            -0.1039,
            -0.0817,
            0.1044,
            0.1482,
            0.0819,
            -0.1573,
            0.0396,
            -0.0094,
            -0.1258,
            -0.0596,
            0.0978,
            -0.0733,
            0.006,
            0.0928
          ]
        }
      }
    },
    {
      "step": 414,
      "word": "naftula",
      "loss": 2.907,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0525,
            0.1685,
            -0.0486,
            0.1905,
            0.1724,
            -0.2233,
            0.1684,
            -0.0527,
            -0.4444,
            0.0809,
            0.1683,
            -0.0399,
            0.3253,
            0.0562,
            -0.12,
            0.1287
          ],
          "after": [
            0.0149,
            0.007,
            0.1429,
            -0.0457,
            0.0216,
            -0.0535,
            -0.1368,
            -0.1475,
            -0.0202,
            0.0997,
            0.1512,
            0.0096,
            -0.0843,
            -0.0042,
            -0.0377,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0969,
            0.1539,
            -0.0694,
            0.0647,
            0.0702,
            -0.1793,
            0.2543,
            0.0049,
            -0.1628,
            0.0264,
            0.0066,
            0.0613,
            0.1052,
            -0.1101,
            -0.1945,
            0.1804
          ],
          "after": [
            0.039,
            0.1006,
            -0.0387,
            -0.0686,
            0.056,
            -0.0055,
            0.0852,
            0.0968,
            -0.0718,
            -0.0487,
            0.0288,
            -0.0189,
            0.1007,
            0.0352,
            -0.0761,
            0.2169
          ]
        },
        "position_0": {
          "grad": [
            -0.013,
            0.086,
            -0.0608,
            0.0423,
            -0.2503,
            0.0081,
            0.0194,
            -0.0082,
            0.0343,
            -0.1366,
            -0.1641,
            -0.0568,
            -0.0057,
            0.0966,
            0.169,
            0.0912
          ],
          "after": [
            0.0197,
            -0.1465,
            -0.2768,
            0.0623,
            0.0364,
            -0.0451,
            -0.149,
            0.0658,
            0.0968,
            -0.0147,
            0.0208,
            0.09,
            0.0356,
            -0.0034,
            -0.1308,
            0.0536
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            0.0022,
            -0.0012,
            0.0018,
            0.0001,
            -0.0011,
            0.0023,
            0.0001,
            -0.0008,
            0.0025,
            0.0001,
            -0.0007,
            0.0026,
            -0.0001,
            -0.0026,
            -0.0005
          ],
          "after": [
            -0.068,
            0.0067,
            -0.103,
            -0.0822,
            0.1042,
            0.1493,
            0.0809,
            -0.1572,
            0.0404,
            -0.0098,
            -0.1259,
            -0.0599,
            0.097,
            -0.0729,
            0.0065,
            0.0921
          ]
        }
      }
    },
    {
      "step": 415,
      "word": "ahlana",
      "loss": 2.0316,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2445,
            0.0976,
            -0.0451,
            -0.114,
            -0.2949,
            -0.0373,
            -0.0837,
            0.0135,
            0.063,
            0.3701,
            0.0406,
            -0.3429,
            0.342,
            0.2105,
            0.0103,
            -0.2034
          ],
          "after": [
            0.0141,
            0.0061,
            0.1432,
            -0.046,
            0.0222,
            -0.0531,
            -0.1368,
            -0.1481,
            -0.0188,
            0.0982,
            0.1512,
            0.0103,
            -0.0861,
            -0.0055,
            -0.0377,
            -0.189
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0119,
            0.0644,
            -0.1257,
            0.0239,
            0.0227,
            -0.139,
            0.2061,
            -0.0234,
            -0.1712,
            0.0348,
            -0.0321,
            -0.0018,
            0.1307,
            -0.0592,
            -0.1806,
            0.1825
          ],
          "after": [
            0.0397,
            0.0993,
            -0.0384,
            -0.0692,
            0.0559,
            -0.0045,
            0.0841,
            0.0971,
            -0.0709,
            -0.0485,
            0.0292,
            -0.0189,
            0.0999,
            0.0358,
            -0.0755,
            0.2158
          ]
        },
        "position_0": {
          "grad": [
            0.0887,
            -0.0533,
            0.0829,
            -0.1727,
            0.0049,
            0.0036,
            -0.278,
            -0.0663,
            0.1364,
            0.0321,
            0.215,
            0.0634,
            -0.2102,
            0.0072,
            -0.0168,
            -0.2186
          ],
          "after": [
            0.0193,
            -0.146,
            -0.2776,
            0.0627,
            0.0369,
            -0.0455,
            -0.1484,
            0.066,
            0.0963,
            -0.0141,
            0.0204,
            0.0899,
            0.0362,
            -0.0034,
            -0.1313,
            0.054
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0013,
            -0.0021,
            -0.0007,
            -0.0015,
            0.0002,
            -0.0015,
            0.0003,
            0.001,
            -0.0026,
            -0.0033,
            -0.0026,
            0.0009,
            0.0032,
            0.001,
            0.0016
          ],
          "after": [
            -0.0691,
            0.0058,
            -0.102,
            -0.0826,
            0.1042,
            0.1503,
            0.0802,
            -0.1572,
            0.0409,
            -0.0099,
            -0.1256,
            -0.0599,
            0.0962,
            -0.073,
            0.0068,
            0.0913
          ]
        }
      }
    },
    {
      "step": 416,
      "word": "shekinah",
      "loss": 2.405,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1765,
            0.0403,
            0.1716,
            0.1408,
            0.0884,
            0.0903,
            0.0848,
            0.1108,
            0.1301,
            -0.0097,
            -0.1282,
            0.0539,
            0.1313,
            -0.0627,
            -0.1044,
            0.1869
          ],
          "after": [
            0.0138,
            0.0052,
            0.1431,
            -0.0465,
            0.0225,
            -0.053,
            -0.1369,
            -0.149,
            -0.0179,
            0.0969,
            0.1515,
            0.0109,
            -0.0878,
            -0.0064,
            -0.0375,
            -0.1888
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1209,
            0.0396,
            0.2656,
            0.0632,
            0.1573,
            0.068,
            -0.063,
            0.039,
            0.0983,
            0.0648,
            0.1864,
            0.1984,
            -0.2572,
            -0.1189,
            0.0327,
            -0.1045
          ],
          "after": [
            0.0405,
            0.0982,
            -0.0384,
            -0.0698,
            0.0557,
            -0.0038,
            0.0831,
            0.0971,
            -0.0702,
            -0.0484,
            0.029,
            -0.0194,
            0.0996,
            0.0366,
            -0.0751,
            0.215
          ]
        },
        "position_0": {
          "grad": [
            0.0008,
            0.0767,
            -0.0254,
            0.082,
            -0.1112,
            0.0513,
            -0.0005,
            0.022,
            -0.0386,
            0.0193,
            -0.0886,
            0.0356,
            0.0709,
            0.0235,
            0.104,
            0.0146
          ],
          "after": [
            0.019,
            -0.1457,
            -0.2781,
            0.0628,
            0.0376,
            -0.0461,
            -0.1479,
            0.0661,
            0.0959,
            -0.0138,
            0.0203,
            0.0897,
            0.0366,
            -0.0034,
            -0.1319,
            0.0543
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            -0.0002,
            0.0013,
            0.0009,
            0.0002,
            0.0009,
            -0.0017,
            -0.0009,
            -0.0007,
            0.0008,
            0.0016,
            -0.0011,
            -0.0003,
            0.0007,
            -0.0015,
            -0.0009
          ],
          "after": [
            -0.0702,
            0.0051,
            -0.1013,
            -0.083,
            0.1042,
            0.151,
            0.0797,
            -0.157,
            0.0414,
            -0.0101,
            -0.1256,
            -0.0598,
            0.0955,
            -0.0731,
            0.0072,
            0.0908
          ]
        }
      }
    },
    {
      "step": 417,
      "word": "seiji",
      "loss": 2.8012,
      "learning_rate": 0.0018,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0136,
            0.0044,
            0.1431,
            -0.0469,
            0.0229,
            -0.053,
            -0.137,
            -0.1497,
            -0.0171,
            0.0959,
            0.1517,
            0.0113,
            -0.0892,
            -0.0072,
            -0.0373,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0174,
            0.1685,
            0.3592,
            0.0866,
            0.4304,
            0.0033,
            -0.2025,
            -0.0163,
            0.2454,
            0.2657,
            -0.0094,
            -0.0419,
            0.0373,
            0.1308,
            -0.4418,
            -0.0689
          ],
          "after": [
            0.0411,
            0.097,
            -0.0389,
            -0.0706,
            0.055,
            -0.0031,
            0.0825,
            0.0972,
            -0.0698,
            -0.0489,
            0.0288,
            -0.0196,
            0.0993,
            0.0371,
            -0.0743,
            0.2144
          ]
        },
        "position_0": {
          "grad": [
            0.0107,
            0.0791,
            -0.0221,
            0.0867,
            -0.2045,
            0.0768,
            -0.064,
            0.0317,
            -0.0214,
            0.0068,
            -0.1281,
            -0.0071,
            0.1033,
            0.0541,
            0.1634,
            -0.0151
          ],
          "after": [
            0.0186,
            -0.1458,
            -0.2785,
            0.0627,
            0.0386,
            -0.0469,
            -0.1474,
            0.0661,
            0.0956,
            -0.0135,
            0.0204,
            0.0896,
            0.0368,
            -0.0036,
            -0.1328,
            0.0545
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0001,
            0.0001,
            -0.0002,
            0.0004,
            -0.0,
            0.0001,
            0.0004,
            -0.0001,
            -0.0007,
            -0.0004,
            0.0008,
            -0.0005,
            -0.0006,
            0.0007,
            -0.0001
          ],
          "after": [
            -0.071,
            0.0045,
            -0.1007,
            -0.0834,
            0.1041,
            0.1516,
            0.0793,
            -0.157,
            0.0419,
            -0.0102,
            -0.1255,
            -0.0598,
            0.095,
            -0.0731,
            0.0075,
            0.0903
          ]
        }
      }
    },
    {
      "step": 418,
      "word": "amore",
      "loss": 2.0287,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1678,
            0.0236,
            0.1645,
            0.0522,
            0.0794,
            0.0165,
            0.2012,
            0.1147,
            0.0955,
            -0.0315,
            0.159,
            0.1004,
            -0.2874,
            -0.1278,
            0.0339,
            -0.1113
          ],
          "after": [
            0.0137,
            0.0037,
            0.1428,
            -0.0474,
            0.023,
            -0.053,
            -0.1374,
            -0.1507,
            -0.0166,
            0.095,
            0.1515,
            0.0115,
            -0.09,
            -0.0076,
            -0.0372,
            -0.1884
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.122,
            -0.3516,
            0.0628,
            -0.0545,
            0.1138,
            0.1167,
            -0.5211,
            0.0299,
            0.0125,
            -0.1369,
            -0.0369,
            -0.0842,
            -0.0839,
            0.2689,
            0.0393,
            -0.0675
          ],
          "after": [
            0.0415,
            0.0965,
            -0.0393,
            -0.0711,
            0.0542,
            -0.0027,
            0.0824,
            0.0972,
            -0.0695,
            -0.049,
            0.0288,
            -0.0197,
            0.0991,
            0.037,
            -0.0737,
            0.214
          ]
        },
        "position_0": {
          "grad": [
            0.0966,
            -0.0665,
            0.0878,
            -0.2141,
            0.0171,
            0.0011,
            -0.323,
            -0.0953,
            0.1448,
            0.0585,
            0.2671,
            0.0556,
            -0.2373,
            0.0041,
            -0.0364,
            -0.261
          ],
          "after": [
            0.0179,
            -0.1456,
            -0.2792,
            0.0631,
            0.0395,
            -0.0475,
            -0.1465,
            0.0664,
            0.0951,
            -0.0134,
            0.02,
            0.0893,
            0.0373,
            -0.0038,
            -0.1335,
            0.0551
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0014,
            -0.0019,
            -0.0019,
            -0.0037,
            0.0033,
            -0.0042,
            -0.001,
            0.0019,
            -0.0014,
            -0.0009,
            0.001,
            -0.0057,
            0.0033,
            0.0082,
            -0.0014
          ],
          "after": [
            -0.0716,
            0.0041,
            -0.1,
            -0.0835,
            0.1044,
            0.1518,
            0.0792,
            -0.1567,
            0.042,
            -0.0101,
            -0.1253,
            -0.0598,
            0.0951,
            -0.0735,
            0.0072,
            0.09
          ]
        }
      }
    },
    {
      "step": 419,
      "word": "misa",
      "loss": 2.512,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.6292,
            -0.1546,
            -0.4983,
            -0.0941,
            -0.3645,
            -0.001,
            -0.1659,
            0.0198,
            -0.1266,
            0.2803,
            0.0769,
            -0.6115,
            0.2334,
            0.3248,
            0.2501,
            -0.2958
          ],
          "after": [
            0.0127,
            0.0034,
            0.1434,
            -0.0476,
            0.0237,
            -0.053,
            -0.1375,
            -0.1516,
            -0.016,
            0.0937,
            0.1511,
            0.0126,
            -0.091,
            -0.0087,
            -0.0377,
            -0.1878
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0068,
            -0.0195,
            -0.1607,
            -0.0273,
            0.0274,
            -0.103,
            0.1331,
            -0.0143,
            -0.1869,
            0.0626,
            -0.0335,
            -0.0349,
            0.121,
            -0.0291,
            -0.173,
            0.1222
          ],
          "after": [
            0.0418,
            0.0961,
            -0.0395,
            -0.0715,
            0.0536,
            -0.0022,
            0.0822,
            0.0972,
            -0.0691,
            -0.0492,
            0.0289,
            -0.0198,
            0.0988,
            0.0369,
            -0.073,
            0.2135
          ]
        },
        "position_0": {
          "grad": [
            -0.0646,
            -0.0112,
            -0.0309,
            0.0573,
            0.1192,
            0.0435,
            0.1651,
            0.1764,
            0.1593,
            -0.0361,
            -0.0626,
            0.097,
            -0.209,
            -0.05,
            0.0632,
            0.0974
          ],
          "after": [
            0.0175,
            -0.1454,
            -0.2796,
            0.0633,
            0.0399,
            -0.0482,
            -0.1459,
            0.0661,
            0.0945,
            -0.0133,
            0.0198,
            0.0889,
            0.038,
            -0.0038,
            -0.1342,
            0.0555
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0042,
            0.002,
            0.0062,
            -0.006,
            0.0065,
            0.0004,
            -0.0033,
            -0.0034,
            0.0022,
            -0.0023,
            -0.0048,
            -0.0121,
            0.0056,
            0.0017,
            0.0023,
            -0.0056
          ],
          "after": [
            -0.0716,
            0.0035,
            -0.1001,
            -0.0829,
            0.104,
            0.1519,
            0.0794,
            -0.156,
            0.0418,
            -0.0099,
            -0.1247,
            -0.0589,
            0.0946,
            -0.0741,
            0.0068,
            0.0904
          ]
        }
      }
    },
    {
      "step": 420,
      "word": "eddison",
      "loss": 2.7521,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0118,
            0.0032,
            0.1439,
            -0.0478,
            0.0243,
            -0.0531,
            -0.1376,
            -0.1523,
            -0.0154,
            0.0926,
            0.1508,
            0.0135,
            -0.0919,
            -0.0095,
            -0.0381,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1859,
            0.4074,
            0.4348,
            0.0186,
            0.1558,
            -0.0789,
            0.4191,
            0.0168,
            -0.0072,
            -0.154,
            -0.0075,
            -0.0233,
            -0.0308,
            0.0247,
            -0.1471,
            -0.0356
          ],
          "after": [
            0.0424,
            0.0952,
            -0.0402,
            -0.0719,
            0.0529,
            -0.0017,
            0.0817,
            0.0972,
            -0.0687,
            -0.0491,
            0.029,
            -0.0197,
            0.0986,
            0.0368,
            -0.0723,
            0.2131
          ]
        },
        "position_0": {
          "grad": [
            -0.0257,
            -0.1342,
            0.0367,
            0.015,
            -0.0252,
            0.0635,
            -0.1395,
            -0.0685,
            0.0711,
            0.0849,
            0.0187,
            0.007,
            -0.0911,
            -0.0139,
            0.0411,
            -0.1935
          ],
          "after": [
            0.0174,
            -0.1449,
            -0.2801,
            0.0634,
            0.0404,
            -0.049,
            -0.1453,
            0.066,
            0.0938,
            -0.0134,
            0.0195,
            0.0884,
            0.0387,
            -0.0037,
            -0.1349,
            0.0561
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0014,
            -0.0016,
            0.0014,
            0.0011,
            -0.0015,
            0.0037,
            -0.001,
            -0.0017,
            -0.0004,
            -0.0013,
            -0.0054,
            0.0043,
            -0.0001,
            0.0006,
            0.0011
          ],
          "after": [
            -0.0716,
            0.0029,
            -0.1,
            -0.0825,
            0.1036,
            0.1521,
            0.0793,
            -0.1553,
            0.0419,
            -0.0097,
            -0.124,
            -0.0576,
            0.094,
            -0.0745,
            0.0064,
            0.0905
          ]
        }
      }
    },
    {
      "step": 421,
      "word": "fisher",
      "loss": 2.5246,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0111,
            0.003,
            0.1443,
            -0.048,
            0.0247,
            -0.0531,
            -0.1376,
            -0.153,
            -0.0149,
            0.0916,
            0.1506,
            0.0143,
            -0.0926,
            -0.0103,
            -0.0385,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0764,
            -0.2449,
            0.1776,
            -0.0243,
            -0.0258,
            0.0394,
            -0.3896,
            -0.1088,
            0.0016,
            -0.0476,
            0.0336,
            -0.0719,
            0.0074,
            0.1525,
            -0.0069,
            -0.1882
          ],
          "after": [
            0.0427,
            0.0948,
            -0.041,
            -0.0721,
            0.0523,
            -0.0014,
            0.0816,
            0.0976,
            -0.0684,
            -0.0489,
            0.0289,
            -0.0196,
            0.0984,
            0.0365,
            -0.0717,
            0.2129
          ]
        },
        "position_0": {
          "grad": [
            0.0158,
            0.1736,
            -0.013,
            0.1839,
            0.0876,
            -0.1903,
            0.155,
            -0.0712,
            -0.2608,
            -0.09,
            0.1204,
            -0.043,
            0.2704,
            0.0094,
            -0.0927,
            0.1947
          ],
          "after": [
            0.0171,
            -0.145,
            -0.2805,
            0.0631,
            0.0405,
            -0.049,
            -0.145,
            0.0662,
            0.0936,
            -0.0132,
            0.0191,
            0.0882,
            0.0389,
            -0.0037,
            -0.1353,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0028,
            -0.0018,
            0.0013,
            -0.0052,
            0.0053,
            -0.0001,
            -0.0006,
            -0.0031,
            0.0019,
            -0.0006,
            0.0005,
            -0.002,
            0.0004,
            0.0029,
            -0.0041,
            -0.0011
          ],
          "after": [
            -0.0719,
            0.0026,
            -0.1001,
            -0.0817,
            0.1028,
            0.1524,
            0.0792,
            -0.1542,
            0.0417,
            -0.0094,
            -0.1234,
            -0.0564,
            0.0933,
            -0.0753,
            0.0063,
            0.0908
          ]
        }
      }
    },
    {
      "step": 422,
      "word": "mayleigh",
      "loss": 2.4478,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0554,
            -0.106,
            0.0085,
            -0.1022,
            0.0619,
            0.1066,
            -0.0651,
            0.0033,
            -0.0463,
            0.0984,
            0.0019,
            0.0869,
            0.011,
            0.046,
            -0.0361,
            0.0409
          ],
          "after": [
            0.0103,
            0.003,
            0.1447,
            -0.0479,
            0.0251,
            -0.0534,
            -0.1376,
            -0.1536,
            -0.0144,
            0.0906,
            0.1504,
            0.0149,
            -0.0932,
            -0.011,
            -0.0387,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0025,
            -0.2673,
            0.1116,
            -0.0401,
            0.1746,
            0.0785,
            -0.2873,
            0.0332,
            0.0771,
            0.0363,
            -0.0613,
            0.0439,
            -0.0892,
            0.1006,
            -0.0737,
            -0.1176
          ],
          "after": [
            0.0431,
            0.0948,
            -0.0419,
            -0.0722,
            0.0516,
            -0.0011,
            0.0817,
            0.0978,
            -0.0682,
            -0.0488,
            0.0291,
            -0.0195,
            0.0984,
            0.036,
            -0.0711,
            0.213
          ]
        },
        "position_0": {
          "grad": [
            -0.0595,
            0.0417,
            -0.0327,
            0.0193,
            0.152,
            0.0229,
            0.145,
            0.0716,
            0.0305,
            0.0257,
            -0.0097,
            0.1222,
            -0.0902,
            -0.0757,
            -0.0328,
            0.0926
          ],
          "after": [
            0.0172,
            -0.1451,
            -0.2808,
            0.0629,
            0.0404,
            -0.0492,
            -0.1449,
            0.0661,
            0.0934,
            -0.0132,
            0.0188,
            0.0877,
            0.0392,
            -0.0034,
            -0.1356,
            0.0564
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0032,
            -0.0,
            0.0016,
            -0.0031,
            0.0035,
            0.0032,
            0.0007,
            -0.0003,
            0.0022,
            -0.0016,
            -0.0019,
            0.0012,
            -0.0013,
            0.001,
            -0.0013,
            0.0007
          ],
          "after": [
            -0.0719,
            0.0023,
            -0.1004,
            -0.0806,
            0.1019,
            0.1522,
            0.0792,
            -0.1533,
            0.0412,
            -0.0091,
            -0.1228,
            -0.0555,
            0.0929,
            -0.076,
            0.0063,
            0.0909
          ]
        }
      }
    },
    {
      "step": 423,
      "word": "hollie",
      "loss": 1.9796,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0097,
            0.003,
            0.145,
            -0.0479,
            0.0253,
            -0.0538,
            -0.1375,
            -0.154,
            -0.014,
            0.0898,
            0.1502,
            0.0153,
            -0.0938,
            -0.0116,
            -0.0389,
            -0.1861
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0747,
            0.0273,
            -0.1755,
            0.1068,
            -0.6058,
            -0.0094,
            -0.0738,
            -0.0076,
            0.0527,
            -0.0252,
            0.0461,
            -0.0552,
            0.1253,
            0.0314,
            0.3701,
            0.0061
          ],
          "after": [
            0.0432,
            0.0947,
            -0.0424,
            -0.0726,
            0.0517,
            -0.0009,
            0.0819,
            0.098,
            -0.0681,
            -0.0487,
            0.0291,
            -0.0194,
            0.0982,
            0.0355,
            -0.0709,
            0.213
          ]
        },
        "position_0": {
          "grad": [
            -0.0072,
            -0.0585,
            0.0719,
            0.0458,
            -0.0411,
            0.1095,
            -0.0439,
            0.0487,
            0.1157,
            0.0549,
            -0.0608,
            -0.1197,
            0.1596,
            0.1075,
            0.0127,
            0.0933
          ],
          "after": [
            0.0173,
            -0.1451,
            -0.2812,
            0.0626,
            0.0404,
            -0.0496,
            -0.1448,
            0.0659,
            0.0931,
            -0.0133,
            0.0186,
            0.0875,
            0.0393,
            -0.0036,
            -0.1358,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0009,
            0.0004,
            0.0025,
            -0.0033,
            -0.0002,
            -0.0031,
            0.0001,
            0.0014,
            0.0008,
            0.0004,
            0.0019,
            -0.0017,
            -0.0001,
            0.0025,
            0.0015
          ],
          "after": [
            -0.0717,
            0.002,
            -0.1006,
            -0.08,
            0.1013,
            0.1521,
            0.0793,
            -0.1524,
            0.0407,
            -0.0089,
            -0.1223,
            -0.0548,
            0.0927,
            -0.0766,
            0.0062,
            0.0909
          ]
        }
      }
    },
    {
      "step": 424,
      "word": "ryzen",
      "loss": 2.5496,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0092,
            0.0031,
            0.1452,
            -0.0478,
            0.0256,
            -0.054,
            -0.1375,
            -0.1544,
            -0.0136,
            0.089,
            0.15,
            0.0157,
            -0.0942,
            -0.0122,
            -0.0391,
            -0.1858
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0087,
            -0.2098,
            0.1764,
            -0.1191,
            0.0203,
            0.0502,
            -0.2733,
            -0.0068,
            0.1618,
            -0.049,
            0.0582,
            0.0011,
            -0.213,
            -0.1367,
            0.4123,
            -0.179
          ],
          "after": [
            0.0433,
            0.0949,
            -0.043,
            -0.0726,
            0.0517,
            -0.0008,
            0.0823,
            0.0982,
            -0.0681,
            -0.0485,
            0.0289,
            -0.0193,
            0.0982,
            0.0353,
            -0.0711,
            0.2132
          ]
        },
        "position_0": {
          "grad": [
            0.0582,
            -0.0453,
            0.0143,
            -0.0452,
            -0.059,
            0.1268,
            0.0971,
            -0.0469,
            0.1452,
            -0.1321,
            -0.1191,
            -0.0569,
            -0.1556,
            -0.0234,
            0.1956,
            0.1259
          ],
          "after": [
            0.0171,
            -0.1449,
            -0.2816,
            0.0624,
            0.0404,
            -0.0504,
            -0.1448,
            0.0658,
            0.0925,
            -0.013,
            0.0187,
            0.0876,
            0.0396,
            -0.0036,
            -0.1365,
            0.0561
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0018,
            0.0004,
            -0.0002,
            -0.001,
            -0.0014,
            0.0018,
            -0.001,
            -0.0009,
            0.0014,
            0.0,
            -0.0023,
            0.0026,
            -0.0014,
            0.0006,
            0.0001
          ],
          "after": [
            -0.0716,
            0.0019,
            -0.1009,
            -0.0795,
            0.1009,
            0.1522,
            0.0793,
            -0.1516,
            0.0403,
            -0.0088,
            -0.1218,
            -0.0541,
            0.0923,
            -0.077,
            0.006,
            0.0909
          ]
        }
      }
    },
    {
      "step": 425,
      "word": "anselmo",
      "loss": 2.7546,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0023,
            0.1651,
            -0.2249,
            0.066,
            -0.4216,
            0.028,
            -0.0383,
            -0.0299,
            0.0066,
            -0.1955,
            -0.2321,
            -0.0741,
            0.0516,
            0.13,
            0.2438,
            0.118
          ],
          "after": [
            0.0087,
            0.0027,
            0.1458,
            -0.0479,
            0.0264,
            -0.0543,
            -0.1374,
            -0.1547,
            -0.0133,
            0.0888,
            0.1504,
            0.0162,
            -0.0947,
            -0.0129,
            -0.0398,
            -0.1858
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2653,
            0.1365,
            0.1298,
            0.0645,
            0.0135,
            -0.0132,
            -0.0073,
            0.0165,
            -0.0526,
            -0.1253,
            -0.1152,
            -0.1324,
            0.159,
            -0.0706,
            0.1569,
            0.0135
          ],
          "after": [
            0.0439,
            0.095,
            -0.0437,
            -0.0728,
            0.0518,
            -0.0007,
            0.0826,
            0.0983,
            -0.0681,
            -0.0481,
            0.0291,
            -0.0189,
            0.0981,
            0.0353,
            -0.0714,
            0.2134
          ]
        },
        "position_0": {
          "grad": [
            0.0719,
            -0.0461,
            0.0519,
            -0.1368,
            0.0066,
            -0.0047,
            -0.2145,
            -0.0559,
            0.0973,
            0.0234,
            0.1745,
            0.0522,
            -0.1671,
            0.0055,
            -0.0285,
            -0.1749
          ],
          "after": [
            0.0167,
            -0.1447,
            -0.2822,
            0.0625,
            0.0405,
            -0.0511,
            -0.1445,
            0.066,
            0.0919,
            -0.0128,
            0.0184,
            0.0875,
            0.04,
            -0.0037,
            -0.137,
            0.0561
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0043,
            -0.0006,
            0.0022,
            0.0015,
            -0.0018,
            0.0049,
            -0.0001,
            -0.0019,
            -0.0016,
            -0.0012,
            0.0009,
            0.0031,
            -0.0027,
            -0.001,
            0.0038
          ],
          "after": [
            -0.0712,
            0.0014,
            -0.101,
            -0.0792,
            0.1005,
            0.1524,
            0.079,
            -0.1509,
            0.0402,
            -0.0086,
            -0.1213,
            -0.0535,
            0.0917,
            -0.077,
            0.006,
            0.0905
          ]
        }
      }
    },
    {
      "step": 426,
      "word": "jerusalem",
      "loss": 2.8527,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1844,
            0.0924,
            -0.0757,
            0.0912,
            0.0922,
            0.0556,
            0.0222,
            0.1583,
            0.0084,
            -0.1638,
            -0.217,
            0.2745,
            -0.0871,
            -0.0675,
            0.0849,
            0.0546
          ],
          "after": [
            0.0087,
            0.0023,
            0.1465,
            -0.0482,
            0.027,
            -0.0548,
            -0.1373,
            -0.1554,
            -0.0131,
            0.089,
            0.1513,
            0.0161,
            -0.095,
            -0.0134,
            -0.0407,
            -0.1858
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.4472,
            -0.4009,
            0.1388,
            -0.1134,
            0.0303,
            0.2313,
            -0.748,
            -0.0113,
            0.458,
            0.3183,
            0.1594,
            -0.1526,
            -0.1619,
            0.1307,
            -0.0029,
            -0.5516
          ],
          "after": [
            0.0436,
            0.0955,
            -0.0444,
            -0.0726,
            0.0517,
            -0.0009,
            0.0835,
            0.0985,
            -0.0686,
            -0.0484,
            0.0288,
            -0.0183,
            0.0982,
            0.0351,
            -0.0717,
            0.2141
          ]
        },
        "position_0": {
          "grad": [
            -0.0297,
            0.0458,
            0.0069,
            0.1003,
            0.0541,
            -0.0712,
            0.1362,
            0.0315,
            -0.1632,
            -0.0235,
            0.0199,
            0.0509,
            0.2188,
            -0.122,
            -0.1239,
            0.0932
          ],
          "after": [
            0.0164,
            -0.1446,
            -0.2826,
            0.0624,
            0.0404,
            -0.0514,
            -0.1444,
            0.066,
            0.0917,
            -0.0125,
            0.0182,
            0.0872,
            0.0401,
            -0.0033,
            -0.1371,
            0.056
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            -0.0012,
            -0.0019,
            -0.0005,
            -0.0002,
            0.0004,
            -0.0024,
            0.0015,
            0.0009,
            0.0017,
            -0.0009,
            -0.0024,
            0.0004,
            0.0013,
            0.0011,
            -0.0017
          ],
          "after": [
            -0.0712,
            0.0011,
            -0.1009,
            -0.079,
            0.1001,
            0.1526,
            0.0788,
            -0.1505,
            0.04,
            -0.0086,
            -0.1208,
            -0.0528,
            0.0912,
            -0.0771,
            0.0058,
            0.0903
          ]
        }
      }
    },
    {
      "step": 427,
      "word": "burhan",
      "loss": 2.3541,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0818,
            0.1421,
            -0.0353,
            0.073,
            -0.0254,
            -0.0888,
            -0.001,
            -0.066,
            0.0243,
            -0.2041,
            -0.0728,
            0.147,
            -0.0866,
            -0.0635,
            0.0002,
            0.1115
          ],
          "after": [
            0.0088,
            0.0016,
            0.1471,
            -0.0485,
            0.0275,
            -0.0549,
            -0.1373,
            -0.1558,
            -0.013,
            0.0896,
            0.1523,
            0.0159,
            -0.0951,
            -0.0137,
            -0.0414,
            -0.186
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0101,
            0.0717,
            -0.0985,
            0.0305,
            0.015,
            -0.1554,
            0.182,
            0.0018,
            -0.1558,
            0.083,
            -0.0093,
            0.0139,
            0.1274,
            -0.0603,
            -0.1801,
            0.1477
          ],
          "after": [
            0.0434,
            0.0958,
            -0.045,
            -0.0726,
            0.0517,
            -0.0009,
            0.084,
            0.0986,
            -0.0688,
            -0.0488,
            0.0286,
            -0.0179,
            0.0981,
            0.035,
            -0.0717,
            0.2146
          ]
        },
        "position_0": {
          "grad": [
            0.0494,
            0.0048,
            0.1218,
            0.0153,
            0.0379,
            -0.0224,
            0.0837,
            -0.0315,
            -0.1283,
            -0.0735,
            0.0401,
            -0.0271,
            0.1858,
            -0.0856,
            -0.0524,
            0.2902
          ],
          "after": [
            0.016,
            -0.1445,
            -0.2835,
            0.0623,
            0.0403,
            -0.0516,
            -0.1445,
            0.0662,
            0.0917,
            -0.012,
            0.0178,
            0.0871,
            0.0399,
            -0.0028,
            -0.1371,
            0.0555
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0004,
            0.0012,
            0.0018,
            -0.0016,
            0.0014,
            0.0009,
            -0.0007,
            0.0005,
            0.001,
            -0.0003,
            0.0011,
            0.0007,
            0.0003,
            -0.0014,
            0.0005
          ],
          "after": [
            -0.0711,
            0.0008,
            -0.101,
            -0.0789,
            0.0999,
            0.1526,
            0.0787,
            -0.1501,
            0.0398,
            -0.0086,
            -0.1203,
            -0.0524,
            0.0907,
            -0.0773,
            0.0058,
            0.0901
          ]
        }
      }
    },
    {
      "step": 428,
      "word": "obryan",
      "loss": 2.4767,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0862,
            0.1418,
            -0.0327,
            0.0728,
            -0.0241,
            -0.0877,
            -0.0003,
            -0.0662,
            0.0244,
            -0.2057,
            -0.0744,
            0.1491,
            -0.0869,
            -0.063,
            -0.0001,
            0.1109
          ],
          "after": [
            0.009,
            0.0008,
            0.1477,
            -0.049,
            0.028,
            -0.0547,
            -0.1373,
            -0.1559,
            -0.0129,
            0.0905,
            0.1533,
            0.0154,
            -0.095,
            -0.0138,
            -0.042,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0045,
            -0.008,
            -0.022,
            0.02,
            0.0188,
            -0.1061,
            0.0838,
            -0.0191,
            -0.1098,
            0.0841,
            0.0096,
            0.0148,
            0.088,
            -0.0386,
            -0.1569,
            0.0677
          ],
          "after": [
            0.0432,
            0.0961,
            -0.0454,
            -0.0726,
            0.0517,
            -0.0007,
            0.0845,
            0.0988,
            -0.0689,
            -0.0492,
            0.0284,
            -0.0175,
            0.098,
            0.035,
            -0.0716,
            0.2149
          ]
        },
        "position_0": {
          "grad": [
            0.1362,
            -0.1241,
            0.1033,
            -0.0989,
            0.0452,
            -0.0426,
            -0.2304,
            -0.0913,
            0.0103,
            0.169,
            0.1092,
            -0.0037,
            -0.0702,
            0.0203,
            -0.0351,
            -0.1979
          ],
          "after": [
            0.0151,
            -0.1441,
            -0.2845,
            0.0624,
            0.0401,
            -0.0516,
            -0.1443,
            0.0666,
            0.0917,
            -0.0122,
            0.0174,
            0.087,
            0.0399,
            -0.0024,
            -0.137,
            0.0554
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0016,
            -0.0006,
            0.0,
            0.0005,
            0.0003,
            -0.002,
            -0.0002,
            0.0018,
            -0.0031,
            -0.002,
            0.0011,
            -0.0021,
            0.0016,
            0.0011,
            0.0008
          ],
          "after": [
            -0.071,
            0.0004,
            -0.101,
            -0.0789,
            0.0997,
            0.1526,
            0.0787,
            -0.1497,
            0.0394,
            -0.0084,
            -0.1197,
            -0.052,
            0.0904,
            -0.0776,
            0.0057,
            0.0899
          ]
        }
      }
    },
    {
      "step": 429,
      "word": "zailen",
      "loss": 2.0188,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0509,
            -0.3066,
            0.0431,
            -0.067,
            0.0734,
            0.1768,
            -0.2534,
            0.0968,
            0.0342,
            0.0083,
            0.0987,
            -0.1471,
            -0.1914,
            0.0478,
            0.0435,
            -0.117
          ],
          "after": [
            0.0093,
            0.0007,
            0.1481,
            -0.0492,
            0.0283,
            -0.0551,
            -0.1368,
            -0.1563,
            -0.0129,
            0.0913,
            0.1539,
            0.0153,
            -0.0947,
            -0.0139,
            -0.0426,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0753,
            -0.3569,
            0.1025,
            -0.0663,
            0.1298,
            0.125,
            -0.4019,
            0.0459,
            0.1156,
            0.047,
            -0.0699,
            -0.0253,
            -0.0789,
            0.1604,
            -0.0547,
            -0.2429
          ],
          "after": [
            0.0429,
            0.0968,
            -0.0459,
            -0.0724,
            0.0515,
            -0.0007,
            0.0852,
            0.0987,
            -0.0691,
            -0.0497,
            0.0284,
            -0.0172,
            0.0979,
            0.0347,
            -0.0715,
            0.2155
          ]
        },
        "position_0": {
          "grad": [
            0.0359,
            0.0646,
            -0.0424,
            0.1717,
            0.0588,
            -0.0813,
            0.2349,
            -0.0091,
            -0.0005,
            0.048,
            -0.011,
            0.0199,
            0.0719,
            -0.0472,
            -0.0262,
            0.0828
          ],
          "after": [
            0.0141,
            -0.144,
            -0.2853,
            0.0622,
            0.0398,
            -0.0514,
            -0.1444,
            0.067,
            0.0916,
            -0.0125,
            0.017,
            0.0869,
            0.0397,
            -0.0019,
            -0.1369,
            0.0551
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.002,
            -0.0013,
            0.0002,
            0.002,
            -0.0005,
            0.0026,
            -0.0015,
            0.0036,
            0.0008,
            -0.0017,
            -0.0018,
            0.0023,
            -0.0019,
            -0.0006,
            0.0017,
            0.0018
          ],
          "after": [
            -0.0708,
            0.0001,
            -0.101,
            -0.0791,
            0.0996,
            0.1522,
            0.0788,
            -0.1498,
            0.0389,
            -0.0081,
            -0.119,
            -0.052,
            0.0903,
            -0.0779,
            0.0055,
            0.0895
          ]
        }
      }
    },
    {
      "step": 430,
      "word": "kaycee",
      "loss": 2.2246,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1004,
            -0.1882,
            0.0107,
            -0.1241,
            0.0251,
            0.1224,
            -0.1233,
            0.0148,
            -0.0307,
            0.0913,
            0.0044,
            0.0853,
            -0.0102,
            0.0892,
            0.0014,
            0.0385
          ],
          "after": [
            0.0094,
            0.0009,
            0.1485,
            -0.0492,
            0.0285,
            -0.0558,
            -0.1363,
            -0.1567,
            -0.0128,
            0.0917,
            0.1544,
            0.015,
            -0.0944,
            -0.0143,
            -0.0431,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0859,
            -0.1795,
            0.0133,
            -0.0886,
            -0.5031,
            0.337,
            -0.3508,
            -0.0594,
            0.3976,
            -0.1141,
            -0.1906,
            -0.0375,
            -0.1462,
            0.418,
            0.6346,
            -0.3607
          ],
          "after": [
            0.0428,
            0.0976,
            -0.0463,
            -0.0721,
            0.0519,
            -0.0012,
            0.086,
            0.0989,
            -0.0696,
            -0.0499,
            0.0289,
            -0.0168,
            0.0981,
            0.0337,
            -0.0719,
            0.2163
          ]
        },
        "position_0": {
          "grad": [
            -0.0924,
            0.0376,
            -0.1077,
            -0.0404,
            0.0445,
            -0.076,
            0.1162,
            -0.0462,
            -0.0448,
            0.0168,
            -0.0092,
            -0.0166,
            0.041,
            -0.0338,
            -0.1161,
            -0.0603
          ],
          "after": [
            0.0137,
            -0.1439,
            -0.2856,
            0.062,
            0.0395,
            -0.051,
            -0.1446,
            0.0676,
            0.0917,
            -0.0129,
            0.0167,
            0.0868,
            0.0395,
            -0.0014,
            -0.1366,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0018,
            -0.0,
            0.0018,
            -0.0022,
            0.0021,
            0.0004,
            -0.0,
            -0.0004,
            0.002,
            -0.0014,
            0.0004,
            -0.0005,
            0.0021,
            0.0006,
            0.0007
          ],
          "after": [
            -0.0705,
            0.0001,
            -0.101,
            -0.0795,
            0.0997,
            0.1518,
            0.0788,
            -0.15,
            0.0386,
            -0.008,
            -0.1182,
            -0.0519,
            0.0903,
            -0.0783,
            0.0053,
            0.0891
          ]
        }
      }
    },
    {
      "step": 431,
      "word": "avan",
      "loss": 2.0262,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1421,
            -0.1078,
            -0.5283,
            0.3405,
            -0.1904,
            -0.0697,
            -0.2924,
            -0.2744,
            -0.1719,
            0.1461,
            0.0568,
            -0.3076,
            0.3329,
            0.0668,
            -0.0766,
            0.4366
          ],
          "after": [
            0.0092,
            0.0014,
            0.1496,
            -0.0498,
            0.029,
            -0.0562,
            -0.1353,
            -0.1562,
            -0.0124,
            0.0918,
            0.1547,
            0.0153,
            -0.0946,
            -0.0147,
            -0.0434,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0011,
            -0.0182,
            -0.0909,
            0.0141,
            0.0047,
            -0.1257,
            0.1428,
            -0.0113,
            -0.133,
            0.0703,
            -0.0245,
            -0.001,
            0.1139,
            -0.0572,
            -0.1185,
            0.0747
          ],
          "after": [
            0.0427,
            0.0984,
            -0.0466,
            -0.0718,
            0.0522,
            -0.0014,
            0.0866,
            0.0991,
            -0.07,
            -0.0502,
            0.0294,
            -0.0165,
            0.098,
            0.033,
            -0.0722,
            0.217
          ]
        },
        "position_0": {
          "grad": [
            0.1273,
            -0.0753,
            0.0942,
            -0.1879,
            -0.0067,
            -0.0099,
            -0.3494,
            -0.1205,
            0.143,
            0.0015,
            0.3005,
            0.0045,
            -0.2091,
            0.0784,
            -0.0215,
            -0.2465
          ],
          "after": [
            0.0128,
            -0.1437,
            -0.2861,
            0.0623,
            0.0392,
            -0.0505,
            -0.1444,
            0.0684,
            0.0915,
            -0.0131,
            0.0159,
            0.0867,
            0.0397,
            -0.0012,
            -0.1362,
            0.0553
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0002,
            0.0,
            -0.0005,
            0.0002,
            0.0018,
            -0.0034,
            0.0008,
            0.0016,
            -0.0022,
            -0.0012,
            0.0,
            -0.0019,
            0.0016,
            0.0008,
            0.0011
          ],
          "after": [
            -0.0702,
            0.0001,
            -0.101,
            -0.0797,
            0.0997,
            0.1511,
            0.0791,
            -0.1502,
            0.038,
            -0.0078,
            -0.1174,
            -0.0519,
            0.0905,
            -0.0788,
            0.0051,
            0.0886
          ]
        }
      }
    },
    {
      "step": 432,
      "word": "alene",
      "loss": 1.9288,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1166,
            0.1015,
            -0.2709,
            -0.1528,
            -0.1932,
            0.0418,
            -0.2966,
            0.1465,
            -0.0515,
            -0.127,
            -0.2491,
            0.2469,
            -0.2286,
            0.0728,
            0.2841,
            -0.4177
          ],
          "after": [
            0.0092,
            0.0016,
            0.1511,
            -0.05,
            0.0297,
            -0.0567,
            -0.1341,
            -0.1563,
            -0.012,
            0.0922,
            0.1555,
            0.0151,
            -0.0945,
            -0.0153,
            -0.0442,
            -0.1874
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.26,
            -0.4323,
            0.3917,
            0.0347,
            0.0328,
            0.4881,
            -1.0417,
            0.0354,
            0.4117,
            -0.0085,
            0.0731,
            -0.0513,
            -0.4029,
            0.4094,
            0.1731,
            -0.5711
          ],
          "after": [
            0.0422,
            0.0995,
            -0.0473,
            -0.0717,
            0.0525,
            -0.0022,
            0.0879,
            0.0992,
            -0.0707,
            -0.0505,
            0.0296,
            -0.0161,
            0.0985,
            0.0317,
            -0.0726,
            0.2181
          ]
        },
        "position_0": {
          "grad": [
            0.1018,
            -0.0955,
            0.0749,
            -0.1891,
            -0.0131,
            -0.0113,
            -0.3396,
            -0.081,
            0.1635,
            0.0209,
            0.2278,
            0.0054,
            -0.2228,
            0.0274,
            -0.0212,
            -0.2643
          ],
          "after": [
            0.0117,
            -0.1433,
            -0.2868,
            0.0629,
            0.039,
            -0.0501,
            -0.1437,
            0.0695,
            0.0911,
            -0.0135,
            0.0147,
            0.0867,
            0.0401,
            -0.0011,
            -0.1359,
            0.0559
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0011,
            -0.0,
            -0.0007,
            -0.0014,
            0.0022,
            -0.0036,
            -0.0004,
            0.0004,
            -0.0002,
            -0.0008,
            0.0,
            -0.0022,
            0.0025,
            0.002,
            -0.002
          ],
          "after": [
            -0.07,
            0.0003,
            -0.101,
            -0.0798,
            0.0999,
            0.1504,
            0.0796,
            -0.1504,
            0.0375,
            -0.0075,
            -0.1166,
            -0.0519,
            0.0907,
            -0.0796,
            0.0048,
            0.0884
          ]
        }
      }
    },
    {
      "step": 433,
      "word": "lojain",
      "loss": 2.3603,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0138,
            -0.275,
            0.1021,
            -0.0847,
            0.1249,
            0.0882,
            -0.2255,
            0.0514,
            0.0276,
            -0.0264,
            0.08,
            -0.1257,
            -0.1411,
            0.0751,
            0.0058,
            -0.1578
          ],
          "after": [
            0.0093,
            0.0023,
            0.1521,
            -0.0501,
            0.0301,
            -0.0574,
            -0.1327,
            -0.1564,
            -0.0116,
            0.0925,
            0.156,
            0.0152,
            -0.0941,
            -0.0159,
            -0.045,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0564,
            0.0839,
            -0.1009,
            0.0042,
            0.0751,
            -0.1211,
            0.2629,
            0.0045,
            -0.2168,
            0.0375,
            -0.0333,
            0.0084,
            0.1227,
            -0.0442,
            -0.236,
            0.1988
          ],
          "after": [
            0.0418,
            0.1003,
            -0.0477,
            -0.0715,
            0.0526,
            -0.0027,
            0.0888,
            0.0992,
            -0.071,
            -0.0507,
            0.0299,
            -0.0158,
            0.0988,
            0.0306,
            -0.0727,
            0.2189
          ]
        },
        "position_0": {
          "grad": [
            -0.0835,
            0.1155,
            -0.102,
            -0.0614,
            0.0234,
            0.0733,
            -0.0475,
            0.0729,
            -0.0596,
            -0.0407,
            -0.1325,
            0.1594,
            -0.0689,
            0.037,
            0.1103,
            -0.0889
          ],
          "after": [
            0.011,
            -0.1432,
            -0.2871,
            0.0635,
            0.0388,
            -0.0501,
            -0.1431,
            0.0701,
            0.0908,
            -0.0136,
            0.014,
            0.0862,
            0.0405,
            -0.0011,
            -0.1358,
            0.0565
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            -0.0055,
            0.0029,
            -0.0031,
            -0.0007,
            -0.0014,
            -0.0006,
            0.0018,
            0.0013,
            -0.0033,
            0.0003,
            -0.003,
            0.0041,
            0.0009,
            -0.0045,
            -0.0021
          ],
          "after": [
            -0.0699,
            0.0009,
            -0.1013,
            -0.0796,
            0.1001,
            0.1499,
            0.0801,
            -0.1508,
            0.0369,
            -0.007,
            -0.116,
            -0.0516,
            0.0907,
            -0.0804,
            0.0048,
            0.0885
          ]
        }
      }
    },
    {
      "step": 434,
      "word": "kayce",
      "loss": 2.2634,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1333,
            -0.2391,
            0.0058,
            -0.1107,
            -0.0021,
            0.1252,
            -0.1559,
            0.0106,
            -0.0304,
            0.0868,
            0.005,
            0.0713,
            -0.0033,
            0.1376,
            0.0364,
            0.0347
          ],
          "after": [
            0.0091,
            0.0033,
            0.153,
            -0.0499,
            0.0304,
            -0.0584,
            -0.1313,
            -0.1566,
            -0.0113,
            0.0926,
            0.1565,
            0.0151,
            -0.0938,
            -0.0167,
            -0.0457,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1903,
            -0.2122,
            0.3,
            0.0622,
            -0.0287,
            -0.0026,
            -0.2177,
            -0.0343,
            0.1495,
            -0.0601,
            -0.1123,
            0.0288,
            -0.1177,
            0.1813,
            0.0035,
            -0.1774
          ],
          "after": [
            0.0418,
            0.1013,
            -0.0485,
            -0.0716,
            0.0528,
            -0.0031,
            0.0898,
            0.0994,
            -0.0715,
            -0.0509,
            0.0304,
            -0.0156,
            0.0991,
            0.0294,
            -0.0728,
            0.2197
          ]
        },
        "position_0": {
          "grad": [
            -0.1087,
            0.0431,
            -0.1236,
            -0.0258,
            0.0333,
            -0.0895,
            0.1449,
            -0.0413,
            -0.0538,
            0.0008,
            -0.0207,
            -0.037,
            0.0529,
            -0.0262,
            -0.1111,
            -0.0558
          ],
          "after": [
            0.0109,
            -0.1432,
            -0.2869,
            0.0641,
            0.0385,
            -0.0497,
            -0.1427,
            0.0707,
            0.0907,
            -0.0137,
            0.0135,
            0.0859,
            0.0409,
            -0.0011,
            -0.1355,
            0.0571
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0018,
            -0.0019,
            -0.0002,
            0.0012,
            -0.0036,
            0.0026,
            0.0,
            -0.0003,
            0.0005,
            0.0023,
            -0.0018,
            0.0009,
            -0.001,
            0.0028,
            0.0019,
            0.0008
          ],
          "after": [
            -0.0697,
            0.0017,
            -0.1016,
            -0.0796,
            0.1006,
            0.1493,
            0.0805,
            -0.1511,
            0.0364,
            -0.0068,
            -0.1153,
            -0.0514,
            0.0907,
            -0.0813,
            0.0047,
            0.0885
          ]
        }
      }
    },
    {
      "step": 435,
      "word": "emrys",
      "loss": 2.6014,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0089,
            0.0042,
            0.1537,
            -0.0498,
            0.0307,
            -0.0592,
            -0.1301,
            -0.1567,
            -0.011,
            0.0927,
            0.1568,
            0.015,
            -0.0936,
            -0.0174,
            -0.0464,
            -0.187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2393,
            0.4767,
            0.5848,
            0.0011,
            0.0962,
            -0.0723,
            0.4336,
            -0.0175,
            0.0744,
            -0.1864,
            -0.0043,
            0.0052,
            -0.0596,
            0.053,
            -0.0571,
            -0.0752
          ],
          "after": [
            0.0423,
            0.1016,
            -0.0498,
            -0.0717,
            0.0528,
            -0.0034,
            0.0902,
            0.0996,
            -0.072,
            -0.0506,
            0.0308,
            -0.0155,
            0.0995,
            0.0283,
            -0.0728,
            0.2205
          ]
        },
        "position_0": {
          "grad": [
            -0.0192,
            -0.1874,
            0.0262,
            -0.0062,
            -0.0553,
            0.0622,
            -0.1818,
            -0.0968,
            0.0987,
            0.1297,
            -0.0058,
            -0.0096,
            -0.1107,
            -0.0171,
            0.045,
            -0.278
          ],
          "after": [
            0.0109,
            -0.1428,
            -0.2868,
            0.0646,
            0.0384,
            -0.0496,
            -0.1422,
            0.0716,
            0.0904,
            -0.0142,
            0.013,
            0.0856,
            0.0413,
            -0.001,
            -0.1354,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0007,
            -0.0006,
            0.0004,
            0.001,
            -0.0019,
            0.0025,
            0.0004,
            -0.0022,
            0.0009,
            -0.0002,
            -0.0018,
            0.0037,
            -0.0017,
            -0.0016,
            0.0015
          ],
          "after": [
            -0.0693,
            0.0024,
            -0.1018,
            -0.0796,
            0.101,
            0.1489,
            0.0807,
            -0.1514,
            0.0362,
            -0.0067,
            -0.1147,
            -0.0512,
            0.0904,
            -0.0819,
            0.0047,
            0.0883
          ]
        }
      }
    },
    {
      "step": 436,
      "word": "mccall",
      "loss": 2.9662,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1353,
            0.1123,
            -0.097,
            -0.0487,
            0.0261,
            0.0088,
            -0.0704,
            0.124,
            -0.0242,
            -0.0432,
            -0.1304,
            0.2745,
            -0.1512,
            -0.018,
            0.1158,
            -0.2125
          ],
          "after": [
            0.009,
            0.0047,
            0.1545,
            -0.0495,
            0.0309,
            -0.0599,
            -0.1289,
            -0.1572,
            -0.0107,
            0.0929,
            0.1575,
            0.0146,
            -0.0931,
            -0.018,
            -0.0472,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0494,
            0.2324,
            -0.2496,
            0.0731,
            0.007,
            -0.2165,
            0.4055,
            -0.0166,
            -0.3914,
            -0.0431,
            -0.0523,
            -0.0152,
            0.248,
            -0.128,
            -0.2504,
            0.4246
          ],
          "after": [
            0.0427,
            0.1015,
            -0.0506,
            -0.0719,
            0.0528,
            -0.0034,
            0.0903,
            0.0998,
            -0.072,
            -0.0503,
            0.0313,
            -0.0153,
            0.0995,
            0.0276,
            -0.0727,
            0.2208
          ]
        },
        "position_0": {
          "grad": [
            -0.0716,
            0.0701,
            -0.0347,
            0.0803,
            0.185,
            0.04,
            0.2348,
            0.1075,
            0.0028,
            0.0052,
            -0.0237,
            0.1496,
            -0.0518,
            -0.0647,
            -0.0241,
            0.1625
          ],
          "after": [
            0.0112,
            -0.1426,
            -0.2866,
            0.0649,
            0.0379,
            -0.0496,
            -0.1421,
            0.072,
            0.0902,
            -0.0147,
            0.0126,
            0.085,
            0.0417,
            -0.0007,
            -0.1352,
            0.0585
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0031,
            -0.0059,
            0.0029,
            -0.0021,
            -0.0009,
            0.0059,
            0.001,
            -0.0057,
            -0.0063,
            -0.0055,
            -0.0046,
            0.0087,
            0.0012,
            0.0022,
            0.0066
          ],
          "after": [
            -0.0691,
            0.0027,
            -0.1013,
            -0.0799,
            0.1014,
            0.1487,
            0.0804,
            -0.1518,
            0.0368,
            -0.0061,
            -0.1136,
            -0.0505,
            0.0895,
            -0.0826,
            0.0046,
            0.0875
          ]
        }
      }
    },
    {
      "step": 437,
      "word": "mei",
      "loss": 2.7037,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0091,
            0.0051,
            0.1552,
            -0.0494,
            0.0311,
            -0.0606,
            -0.128,
            -0.1576,
            -0.0105,
            0.093,
            0.158,
            0.0142,
            -0.0928,
            -0.0185,
            -0.0479,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.5852,
            -0.4484,
            0.5867,
            -0.2175,
            0.4148,
            0.2376,
            -0.8367,
            -0.1513,
            0.8622,
            0.2935,
            0.4641,
            0.1292,
            -0.6163,
            0.0484,
            -0.115,
            -0.5738
          ],
          "after": [
            0.0422,
            0.102,
            -0.0519,
            -0.0716,
            0.0523,
            -0.0036,
            0.091,
            0.1007,
            -0.0728,
            -0.0506,
            0.0307,
            -0.0154,
            0.1002,
            0.0269,
            -0.0724,
            0.2215
          ]
        },
        "position_0": {
          "grad": [
            -0.0755,
            -0.0252,
            -0.0448,
            0.0818,
            0.0836,
            0.0199,
            0.1687,
            0.2292,
            0.2215,
            -0.0557,
            -0.1042,
            0.0393,
            -0.2798,
            -0.0157,
            0.1157,
            0.0982
          ],
          "after": [
            0.0118,
            -0.1424,
            -0.2863,
            0.065,
            0.0373,
            -0.0497,
            -0.1422,
            0.0716,
            0.0896,
            -0.0149,
            0.0125,
            0.0844,
            0.0424,
            -0.0004,
            -0.1353,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            -0.0,
            -0.0003,
            -0.0016,
            0.0004,
            0.0037,
            -0.0014,
            -0.0003,
            0.0026,
            0.0016,
            0.001,
            0.0051,
            -0.0067,
            0.0002,
            0.0029,
            -0.0019
          ],
          "after": [
            -0.0689,
            0.0029,
            -0.1008,
            -0.08,
            0.1018,
            0.1481,
            0.0802,
            -0.1521,
            0.0369,
            -0.0057,
            -0.1127,
            -0.0504,
            0.0892,
            -0.0832,
            0.0042,
            0.087
          ]
        }
      }
    },
    {
      "step": 438,
      "word": "maclyn",
      "loss": 1.9722,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0448,
            0.026,
            0.0687,
            0.143,
            -0.0586,
            -0.0568,
            0.1259,
            0.0869,
            -0.1026,
            0.0595,
            0.0816,
            -0.0084,
            0.0692,
            0.0185,
            -0.1067,
            -0.0121
          ],
          "after": [
            0.0093,
            0.0055,
            0.1557,
            -0.0495,
            0.0313,
            -0.0609,
            -0.1273,
            -0.1582,
            -0.01,
            0.093,
            0.1583,
            0.0139,
            -0.0925,
            -0.0189,
            -0.0482,
            -0.186
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0531,
            0.1101,
            -0.1711,
            0.0544,
            0.0093,
            -0.1404,
            0.2773,
            -0.004,
            -0.2633,
            0.0414,
            -0.0229,
            -0.0044,
            0.1677,
            -0.1047,
            -0.1736,
            0.2581
          ],
          "after": [
            0.0418,
            0.1023,
            -0.0529,
            -0.0714,
            0.0519,
            -0.0037,
            0.0914,
            0.1014,
            -0.0732,
            -0.051,
            0.0302,
            -0.0155,
            0.1007,
            0.0265,
            -0.0721,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            -0.0665,
            0.0568,
            -0.0433,
            0.0581,
            0.1628,
            0.022,
            0.2109,
            0.1063,
            0.0193,
            0.0175,
            -0.0298,
            0.1271,
            -0.0837,
            -0.0676,
            -0.0163,
            0.1302
          ],
          "after": [
            0.0125,
            -0.1423,
            -0.2859,
            0.0649,
            0.0365,
            -0.0499,
            -0.1425,
            0.0708,
            0.0891,
            -0.0152,
            0.0125,
            0.0836,
            0.0432,
            0.0,
            -0.1353,
            0.0589
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0007,
            0.0006,
            -0.0006,
            0.0012,
            -0.0023,
            0.0021,
            -0.0018,
            -0.0011,
            0.0012,
            0.0016,
            -0.0005,
            0.0016,
            -0.0016,
            -0.0016,
            -0.0001
          ],
          "after": [
            -0.0688,
            0.003,
            -0.1004,
            -0.08,
            0.102,
            0.1478,
            0.08,
            -0.1521,
            0.0372,
            -0.0055,
            -0.1122,
            -0.0503,
            0.0889,
            -0.0836,
            0.0041,
            0.0866
          ]
        }
      }
    },
    {
      "step": 439,
      "word": "celia",
      "loss": 1.9755,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3862,
            -0.0695,
            -0.166,
            -0.1278,
            -0.0428,
            -0.0185,
            -0.0578,
            -0.0278,
            -0.1277,
            0.2213,
            0.128,
            -0.3461,
            0.1221,
            0.167,
            -0.0038,
            -0.2522
          ],
          "after": [
            0.0087,
            0.0059,
            0.1563,
            -0.0493,
            0.0316,
            -0.0612,
            -0.1267,
            -0.1587,
            -0.0094,
            0.0926,
            0.1582,
            0.0141,
            -0.0926,
            -0.0197,
            -0.0485,
            -0.1854
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1115,
            -0.0464,
            0.4236,
            -0.0299,
            0.3781,
            -0.0723,
            -0.1473,
            -0.0404,
            0.4432,
            0.1528,
            0.1382,
            0.2684,
            -0.3364,
            0.0256,
            -0.2637,
            -0.1969
          ],
          "after": [
            0.0416,
            0.1025,
            -0.0541,
            -0.0712,
            0.0512,
            -0.0036,
            0.0918,
            0.1022,
            -0.074,
            -0.0515,
            0.0294,
            -0.016,
            0.1014,
            0.0261,
            -0.0716,
            0.2225
          ]
        },
        "position_0": {
          "grad": [
            -0.0176,
            -0.0074,
            -0.0044,
            0.0991,
            -0.1534,
            -0.0317,
            0.0367,
            0.0859,
            -0.0196,
            -0.0035,
            -0.0103,
            -0.063,
            0.0405,
            0.0421,
            -0.0212,
            0.0148
          ],
          "after": [
            0.0132,
            -0.1422,
            -0.2856,
            0.0647,
            0.0361,
            -0.0499,
            -0.1429,
            0.0699,
            0.0887,
            -0.0154,
            0.0125,
            0.083,
            0.0438,
            0.0003,
            -0.1353,
            0.059
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0003,
            0.004,
            -0.0001,
            0.0027,
            -0.0026,
            -0.0027,
            -0.0011,
            -0.0001,
            0.0002,
            0.0013,
            0.0023,
            -0.0017,
            -0.0016,
            -0.0027,
            -0.0031
          ],
          "after": [
            -0.0687,
            0.0032,
            -0.1006,
            -0.08,
            0.102,
            0.1479,
            0.0799,
            -0.1519,
            0.0375,
            -0.0053,
            -0.1119,
            -0.0504,
            0.0887,
            -0.0836,
            0.0041,
            0.0866
          ]
        }
      }
    },
    {
      "step": 440,
      "word": "carlyann",
      "loss": 2.233,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1441,
            0.0641,
            -0.0467,
            0.002,
            0.0518,
            0.0299,
            0.1154,
            -0.1112,
            0.1921,
            -0.2572,
            -0.1222,
            0.2441,
            -0.1844,
            -0.2118,
            0.053,
            0.2929
          ],
          "after": [
            0.0085,
            0.0061,
            0.157,
            -0.0492,
            0.0318,
            -0.0615,
            -0.1263,
            -0.1587,
            -0.0093,
            0.0927,
            0.1584,
            0.014,
            -0.0923,
            -0.0198,
            -0.0488,
            -0.1853
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0247,
            0.0718,
            -0.1095,
            0.0397,
            0.0037,
            -0.1084,
            0.208,
            -0.0272,
            -0.1855,
            0.0332,
            0.0008,
            -0.0022,
            0.1163,
            -0.1017,
            -0.1306,
            0.1823
          ],
          "after": [
            0.0415,
            0.1027,
            -0.0551,
            -0.0711,
            0.0505,
            -0.0035,
            0.092,
            0.1029,
            -0.0745,
            -0.0521,
            0.0288,
            -0.0165,
            0.1019,
            0.0259,
            -0.071,
            0.2227
          ]
        },
        "position_0": {
          "grad": [
            -0.0233,
            0.026,
            -0.0226,
            0.0748,
            -0.0677,
            -0.0219,
            0.0849,
            0.0579,
            -0.0637,
            0.0352,
            -0.0103,
            0.0091,
            0.0332,
            -0.0142,
            -0.0392,
            0.0281
          ],
          "after": [
            0.0139,
            -0.1423,
            -0.2852,
            0.0643,
            0.0359,
            -0.0498,
            -0.1433,
            0.0689,
            0.0884,
            -0.0157,
            0.0125,
            0.0825,
            0.0442,
            0.0006,
            -0.1353,
            0.059
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0006,
            -0.0005,
            0.0004,
            -0.0006,
            0.0001,
            0.0005,
            0.0002,
            0.0004,
            0.0,
            -0.0001,
            0.0002,
            0.0,
            0.0001,
            0.0001,
            0.0005
          ],
          "after": [
            -0.0686,
            0.0032,
            -0.1007,
            -0.0801,
            0.102,
            0.1479,
            0.0799,
            -0.1518,
            0.0377,
            -0.0052,
            -0.1116,
            -0.0505,
            0.0886,
            -0.0837,
            0.0042,
            0.0865
          ]
        }
      }
    },
    {
      "step": 441,
      "word": "tywan",
      "loss": 2.5958,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0124,
            0.1145,
            -0.2134,
            -0.0014,
            -0.202,
            -0.152,
            0.0139,
            -0.0663,
            0.0523,
            -0.0032,
            0.0016,
            0.0197,
            -0.0589,
            -0.0334,
            0.1065,
            -0.0004
          ],
          "after": [
            0.0083,
            0.0061,
            0.1579,
            -0.0491,
            0.0322,
            -0.0613,
            -0.1261,
            -0.1585,
            -0.0093,
            0.0928,
            0.1586,
            0.0138,
            -0.0919,
            -0.0199,
            -0.0494,
            -0.1852
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0326,
            0.0386,
            -0.1008,
            0.0624,
            0.0392,
            -0.1259,
            0.2406,
            -0.0053,
            -0.204,
            0.066,
            -0.0075,
            -0.0334,
            0.1231,
            -0.0774,
            -0.1906,
            0.1478
          ],
          "after": [
            0.0415,
            0.1028,
            -0.0558,
            -0.0712,
            0.05,
            -0.0032,
            0.092,
            0.1036,
            -0.0747,
            -0.0526,
            0.0283,
            -0.0168,
            0.1022,
            0.0259,
            -0.0704,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0125,
            -0.1856,
            0.124,
            0.1249,
            -0.1537,
            0.0993,
            0.0873,
            0.047,
            -0.1236,
            -0.2102,
            -0.135,
            -0.3244,
            0.1594,
            0.0813,
            0.1607,
            0.2019
          ],
          "after": [
            0.0145,
            -0.1418,
            -0.2853,
            0.0638,
            0.0361,
            -0.0501,
            -0.1437,
            0.068,
            0.0884,
            -0.0152,
            0.0128,
            0.0829,
            0.0444,
            0.0005,
            -0.1355,
            0.0587
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0009,
            -0.0003,
            -0.0004,
            -0.0005,
            0.0001,
            -0.0006,
            -0.0004,
            0.0003,
            -0.0004,
            -0.0007,
            -0.001,
            0.0011,
            0.0004,
            0.0007,
            0.0
          ],
          "after": [
            -0.0684,
            0.0034,
            -0.1007,
            -0.0801,
            0.102,
            0.148,
            0.0799,
            -0.1516,
            0.0378,
            -0.005,
            -0.1113,
            -0.0505,
            0.0884,
            -0.0838,
            0.0041,
            0.0864
          ]
        }
      }
    },
    {
      "step": 442,
      "word": "ily",
      "loss": 3.0796,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0082,
            0.0061,
            0.1587,
            -0.049,
            0.0326,
            -0.0611,
            -0.1258,
            -0.1584,
            -0.0092,
            0.0929,
            0.1587,
            0.0137,
            -0.0917,
            -0.02,
            -0.0499,
            -0.1852
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0147,
            0.0505,
            -0.2149,
            -0.0333,
            -0.0488,
            -0.1591,
            0.2683,
            -0.0966,
            -0.3007,
            0.0287,
            -0.0239,
            -0.163,
            0.2626,
            -0.0522,
            -0.2327,
            0.26
          ],
          "after": [
            0.0414,
            0.1028,
            -0.0561,
            -0.0712,
            0.0495,
            -0.0027,
            0.0918,
            0.1046,
            -0.0746,
            -0.0532,
            0.0279,
            -0.0168,
            0.1021,
            0.026,
            -0.0697,
            0.2226
          ]
        },
        "position_0": {
          "grad": [
            -0.0521,
            -0.4661,
            0.1437,
            -0.1827,
            -0.0773,
            0.2026,
            -0.3954,
            0.0949,
            0.2249,
            -0.0745,
            -0.0026,
            -0.3236,
            -0.3313,
            0.1591,
            0.1705,
            -0.2815
          ],
          "after": [
            0.0152,
            -0.1403,
            -0.2858,
            0.0636,
            0.0364,
            -0.051,
            -0.1436,
            0.0668,
            0.0881,
            -0.0145,
            0.013,
            0.084,
            0.045,
            0.0,
            -0.1362,
            0.0589
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0017,
            -0.0007,
            0.0003,
            -0.0011,
            -0.002,
            0.0012,
            -0.0006,
            0.0,
            -0.0017,
            -0.0001,
            -0.0004,
            0.0012,
            -0.0003,
            0.0006,
            0.0015
          ],
          "after": [
            -0.0684,
            0.0033,
            -0.1006,
            -0.0801,
            0.1022,
            0.1482,
            0.0798,
            -0.1514,
            0.0379,
            -0.0047,
            -0.111,
            -0.0504,
            0.0881,
            -0.0839,
            0.0041,
            0.0862
          ]
        }
      }
    },
    {
      "step": 443,
      "word": "lexy",
      "loss": 3.2672,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.008,
            0.0061,
            0.1593,
            -0.0489,
            0.0329,
            -0.0609,
            -0.1256,
            -0.1583,
            -0.0092,
            0.093,
            0.1588,
            0.0136,
            -0.0914,
            -0.02,
            -0.0503,
            -0.1851
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1653,
            -0.42,
            0.3264,
            -0.0916,
            0.4631,
            0.1598,
            -0.6166,
            0.0666,
            0.5059,
            0.3082,
            0.0701,
            0.2066,
            -0.3487,
            0.0854,
            -0.1247,
            -0.4277
          ],
          "after": [
            0.0411,
            0.1033,
            -0.0568,
            -0.0709,
            0.0486,
            -0.0025,
            0.0921,
            0.1051,
            -0.075,
            -0.0542,
            0.0274,
            -0.0172,
            0.1025,
            0.026,
            -0.0689,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0948,
            0.1069,
            -0.1293,
            -0.0555,
            -0.0733,
            0.0988,
            -0.1241,
            0.1446,
            -0.0145,
            -0.1305,
            -0.2188,
            0.1357,
            -0.1243,
            0.1387,
            0.259,
            -0.1543
          ],
          "after": [
            0.0163,
            -0.1394,
            -0.2859,
            0.0637,
            0.0368,
            -0.0521,
            -0.1434,
            0.0654,
            0.0878,
            -0.0136,
            0.0136,
            0.0846,
            0.0456,
            -0.0008,
            -0.1372,
            0.0592
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0012,
            0.001,
            -0.0013,
            0.0017,
            -0.0003,
            -0.0029,
            0.0005,
            0.0016,
            -0.0001,
            -0.0006,
            0.0005,
            -0.0012,
            0.001,
            -0.0011,
            -0.0015
          ],
          "after": [
            -0.0684,
            0.0034,
            -0.1007,
            -0.08,
            0.1021,
            0.1484,
            0.0799,
            -0.1513,
            0.0377,
            -0.0045,
            -0.1106,
            -0.0504,
            0.088,
            -0.0841,
            0.0041,
            0.0862
          ]
        }
      }
    },
    {
      "step": 444,
      "word": "locklan",
      "loss": 2.4248,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2102,
            0.0946,
            -0.0227,
            0.1248,
            -0.0564,
            -0.0395,
            0.0269,
            -0.0049,
            0.1061,
            -0.247,
            -0.1554,
            0.2097,
            -0.0908,
            -0.0737,
            0.0484,
            0.2397
          ],
          "after": [
            0.0083,
            0.0059,
            0.1599,
            -0.0491,
            0.0333,
            -0.0606,
            -0.1255,
            -0.1582,
            -0.0095,
            0.0936,
            0.1593,
            0.0132,
            -0.0911,
            -0.0199,
            -0.0507,
            -0.1854
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0639,
            0.1958,
            -0.1299,
            0.0589,
            0.0442,
            -0.2011,
            0.3637,
            -0.0134,
            -0.283,
            -0.0099,
            -0.0029,
            0.014,
            0.1378,
            -0.1411,
            -0.2103,
            0.3256
          ],
          "after": [
            0.0409,
            0.1035,
            -0.0572,
            -0.0709,
            0.0478,
            -0.0021,
            0.0921,
            0.1057,
            -0.0751,
            -0.0551,
            0.027,
            -0.0175,
            0.1026,
            0.0261,
            -0.0682,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0816,
            0.1121,
            -0.0986,
            -0.0442,
            0.0434,
            0.06,
            -0.0162,
            0.0718,
            -0.0708,
            -0.0256,
            -0.1179,
            0.1694,
            -0.0464,
            0.018,
            0.0867,
            -0.0687
          ],
          "after": [
            0.0174,
            -0.1388,
            -0.2856,
            0.0638,
            0.037,
            -0.0532,
            -0.1431,
            0.0639,
            0.0876,
            -0.0127,
            0.0144,
            0.0848,
            0.0463,
            -0.0016,
            -0.1383,
            0.0596
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0042,
            0.0005,
            -0.001,
            0.0,
            0.0001,
            0.0001,
            0.0018,
            0.0001,
            -0.0007,
            0.0016,
            0.0016,
            -0.0016,
            0.0025,
            0.001,
            -0.0032,
            0.0015
          ],
          "after": [
            -0.0689,
            0.0034,
            -0.1007,
            -0.0799,
            0.1021,
            0.1486,
            0.0799,
            -0.1512,
            0.0377,
            -0.0044,
            -0.1106,
            -0.0503,
            0.0877,
            -0.0844,
            0.0044,
            0.086
          ]
        }
      }
    },
    {
      "step": 445,
      "word": "gema",
      "loss": 2.4404,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5585,
            -0.1338,
            -0.4076,
            -0.1268,
            -0.2877,
            -0.0159,
            -0.1684,
            -0.018,
            -0.1634,
            0.2936,
            0.1202,
            -0.532,
            0.187,
            0.2753,
            0.1917,
            -0.2869
          ],
          "after": [
            0.0075,
            0.006,
            0.1611,
            -0.049,
            0.034,
            -0.0603,
            -0.1251,
            -0.158,
            -0.0093,
            0.0935,
            0.1594,
            0.0136,
            -0.0911,
            -0.0204,
            -0.0516,
            -0.1852
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1992,
            -0.309,
            0.4733,
            0.1472,
            0.379,
            -0.0409,
            -0.3112,
            -0.0031,
            0.4879,
            0.0872,
            0.0544,
            0.1657,
            -0.3249,
            0.1761,
            -0.2656,
            -0.2965
          ],
          "after": [
            0.0405,
            0.104,
            -0.0581,
            -0.0712,
            0.0468,
            -0.0017,
            0.0923,
            0.1061,
            -0.0756,
            -0.0559,
            0.0265,
            -0.0181,
            0.1031,
            0.026,
            -0.0673,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            0.0312,
            0.0598,
            0.0349,
            0.1702,
            -0.102,
            -0.1374,
            0.0092,
            -0.0177,
            -0.1208,
            0.1005,
            0.0854,
            -0.1683,
            0.115,
            0.0584,
            -0.1752,
            0.1138
          ],
          "after": [
            0.0183,
            -0.1384,
            -0.2854,
            0.0635,
            0.0374,
            -0.0537,
            -0.1429,
            0.0627,
            0.0877,
            -0.0122,
            0.0148,
            0.0852,
            0.0466,
            -0.0025,
            -0.1389,
            0.0598
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0016,
            -0.0013,
            0.002,
            -0.0019,
            0.0024,
            -0.0014,
            0.0022,
            0.001,
            -0.0043,
            -0.0047,
            -0.0011,
            0.0013,
            0.0035,
            0.001,
            0.0041
          ],
          "after": [
            -0.0694,
            0.0036,
            -0.1005,
            -0.08,
            0.1022,
            0.1485,
            0.08,
            -0.1514,
            0.0375,
            -0.004,
            -0.11,
            -0.0501,
            0.0874,
            -0.085,
            0.0045,
            0.0855
          ]
        }
      }
    },
    {
      "step": 446,
      "word": "darel",
      "loss": 2.0835,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1343,
            -0.1317,
            -0.0671,
            -0.1229,
            0.054,
            0.0676,
            0.0267,
            -0.1636,
            0.2025,
            -0.0783,
            0.0413,
            0.0151,
            -0.1654,
            -0.1116,
            0.0848,
            0.0851
          ],
          "after": [
            0.0067,
            0.0063,
            0.1622,
            -0.0487,
            0.0346,
            -0.0603,
            -0.1248,
            -0.1574,
            -0.0096,
            0.0936,
            0.1595,
            0.0139,
            -0.0908,
            -0.0206,
            -0.0525,
            -0.1852
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1266,
            -0.0849,
            -0.0589,
            -0.0979,
            0.1433,
            -0.0205,
            -0.1635,
            0.0931,
            -0.0876,
            -0.2702,
            -0.0215,
            -0.0026,
            -0.0869,
            -0.0912,
            0.3021,
            0.0009
          ],
          "after": [
            0.0403,
            0.1046,
            -0.0587,
            -0.0712,
            0.0457,
            -0.0014,
            0.0926,
            0.1061,
            -0.0759,
            -0.0562,
            0.0261,
            -0.0186,
            0.1036,
            0.026,
            -0.0668,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            -0.0358,
            0.0431,
            -0.0113,
            -0.0833,
            0.115,
            -0.0939,
            0.0774,
            -0.0642,
            -0.0099,
            -0.0052,
            -0.0954,
            -0.0621,
            0.0827,
            0.0599,
            0.0326,
            0.1671
          ],
          "after": [
            0.0192,
            -0.1382,
            -0.2853,
            0.0635,
            0.0376,
            -0.0538,
            -0.1429,
            0.062,
            0.0878,
            -0.0118,
            0.0154,
            0.0858,
            0.0469,
            -0.0034,
            -0.1394,
            0.0597
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0023,
            -0.0026,
            -0.0002,
            -0.0016,
            0.0008,
            0.001,
            -0.0031,
            0.0027,
            0.0003,
            -0.0018,
            -0.0006,
            -0.0003,
            -0.002,
            -0.0002,
            0.0035,
            -0.0008
          ],
          "after": [
            -0.0696,
            0.004,
            -0.1003,
            -0.08,
            0.1023,
            0.1484,
            0.0802,
            -0.1521,
            0.0374,
            -0.0035,
            -0.1094,
            -0.0499,
            0.0872,
            -0.0855,
            0.0044,
            0.0851
          ]
        }
      }
    },
    {
      "step": 447,
      "word": "jaryia",
      "loss": 2.1038,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1925,
            -0.0892,
            -0.0624,
            -0.2267,
            0.0673,
            0.0756,
            0.0048,
            -0.1766,
            0.119,
            0.0532,
            0.1021,
            -0.0521,
            -0.1114,
            -0.0751,
            0.0311,
            -0.0318
          ],
          "after": [
            0.0057,
            0.0067,
            0.1632,
            -0.048,
            0.0349,
            -0.0605,
            -0.1246,
            -0.1564,
            -0.0101,
            0.0935,
            0.1592,
            0.0143,
            -0.0904,
            -0.0206,
            -0.0533,
            -0.1851
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0827,
            0.0544,
            -0.0798,
            0.0058,
            0.0658,
            -0.1103,
            0.2295,
            -0.0068,
            -0.1956,
            0.0309,
            -0.0376,
            -0.0001,
            0.1065,
            -0.066,
            -0.1741,
            0.1585
          ],
          "after": [
            0.0403,
            0.105,
            -0.0592,
            -0.0713,
            0.0447,
            -0.0009,
            0.0927,
            0.1062,
            -0.0761,
            -0.0565,
            0.0259,
            -0.019,
            0.1039,
            0.0262,
            -0.0662,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            -0.044,
            0.0981,
            0.0212,
            0.1369,
            0.0811,
            -0.0851,
            0.2141,
            0.0379,
            -0.2753,
            -0.0233,
            0.0269,
            0.0658,
            0.3253,
            -0.1938,
            -0.2113,
            0.1696
          ],
          "after": [
            0.0201,
            -0.1383,
            -0.2852,
            0.0632,
            0.0375,
            -0.0536,
            -0.1431,
            0.0612,
            0.0883,
            -0.0114,
            0.0159,
            0.0861,
            0.0466,
            -0.0035,
            -0.1394,
            0.0594
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0006,
            -0.0005,
            0.001,
            -0.0029,
            0.0001,
            -0.0013,
            0.0009,
            0.0004,
            -0.0016,
            -0.0008,
            0.0007,
            -0.0006,
            0.001,
            0.0022,
            0.0004
          ],
          "after": [
            -0.0697,
            0.0043,
            -0.1,
            -0.08,
            0.1026,
            0.1482,
            0.0806,
            -0.1527,
            0.0371,
            -0.0029,
            -0.1089,
            -0.0498,
            0.0871,
            -0.0861,
            0.0041,
            0.0847
          ]
        }
      }
    },
    {
      "step": 448,
      "word": "bohdi",
      "loss": 2.8017,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0049,
            0.0071,
            0.1641,
            -0.0474,
            0.0352,
            -0.0607,
            -0.1244,
            -0.1556,
            -0.0105,
            0.0935,
            0.159,
            0.0146,
            -0.0901,
            -0.0205,
            -0.0541,
            -0.1851
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0177,
            0.1006,
            -0.0874,
            0.0795,
            0.1078,
            -0.1608,
            0.2693,
            0.0066,
            -0.2405,
            0.062,
            -0.0174,
            -0.0108,
            0.1458,
            -0.0547,
            -0.3124,
            0.2031
          ],
          "after": [
            0.0403,
            0.1052,
            -0.0595,
            -0.0715,
            0.0438,
            -0.0004,
            0.0925,
            0.1062,
            -0.076,
            -0.0568,
            0.0258,
            -0.0194,
            0.104,
            0.0264,
            -0.0654,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            0.0451,
            0.0279,
            0.159,
            0.0353,
            0.0741,
            -0.0005,
            0.1089,
            -0.0589,
            -0.1546,
            -0.0825,
            0.079,
            -0.0281,
            0.2133,
            -0.0974,
            -0.0761,
            0.3765
          ],
          "after": [
            0.0208,
            -0.1384,
            -0.2857,
            0.0628,
            0.0373,
            -0.0535,
            -0.1434,
            0.0607,
            0.089,
            -0.0108,
            0.0161,
            0.0865,
            0.0461,
            -0.0034,
            -0.1393,
            0.0586
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0053,
            -0.001,
            -0.0024,
            0.0007,
            0.0035,
            0.0028,
            0.0042,
            0.0001,
            0.0024,
            0.0033,
            0.0003,
            -0.0002,
            0.0001,
            0.0018,
            -0.0103,
            0.0038
          ],
          "after": [
            -0.0704,
            0.0047,
            -0.0996,
            -0.0801,
            0.1025,
            0.1477,
            0.0806,
            -0.1533,
            0.0366,
            -0.0027,
            -0.1084,
            -0.0497,
            0.087,
            -0.0868,
            0.0046,
            0.084
          ]
        }
      }
    },
    {
      "step": 449,
      "word": "olenna",
      "loss": 2.1581,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0997,
            -0.0119,
            -0.0163,
            -0.0946,
            -0.0202,
            -0.0043,
            -0.0479,
            -0.0247,
            -0.0242,
            0.0891,
            0.0682,
            -0.0929,
            0.0228,
            0.0535,
            -0.0031,
            -0.1311
          ],
          "after": [
            0.004,
            0.0075,
            0.1649,
            -0.0468,
            0.0355,
            -0.0609,
            -0.1242,
            -0.1548,
            -0.0108,
            0.0933,
            0.1587,
            0.015,
            -0.0898,
            -0.0207,
            -0.0547,
            -0.1849
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0286,
            -0.2567,
            0.1914,
            0.0391,
            0.2223,
            0.2208,
            -0.299,
            0.1434,
            0.2301,
            0.1757,
            0.0349,
            0.2198,
            -0.2787,
            -0.0068,
            0.0477,
            -0.2708
          ],
          "after": [
            0.0403,
            0.1057,
            -0.06,
            -0.0718,
            0.0427,
            -0.0002,
            0.0926,
            0.1056,
            -0.0761,
            -0.0574,
            0.0256,
            -0.0201,
            0.1044,
            0.0266,
            -0.0648,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            0.1242,
            -0.1504,
            0.093,
            -0.1191,
            0.0402,
            -0.0359,
            -0.2738,
            -0.0561,
            0.0596,
            0.1745,
            0.0632,
            0.0143,
            -0.1168,
            -0.0173,
            -0.0536,
            -0.2357
          ],
          "after": [
            0.0208,
            -0.1382,
            -0.2864,
            0.0628,
            0.037,
            -0.0532,
            -0.1433,
            0.0604,
            0.0895,
            -0.0108,
            0.0162,
            0.0867,
            0.0458,
            -0.0032,
            -0.139,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0015,
            -0.0003,
            -0.0006,
            -0.0001,
            0.0016,
            -0.0017,
            0.0012,
            0.0003,
            0.0003,
            -0.0006,
            0.0007,
            -0.0017,
            0.0004,
            0.0015,
            -0.0016
          ],
          "after": [
            -0.0709,
            0.0052,
            -0.0992,
            -0.0802,
            0.1025,
            0.1472,
            0.0807,
            -0.154,
            0.0362,
            -0.0025,
            -0.108,
            -0.0496,
            0.0871,
            -0.0875,
            0.0049,
            0.0836
          ]
        }
      }
    },
    {
      "step": 450,
      "word": "west",
      "loss": 2.8875,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0032,
            0.0078,
            0.1656,
            -0.0462,
            0.0358,
            -0.061,
            -0.124,
            -0.1541,
            -0.011,
            0.0931,
            0.1584,
            0.0153,
            -0.0896,
            -0.0207,
            -0.0552,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1711,
            -0.2064,
            0.153,
            -0.135,
            0.2873,
            0.1447,
            -0.6617,
            -0.1092,
            0.6002,
            0.1967,
            0.1268,
            0.2646,
            -0.2213,
            0.0832,
            -0.1063,
            -0.3251
          ],
          "after": [
            0.0401,
            0.1064,
            -0.0606,
            -0.0717,
            0.0415,
            -0.0001,
            0.0932,
            0.1056,
            -0.0767,
            -0.0583,
            0.0251,
            -0.0211,
            0.105,
            0.0266,
            -0.0642,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            0.1826,
            0.1538,
            -0.0728,
            0.2044,
            -0.1065,
            -0.0911,
            0.2051,
            0.0509,
            -0.0283,
            -0.0836,
            -0.0708,
            -0.2262,
            0.1914,
            0.0962,
            -0.0857,
            0.0666
          ],
          "after": [
            0.0201,
            -0.1383,
            -0.2868,
            0.0623,
            0.037,
            -0.0528,
            -0.1435,
            0.0601,
            0.09,
            -0.0106,
            0.0164,
            0.0875,
            0.0453,
            -0.0034,
            -0.1386,
            0.0579
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            0.0006,
            -0.0014,
            -0.0013,
            0.0026,
            -0.0016,
            0.0008,
            -0.0014,
            0.0027,
            0.0018,
            0.0022,
            0.0021,
            0.0009,
            -0.0006,
            -0.005,
            0.0001
          ],
          "after": [
            -0.0716,
            0.0055,
            -0.0986,
            -0.0801,
            0.1022,
            0.1469,
            0.0807,
            -0.1544,
            0.0354,
            -0.0026,
            -0.1078,
            -0.0498,
            0.0871,
            -0.088,
            0.0055,
            0.0832
          ]
        }
      }
    },
    {
      "step": 451,
      "word": "maccabee",
      "loss": 2.7975,
      "learning_rate": 0.0017,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.465,
            0.1906,
            0.3529,
            0.1545,
            0.1118,
            0.0543,
            0.4524,
            0.0756,
            -0.1681,
            0.0199,
            -0.0165,
            0.2725,
            0.2126,
            -0.2916,
            -0.2468,
            0.5622
          ],
          "after": [
            0.0033,
            0.0077,
            0.1656,
            -0.046,
            0.0358,
            -0.0613,
            -0.1245,
            -0.1538,
            -0.0109,
            0.0929,
            0.1582,
            0.0152,
            -0.0897,
            -0.0202,
            -0.055,
            -0.1853
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.6435,
            0.0122,
            -0.7933,
            -0.2323,
            -0.8399,
            0.5967,
            -0.1285,
            0.0517,
            0.1532,
            0.0299,
            -0.1194,
            -0.6865,
            0.2507,
            0.2687,
            0.6961,
            -0.3701
          ],
          "after": [
            0.0389,
            0.1069,
            -0.0602,
            -0.0711,
            0.0414,
            -0.0008,
            0.0938,
            0.1053,
            -0.0773,
            -0.059,
            0.025,
            -0.0208,
            0.1052,
            0.0261,
            -0.0643,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            -0.0493,
            0.0803,
            -0.0486,
            0.0382,
            0.1685,
            -0.0034,
            0.2062,
            0.033,
            -0.0307,
            0.0645,
            0.0007,
            0.1318,
            -0.0474,
            -0.0747,
            -0.063,
            0.1214
          ],
          "after": [
            0.0197,
            -0.1386,
            -0.2869,
            0.0619,
            0.0367,
            -0.0523,
            -0.144,
            0.0597,
            0.0905,
            -0.0106,
            0.0165,
            0.0878,
            0.045,
            -0.0032,
            -0.1382,
            0.0574
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0043,
            0.0015,
            0.0022,
            -0.0,
            0.0035,
            -0.0019,
            0.0,
            -0.0032,
            -0.002,
            0.0019,
            0.003,
            -0.0003,
            0.0007,
            -0.0,
            -0.0052,
            -0.0019
          ],
          "after": [
            -0.0727,
            0.0056,
            -0.0984,
            -0.08,
            0.1017,
            0.1469,
            0.0808,
            -0.1542,
            0.0351,
            -0.0027,
            -0.1081,
            -0.0499,
            0.087,
            -0.0884,
            0.0064,
            0.0831
          ]
        }
      }
    },
    {
      "step": 452,
      "word": "emberlynn",
      "loss": 2.9999,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0034,
            0.0076,
            0.1655,
            -0.0458,
            0.0359,
            -0.0615,
            -0.1249,
            -0.1535,
            -0.0108,
            0.0927,
            0.158,
            0.0151,
            -0.0899,
            -0.0198,
            -0.0549,
            -0.1858
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2422,
            0.1853,
            0.3027,
            -0.0857,
            0.1295,
            0.0954,
            0.2007,
            0.0295,
            0.2338,
            -0.2833,
            -0.0394,
            0.0027,
            -0.2183,
            -0.1126,
            0.2349,
            -0.1409
          ],
          "after": [
            0.0383,
            0.1072,
            -0.0603,
            -0.0703,
            0.0412,
            -0.0016,
            0.0941,
            0.105,
            -0.0781,
            -0.0592,
            0.025,
            -0.0206,
            0.1057,
            0.026,
            -0.0646,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            -0.0285,
            -0.0645,
            0.006,
            -0.0029,
            0.0496,
            0.039,
            -0.0551,
            -0.0936,
            -0.0059,
            0.1479,
            0.0249,
            0.0546,
            -0.0436,
            -0.0637,
            -0.0534,
            -0.1303
          ],
          "after": [
            0.0195,
            -0.1388,
            -0.2871,
            0.0615,
            0.0363,
            -0.0521,
            -0.1443,
            0.0596,
            0.0909,
            -0.0111,
            0.0166,
            0.0879,
            0.0447,
            -0.003,
            -0.1376,
            0.0572
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0025,
            0.0005,
            0.001,
            0.001,
            0.002,
            -0.0022,
            0.0037,
            -0.0004,
            -0.0014,
            -0.0014,
            -0.0006,
            -0.0005,
            0.0038,
            -0.0032,
            -0.0029,
            0.003
          ],
          "after": [
            -0.0733,
            0.0057,
            -0.0984,
            -0.08,
            0.101,
            0.1471,
            0.0805,
            -0.154,
            0.0349,
            -0.0028,
            -0.1082,
            -0.05,
            0.0866,
            -0.0883,
            0.0073,
            0.0827
          ]
        }
      }
    },
    {
      "step": 453,
      "word": "preston",
      "loss": 2.6033,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0035,
            0.0075,
            0.1655,
            -0.0457,
            0.0359,
            -0.0617,
            -0.1253,
            -0.1532,
            -0.0107,
            0.0926,
            0.1578,
            0.015,
            -0.09,
            -0.0194,
            -0.0548,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0786,
            -0.0642,
            0.0713,
            0.0734,
            0.207,
            0.1563,
            -0.1138,
            0.1435,
            0.0218,
            -0.0008,
            0.0984,
            0.2206,
            -0.2871,
            -0.0693,
            0.1019,
            -0.0048
          ],
          "after": [
            0.0379,
            0.1074,
            -0.0603,
            -0.0698,
            0.0408,
            -0.0023,
            0.0945,
            0.1042,
            -0.0788,
            -0.0593,
            0.0247,
            -0.0207,
            0.1063,
            0.0259,
            -0.0649,
            0.2255
          ]
        },
        "position_0": {
          "grad": [
            0.0613,
            0.156,
            -0.0591,
            0.0687,
            0.0336,
            -0.0947,
            0.305,
            0.0001,
            -0.274,
            0.0207,
            -0.0249,
            0.0078,
            0.0623,
            0.0692,
            -0.0508,
            0.2282
          ],
          "after": [
            0.0191,
            -0.1392,
            -0.287,
            0.061,
            0.0359,
            -0.0516,
            -0.1449,
            0.0596,
            0.0916,
            -0.0116,
            0.0168,
            0.088,
            0.0444,
            -0.0029,
            -0.1371,
            0.0567
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0002,
            -0.0022,
            -0.0001,
            -0.0014,
            -0.0016,
            0.0027,
            0.0011,
            -0.0018,
            -0.0009,
            0.0008,
            0.0005,
            -0.0008,
            -0.0016,
            0.0008,
            0.0002
          ],
          "after": [
            -0.0738,
            0.0057,
            -0.0981,
            -0.08,
            0.1006,
            0.1474,
            0.0801,
            -0.154,
            0.035,
            -0.0027,
            -0.1084,
            -0.05,
            0.0864,
            -0.0881,
            0.0081,
            0.0823
          ]
        }
      }
    },
    {
      "step": 454,
      "word": "kaihan",
      "loss": 1.9922,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2043,
            -0.1108,
            0.052,
            0.0153,
            0.087,
            0.0746,
            -0.144,
            0.0286,
            0.0367,
            -0.1931,
            -0.0104,
            0.0492,
            -0.235,
            -0.0502,
            -0.004,
            0.0714
          ],
          "after": [
            0.0039,
            0.0076,
            0.1654,
            -0.0456,
            0.0358,
            -0.0622,
            -0.1254,
            -0.1531,
            -0.0107,
            0.0929,
            0.1577,
            0.0149,
            -0.0897,
            -0.0189,
            -0.0547,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0975,
            0.1083,
            -0.1,
            0.0195,
            0.0698,
            -0.14,
            0.2963,
            0.0188,
            -0.1823,
            0.0213,
            -0.0382,
            -0.0001,
            0.0737,
            -0.1016,
            -0.152,
            0.2305
          ],
          "after": [
            0.0377,
            0.1076,
            -0.0603,
            -0.0695,
            0.0404,
            -0.0029,
            0.0946,
            0.1034,
            -0.0792,
            -0.0595,
            0.0246,
            -0.0208,
            0.1068,
            0.0261,
            -0.065,
            0.2258
          ]
        },
        "position_0": {
          "grad": [
            -0.0909,
            0.0544,
            -0.1133,
            -0.0398,
            0.0565,
            -0.0846,
            0.1339,
            -0.0595,
            -0.0596,
            0.0367,
            -0.0069,
            -0.0028,
            0.0474,
            -0.0535,
            -0.1358,
            -0.0532
          ],
          "after": [
            0.0191,
            -0.1397,
            -0.2866,
            0.0607,
            0.0354,
            -0.0509,
            -0.1456,
            0.0597,
            0.0924,
            -0.0121,
            0.0169,
            0.0881,
            0.0441,
            -0.0028,
            -0.1363,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            -0.0003,
            -0.0004,
            0.0018,
            0.0001,
            0.0047,
            0.0009,
            0.0018,
            0.0001,
            -0.0004,
            -0.0015,
            0.0005,
            -0.0004,
            0.0012,
            0.0001,
            0.0026
          ],
          "after": [
            -0.0742,
            0.0058,
            -0.0978,
            -0.0803,
            0.1002,
            0.1472,
            0.0797,
            -0.1543,
            0.0351,
            -0.0027,
            -0.1083,
            -0.0502,
            0.0862,
            -0.088,
            0.0087,
            0.0818
          ]
        }
      }
    },
    {
      "step": 455,
      "word": "kenya",
      "loss": 2.1281,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.283,
            -0.0672,
            -0.0922,
            -0.0931,
            -0.0316,
            -0.002,
            -0.0659,
            -0.0082,
            -0.0886,
            0.1661,
            0.0945,
            -0.2438,
            0.0928,
            0.1225,
            0.0001,
            -0.2161
          ],
          "after": [
            0.0038,
            0.0079,
            0.1655,
            -0.0453,
            0.0357,
            -0.0625,
            -0.1254,
            -0.1529,
            -0.0105,
            0.0928,
            0.1574,
            0.0151,
            -0.0896,
            -0.0188,
            -0.0546,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2708,
            -0.3662,
            0.1786,
            -0.014,
            0.2786,
            0.198,
            -0.5794,
            -0.1049,
            0.4345,
            0.2588,
            0.2109,
            0.198,
            -0.2466,
            0.0422,
            -0.1974,
            -0.4064
          ],
          "after": [
            0.0372,
            0.1081,
            -0.0605,
            -0.0692,
            0.0397,
            -0.0035,
            0.0951,
            0.1031,
            -0.0799,
            -0.0601,
            0.024,
            -0.0212,
            0.1075,
            0.0261,
            -0.0649,
            0.2265
          ]
        },
        "position_0": {
          "grad": [
            -0.0963,
            0.0121,
            -0.1156,
            -0.0509,
            0.0128,
            -0.0858,
            0.0921,
            -0.0406,
            -0.0051,
            -0.0067,
            -0.0248,
            -0.048,
            0.0166,
            -0.0266,
            -0.0986,
            -0.098
          ],
          "after": [
            0.0194,
            -0.1402,
            -0.2858,
            0.0606,
            0.035,
            -0.05,
            -0.1464,
            0.06,
            0.0931,
            -0.0125,
            0.0171,
            0.0883,
            0.0438,
            -0.0025,
            -0.1355,
            0.0562
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            0.0004,
            -0.0003,
            0.0,
            0.0001,
            0.0012,
            -0.003,
            -0.0005,
            0.0014,
            -0.0017,
            -0.0003,
            0.0013,
            -0.0015,
            0.0019,
            -0.0005,
            -0.0015
          ],
          "after": [
            -0.0747,
            0.0058,
            -0.0975,
            -0.0804,
            0.0999,
            0.1469,
            0.0796,
            -0.1544,
            0.035,
            -0.0025,
            -0.1083,
            -0.0504,
            0.0862,
            -0.0882,
            0.0093,
            0.0814
          ]
        }
      }
    },
    {
      "step": 456,
      "word": "izekiel",
      "loss": 2.7609,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0037,
            0.0081,
            0.1656,
            -0.0451,
            0.0357,
            -0.0628,
            -0.1254,
            -0.1528,
            -0.0104,
            0.0927,
            0.1571,
            0.0153,
            -0.0895,
            -0.0187,
            -0.0546,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1832,
            -0.0575,
            0.1143,
            0.0929,
            -0.5396,
            0.2709,
            -0.3609,
            0.0278,
            0.3215,
            0.0718,
            0.1672,
            0.0346,
            -0.191,
            0.0557,
            0.656,
            -0.2239
          ],
          "after": [
            0.0364,
            0.1086,
            -0.0607,
            -0.0691,
            0.0397,
            -0.0044,
            0.0958,
            0.1028,
            -0.0808,
            -0.0607,
            0.0232,
            -0.0217,
            0.1084,
            0.0261,
            -0.0654,
            0.2273
          ]
        },
        "position_0": {
          "grad": [
            -0.0577,
            -0.1204,
            0.0419,
            -0.0805,
            0.1086,
            0.1057,
            -0.0843,
            0.0006,
            -0.0167,
            0.0352,
            0.0313,
            -0.063,
            -0.0754,
            0.0175,
            -0.0409,
            -0.0386
          ],
          "after": [
            0.0199,
            -0.1403,
            -0.2854,
            0.0606,
            0.0344,
            -0.0497,
            -0.1469,
            0.0602,
            0.0936,
            -0.013,
            0.0171,
            0.0886,
            0.0437,
            -0.0024,
            -0.1347,
            0.0561
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0005,
            0.0002,
            0.0006,
            -0.0006,
            -0.0008,
            0.0003,
            0.0004,
            0.0001,
            0.0008,
            0.0005,
            0.0005,
            -0.0002,
            -0.0005,
            0.0001,
            0.0008
          ],
          "after": [
            -0.0751,
            0.0057,
            -0.0973,
            -0.0807,
            0.0997,
            0.1467,
            0.0795,
            -0.1546,
            0.0349,
            -0.0024,
            -0.1083,
            -0.0506,
            0.0861,
            -0.0883,
            0.0098,
            0.0811
          ]
        }
      }
    },
    {
      "step": 457,
      "word": "adom",
      "loss": 2.559,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1322,
            0.0433,
            0.0219,
            -0.168,
            0.2508,
            -0.2358,
            0.0715,
            -0.2323,
            -0.0594,
            -0.0079,
            0.0249,
            -0.0728,
            0.1909,
            0.1279,
            0.0206,
            0.2648
          ],
          "after": [
            0.0038,
            0.0082,
            0.1656,
            -0.0446,
            0.0353,
            -0.0623,
            -0.1255,
            -0.152,
            -0.0101,
            0.0926,
            0.1568,
            0.0156,
            -0.0898,
            -0.0189,
            -0.0546,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0453,
            0.0881,
            -0.1463,
            0.0506,
            0.1058,
            -0.1845,
            0.2367,
            0.009,
            -0.2213,
            0.0245,
            -0.0537,
            -0.021,
            0.1737,
            -0.0786,
            -0.2401,
            0.1737
          ],
          "after": [
            0.0358,
            0.1089,
            -0.0608,
            -0.0692,
            0.0397,
            -0.0049,
            0.0962,
            0.1025,
            -0.0814,
            -0.0613,
            0.0226,
            -0.022,
            0.1088,
            0.0261,
            -0.0657,
            0.2278
          ]
        },
        "position_0": {
          "grad": [
            0.1226,
            -0.0691,
            0.1027,
            -0.1854,
            0.0036,
            -0.0174,
            -0.3723,
            -0.1631,
            0.1559,
            0.0411,
            0.3336,
            -0.0056,
            -0.2008,
            0.0512,
            -0.0325,
            -0.2706
          ],
          "after": [
            0.0199,
            -0.1402,
            -0.2853,
            0.061,
            0.0339,
            -0.0493,
            -0.1468,
            0.061,
            0.0939,
            -0.0135,
            0.0165,
            0.0889,
            0.0438,
            -0.0024,
            -0.1339,
            0.0564
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            -0.0055,
            0.0051,
            -0.0019,
            -0.004,
            0.0042,
            -0.0063,
            -0.0008,
            0.0031,
            0.0021,
            0.0053,
            0.0009,
            -0.0055,
            0.0024,
            -0.0014,
            -0.0037
          ],
          "after": [
            -0.0757,
            0.0063,
            -0.0976,
            -0.0807,
            0.0999,
            0.1461,
            0.0798,
            -0.1547,
            0.0344,
            -0.0025,
            -0.1089,
            -0.0509,
            0.0866,
            -0.0887,
            0.0103,
            0.0811
          ]
        }
      }
    },
    {
      "step": 458,
      "word": "tywaun",
      "loss": 2.7908,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0369,
            -0.0917,
            0.1034,
            0.0738,
            0.2352,
            -0.0344,
            -0.2853,
            -0.0367,
            0.0694,
            -0.0843,
            0.0523,
            -0.0174,
            -0.0095,
            -0.0377,
            -0.2539,
            0.1457
          ],
          "after": [
            0.0038,
            0.0085,
            0.1654,
            -0.0443,
            0.0346,
            -0.0618,
            -0.1251,
            -0.1513,
            -0.01,
            0.0927,
            0.1565,
            0.0159,
            -0.09,
            -0.019,
            -0.054,
            -0.1878
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0276,
            0.0628,
            -0.0773,
            0.0582,
            0.0739,
            -0.1491,
            0.209,
            -0.0032,
            -0.1849,
            0.0494,
            0.0098,
            -0.0289,
            0.1093,
            -0.0713,
            -0.2085,
            0.1453
          ],
          "after": [
            0.0354,
            0.1091,
            -0.0607,
            -0.0694,
            0.0395,
            -0.0052,
            0.0964,
            0.1022,
            -0.0817,
            -0.0618,
            0.022,
            -0.0222,
            0.1091,
            0.0263,
            -0.0657,
            0.228
          ]
        },
        "position_0": {
          "grad": [
            -0.0185,
            -0.1525,
            0.0894,
            0.0873,
            -0.1029,
            0.0896,
            0.0637,
            0.0245,
            -0.1114,
            -0.1629,
            -0.094,
            -0.2461,
            0.1277,
            0.0567,
            0.1192,
            0.1669
          ],
          "after": [
            0.02,
            -0.1398,
            -0.2855,
            0.0611,
            0.0337,
            -0.0492,
            -0.1469,
            0.0615,
            0.0943,
            -0.0134,
            0.0162,
            0.0897,
            0.0438,
            -0.0026,
            -0.1335,
            0.0564
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0012,
            0.0005,
            -0.0017,
            0.0005,
            -0.0014,
            -0.0002,
            -0.0016,
            0.0001,
            0.0018,
            0.0008,
            -0.0008,
            0.0012,
            -0.0011,
            -0.0001,
            -0.0014
          ],
          "after": [
            -0.076,
            0.0069,
            -0.098,
            -0.0804,
            0.1,
            0.1458,
            0.0801,
            -0.1545,
            0.034,
            -0.0027,
            -0.1095,
            -0.051,
            0.0868,
            -0.0889,
            0.0107,
            0.0813
          ]
        }
      }
    },
    {
      "step": 459,
      "word": "jerin",
      "loss": 1.7458,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0039,
            0.0087,
            0.1653,
            -0.0441,
            0.0339,
            -0.0613,
            -0.1248,
            -0.1507,
            -0.01,
            0.0928,
            0.1561,
            0.0161,
            -0.0901,
            -0.0191,
            -0.0535,
            -0.1883
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2964,
            -0.2581,
            0.2879,
            0.042,
            0.25,
            0.0888,
            -0.4993,
            -0.1416,
            0.4478,
            0.2974,
            0.2577,
            0.1105,
            -0.3119,
            -0.025,
            -0.1936,
            -0.3132
          ],
          "after": [
            0.0346,
            0.1096,
            -0.061,
            -0.0697,
            0.0391,
            -0.0055,
            0.0969,
            0.1026,
            -0.0823,
            -0.0628,
            0.021,
            -0.0226,
            0.1097,
            0.0266,
            -0.0655,
            0.2286
          ]
        },
        "position_0": {
          "grad": [
            -0.0329,
            0.0469,
            0.0193,
            0.16,
            0.0234,
            -0.0921,
            0.1745,
            0.0646,
            -0.2211,
            -0.0834,
            0.0125,
            0.0436,
            0.3291,
            -0.1633,
            -0.1425,
            0.1338
          ],
          "after": [
            0.0201,
            -0.1396,
            -0.2857,
            0.0609,
            0.0334,
            -0.0489,
            -0.1471,
            0.0617,
            0.095,
            -0.0131,
            0.0159,
            0.0903,
            0.0433,
            -0.0023,
            -0.1329,
            0.0562
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0009,
            0.0005,
            -0.0007,
            0.002,
            -0.0003,
            -0.0008,
            0.0001,
            0.0007,
            0.0004,
            0.0007,
            0.0008,
            -0.0011,
            -0.0011,
            -0.0004,
            -0.0006
          ],
          "after": [
            -0.0763,
            0.0074,
            -0.0984,
            -0.0802,
            0.0999,
            0.1455,
            0.0805,
            -0.1543,
            0.0335,
            -0.0029,
            -0.11,
            -0.0512,
            0.0871,
            -0.0889,
            0.0111,
            0.0815
          ]
        }
      }
    },
    {
      "step": 460,
      "word": "kyreece",
      "loss": 2.462,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0039,
            0.0089,
            0.1652,
            -0.0439,
            0.0334,
            -0.0609,
            -0.1246,
            -0.1501,
            -0.0099,
            0.0929,
            0.1559,
            0.0163,
            -0.0903,
            -0.0191,
            -0.053,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1032,
            -0.3613,
            -0.1489,
            -0.1986,
            -0.5555,
            0.4172,
            -0.5816,
            0.0521,
            0.2792,
            -0.2389,
            -0.2241,
            -0.2264,
            -0.1457,
            0.4421,
            1.0158,
            -0.6243
          ],
          "after": [
            0.0337,
            0.1104,
            -0.0611,
            -0.0694,
            0.0393,
            -0.0063,
            0.0978,
            0.1027,
            -0.0831,
            -0.0632,
            0.0206,
            -0.0225,
            0.1104,
            0.026,
            -0.0662,
            0.2296
          ]
        },
        "position_0": {
          "grad": [
            -0.084,
            0.0379,
            -0.0993,
            -0.0487,
            0.0478,
            -0.0543,
            0.1115,
            -0.0532,
            -0.0393,
            0.026,
            0.0089,
            -0.0014,
            0.0307,
            -0.0522,
            -0.1112,
            -0.0489
          ],
          "after": [
            0.0206,
            -0.1395,
            -0.2856,
            0.0609,
            0.0331,
            -0.0484,
            -0.1475,
            0.0621,
            0.0956,
            -0.0129,
            0.0157,
            0.0908,
            0.0429,
            -0.0018,
            -0.1321,
            0.0561
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0016,
            0.0009,
            -0.0007,
            0.0009,
            -0.0003,
            -0.0018,
            0.0009,
            -0.0011,
            -0.003,
            -0.0018,
            -0.0004,
            -0.0015,
            0.0007,
            0.0023,
            -0.0012
          ],
          "after": [
            -0.0764,
            0.0081,
            -0.0989,
            -0.0799,
            0.0997,
            0.1453,
            0.0809,
            -0.1543,
            0.0332,
            -0.0028,
            -0.1103,
            -0.0514,
            0.0875,
            -0.0891,
            0.0113,
            0.0818
          ]
        }
      }
    },
    {
      "step": 461,
      "word": "maely",
      "loss": 1.9651,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0424,
            -0.2435,
            0.0822,
            0.0976,
            -0.1566,
            0.1803,
            -0.281,
            -0.0198,
            0.1524,
            0.1161,
            0.0688,
            -0.007,
            -0.252,
            0.0016,
            0.1272,
            -0.316
          ],
          "after": [
            0.004,
            0.0095,
            0.1649,
            -0.0439,
            0.0332,
            -0.0612,
            -0.1239,
            -0.1496,
            -0.0102,
            0.0927,
            0.1555,
            0.0164,
            -0.09,
            -0.0192,
            -0.0529,
            -0.1886
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3042,
            0.284,
            -0.2813,
            0.2704,
            -0.0953,
            0.3485,
            0.5483,
            0.2856,
            -0.1087,
            -0.2485,
            -0.2258,
            0.1451,
            0.1221,
            -0.1362,
            0.2945,
            0.4706
          ],
          "after": [
            0.0335,
            0.1108,
            -0.0609,
            -0.0699,
            0.0396,
            -0.0074,
            0.0981,
            0.1017,
            -0.0837,
            -0.0632,
            0.0208,
            -0.0227,
            0.1108,
            0.0257,
            -0.067,
            0.2301
          ]
        },
        "position_0": {
          "grad": [
            -0.0463,
            0.0262,
            -0.0491,
            -0.0066,
            0.158,
            0.0105,
            0.1608,
            0.0873,
            0.0871,
            0.0381,
            -0.0202,
            0.1145,
            -0.1437,
            -0.067,
            -0.0137,
            0.0792
          ],
          "after": [
            0.0211,
            -0.1395,
            -0.2854,
            0.0608,
            0.0326,
            -0.0481,
            -0.148,
            0.0622,
            0.096,
            -0.0129,
            0.0155,
            0.0909,
            0.0427,
            -0.0013,
            -0.1314,
            0.056
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0031,
            0.0012,
            -0.0036,
            0.0043,
            -0.0025,
            0.006,
            0.003,
            0.0052,
            0.0004,
            -0.0025,
            -0.0024,
            0.0035,
            -0.002,
            -0.0016,
            0.0042,
            0.0064
          ],
          "after": [
            -0.0761,
            0.0085,
            -0.0988,
            -0.0801,
            0.0998,
            0.1446,
            0.081,
            -0.1551,
            0.033,
            -0.0026,
            -0.1103,
            -0.0518,
            0.088,
            -0.089,
            0.0111,
            0.0815
          ]
        }
      }
    },
    {
      "step": 462,
      "word": "hanzo",
      "loss": 2.6802,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1156,
            0.0621,
            -0.2852,
            -0.0414,
            -0.3448,
            -0.0994,
            -0.1327,
            -0.1044,
            0.0761,
            -0.1494,
            -0.1028,
            -0.1844,
            0.107,
            0.1857,
            0.1422,
            0.0948
          ],
          "after": [
            0.0039,
            0.0099,
            0.1652,
            -0.0438,
            0.0336,
            -0.0611,
            -0.1232,
            -0.1489,
            -0.0106,
            0.0929,
            0.1554,
            0.0169,
            -0.0899,
            -0.0196,
            -0.0532,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1023,
            0.0812,
            -0.1989,
            0.0572,
            0.0329,
            -0.1422,
            0.3319,
            0.0358,
            -0.2378,
            0.024,
            -0.052,
            -0.0039,
            0.16,
            -0.1147,
            -0.2126,
            0.2298
          ],
          "after": [
            0.0334,
            0.111,
            -0.0605,
            -0.0704,
            0.0398,
            -0.0081,
            0.0982,
            0.1008,
            -0.084,
            -0.0632,
            0.0211,
            -0.0229,
            0.111,
            0.0257,
            -0.0675,
            0.2302
          ]
        },
        "position_0": {
          "grad": [
            0.0077,
            -0.0536,
            0.0732,
            0.0497,
            -0.0754,
            0.1077,
            -0.056,
            0.0861,
            0.1383,
            0.0506,
            -0.1088,
            -0.138,
            0.1639,
            0.1271,
            0.0502,
            0.098
          ],
          "after": [
            0.0216,
            -0.1393,
            -0.2854,
            0.0607,
            0.0322,
            -0.0481,
            -0.1484,
            0.0619,
            0.0962,
            -0.013,
            0.0155,
            0.0914,
            0.0423,
            -0.0012,
            -0.1309,
            0.0557
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            0.0004,
            -0.0019,
            0.003,
            0.0013,
            0.001,
            0.0058,
            0.0022,
            0.0009,
            0.0045,
            0.0003,
            0.0039,
            0.0013,
            -0.0021,
            -0.0036,
            0.0035
          ],
          "after": [
            -0.0758,
            0.0089,
            -0.0986,
            -0.0806,
            0.0997,
            0.1438,
            0.0807,
            -0.156,
            0.0326,
            -0.0027,
            -0.1103,
            -0.0525,
            0.0883,
            -0.0886,
            0.0113,
            0.0808
          ]
        }
      }
    },
    {
      "step": 463,
      "word": "amoni",
      "loss": 2.0854,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1439,
            0.0682,
            0.1994,
            0.0144,
            0.1604,
            -0.0181,
            0.2473,
            0.0787,
            0.1365,
            -0.0289,
            0.154,
            0.104,
            -0.2837,
            -0.12,
            -0.04,
            -0.0962
          ],
          "after": [
            0.004,
            0.0102,
            0.1651,
            -0.0438,
            0.0336,
            -0.0609,
            -0.1229,
            -0.1485,
            -0.0112,
            0.0931,
            0.1549,
            0.017,
            -0.0894,
            -0.0197,
            -0.0534,
            -0.1886
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0757,
            0.0116,
            -0.0945,
            -0.0168,
            0.1462,
            -0.1497,
            0.1597,
            -0.0039,
            -0.1614,
            0.039,
            -0.0515,
            0.0043,
            0.1119,
            -0.0541,
            -0.2181,
            0.141
          ],
          "after": [
            0.0335,
            0.1112,
            -0.06,
            -0.0708,
            0.0399,
            -0.0085,
            0.0981,
            0.1001,
            -0.0841,
            -0.0632,
            0.0214,
            -0.023,
            0.1111,
            0.0258,
            -0.0678,
            0.2302
          ]
        },
        "position_0": {
          "grad": [
            0.1109,
            -0.0659,
            0.0825,
            -0.1913,
            -0.0132,
            -0.0181,
            -0.3369,
            -0.1389,
            0.1516,
            0.0458,
            0.2579,
            -0.0009,
            -0.1952,
            0.0317,
            -0.0424,
            -0.2541
          ],
          "after": [
            0.0216,
            -0.139,
            -0.2857,
            0.061,
            0.032,
            -0.0481,
            -0.1482,
            0.0622,
            0.096,
            -0.0133,
            0.0151,
            0.0917,
            0.0422,
            -0.0012,
            -0.1304,
            0.0558
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0006,
            -0.0006,
            0.0002,
            -0.0005,
            -0.0002,
            0.0001,
            -0.0001,
            0.0002,
            -0.0007,
            -0.0004,
            0.0008,
            -0.0016,
            -0.0014,
            0.0035,
            0.0003
          ],
          "after": [
            -0.0753,
            0.0091,
            -0.0983,
            -0.0811,
            0.0997,
            0.1432,
            0.0804,
            -0.1568,
            0.0323,
            -0.0028,
            -0.1103,
            -0.0531,
            0.0887,
            -0.0882,
            0.0111,
            0.0803
          ]
        }
      }
    },
    {
      "step": 464,
      "word": "jerico",
      "loss": 2.2307,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0042,
            0.0104,
            0.165,
            -0.0437,
            0.0337,
            -0.0608,
            -0.1227,
            -0.1481,
            -0.0117,
            0.0933,
            0.1545,
            0.0172,
            -0.089,
            -0.0198,
            -0.0535,
            -0.1885
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2703,
            -0.1528,
            0.2061,
            0.0579,
            0.303,
            0.048,
            -0.3312,
            -0.1005,
            0.2774,
            0.2074,
            0.212,
            0.1093,
            -0.2365,
            -0.0652,
            -0.2578,
            -0.1465
          ],
          "after": [
            0.0331,
            0.1115,
            -0.0599,
            -0.0713,
            0.0396,
            -0.009,
            0.0983,
            0.0998,
            -0.0845,
            -0.0636,
            0.0212,
            -0.0233,
            0.1114,
            0.026,
            -0.0678,
            0.2304
          ]
        },
        "position_0": {
          "grad": [
            -0.0348,
            0.0618,
            0.0077,
            0.1247,
            0.0554,
            -0.0739,
            0.1622,
            0.038,
            -0.2102,
            -0.0545,
            0.0262,
            0.0704,
            0.2934,
            -0.1465,
            -0.1554,
            0.1365
          ],
          "after": [
            0.0217,
            -0.1389,
            -0.2859,
            0.061,
            0.0316,
            -0.0478,
            -0.1483,
            0.0622,
            0.0962,
            -0.0133,
            0.0147,
            0.0919,
            0.0418,
            -0.0008,
            -0.1297,
            0.0557
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0009,
            0.0012,
            -0.0012,
            0.0018,
            0.0001,
            -0.0012,
            -0.0,
            0.0008,
            0.0003,
            0.0014,
            0.0005,
            -0.0008,
            -0.0007,
            -0.0012,
            -0.0009
          ],
          "after": [
            -0.0749,
            0.0094,
            -0.0982,
            -0.0814,
            0.0996,
            0.1426,
            0.0803,
            -0.1575,
            0.0319,
            -0.0029,
            -0.1105,
            -0.0537,
            0.0891,
            -0.0877,
            0.0111,
            0.0799
          ]
        }
      }
    },
    {
      "step": 465,
      "word": "evers",
      "loss": 2.711,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0043,
            0.0105,
            0.165,
            -0.0437,
            0.0337,
            -0.0607,
            -0.1225,
            -0.1478,
            -0.0121,
            0.0934,
            0.1542,
            0.0174,
            -0.0886,
            -0.0199,
            -0.0536,
            -0.1885
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2254,
            0.4063,
            0.8496,
            -0.0241,
            0.2037,
            0.3056,
            -0.0697,
            0.0086,
            0.5094,
            -0.1437,
            0.2868,
            0.1494,
            -0.6117,
            -0.0188,
            0.2061,
            -0.3836
          ],
          "after": [
            0.0331,
            0.1113,
            -0.0606,
            -0.0716,
            0.0392,
            -0.0097,
            0.0985,
            0.0995,
            -0.0852,
            -0.0637,
            0.0205,
            -0.0238,
            0.1123,
            0.0261,
            -0.068,
            0.2308
          ]
        },
        "position_0": {
          "grad": [
            -0.0076,
            -0.2265,
            0.0713,
            -0.0178,
            -0.0598,
            0.0894,
            -0.2822,
            -0.0935,
            0.1576,
            0.1165,
            0.023,
            -0.0381,
            -0.1586,
            0.0296,
            0.0515,
            -0.3115
          ],
          "after": [
            0.0218,
            -0.1383,
            -0.2864,
            0.061,
            0.0315,
            -0.0479,
            -0.1481,
            0.0626,
            0.0962,
            -0.0137,
            0.0143,
            0.0921,
            0.0416,
            -0.0005,
            -0.1291,
            0.056
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            0.0023,
            0.0003,
            0.0006,
            -0.0009,
            0.0008,
            0.0011,
            -0.0001,
            0.0009,
            0.0021,
            0.0028,
            -0.0,
            -0.0006,
            -0.0026,
            -0.001,
            -0.0002
          ],
          "after": [
            -0.0744,
            0.0094,
            -0.0981,
            -0.0817,
            0.0995,
            0.1421,
            0.08,
            -0.158,
            0.0315,
            -0.0032,
            -0.1109,
            -0.0543,
            0.0895,
            -0.087,
            0.0112,
            0.0796
          ]
        }
      }
    },
    {
      "step": 466,
      "word": "kylah",
      "loss": 2.2256,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1623,
            0.0102,
            0.2947,
            0.0933,
            0.1327,
            0.1357,
            0.0783,
            0.0849,
            0.1127,
            0.0746,
            -0.032,
            -0.0293,
            0.2108,
            0.0062,
            -0.1852,
            0.1558
          ],
          "after": [
            0.0046,
            0.0106,
            0.1644,
            -0.0439,
            0.0336,
            -0.061,
            -0.1225,
            -0.1478,
            -0.0128,
            0.0934,
            0.154,
            0.0175,
            -0.0886,
            -0.02,
            -0.0532,
            -0.1886
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1161,
            0.214,
            -0.1561,
            0.0814,
            0.0268,
            -0.2258,
            0.447,
            0.0057,
            -0.3278,
            -0.0322,
            -0.0624,
            -0.0252,
            0.1736,
            -0.1217,
            -0.2259,
            0.3914
          ],
          "after": [
            0.0333,
            0.1109,
            -0.061,
            -0.0721,
            0.0388,
            -0.01,
            0.0983,
            0.0992,
            -0.0855,
            -0.0637,
            0.02,
            -0.0242,
            0.1128,
            0.0265,
            -0.0679,
            0.2309
          ]
        },
        "position_0": {
          "grad": [
            -0.1124,
            0.041,
            -0.1275,
            -0.028,
            0.0425,
            -0.0718,
            0.1533,
            -0.055,
            -0.0467,
            0.002,
            0.0053,
            0.0018,
            0.0366,
            -0.0398,
            -0.1016,
            -0.0539
          ],
          "after": [
            0.0223,
            -0.1379,
            -0.2864,
            0.061,
            0.0312,
            -0.0477,
            -0.1481,
            0.0631,
            0.0962,
            -0.0141,
            0.0139,
            0.0923,
            0.0414,
            -0.0002,
            -0.1285,
            0.0564
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            0.0024,
            0.0011,
            -0.0009,
            0.0008,
            -0.0011,
            0.0003,
            0.0003,
            -0.0006,
            0.0008,
            0.0021,
            0.0014,
            -0.0022,
            -0.002,
            0.0007,
            -0.0032
          ],
          "after": [
            -0.0739,
            0.0091,
            -0.0982,
            -0.0818,
            0.0994,
            0.1417,
            0.0799,
            -0.1585,
            0.0312,
            -0.0034,
            -0.1114,
            -0.0548,
            0.09,
            -0.0862,
            0.0111,
            0.0796
          ]
        }
      }
    },
    {
      "step": 467,
      "word": "sonnie",
      "loss": 2.1884,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0049,
            0.0107,
            0.164,
            -0.044,
            0.0334,
            -0.0613,
            -0.1224,
            -0.1478,
            -0.0133,
            0.0934,
            0.1538,
            0.0177,
            -0.0887,
            -0.0201,
            -0.0529,
            -0.1888
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1329,
            -0.0582,
            -0.1745,
            0.0289,
            -0.6259,
            0.0251,
            -0.1712,
            0.0427,
            0.1486,
            -0.0049,
            0.0596,
            -0.0199,
            0.0766,
            0.03,
            0.4724,
            -0.1167
          ],
          "after": [
            0.0333,
            0.1106,
            -0.0612,
            -0.0726,
            0.0391,
            -0.0104,
            0.0983,
            0.0988,
            -0.0859,
            -0.0637,
            0.0195,
            -0.0244,
            0.1133,
            0.0267,
            -0.0683,
            0.231
          ]
        },
        "position_0": {
          "grad": [
            0.0132,
            0.05,
            0.0001,
            0.0797,
            -0.2102,
            0.0708,
            -0.0928,
            0.0536,
            0.0092,
            -0.0308,
            -0.118,
            -0.0041,
            0.047,
            0.0693,
            0.1667,
            -0.0408
          ],
          "after": [
            0.0226,
            -0.1377,
            -0.2863,
            0.0609,
            0.0315,
            -0.0478,
            -0.1479,
            0.0633,
            0.0962,
            -0.0143,
            0.0138,
            0.0925,
            0.0412,
            -0.0001,
            -0.1282,
            0.0568
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0002,
            -0.0,
            0.0003,
            -0.0003,
            0.0,
            -0.0003,
            0.0001,
            0.0003,
            0.0006,
            -0.0001,
            0.0001,
            0.0001,
            0.0001,
            0.0002,
            -0.0
          ],
          "after": [
            -0.0734,
            0.0089,
            -0.0983,
            -0.082,
            0.0993,
            0.1414,
            0.0797,
            -0.159,
            0.0309,
            -0.0037,
            -0.1119,
            -0.0553,
            0.0904,
            -0.0854,
            0.0111,
            0.0796
          ]
        }
      }
    },
    {
      "step": 468,
      "word": "faolan",
      "loss": 2.2239,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0241,
            -0.0269,
            0.0377,
            0.1132,
            0.0143,
            0.0158,
            -0.1766,
            0.0163,
            -0.0186,
            0.0566,
            0.0278,
            0.2961,
            -0.2413,
            -0.1672,
            -0.0047,
            0.0102
          ],
          "after": [
            0.0052,
            0.0109,
            0.1635,
            -0.0443,
            0.0333,
            -0.0616,
            -0.1221,
            -0.1479,
            -0.0137,
            0.0932,
            0.1536,
            0.0174,
            -0.0883,
            -0.0198,
            -0.0527,
            -0.1889
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0865,
            0.1267,
            -0.1048,
            0.0565,
            -0.0237,
            -0.1572,
            0.2449,
            -0.0192,
            -0.1946,
            0.0076,
            0.0083,
            0.0034,
            0.1,
            -0.1149,
            -0.1252,
            0.1681
          ],
          "after": [
            0.0334,
            0.1103,
            -0.0613,
            -0.0732,
            0.0393,
            -0.0105,
            0.0981,
            0.0986,
            -0.0861,
            -0.0638,
            0.019,
            -0.0247,
            0.1135,
            0.0272,
            -0.0685,
            0.231
          ]
        },
        "position_0": {
          "grad": [
            0.0005,
            0.1832,
            -0.0391,
            0.1189,
            0.1186,
            -0.2088,
            0.1455,
            -0.0448,
            -0.2618,
            -0.0631,
            0.1074,
            0.0232,
            0.225,
            -0.0349,
            -0.0955,
            0.1568
          ],
          "after": [
            0.023,
            -0.1379,
            -0.2862,
            0.0606,
            0.0314,
            -0.0473,
            -0.148,
            0.0637,
            0.0966,
            -0.0142,
            0.0136,
            0.0926,
            0.0407,
            0.0001,
            -0.1279,
            0.0569
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0021,
            -0.001,
            -0.0009,
            0.0006,
            0.0006,
            0.0027,
            -0.0018,
            0.002,
            0.0013,
            -0.0021,
            -0.003,
            0.0019,
            -0.0024,
            0.0001,
            0.0036,
            0.0019
          ],
          "after": [
            -0.0727,
            0.0088,
            -0.0982,
            -0.0822,
            0.0992,
            0.1409,
            0.0797,
            -0.1596,
            0.0305,
            -0.0038,
            -0.112,
            -0.0559,
            0.091,
            -0.0849,
            0.0109,
            0.0795
          ]
        }
      }
    },
    {
      "step": 469,
      "word": "nature",
      "loss": 2.4122,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.065,
            -0.1509,
            0.1913,
            0.059,
            0.0925,
            0.0629,
            0.1099,
            0.0078,
            -0.2136,
            -0.0768,
            -0.0379,
            -0.2498,
            0.1998,
            -0.0184,
            -0.065,
            0.1408
          ],
          "after": [
            0.0055,
            0.0113,
            0.1628,
            -0.0447,
            0.033,
            -0.062,
            -0.122,
            -0.1479,
            -0.0136,
            0.0933,
            0.1535,
            0.0175,
            -0.0883,
            -0.0195,
            -0.0523,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2038,
            -0.2629,
            -0.1374,
            -0.1691,
            -0.3411,
            0.1782,
            -0.4162,
            0.0116,
            0.1565,
            0.1267,
            0.1012,
            -0.2454,
            0.0583,
            0.2477,
            0.3365,
            -0.2234
          ],
          "after": [
            0.0332,
            0.1102,
            -0.0612,
            -0.0732,
            0.0399,
            -0.0107,
            0.0983,
            0.0983,
            -0.0863,
            -0.064,
            0.0184,
            -0.0245,
            0.1136,
            0.0271,
            -0.0689,
            0.2311
          ]
        },
        "position_0": {
          "grad": [
            0.0054,
            0.0791,
            -0.0569,
            0.0055,
            -0.3593,
            0.0029,
            -0.02,
            0.0109,
            0.0857,
            -0.1825,
            -0.2188,
            -0.1355,
            -0.0288,
            0.1392,
            0.2349,
            0.06
          ],
          "after": [
            0.0232,
            -0.1383,
            -0.2859,
            0.0603,
            0.0321,
            -0.0468,
            -0.148,
            0.0639,
            0.0968,
            -0.0136,
            0.0137,
            0.093,
            0.0403,
            -0.0001,
            -0.128,
            0.0569
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0015,
            -0.0013,
            0.0014,
            0.0001,
            0.0004,
            0.0023,
            -0.0003,
            -0.0005,
            -0.0001,
            -0.0009,
            0.0003,
            0.0006,
            -0.0003,
            -0.0003,
            -0.0
          ],
          "after": [
            -0.0723,
            0.0086,
            -0.098,
            -0.0825,
            0.0991,
            0.1404,
            0.0796,
            -0.1602,
            0.0302,
            -0.0038,
            -0.112,
            -0.0564,
            0.0914,
            -0.0843,
            0.0106,
            0.0793
          ]
        }
      }
    },
    {
      "step": 470,
      "word": "ivvy",
      "loss": 3.5831,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0058,
            0.0116,
            0.1622,
            -0.0451,
            0.0328,
            -0.0624,
            -0.122,
            -0.148,
            -0.0135,
            0.0933,
            0.1534,
            0.0175,
            -0.0883,
            -0.0193,
            -0.052,
            -0.1894
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0391,
            0.0215,
            -0.1552,
            0.0391,
            0.0373,
            -0.1951,
            0.3658,
            -0.013,
            -0.3133,
            0.0797,
            -0.094,
            -0.0782,
            0.2655,
            -0.0438,
            -0.2937,
            0.2214
          ],
          "after": [
            0.0331,
            0.1102,
            -0.061,
            -0.0734,
            0.0403,
            -0.0108,
            0.0982,
            0.0982,
            -0.0863,
            -0.0643,
            0.018,
            -0.0242,
            0.1135,
            0.0271,
            -0.069,
            0.2311
          ]
        },
        "position_0": {
          "grad": [
            -0.0741,
            -0.2565,
            0.1055,
            -0.0784,
            0.0634,
            0.2089,
            -0.1736,
            0.0438,
            0.0112,
            -0.0667,
            0.0349,
            -0.1858,
            -0.147,
            0.1282,
            0.0779,
            -0.0545
          ],
          "after": [
            0.0237,
            -0.138,
            -0.286,
            0.0603,
            0.0326,
            -0.047,
            -0.1478,
            0.064,
            0.097,
            -0.0129,
            0.0138,
            0.0937,
            0.0402,
            -0.0007,
            -0.1283,
            0.0569
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0058,
            0.0013,
            -0.0012,
            0.0029,
            -0.0018,
            -0.0059,
            0.0045,
            -0.0002,
            -0.0025,
            -0.0059,
            -0.008,
            -0.0019,
            0.008,
            0.0052,
            -0.0019,
            0.0016
          ],
          "after": [
            -0.0726,
            0.0082,
            -0.0978,
            -0.0831,
            0.0991,
            0.1406,
            0.0791,
            -0.1606,
            0.0303,
            -0.0034,
            -0.1112,
            -0.0567,
            0.0911,
            -0.0845,
            0.0106,
            0.0791
          ]
        }
      }
    },
    {
      "step": 471,
      "word": "myrikal",
      "loss": 2.6414,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2331,
            0.0622,
            -0.0815,
            0.126,
            0.1224,
            0.0533,
            0.0034,
            0.1896,
            0.0194,
            -0.2103,
            -0.243,
            0.3484,
            -0.1431,
            -0.0898,
            0.1296,
            0.0418
          ],
          "after": [
            0.0064,
            0.0118,
            0.1619,
            -0.0456,
            0.0324,
            -0.0629,
            -0.1219,
            -0.1485,
            -0.0135,
            0.0938,
            0.154,
            0.0171,
            -0.0881,
            -0.0189,
            -0.052,
            -0.1897
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1576,
            0.1318,
            -0.0918,
            0.0536,
            0.121,
            -0.1915,
            0.3282,
            -0.0039,
            -0.2473,
            -0.0019,
            -0.0568,
            0.0265,
            0.118,
            -0.1088,
            -0.2309,
            0.2742
          ],
          "after": [
            0.0332,
            0.11,
            -0.0608,
            -0.0736,
            0.0405,
            -0.0106,
            0.0978,
            0.098,
            -0.0861,
            -0.0646,
            0.0179,
            -0.024,
            0.1132,
            0.0273,
            -0.069,
            0.2308
          ]
        },
        "position_0": {
          "grad": [
            -0.0447,
            0.0464,
            -0.0432,
            0.019,
            0.1426,
            0.0257,
            0.1757,
            0.0638,
            0.0333,
            0.0442,
            -0.0049,
            0.1237,
            -0.102,
            -0.0664,
            -0.022,
            0.0972
          ],
          "after": [
            0.0243,
            -0.1379,
            -0.2859,
            0.0601,
            0.0327,
            -0.0473,
            -0.1479,
            0.0639,
            0.0971,
            -0.0125,
            0.0139,
            0.0941,
            0.0402,
            -0.001,
            -0.1286,
            0.0569
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0004,
            -0.0015,
            -0.0014,
            0.0019,
            0.0014,
            0.0013,
            -0.0002,
            -0.0003,
            -0.0006,
            0.0005,
            -0.0009,
            -0.0001,
            -0.0,
            0.0001,
            -0.0001
          ],
          "after": [
            -0.0727,
            0.0079,
            -0.0973,
            -0.0835,
            0.099,
            0.1406,
            0.0786,
            -0.1609,
            0.0304,
            -0.0029,
            -0.1105,
            -0.0568,
            0.0909,
            -0.0846,
            0.0106,
            0.0789
          ]
        }
      }
    },
    {
      "step": 472,
      "word": "ahzab",
      "loss": 2.7894,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2105,
            0.0713,
            0.7697,
            0.227,
            0.1406,
            0.1747,
            0.3098,
            0.0961,
            -0.0598,
            -0.0486,
            -0.0732,
            -0.0764,
            0.5637,
            -0.0487,
            -0.2774,
            0.6548
          ],
          "after": [
            0.0073,
            0.0118,
            0.1605,
            -0.0465,
            0.0318,
            -0.0638,
            -0.1223,
            -0.1493,
            -0.0133,
            0.0943,
            0.1546,
            0.0169,
            -0.0887,
            -0.0184,
            -0.0514,
            -0.1907
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0192,
            0.0801,
            -0.129,
            0.0368,
            0.0733,
            -0.1944,
            0.2018,
            -0.0053,
            -0.2168,
            0.0152,
            -0.0547,
            -0.0161,
            0.1821,
            0.012,
            -0.2975,
            0.2077
          ],
          "after": [
            0.0333,
            0.1098,
            -0.0604,
            -0.0739,
            0.0407,
            -0.0102,
            0.0974,
            0.0979,
            -0.0857,
            -0.0649,
            0.0179,
            -0.0238,
            0.1128,
            0.0274,
            -0.0687,
            0.2303
          ]
        },
        "position_0": {
          "grad": [
            0.1237,
            -0.0385,
            0.088,
            -0.1566,
            0.0013,
            -0.0249,
            -0.3085,
            -0.1297,
            0.134,
            0.0218,
            0.2481,
            0.0141,
            -0.1687,
            0.0644,
            -0.029,
            -0.1954
          ],
          "after": [
            0.0243,
            -0.1377,
            -0.2861,
            0.0604,
            0.0328,
            -0.0474,
            -0.1475,
            0.0642,
            0.0969,
            -0.0121,
            0.0135,
            0.0943,
            0.0405,
            -0.0014,
            -0.1287,
            0.0571
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0015,
            -0.0034,
            -0.0041,
            0.0003,
            -0.0018,
            0.0016,
            -0.002,
            0.0029,
            0.0037,
            0.0032,
            -0.002,
            -0.0028,
            -0.0035,
            0.0005,
            -0.0032
          ],
          "after": [
            -0.0729,
            0.0078,
            -0.0966,
            -0.0834,
            0.0989,
            0.1408,
            0.0781,
            -0.1609,
            0.0301,
            -0.0029,
            -0.1103,
            -0.0568,
            0.0909,
            -0.0844,
            0.0105,
            0.079
          ]
        }
      }
    },
    {
      "step": 473,
      "word": "navaeh",
      "loss": 2.5515,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1383,
            -0.1676,
            0.1567,
            0.1292,
            0.3422,
            0.0681,
            -0.1968,
            -0.0948,
            -0.0322,
            0.1694,
            0.0355,
            0.1069,
            -0.0434,
            -0.1121,
            -0.1746,
            -0.1239
          ],
          "after": [
            0.0082,
            0.0122,
            0.1591,
            -0.0475,
            0.0309,
            -0.0648,
            -0.1224,
            -0.1497,
            -0.0131,
            0.0943,
            0.1551,
            0.0165,
            -0.0892,
            -0.0178,
            -0.0505,
            -0.1915
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0059,
            -0.0088,
            -0.2281,
            0.1093,
            -0.2939,
            0.235,
            0.1465,
            0.1542,
            0.0641,
            -0.3058,
            -0.3091,
            -0.122,
            0.2293,
            0.1578,
            0.2548,
            0.2422
          ],
          "after": [
            0.0334,
            0.1096,
            -0.0599,
            -0.0745,
            0.0411,
            -0.0101,
            0.0969,
            0.0973,
            -0.0854,
            -0.0646,
            0.0185,
            -0.0235,
            0.1122,
            0.0273,
            -0.0687,
            0.2297
          ]
        },
        "position_0": {
          "grad": [
            0.0033,
            0.0945,
            -0.0674,
            0.005,
            -0.3314,
            -0.0036,
            -0.0055,
            0.0014,
            0.07,
            -0.1664,
            -0.2156,
            -0.114,
            -0.0159,
            0.1226,
            0.2097,
            0.0701
          ],
          "after": [
            0.0244,
            -0.1377,
            -0.2861,
            0.0605,
            0.0335,
            -0.0476,
            -0.1472,
            0.0644,
            0.0967,
            -0.0114,
            0.0136,
            0.0948,
            0.0407,
            -0.0022,
            -0.1292,
            0.0572
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0013,
            0.0022,
            0.0011,
            0.002,
            0.0005,
            -0.0025,
            0.0017,
            0.0002,
            -0.0038,
            -0.0008,
            -0.0016,
            0.0006,
            -0.0001,
            -0.0018,
            0.0017
          ],
          "after": [
            -0.073,
            0.0078,
            -0.0962,
            -0.0833,
            0.0986,
            0.1409,
            0.0778,
            -0.1611,
            0.0299,
            -0.0025,
            -0.11,
            -0.0566,
            0.0909,
            -0.0841,
            0.0106,
            0.0789
          ]
        }
      }
    },
    {
      "step": 474,
      "word": "meziah",
      "loss": 2.257,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2892,
            -0.0016,
            0.2458,
            0.1161,
            0.0931,
            0.1257,
            0.1141,
            0.1179,
            0.1327,
            0.0228,
            -0.1023,
            0.0903,
            0.1545,
            -0.0495,
            -0.1116,
            0.2584
          ],
          "after": [
            0.0094,
            0.0125,
            0.1576,
            -0.0486,
            0.0299,
            -0.0661,
            -0.1226,
            -0.1503,
            -0.0132,
            0.0943,
            0.1557,
            0.0161,
            -0.0899,
            -0.0172,
            -0.0495,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.25,
            -0.1813,
            0.2888,
            -0.0818,
            0.2762,
            0.088,
            -0.2872,
            -0.0784,
            0.3737,
            0.1934,
            0.2251,
            0.1556,
            -0.3023,
            -0.0146,
            -0.1775,
            -0.2208
          ],
          "after": [
            0.0332,
            0.1096,
            -0.0597,
            -0.0747,
            0.0411,
            -0.0101,
            0.0967,
            0.0971,
            -0.0855,
            -0.0647,
            0.0185,
            -0.0234,
            0.112,
            0.0272,
            -0.0685,
            0.2294
          ]
        },
        "position_0": {
          "grad": [
            -0.0387,
            0.0464,
            -0.042,
            0.0238,
            0.1477,
            0.0116,
            0.1652,
            0.071,
            0.0625,
            0.0301,
            -0.0212,
            0.1109,
            -0.1135,
            -0.0392,
            -0.0111,
            0.1017
          ],
          "after": [
            0.0245,
            -0.1379,
            -0.286,
            0.0606,
            0.0338,
            -0.0477,
            -0.1472,
            0.0644,
            0.0964,
            -0.0108,
            0.0137,
            0.095,
            0.0411,
            -0.0027,
            -0.1297,
            0.0571
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            -0.0013,
            0.0034,
            -0.0004,
            -0.0006,
            -0.0021,
            -0.0042,
            -0.0003,
            0.0023,
            -0.0008,
            0.0016,
            0.0012,
            -0.0022,
            -0.0004,
            -0.0002,
            -0.0012
          ],
          "after": [
            -0.0732,
            0.008,
            -0.0963,
            -0.0833,
            0.0984,
            0.1412,
            0.0779,
            -0.1613,
            0.0293,
            -0.0021,
            -0.1099,
            -0.0566,
            0.0911,
            -0.0839,
            0.0106,
            0.079
          ]
        }
      }
    },
    {
      "step": 475,
      "word": "keyonni",
      "loss": 2.4234,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0105,
            0.0127,
            0.1563,
            -0.0495,
            0.0291,
            -0.0671,
            -0.1228,
            -0.1509,
            -0.0133,
            0.0943,
            0.1562,
            0.0157,
            -0.0904,
            -0.0166,
            -0.0487,
            -0.1932
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1537,
            -0.2896,
            0.1769,
            -0.0164,
            0.2763,
            0.1586,
            -0.4054,
            -0.0766,
            0.3145,
            0.1819,
            0.1467,
            0.209,
            -0.2094,
            0.0147,
            -0.1709,
            -0.2731
          ],
          "after": [
            0.0327,
            0.1099,
            -0.0597,
            -0.0749,
            0.0409,
            -0.0104,
            0.0968,
            0.0971,
            -0.0859,
            -0.065,
            0.0183,
            -0.0237,
            0.112,
            0.0271,
            -0.0682,
            0.2294
          ]
        },
        "position_0": {
          "grad": [
            -0.084,
            0.0213,
            -0.096,
            -0.0477,
            0.0289,
            -0.0593,
            0.0772,
            -0.0454,
            -0.0137,
            0.009,
            -0.0001,
            -0.0097,
            0.0087,
            -0.0345,
            -0.0854,
            -0.0663
          ],
          "after": [
            0.0249,
            -0.138,
            -0.2855,
            0.0608,
            0.034,
            -0.0476,
            -0.1472,
            0.0646,
            0.0962,
            -0.0103,
            0.0138,
            0.0951,
            0.0414,
            -0.003,
            -0.1298,
            0.0571
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0002,
            -0.0015,
            -0.001,
            0.0016,
            0.0022,
            -0.0023,
            -0.001,
            0.0,
            -0.0026,
            -0.0005,
            0.0009,
            -0.0027,
            0.0014,
            0.0014,
            -0.0017
          ],
          "after": [
            -0.0735,
            0.0081,
            -0.0962,
            -0.0831,
            0.0981,
            0.1412,
            0.0781,
            -0.1613,
            0.0289,
            -0.0016,
            -0.1098,
            -0.0566,
            0.0914,
            -0.0838,
            0.0106,
            0.0792
          ]
        }
      }
    },
    {
      "step": 476,
      "word": "yannai",
      "loss": 2.3363,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1381,
            -0.2393,
            -0.0759,
            -0.0978,
            -0.2357,
            0.0298,
            -0.3064,
            0.0265,
            0.1939,
            -0.2084,
            -0.0835,
            -0.1506,
            -0.1435,
            0.136,
            0.1755,
            0.0449
          ],
          "after": [
            0.0116,
            0.0134,
            0.1553,
            -0.05,
            0.0287,
            -0.0681,
            -0.1225,
            -0.1514,
            -0.0138,
            0.0948,
            0.1569,
            0.0156,
            -0.0906,
            -0.0165,
            -0.0483,
            -0.194
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0522,
            -0.0223,
            -0.0247,
            0.0076,
            0.0961,
            -0.0761,
            0.1238,
            -0.009,
            -0.0725,
            0.087,
            0.0051,
            0.047,
            0.0251,
            -0.0815,
            -0.1286,
            0.0772
          ],
          "after": [
            0.0324,
            0.1103,
            -0.0597,
            -0.0751,
            0.0407,
            -0.0105,
            0.0969,
            0.0972,
            -0.0861,
            -0.0655,
            0.0181,
            -0.024,
            0.1121,
            0.0271,
            -0.0679,
            0.2293
          ]
        },
        "position_0": {
          "grad": [
            0.0546,
            -0.1884,
            0.0965,
            -0.1849,
            -0.0538,
            0.1264,
            -0.2057,
            0.0416,
            0.0978,
            0.0557,
            -0.0734,
            -0.0082,
            -0.1385,
            0.0954,
            0.0924,
            -0.1179
          ],
          "after": [
            0.0251,
            -0.1378,
            -0.2855,
            0.0613,
            0.0343,
            -0.0479,
            -0.147,
            0.0645,
            0.0958,
            -0.0101,
            0.014,
            0.0953,
            0.0418,
            -0.0035,
            -0.1302,
            0.0573
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0007,
            0.0005,
            -0.0008,
            0.0017,
            0.0001,
            0.0004,
            -0.0004,
            0.0006,
            -0.0,
            0.0,
            0.0006,
            -0.0001,
            0.0002,
            -0.0015,
            -0.0
          ],
          "after": [
            -0.0737,
            0.0082,
            -0.0961,
            -0.0829,
            0.0977,
            0.1412,
            0.0783,
            -0.1612,
            0.0284,
            -0.0011,
            -0.1097,
            -0.0567,
            0.0917,
            -0.0838,
            0.0107,
            0.0794
          ]
        }
      }
    },
    {
      "step": 477,
      "word": "izzabelle",
      "loss": 3.0008,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1426,
            0.0384,
            0.3204,
            -0.0066,
            0.2063,
            0.0801,
            0.1264,
            -0.01,
            -0.0995,
            -0.107,
            -0.0312,
            0.1064,
            0.1373,
            -0.1545,
            -0.2265,
            0.3511
          ],
          "after": [
            0.0127,
            0.0139,
            0.154,
            -0.0505,
            0.0281,
            -0.0692,
            -0.1224,
            -0.1518,
            -0.014,
            0.0953,
            0.1575,
            0.0153,
            -0.0911,
            -0.016,
            -0.0476,
            -0.195
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.4231,
            -0.3273,
            -0.3249,
            -0.2737,
            -0.3572,
            0.3389,
            -0.3701,
            0.0128,
            0.193,
            0.1331,
            0.0046,
            -0.41,
            0.082,
            0.2294,
            0.3123,
            -0.497
          ],
          "after": [
            0.0316,
            0.1109,
            -0.0594,
            -0.0745,
            0.0408,
            -0.0109,
            0.0971,
            0.0972,
            -0.0865,
            -0.0661,
            0.0179,
            -0.0237,
            0.112,
            0.0268,
            -0.0678,
            0.2297
          ]
        },
        "position_0": {
          "grad": [
            -0.0465,
            -0.0842,
            0.0221,
            -0.0434,
            0.0691,
            0.0831,
            -0.0338,
            0.0119,
            -0.0408,
            0.0171,
            0.0103,
            -0.0352,
            -0.0631,
            0.0178,
            -0.0078,
            -0.0138
          ],
          "after": [
            0.0254,
            -0.1373,
            -0.2855,
            0.0619,
            0.0344,
            -0.0485,
            -0.1468,
            0.0645,
            0.0956,
            -0.01,
            0.0141,
            0.0955,
            0.0422,
            -0.004,
            -0.1305,
            0.0575
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0024,
            0.0033,
            0.0031,
            -0.0004,
            0.0017,
            -0.0041,
            0.0028,
            -0.0021,
            -0.0012,
            0.0002,
            0.003,
            0.0021,
            0.0016,
            -0.0032,
            -0.0029,
            0.0009
          ],
          "after": [
            -0.0736,
            0.0079,
            -0.0965,
            -0.0827,
            0.0972,
            0.1417,
            0.0782,
            -0.1609,
            0.0282,
            -0.0007,
            -0.1099,
            -0.0569,
            0.0919,
            -0.0834,
            0.0109,
            0.0795
          ]
        }
      }
    },
    {
      "step": 478,
      "word": "jazlene",
      "loss": 2.0776,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0166,
            0.061,
            0.0205,
            0.1902,
            0.1405,
            -0.1096,
            0.2445,
            -0.0246,
            -0.0074,
            0.1007,
            0.0711,
            0.1196,
            0.0439,
            -0.0927,
            -0.0815,
            0.0143
          ],
          "after": [
            0.0137,
            0.0142,
            0.1529,
            -0.0513,
            0.0274,
            -0.0697,
            -0.1227,
            -0.1521,
            -0.0141,
            0.0956,
            0.1579,
            0.015,
            -0.0915,
            -0.0154,
            -0.0468,
            -0.1959
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.511,
            -0.3204,
            -0.1261,
            -0.1153,
            -0.4657,
            0.3737,
            -0.682,
            0.0014,
            0.0517,
            -0.0301,
            -0.06,
            -0.4897,
            0.1388,
            0.5278,
            0.2763,
            -0.457
          ],
          "after": [
            0.0302,
            0.1118,
            -0.059,
            -0.0738,
            0.0413,
            -0.0117,
            0.0978,
            0.0972,
            -0.0868,
            -0.0666,
            0.0178,
            -0.0226,
            0.1118,
            0.0257,
            -0.068,
            0.2304
          ]
        },
        "position_0": {
          "grad": [
            -0.0367,
            0.0786,
            0.0018,
            0.1313,
            0.0732,
            -0.0755,
            0.1849,
            0.0448,
            -0.2165,
            -0.0218,
            0.0214,
            0.1115,
            0.2653,
            -0.1604,
            -0.1436,
            0.1325
          ],
          "after": [
            0.0258,
            -0.1372,
            -0.2855,
            0.0621,
            0.0344,
            -0.0487,
            -0.1469,
            0.0643,
            0.0957,
            -0.0098,
            0.0142,
            0.0954,
            0.0423,
            -0.004,
            -0.1304,
            0.0575
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.0005,
            0.0002,
            0.0004,
            0.0012,
            -0.0007,
            0.0004,
            0.0,
            -0.0008,
            0.0004,
            -0.0002,
            -0.0007,
            0.0017,
            0.0005,
            -0.0032,
            -0.0005
          ],
          "after": [
            -0.0737,
            0.0075,
            -0.0968,
            -0.0825,
            0.0966,
            0.1421,
            0.0781,
            -0.1606,
            0.0281,
            -0.0004,
            -0.1101,
            -0.0571,
            0.0918,
            -0.0831,
            0.0114,
            0.0796
          ]
        }
      }
    },
    {
      "step": 479,
      "word": "ashari",
      "loss": 2.2299,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0215,
            0.1426,
            0.062,
            0.1092,
            -0.042,
            0.1844,
            0.0778,
            -0.0111,
            0.1008,
            -0.0103,
            -0.2292,
            0.2714,
            -0.0349,
            -0.1121,
            0.1485,
            0.0959
          ],
          "after": [
            0.0146,
            0.0142,
            0.1518,
            -0.0522,
            0.0269,
            -0.0708,
            -0.1231,
            -0.1524,
            -0.0145,
            0.0959,
            0.1588,
            0.0143,
            -0.0918,
            -0.0147,
            -0.0464,
            -0.1968
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0188,
            0.0383,
            -0.0519,
            0.0147,
            0.1121,
            -0.1107,
            0.1636,
            -0.022,
            -0.1026,
            0.0261,
            -0.0153,
            0.0466,
            0.0271,
            -0.0647,
            -0.2005,
            0.1236
          ],
          "after": [
            0.0291,
            0.1125,
            -0.0586,
            -0.0733,
            0.0417,
            -0.0123,
            0.0983,
            0.0973,
            -0.087,
            -0.067,
            0.0178,
            -0.0218,
            0.1116,
            0.0249,
            -0.068,
            0.231
          ]
        },
        "position_0": {
          "grad": [
            0.1097,
            -0.0688,
            0.065,
            -0.177,
            -0.0212,
            -0.0298,
            -0.3055,
            -0.1096,
            0.1465,
            0.029,
            0.2003,
            -0.0201,
            -0.1802,
            0.0392,
            -0.0278,
            -0.2328
          ],
          "after": [
            0.0258,
            -0.1369,
            -0.2857,
            0.0626,
            0.0344,
            -0.0488,
            -0.1465,
            0.0645,
            0.0956,
            -0.0097,
            0.0139,
            0.0954,
            0.0425,
            -0.0041,
            -0.1303,
            0.0577
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0006,
            -0.0001,
            0.0015,
            -0.0012,
            -0.001,
            0.0004,
            0.0005,
            -0.0007,
            -0.0009,
            -0.0004,
            0.0004,
            0.0004,
            -0.0013,
            0.0008,
            -0.0002
          ],
          "after": [
            -0.0738,
            0.0072,
            -0.097,
            -0.0826,
            0.0962,
            0.1426,
            0.078,
            -0.1604,
            0.0281,
            -0.0001,
            -0.1102,
            -0.0573,
            0.0918,
            -0.0827,
            0.0117,
            0.0797
          ]
        }
      }
    },
    {
      "step": 480,
      "word": "emilene",
      "loss": 2.0483,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0154,
            0.0142,
            0.1509,
            -0.0529,
            0.0264,
            -0.0716,
            -0.1234,
            -0.1526,
            -0.0148,
            0.0961,
            0.1595,
            0.0137,
            -0.092,
            -0.014,
            -0.0461,
            -0.1976
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3646,
            -0.1031,
            0.4757,
            -0.145,
            -0.3335,
            0.4285,
            -0.6413,
            -0.0366,
            0.3208,
            -0.147,
            0.0042,
            -0.5041,
            -0.079,
            0.6231,
            0.3433,
            -0.714
          ],
          "after": [
            0.0277,
            0.1133,
            -0.0588,
            -0.0725,
            0.0423,
            -0.0133,
            0.0991,
            0.0975,
            -0.0875,
            -0.0671,
            0.0178,
            -0.0205,
            0.1115,
            0.0233,
            -0.0683,
            0.232
          ]
        },
        "position_0": {
          "grad": [
            -0.0086,
            -0.1532,
            0.0338,
            -0.0408,
            0.0021,
            0.0634,
            -0.2049,
            -0.0861,
            0.105,
            0.1414,
            0.0129,
            0.0052,
            -0.1211,
            -0.0265,
            -0.0143,
            -0.2494
          ],
          "after": [
            0.0258,
            -0.1363,
            -0.286,
            0.0631,
            0.0343,
            -0.049,
            -0.146,
            0.0649,
            0.0954,
            -0.0101,
            0.0137,
            0.0954,
            0.0429,
            -0.0041,
            -0.1302,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            -0.0011,
            0.0009,
            0.0004,
            -0.0009,
            0.0004,
            -0.0026,
            -0.0,
            -0.0005,
            -0.0004,
            0.0008,
            -0.0016,
            -0.0007,
            0.0007,
            0.0003,
            -0.0013
          ],
          "after": [
            -0.074,
            0.007,
            -0.0973,
            -0.0827,
            0.096,
            0.1429,
            0.0781,
            -0.1602,
            0.0282,
            0.0002,
            -0.1104,
            -0.0573,
            0.0918,
            -0.0824,
            0.012,
            0.0799
          ]
        }
      }
    },
    {
      "step": 481,
      "word": "oluwasemilore",
      "loss": 3.0986,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1349,
            0.1011,
            0.0644,
            0.0509,
            -0.0114,
            0.048,
            0.0737,
            0.0402,
            -0.0262,
            0.027,
            -0.0712,
            0.1153,
            0.0747,
            -0.0551,
            -0.018,
            0.0795
          ],
          "after": [
            0.0162,
            0.014,
            0.1501,
            -0.0536,
            0.026,
            -0.0725,
            -0.1238,
            -0.1528,
            -0.015,
            0.0963,
            0.1603,
            0.013,
            -0.0924,
            -0.0134,
            -0.0457,
            -0.1983
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.189,
            -0.0538,
            -0.058,
            -0.0531,
            -0.086,
            0.1085,
            -0.0821,
            0.0831,
            -0.0405,
            0.0497,
            0.0214,
            -0.24,
            0.1261,
            0.196,
            0.1078,
            -0.0879
          ],
          "after": [
            0.0262,
            0.1139,
            -0.0589,
            -0.0717,
            0.0429,
            -0.0142,
            0.0998,
            0.0974,
            -0.0878,
            -0.0673,
            0.0177,
            -0.019,
            0.1113,
            0.0217,
            -0.0686,
            0.233
          ]
        },
        "position_0": {
          "grad": [
            0.0437,
            -0.0436,
            0.0399,
            -0.0542,
            0.0624,
            -0.0027,
            -0.1042,
            -0.0461,
            -0.0035,
            0.1151,
            0.0562,
            0.0424,
            -0.0429,
            -0.023,
            -0.0601,
            -0.091
          ],
          "after": [
            0.0256,
            -0.1357,
            -0.2864,
            0.0636,
            0.0342,
            -0.0493,
            -0.1454,
            0.0654,
            0.0951,
            -0.0108,
            0.0133,
            0.0953,
            0.0433,
            -0.0041,
            -0.13,
            0.0589
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.003,
            0.0014,
            -0.0003,
            -0.0006,
            0.0017,
            -0.003,
            0.0004,
            -0.0006,
            -0.0014,
            0.0003,
            0.001,
            -0.002,
            0.0018,
            0.0013,
            -0.0009,
            -0.0002
          ],
          "after": [
            -0.0745,
            0.0067,
            -0.0975,
            -0.0827,
            0.0957,
            0.1435,
            0.0782,
            -0.16,
            0.0284,
            0.0004,
            -0.1106,
            -0.0571,
            0.0917,
            -0.0824,
            0.0123,
            0.0801
          ]
        }
      }
    },
    {
      "step": 482,
      "word": "estephany",
      "loss": 2.8921,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1148,
            0.0765,
            -0.0286,
            0.0565,
            -0.0398,
            -0.0248,
            0.0165,
            -0.03,
            0.0481,
            -0.1542,
            -0.092,
            0.1175,
            -0.0526,
            -0.0312,
            0.0345,
            0.1351
          ],
          "after": [
            0.0171,
            0.0136,
            0.1494,
            -0.0543,
            0.0258,
            -0.0732,
            -0.1242,
            -0.153,
            -0.0152,
            0.0967,
            0.1612,
            0.0123,
            -0.0926,
            -0.0128,
            -0.0455,
            -0.1992
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1744,
            0.0537,
            0.479,
            -0.1434,
            0.1373,
            -0.049,
            0.0129,
            0.0476,
            0.1445,
            -0.1956,
            0.0228,
            0.1572,
            -0.3163,
            -0.0497,
            0.1612,
            -0.1255
          ],
          "after": [
            0.0252,
            0.1145,
            -0.0594,
            -0.0706,
            0.0433,
            -0.0149,
            0.1005,
            0.0972,
            -0.0882,
            -0.0672,
            0.0177,
            -0.0179,
            0.1115,
            0.0204,
            -0.069,
            0.2339
          ]
        },
        "position_0": {
          "grad": [
            -0.0008,
            -0.126,
            0.0288,
            -0.0129,
            -0.0218,
            0.0436,
            -0.159,
            -0.0564,
            0.0863,
            0.094,
            0.0062,
            -0.0001,
            -0.1023,
            0.0005,
            0.019,
            -0.1885
          ],
          "after": [
            0.0255,
            -0.1349,
            -0.2868,
            0.0641,
            0.0342,
            -0.0496,
            -0.1447,
            0.066,
            0.0948,
            -0.0116,
            0.013,
            0.0952,
            0.0438,
            -0.004,
            -0.1298,
            0.0597
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0005,
            0.0004,
            -0.001,
            0.0007,
            -0.0021,
            0.0005,
            0.0004,
            -0.0007,
            -0.0001,
            0.0011,
            0.0001,
            -0.0001,
            -0.0014,
            -0.0011,
            -0.0001
          ],
          "after": [
            -0.075,
            0.0065,
            -0.0978,
            -0.0825,
            0.0953,
            0.1443,
            0.0782,
            -0.1598,
            0.0287,
            0.0007,
            -0.111,
            -0.0569,
            0.0916,
            -0.0822,
            0.0126,
            0.0803
          ]
        }
      }
    },
    {
      "step": 483,
      "word": "leidy",
      "loss": 2.4998,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0179,
            0.0133,
            0.1488,
            -0.055,
            0.0255,
            -0.0738,
            -0.1245,
            -0.1531,
            -0.0155,
            0.0971,
            0.1619,
            0.0117,
            -0.0928,
            -0.0122,
            -0.0454,
            -0.1998
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1122,
            -0.2661,
            0.261,
            -0.0252,
            0.4478,
            0.113,
            -0.3219,
            0.0583,
            0.3289,
            0.2285,
            0.0431,
            0.2131,
            -0.2601,
            0.0187,
            -0.1651,
            -0.2841
          ],
          "after": [
            0.0242,
            0.1152,
            -0.06,
            -0.0697,
            0.0432,
            -0.0157,
            0.1012,
            0.0967,
            -0.0889,
            -0.0674,
            0.0175,
            -0.0173,
            0.1119,
            0.0193,
            -0.0691,
            0.235
          ]
        },
        "position_0": {
          "grad": [
            -0.066,
            0.1025,
            -0.1195,
            -0.1056,
            -0.0283,
            0.0611,
            -0.1188,
            0.0979,
            -0.0151,
            -0.0543,
            -0.182,
            0.1187,
            -0.1151,
            0.0674,
            0.1543,
            -0.1686
          ],
          "after": [
            0.0256,
            -0.1344,
            -0.2867,
            0.0647,
            0.0342,
            -0.05,
            -0.144,
            0.0663,
            0.0946,
            -0.0122,
            0.0131,
            0.0948,
            0.0443,
            -0.0042,
            -0.13,
            0.0606
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0007,
            0.0001,
            -0.0002,
            0.0041,
            0.0024,
            0.001,
            0.0003,
            0.0,
            0.0026,
            -0.0006,
            0.002,
            -0.0012,
            -0.0003,
            -0.0022,
            -0.0006
          ],
          "after": [
            -0.0754,
            0.0064,
            -0.098,
            -0.0824,
            0.0946,
            0.1446,
            0.0781,
            -0.1598,
            0.029,
            0.0006,
            -0.1112,
            -0.057,
            0.0916,
            -0.0819,
            0.013,
            0.0806
          ]
        }
      }
    },
    {
      "step": 484,
      "word": "kendrix",
      "loss": 2.7755,
      "learning_rate": 0.0016,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0186,
            0.0131,
            0.1483,
            -0.0555,
            0.0253,
            -0.0743,
            -0.1247,
            -0.1532,
            -0.0157,
            0.0974,
            0.1626,
            0.0111,
            -0.0929,
            -0.0118,
            -0.0452,
            -0.2004
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0644,
            -0.0804,
            0.1303,
            0.0672,
            0.3721,
            0.0388,
            -0.0994,
            -0.0621,
            0.184,
            0.1461,
            0.1052,
            0.246,
            -0.141,
            -0.093,
            -0.3226,
            -0.1204
          ],
          "after": [
            0.0233,
            0.1159,
            -0.0607,
            -0.0691,
            0.0427,
            -0.0163,
            0.1019,
            0.0966,
            -0.0896,
            -0.0679,
            0.0171,
            -0.0172,
            0.1124,
            0.0184,
            -0.0691,
            0.236
          ]
        },
        "position_0": {
          "grad": [
            -0.1004,
            0.0609,
            -0.0998,
            0.019,
            0.0411,
            -0.0513,
            0.1602,
            -0.023,
            -0.0686,
            -0.0088,
            -0.0114,
            0.0367,
            0.0533,
            -0.0347,
            -0.0771,
            0.0037
          ],
          "after": [
            0.026,
            -0.1342,
            -0.2864,
            0.0652,
            0.0341,
            -0.0502,
            -0.1436,
            0.0665,
            0.0945,
            -0.0126,
            0.0132,
            0.0944,
            0.0447,
            -0.0042,
            -0.13,
            0.0614
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            0.0008,
            0.0014,
            -0.0011,
            0.0069,
            0.0012,
            0.0046,
            -0.0011,
            0.0012,
            0.0024,
            0.0008,
            0.0023,
            0.0016,
            -0.0018,
            -0.0063,
            0.0009
          ],
          "after": [
            -0.0756,
            0.0063,
            -0.0984,
            -0.0822,
            0.0934,
            0.1448,
            0.0778,
            -0.1595,
            0.029,
            0.0004,
            -0.1114,
            -0.0572,
            0.0915,
            -0.0815,
            0.0139,
            0.0807
          ]
        }
      }
    },
    {
      "step": 485,
      "word": "deziray",
      "loss": 2.4346,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0322,
            -0.2235,
            0.0963,
            -0.0053,
            0.1284,
            0.1549,
            -0.0383,
            0.0989,
            0.0329,
            -0.0039,
            -0.0989,
            0.2211,
            -0.0758,
            0.0205,
            0.0118,
            0.1356
          ],
          "after": [
            0.0192,
            0.0133,
            0.1477,
            -0.0559,
            0.025,
            -0.0751,
            -0.1249,
            -0.1536,
            -0.0159,
            0.0977,
            0.1634,
            0.0104,
            -0.0929,
            -0.0114,
            -0.0451,
            -0.2011
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0164,
            -0.1309,
            0.1851,
            -0.094,
            0.2597,
            -0.0031,
            -0.1612,
            -0.0581,
            0.2285,
            0.1862,
            0.1094,
            0.2246,
            -0.1998,
            -0.0314,
            -0.1847,
            -0.0422
          ],
          "after": [
            0.0225,
            0.1167,
            -0.0615,
            -0.0683,
            0.0422,
            -0.0169,
            0.1027,
            0.0967,
            -0.0903,
            -0.0685,
            0.0166,
            -0.0174,
            0.113,
            0.0178,
            -0.0689,
            0.2368
          ]
        },
        "position_0": {
          "grad": [
            -0.0383,
            0.0389,
            -0.0137,
            -0.0625,
            0.0798,
            -0.0707,
            0.0588,
            -0.0423,
            -0.0016,
            -0.0199,
            -0.0724,
            -0.0243,
            0.0508,
            0.0603,
            0.0428,
            0.1363
          ],
          "after": [
            0.0265,
            -0.1341,
            -0.2861,
            0.0658,
            0.0339,
            -0.0502,
            -0.1433,
            0.0669,
            0.0944,
            -0.0129,
            0.0134,
            0.0942,
            0.045,
            -0.0044,
            -0.1301,
            0.0618
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            -0.0029,
            0.0005,
            -0.0025,
            0.0004,
            -0.0003,
            -0.0027,
            -0.0002,
            0.0014,
            0.0012,
            0.0008,
            -0.0001,
            -0.0014,
            0.0007,
            0.0009,
            0.0
          ],
          "after": [
            -0.0757,
            0.0064,
            -0.0987,
            -0.0817,
            0.0924,
            0.145,
            0.0776,
            -0.1593,
            0.0289,
            0.0001,
            -0.1117,
            -0.0575,
            0.0915,
            -0.0812,
            0.0145,
            0.0807
          ]
        }
      }
    },
    {
      "step": 486,
      "word": "leeana",
      "loss": 2.1442,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2742,
            0.1212,
            -0.2899,
            -0.172,
            -0.2887,
            -0.2039,
            -0.0454,
            -0.1067,
            -0.0304,
            0.1761,
            0.1068,
            -0.2427,
            0.0394,
            0.0958,
            0.1248,
            -0.2563
          ],
          "after": [
            0.0193,
            0.0133,
            0.1477,
            -0.056,
            0.0251,
            -0.0753,
            -0.125,
            -0.1536,
            -0.016,
            0.0975,
            0.1638,
            0.0101,
            -0.093,
            -0.0113,
            -0.0453,
            -0.2014
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0688,
            -0.1187,
            0.1449,
            0.0868,
            0.3143,
            0.4941,
            -0.2211,
            0.1251,
            0.4648,
            0.096,
            -0.065,
            0.5399,
            -0.5315,
            -0.0111,
            0.3787,
            -0.3156
          ],
          "after": [
            0.0219,
            0.1175,
            -0.0623,
            -0.0679,
            0.0414,
            -0.0179,
            0.1034,
            0.0963,
            -0.0914,
            -0.0693,
            0.0163,
            -0.0182,
            0.1141,
            0.0172,
            -0.069,
            0.2379
          ]
        },
        "position_0": {
          "grad": [
            -0.0412,
            0.0637,
            -0.1144,
            -0.1368,
            -0.0457,
            0.0259,
            -0.1551,
            0.0907,
            0.0267,
            -0.0382,
            -0.1871,
            0.0689,
            -0.126,
            0.0493,
            0.1273,
            -0.2007
          ],
          "after": [
            0.0271,
            -0.1341,
            -0.2855,
            0.0665,
            0.0338,
            -0.0503,
            -0.1428,
            0.0669,
            0.0943,
            -0.0131,
            0.014,
            0.0938,
            0.0454,
            -0.0047,
            -0.1305,
            0.0625
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0006,
            -0.0007,
            -0.0008,
            -0.0001,
            0.0022,
            -0.0022,
            0.0004,
            0.0008,
            -0.0002,
            -0.0015,
            0.0021,
            -0.0027,
            0.001,
            0.0023,
            -0.0009
          ],
          "after": [
            -0.0756,
            0.0067,
            -0.0989,
            -0.0812,
            0.0915,
            0.145,
            0.0777,
            -0.1592,
            0.0287,
            -0.0002,
            -0.1119,
            -0.0578,
            0.0917,
            -0.0811,
            0.0149,
            0.0809
          ]
        }
      }
    },
    {
      "step": 487,
      "word": "akshitha",
      "loss": 2.8939,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1196,
            0.1504,
            0.0628,
            -0.1359,
            0.041,
            -0.0971,
            0.1441,
            -0.0813,
            0.0022,
            0.0845,
            0.009,
            -0.0405,
            0.0302,
            0.0421,
            -0.175,
            -0.1567
          ],
          "after": [
            0.0195,
            0.013,
            0.1475,
            -0.0557,
            0.0252,
            -0.0751,
            -0.1252,
            -0.1534,
            -0.0161,
            0.0973,
            0.1641,
            0.0099,
            -0.0931,
            -0.0114,
            -0.0451,
            -0.2014
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1135,
            0.1364,
            -0.0955,
            -0.0031,
            0.1822,
            -0.2019,
            0.3307,
            0.0124,
            -0.1578,
            -0.0165,
            -0.0438,
            0.0539,
            0.0581,
            -0.1088,
            -0.2306,
            0.2623
          ],
          "after": [
            0.0216,
            0.118,
            -0.0629,
            -0.0675,
            0.0405,
            -0.0186,
            0.1038,
            0.0959,
            -0.0921,
            -0.0699,
            0.0161,
            -0.019,
            0.115,
            0.0169,
            -0.0689,
            0.2385
          ]
        },
        "position_0": {
          "grad": [
            0.0768,
            -0.0212,
            0.0434,
            -0.101,
            0.0468,
            -0.013,
            -0.1655,
            -0.1,
            0.0558,
            0.0325,
            0.1791,
            0.0337,
            -0.0926,
            0.0136,
            -0.0422,
            -0.1203
          ],
          "after": [
            0.0274,
            -0.1341,
            -0.2851,
            0.0673,
            0.0336,
            -0.0503,
            -0.1422,
            0.0672,
            0.0941,
            -0.0133,
            0.0141,
            0.0934,
            0.0459,
            -0.005,
            -0.1307,
            0.0632
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0018,
            0.0005,
            -0.0017,
            0.0018,
            -0.0024,
            0.0025,
            0.0,
            0.0,
            -0.0005,
            -0.0013,
            -0.002,
            0.0018,
            0.0005,
            -0.0008,
            0.0004
          ],
          "after": [
            -0.0756,
            0.0066,
            -0.0992,
            -0.0806,
            0.0905,
            0.1452,
            0.0775,
            -0.1591,
            0.0285,
            -0.0004,
            -0.1118,
            -0.058,
            0.0917,
            -0.0811,
            0.0152,
            0.081
          ]
        }
      }
    },
    {
      "step": 488,
      "word": "noemy",
      "loss": 2.6853,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0197,
            0.0127,
            0.1474,
            -0.0555,
            0.0252,
            -0.075,
            -0.1255,
            -0.1532,
            -0.0162,
            0.097,
            0.1644,
            0.0097,
            -0.0932,
            -0.0114,
            -0.0449,
            -0.2014
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3133,
            0.4169,
            -0.3448,
            0.3042,
            0.0301,
            0.1504,
            0.4396,
            0.0798,
            -0.1065,
            -0.0711,
            -0.2383,
            0.188,
            -0.0079,
            -0.2162,
            0.3835,
            0.4202
          ],
          "after": [
            0.0217,
            0.1179,
            -0.0631,
            -0.0679,
            0.0398,
            -0.0193,
            0.1039,
            0.0954,
            -0.0927,
            -0.0702,
            0.0164,
            -0.0199,
            0.1157,
            0.017,
            -0.0691,
            0.2387
          ]
        },
        "position_0": {
          "grad": [
            -0.0217,
            0.1191,
            -0.0769,
            0.0301,
            -0.3917,
            -0.0004,
            0.0102,
            0.0025,
            0.0682,
            -0.2345,
            -0.2524,
            -0.1462,
            0.009,
            0.1498,
            0.2469,
            0.1283
          ],
          "after": [
            0.0276,
            -0.1343,
            -0.2845,
            0.068,
            0.0342,
            -0.0503,
            -0.1418,
            0.0674,
            0.0938,
            -0.0128,
            0.0146,
            0.0933,
            0.0462,
            -0.0057,
            -0.1313,
            0.0636
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0029,
            -0.0017,
            0.0014,
            -0.0006,
            0.0007,
            -0.0015,
            -0.0004,
            0.0005,
            0.0001,
            -0.0008,
            0.0003,
            -0.0012,
            0.0013,
            0.0012,
            0.0015
          ],
          "after": [
            -0.0757,
            0.0063,
            -0.0992,
            -0.0802,
            0.0898,
            0.1453,
            0.0775,
            -0.1589,
            0.0282,
            -0.0005,
            -0.1117,
            -0.0581,
            0.0919,
            -0.0812,
            0.0155,
            0.0809
          ]
        }
      }
    },
    {
      "step": 489,
      "word": "zaavan",
      "loss": 2.2695,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.103,
            0.1291,
            0.0752,
            0.0595,
            0.148,
            0.1187,
            -0.2228,
            -0.0559,
            -0.008,
            -0.1906,
            0.2736,
            0.175,
            -0.2481,
            -0.1751,
            -0.0587,
            0.1887
          ],
          "after": [
            0.0201,
            0.0122,
            0.1472,
            -0.0555,
            0.025,
            -0.0752,
            -0.1253,
            -0.1529,
            -0.0163,
            0.0972,
            0.164,
            0.0093,
            -0.0929,
            -0.011,
            -0.0447,
            -0.2016
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1052,
            0.0958,
            -0.1791,
            0.0345,
            -0.0091,
            -0.1805,
            0.3016,
            0.027,
            -0.2661,
            -0.0109,
            -0.067,
            -0.0178,
            0.1901,
            -0.0923,
            -0.1564,
            0.1854
          ],
          "after": [
            0.0219,
            0.1178,
            -0.0631,
            -0.0683,
            0.0392,
            -0.0197,
            0.1037,
            0.0948,
            -0.093,
            -0.0706,
            0.0169,
            -0.0207,
            0.1162,
            0.0172,
            -0.0692,
            0.2387
          ]
        },
        "position_0": {
          "grad": [
            0.0246,
            0.0853,
            -0.0319,
            0.1825,
            0.0472,
            -0.0702,
            0.228,
            0.0125,
            -0.0172,
            0.0505,
            -0.0093,
            0.0724,
            0.0488,
            -0.0605,
            -0.0151,
            0.0816
          ],
          "after": [
            0.0278,
            -0.1347,
            -0.2839,
            0.0682,
            0.0346,
            -0.0501,
            -0.1416,
            0.0676,
            0.0936,
            -0.0125,
            0.0151,
            0.0932,
            0.0465,
            -0.0061,
            -0.1319,
            0.0639
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            0.0003,
            0.0004,
            0.0017,
            0.0002,
            0.0029,
            -0.0004,
            0.0007,
            0.0004,
            0.0008,
            0.0007,
            0.002,
            -0.0017,
            -0.0004,
            -0.0008,
            0.0012
          ],
          "after": [
            -0.0757,
            0.006,
            -0.0993,
            -0.0801,
            0.0892,
            0.1451,
            0.0775,
            -0.1589,
            0.028,
            -0.0007,
            -0.1117,
            -0.0584,
            0.0921,
            -0.0813,
            0.0157,
            0.0808
          ]
        }
      }
    },
    {
      "step": 490,
      "word": "bellemy",
      "loss": 2.3903,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0204,
            0.0118,
            0.147,
            -0.0555,
            0.0249,
            -0.0754,
            -0.1252,
            -0.1526,
            -0.0164,
            0.0974,
            0.1636,
            0.009,
            -0.0926,
            -0.0107,
            -0.0444,
            -0.2018
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0683,
            -0.367,
            0.1823,
            -0.028,
            0.421,
            0.2766,
            -0.356,
            -0.002,
            0.4603,
            0.0918,
            0.0083,
            0.232,
            -0.4006,
            -0.0614,
            -0.0476,
            -0.1997
          ],
          "after": [
            0.0221,
            0.1181,
            -0.0632,
            -0.0685,
            0.0383,
            -0.0203,
            0.1038,
            0.0943,
            -0.0935,
            -0.071,
            0.0172,
            -0.0217,
            0.1169,
            0.0174,
            -0.0692,
            0.2388
          ]
        },
        "position_0": {
          "grad": [
            0.0313,
            0.027,
            0.0948,
            0.0095,
            0.0204,
            0.0047,
            0.0846,
            0.0037,
            -0.1018,
            -0.0445,
            0.0017,
            0.0123,
            0.1376,
            -0.1022,
            -0.0422,
            0.2392
          ],
          "after": [
            0.0278,
            -0.1351,
            -0.2837,
            0.0683,
            0.0349,
            -0.05,
            -0.1416,
            0.0678,
            0.0936,
            -0.0122,
            0.0155,
            0.093,
            0.0465,
            -0.0061,
            -0.1323,
            0.0638
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.001,
            -0.0003,
            -0.0002,
            0.0027,
            0.0007,
            -0.0003,
            0.0001,
            0.0018,
            0.0005,
            0.0009,
            0.0005,
            -0.002,
            -0.0022,
            -0.0023,
            0.0002
          ],
          "after": [
            -0.0757,
            0.0059,
            -0.0993,
            -0.08,
            0.0884,
            0.1448,
            0.0776,
            -0.1589,
            0.0275,
            -0.0009,
            -0.1118,
            -0.0587,
            0.0925,
            -0.081,
            0.0161,
            0.0806
          ]
        }
      }
    },
    {
      "step": 491,
      "word": "norman",
      "loss": 2.0877,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0129,
            0.1494,
            -0.0752,
            0.0483,
            -0.0346,
            -0.1053,
            0.0148,
            -0.0749,
            -0.0064,
            -0.1686,
            -0.059,
            0.0784,
            -0.06,
            -0.0377,
            0.0091,
            0.05
          ],
          "after": [
            0.0207,
            0.0112,
            0.147,
            -0.0555,
            0.0248,
            -0.0752,
            -0.1251,
            -0.1522,
            -0.0164,
            0.0978,
            0.1635,
            0.0086,
            -0.0923,
            -0.0104,
            -0.0442,
            -0.2021
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1497,
            0.1464,
            -0.1775,
            0.0453,
            0.0383,
            -0.1964,
            0.3992,
            0.046,
            -0.2255,
            0.0523,
            -0.0625,
            0.0426,
            0.1751,
            -0.1416,
            -0.1891,
            0.2333
          ],
          "after": [
            0.0224,
            0.1182,
            -0.0632,
            -0.0689,
            0.0375,
            -0.0207,
            0.1037,
            0.0937,
            -0.0939,
            -0.0714,
            0.0176,
            -0.0225,
            0.1174,
            0.0178,
            -0.0691,
            0.2388
          ]
        },
        "position_0": {
          "grad": [
            -0.0227,
            0.1127,
            -0.0729,
            0.06,
            -0.336,
            0.0031,
            0.0517,
            0.0164,
            0.0354,
            -0.2023,
            -0.2248,
            -0.0997,
            0.0207,
            0.1165,
            0.2226,
            0.1311
          ],
          "after": [
            0.0279,
            -0.1357,
            -0.2833,
            0.0683,
            0.0358,
            -0.0499,
            -0.1417,
            0.0679,
            0.0935,
            -0.0113,
            0.0163,
            0.0931,
            0.0465,
            -0.0065,
            -0.133,
            0.0635
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0021,
            0.0014,
            0.0008,
            0.0011,
            -0.0005,
            0.0002,
            0.0002,
            -0.0013,
            0.0003,
            0.0013,
            0.0005,
            0.0002,
            0.0001,
            0.0006,
            -0.0022,
            -0.0001
          ],
          "after": [
            -0.076,
            0.0056,
            -0.0994,
            -0.08,
            0.0878,
            0.1446,
            0.0776,
            -0.1587,
            0.027,
            -0.0012,
            -0.1119,
            -0.059,
            0.0928,
            -0.0809,
            0.0166,
            0.0805
          ]
        }
      }
    },
    {
      "step": 492,
      "word": "paidyn",
      "loss": 2.4634,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0451,
            -0.2356,
            0.0249,
            -0.0576,
            0.0341,
            0.153,
            -0.1864,
            0.0658,
            0.0015,
            -0.0327,
            0.0867,
            -0.1591,
            -0.1429,
            0.0628,
            0.0539,
            -0.0869
          ],
          "after": [
            0.021,
            0.0111,
            0.1469,
            -0.0555,
            0.0247,
            -0.0756,
            -0.1248,
            -0.152,
            -0.0164,
            0.0983,
            0.1632,
            0.0085,
            -0.0919,
            -0.0102,
            -0.0442,
            -0.2022
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0258,
            0.1033,
            -0.155,
            0.0263,
            0.0536,
            -0.1823,
            0.2996,
            0.0244,
            -0.2313,
            0.0046,
            -0.0162,
            -0.0074,
            0.1158,
            -0.1345,
            -0.1722,
            0.1595
          ],
          "after": [
            0.0226,
            0.1181,
            -0.063,
            -0.0692,
            0.0368,
            -0.0208,
            0.1033,
            0.0932,
            -0.0939,
            -0.0718,
            0.018,
            -0.0233,
            0.1177,
            0.0184,
            -0.0688,
            0.2386
          ]
        },
        "position_0": {
          "grad": [
            0.0559,
            0.1662,
            -0.0541,
            0.0748,
            -0.0117,
            -0.1146,
            0.3076,
            0.0204,
            -0.2975,
            -0.0093,
            -0.0354,
            -0.0,
            0.061,
            0.0802,
            -0.0293,
            0.2471
          ],
          "after": [
            0.0278,
            -0.1366,
            -0.2828,
            0.0681,
            0.0365,
            -0.0494,
            -0.1421,
            0.0679,
            0.0939,
            -0.0105,
            0.017,
            0.0931,
            0.0465,
            -0.007,
            -0.1336,
            0.063
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0,
            0.0001,
            0.0009,
            0.0013,
            0.0008,
            0.0022,
            0.0009,
            -0.0003,
            0.0004,
            -0.0003,
            0.0011,
            0.0004,
            -0.0014,
            -0.0009,
            0.0013
          ],
          "after": [
            -0.0762,
            0.0054,
            -0.0995,
            -0.0802,
            0.0871,
            0.1443,
            0.0774,
            -0.1587,
            0.0267,
            -0.0015,
            -0.1119,
            -0.0593,
            0.0931,
            -0.0807,
            0.0171,
            0.0802
          ]
        }
      }
    },
    {
      "step": 493,
      "word": "taisley",
      "loss": 2.4633,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0362,
            -0.1992,
            0.0351,
            -0.0689,
            0.0508,
            0.1478,
            -0.1706,
            0.0654,
            0.0044,
            -0.0173,
            0.0576,
            -0.1231,
            -0.1304,
            0.0427,
            0.0167,
            -0.078
          ],
          "after": [
            0.0213,
            0.0114,
            0.1468,
            -0.0553,
            0.0245,
            -0.0762,
            -0.1242,
            -0.152,
            -0.0165,
            0.0987,
            0.1627,
            0.0085,
            -0.0913,
            -0.0102,
            -0.0442,
            -0.2022
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0505,
            -0.2623,
            -0.0286,
            -0.0869,
            -0.0401,
            0.0903,
            -0.2487,
            0.0102,
            0.1193,
            0.1489,
            0.0153,
            -0.065,
            -0.0017,
            0.0751,
            0.0612,
            -0.2183
          ],
          "after": [
            0.0228,
            0.1184,
            -0.0628,
            -0.0693,
            0.0362,
            -0.0209,
            0.1032,
            0.0926,
            -0.0941,
            -0.0724,
            0.0183,
            -0.0238,
            0.118,
            0.0187,
            -0.0687,
            0.2386
          ]
        },
        "position_0": {
          "grad": [
            -0.016,
            -0.1238,
            0.0641,
            0.0591,
            -0.0879,
            0.0557,
            0.0469,
            0.027,
            -0.0963,
            -0.102,
            -0.0959,
            -0.2122,
            0.108,
            0.0061,
            0.0706,
            0.121
          ],
          "after": [
            0.0277,
            -0.137,
            -0.2826,
            0.0679,
            0.0373,
            -0.0492,
            -0.1425,
            0.0678,
            0.0945,
            -0.0096,
            0.0177,
            0.0937,
            0.0462,
            -0.0075,
            -0.1343,
            0.0624
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0012,
            -0.0007,
            0.0004,
            -0.0007,
            0.0021,
            -0.0012,
            0.0006,
            0.0005,
            0.0007,
            -0.0012,
            -0.0003,
            -0.0006,
            0.0014,
            0.0012,
            -0.0003
          ],
          "after": [
            -0.0763,
            0.0053,
            -0.0995,
            -0.0803,
            0.0866,
            0.1439,
            0.0774,
            -0.1588,
            0.0263,
            -0.0018,
            -0.1119,
            -0.0596,
            0.0933,
            -0.0806,
            0.0174,
            0.08
          ]
        }
      }
    },
    {
      "step": 494,
      "word": "dalon",
      "loss": 2.0598,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0617,
            0.0864,
            -0.1785,
            -0.1225,
            -0.0488,
            -0.0069,
            -0.1918,
            0.0659,
            -0.0134,
            -0.0589,
            -0.116,
            0.1712,
            -0.1202,
            0.0755,
            0.1339,
            -0.2428
          ],
          "after": [
            0.0216,
            0.0115,
            0.147,
            -0.0549,
            0.0244,
            -0.0768,
            -0.1235,
            -0.1522,
            -0.0165,
            0.0992,
            0.1626,
            0.0084,
            -0.0906,
            -0.0103,
            -0.0445,
            -0.2018
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.023,
            0.2046,
            -0.224,
            0.1074,
            -0.0308,
            -0.2068,
            0.4182,
            0.0075,
            -0.3575,
            -0.0246,
            -0.0357,
            -0.0852,
            0.253,
            -0.1083,
            -0.2491,
            0.288
          ],
          "after": [
            0.023,
            0.1184,
            -0.0624,
            -0.0696,
            0.0357,
            -0.0209,
            0.1028,
            0.0922,
            -0.094,
            -0.0728,
            0.0186,
            -0.0241,
            0.1179,
            0.0192,
            -0.0684,
            0.2384
          ]
        },
        "position_0": {
          "grad": [
            -0.0562,
            0.0318,
            -0.0088,
            -0.0752,
            0.1168,
            -0.0986,
            0.0774,
            -0.0566,
            -0.0086,
            -0.0484,
            -0.0825,
            -0.034,
            0.0857,
            0.07,
            0.0764,
            0.1771
          ],
          "after": [
            0.0279,
            -0.1375,
            -0.2824,
            0.0678,
            0.0378,
            -0.0487,
            -0.143,
            0.0679,
            0.0949,
            -0.0087,
            0.0185,
            0.0942,
            0.0459,
            -0.0081,
            -0.135,
            0.0616
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0007,
            -0.0006,
            0.0022,
            0.0011,
            0.001,
            0.0032,
            0.0014,
            -0.0004,
            -0.0004,
            -0.0008,
            0.0019,
            0.0004,
            -0.0018,
            -0.0005,
            0.0025
          ],
          "after": [
            -0.0763,
            0.0052,
            -0.0994,
            -0.0807,
            0.0861,
            0.1434,
            0.0771,
            -0.159,
            0.0261,
            -0.0021,
            -0.1117,
            -0.06,
            0.0935,
            -0.0803,
            0.0177,
            0.0796
          ]
        }
      }
    },
    {
      "step": 495,
      "word": "yoana",
      "loss": 2.5886,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5735,
            0.0774,
            -0.4061,
            -0.0196,
            -0.4887,
            -0.0779,
            -0.1616,
            -0.1521,
            -0.0406,
            0.2322,
            0.0199,
            -0.4914,
            0.1761,
            0.3295,
            0.1209,
            -0.2449
          ],
          "after": [
            0.0211,
            0.0114,
            0.1477,
            -0.0545,
            0.025,
            -0.077,
            -0.1226,
            -0.152,
            -0.0164,
            0.0991,
            0.1625,
            0.0089,
            -0.0903,
            -0.0111,
            -0.045,
            -0.2013
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0173,
            -0.0468,
            -0.0573,
            0.0002,
            0.0158,
            -0.0757,
            0.0852,
            -0.0031,
            -0.0981,
            0.0818,
            0.0122,
            0.0003,
            0.0611,
            -0.0756,
            -0.0813,
            0.0493
          ],
          "after": [
            0.0232,
            0.1184,
            -0.0621,
            -0.0699,
            0.0353,
            -0.0207,
            0.1024,
            0.0918,
            -0.0937,
            -0.0733,
            0.0188,
            -0.0244,
            0.1178,
            0.0197,
            -0.068,
            0.2381
          ]
        },
        "position_0": {
          "grad": [
            0.0777,
            -0.23,
            0.103,
            -0.2465,
            -0.0645,
            0.1322,
            -0.2567,
            0.0245,
            0.1169,
            0.0433,
            -0.0586,
            -0.0645,
            -0.1361,
            0.151,
            0.0955,
            -0.1232
          ],
          "after": [
            0.0277,
            -0.1374,
            -0.2825,
            0.0683,
            0.0383,
            -0.0487,
            -0.1431,
            0.0679,
            0.0951,
            -0.008,
            0.0193,
            0.0948,
            0.0459,
            -0.009,
            -0.1357,
            0.0611
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            0.0019,
            -0.0006,
            0.0,
            0.0006,
            0.0001,
            -0.0015,
            0.0,
            0.0013,
            -0.001,
            -0.0008,
            0.0012,
            -0.0016,
            0.0011,
            0.0002,
            0.0003
          ],
          "after": [
            -0.0764,
            0.0048,
            -0.0993,
            -0.081,
            0.0856,
            0.143,
            0.077,
            -0.1593,
            0.0257,
            -0.0022,
            -0.1115,
            -0.0604,
            0.0938,
            -0.0802,
            0.0179,
            0.0793
          ]
        }
      }
    },
    {
      "step": 496,
      "word": "xolani",
      "loss": 2.4816,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0579,
            0.1278,
            -0.2317,
            0.0043,
            -0.2207,
            -0.1562,
            0.0196,
            -0.0535,
            0.0327,
            -0.0006,
            -0.0279,
            -0.0508,
            -0.0105,
            0.0088,
            0.1181,
            -0.0229
          ],
          "after": [
            0.0205,
            0.0111,
            0.1486,
            -0.0542,
            0.0259,
            -0.0768,
            -0.1219,
            -0.1516,
            -0.0164,
            0.0991,
            0.1625,
            0.0094,
            -0.09,
            -0.0118,
            -0.0457,
            -0.2007
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0823,
            0.1302,
            -0.1517,
            0.0268,
            0.0209,
            -0.1712,
            0.302,
            -0.0274,
            -0.2705,
            0.0204,
            -0.0326,
            -0.0529,
            0.1751,
            -0.1098,
            -0.1997,
            0.2276
          ],
          "after": [
            0.0234,
            0.1183,
            -0.0616,
            -0.0702,
            0.0349,
            -0.0204,
            0.1019,
            0.0915,
            -0.0933,
            -0.0738,
            0.0191,
            -0.0246,
            0.1175,
            0.0203,
            -0.0676,
            0.2377
          ]
        },
        "position_0": {
          "grad": [
            0.0127,
            0.0606,
            0.0403,
            0.3019,
            -0.0899,
            -0.1056,
            0.0593,
            0.0698,
            -0.188,
            -0.1505,
            -0.0021,
            -0.2012,
            0.2892,
            -0.1266,
            -0.0572,
            0.1255
          ],
          "after": [
            0.0276,
            -0.1374,
            -0.2827,
            0.0681,
            0.0389,
            -0.0484,
            -0.1432,
            0.0677,
            0.0956,
            -0.007,
            0.02,
            0.0957,
            0.0454,
            -0.0094,
            -0.1363,
            0.0605
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0006,
            0.0016,
            -0.0028,
            0.003,
            0.0026,
            -0.0021,
            -0.0009,
            0.0023,
            -0.0009,
            -0.0005,
            0.0007,
            -0.0022,
            0.0021,
            -0.0009,
            0.0002
          ],
          "after": [
            -0.0764,
            0.0046,
            -0.0993,
            -0.081,
            0.0849,
            0.1424,
            0.0771,
            -0.1593,
            0.025,
            -0.0022,
            -0.1113,
            -0.0609,
            0.0942,
            -0.0804,
            0.0182,
            0.0789
          ]
        }
      }
    },
    {
      "step": 497,
      "word": "trail",
      "loss": 2.8353,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0004,
            -0.2905,
            0.0311,
            -0.082,
            0.044,
            0.0842,
            -0.2273,
            0.0695,
            0.0365,
            -0.0432,
            0.1228,
            -0.2112,
            -0.1546,
            0.0724,
            0.028,
            -0.1512
          ],
          "after": [
            0.0201,
            0.0114,
            0.1493,
            -0.0538,
            0.0266,
            -0.0768,
            -0.121,
            -0.1515,
            -0.0165,
            0.0991,
            0.1622,
            0.0101,
            -0.0895,
            -0.0125,
            -0.0463,
            -0.2001
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1,
            0.3078,
            -0.2376,
            0.0563,
            0.0049,
            -0.267,
            0.4951,
            -0.0523,
            -0.3634,
            -0.1024,
            -0.044,
            -0.128,
            0.2911,
            -0.13,
            -0.2828,
            0.321
          ],
          "after": [
            0.0238,
            0.1179,
            -0.061,
            -0.0706,
            0.0346,
            -0.0198,
            0.1011,
            0.0915,
            -0.0927,
            -0.074,
            0.0194,
            -0.0246,
            0.117,
            0.021,
            -0.067,
            0.2371
          ]
        },
        "position_0": {
          "grad": [
            -0.0215,
            -0.1773,
            0.0868,
            0.1103,
            -0.1366,
            0.0941,
            0.0848,
            0.0565,
            -0.1338,
            -0.1585,
            -0.1286,
            -0.2826,
            0.1548,
            0.0017,
            0.1251,
            0.1785
          ],
          "after": [
            0.0275,
            -0.1371,
            -0.2832,
            0.0677,
            0.0397,
            -0.0484,
            -0.1434,
            0.0673,
            0.0962,
            -0.0058,
            0.0208,
            0.0971,
            0.0448,
            -0.0098,
            -0.137,
            0.0598
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            0.0034,
            -0.0007,
            0.0036,
            -0.0012,
            0.0003,
            0.0036,
            0.0004,
            -0.0013,
            -0.0045,
            -0.0007,
            -0.0006,
            0.0022,
            -0.0015,
            0.0014,
            0.0025
          ],
          "after": [
            -0.0762,
            0.0041,
            -0.0993,
            -0.0813,
            0.0844,
            0.1418,
            0.0769,
            -0.1594,
            0.0247,
            -0.0018,
            -0.111,
            -0.0612,
            0.0943,
            -0.0804,
            0.0183,
            0.0784
          ]
        }
      }
    },
    {
      "step": 498,
      "word": "aditi",
      "loss": 2.4251,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0908,
            0.0175,
            -0.0239,
            -0.1512,
            0.125,
            -0.2002,
            0.018,
            -0.1567,
            -0.043,
            -0.0564,
            -0.0138,
            -0.0847,
            0.1342,
            0.1239,
            0.0669,
            0.1823
          ],
          "after": [
            0.0198,
            0.0116,
            0.15,
            -0.0531,
            0.027,
            -0.0764,
            -0.1202,
            -0.151,
            -0.0165,
            0.0993,
            0.1619,
            0.0108,
            -0.0893,
            -0.0134,
            -0.047,
            -0.1998
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0009,
            0.1089,
            -0.1216,
            0.0259,
            0.1305,
            -0.1432,
            0.232,
            -0.0233,
            -0.1869,
            0.0102,
            -0.0246,
            -0.103,
            0.168,
            -0.0436,
            -0.265,
            0.1092
          ],
          "after": [
            0.0241,
            0.1174,
            -0.0603,
            -0.0709,
            0.0342,
            -0.0192,
            0.1003,
            0.0916,
            -0.092,
            -0.0743,
            0.0198,
            -0.0244,
            0.1164,
            0.0216,
            -0.0663,
            0.2365
          ]
        },
        "position_0": {
          "grad": [
            0.114,
            -0.0706,
            0.0797,
            -0.1805,
            0.0192,
            -0.0233,
            -0.3303,
            -0.1342,
            0.1507,
            0.0254,
            0.2593,
            -0.0127,
            -0.1776,
            0.0468,
            -0.0454,
            -0.2334
          ],
          "after": [
            0.0271,
            -0.1366,
            -0.2838,
            0.0677,
            0.0403,
            -0.0484,
            -0.1432,
            0.0674,
            0.0965,
            -0.0048,
            0.021,
            0.0983,
            0.0446,
            -0.0102,
            -0.1376,
            0.0595
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0013,
            0.0007,
            -0.0018,
            0.0024,
            0.002,
            -0.0001,
            -0.0007,
            0.0,
            0.0012,
            0.0003,
            0.0003,
            -0.0011,
            0.0005,
            -0.0019,
            0.0004
          ],
          "after": [
            -0.076,
            0.0038,
            -0.0994,
            -0.0814,
            0.0838,
            0.1412,
            0.0767,
            -0.1594,
            0.0243,
            -0.0016,
            -0.1108,
            -0.0615,
            0.0946,
            -0.0804,
            0.0186,
            0.0779
          ]
        }
      }
    },
    {
      "step": 499,
      "word": "zayda",
      "loss": 2.3042,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5208,
            -0.2478,
            -0.1023,
            -0.2548,
            -0.0945,
            0.1325,
            -0.2161,
            0.011,
            -0.1173,
            0.3232,
            0.1743,
            -0.2718,
            0.0954,
            0.2506,
            0.046,
            -0.2714
          ],
          "after": [
            0.0189,
            0.0123,
            0.1507,
            -0.0521,
            0.0274,
            -0.0763,
            -0.1192,
            -0.1506,
            -0.0162,
            0.0988,
            0.1613,
            0.0118,
            -0.0893,
            -0.0146,
            -0.0477,
            -0.1992
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0708,
            0.1431,
            -0.2274,
            0.0669,
            -0.0175,
            -0.1726,
            0.3078,
            0.0341,
            -0.3004,
            -0.0269,
            -0.0619,
            -0.0879,
            0.2196,
            -0.0904,
            -0.1843,
            0.1869
          ],
          "after": [
            0.0245,
            0.1169,
            -0.0596,
            -0.0714,
            0.0339,
            -0.0185,
            0.0994,
            0.0915,
            -0.0911,
            -0.0744,
            0.0202,
            -0.0242,
            0.1156,
            0.0223,
            -0.0656,
            0.2358
          ]
        },
        "position_0": {
          "grad": [
            0.036,
            0.08,
            -0.0285,
            0.2042,
            0.0525,
            -0.0794,
            0.2458,
            0.0055,
            0.0105,
            0.0544,
            0.0143,
            0.0619,
            0.0385,
            -0.0593,
            -0.0039,
            0.0771
          ],
          "after": [
            0.0265,
            -0.1364,
            -0.2842,
            0.0673,
            0.0408,
            -0.0481,
            -0.1433,
            0.0675,
            0.0967,
            -0.0041,
            0.0212,
            0.0992,
            0.0443,
            -0.0105,
            -0.138,
            0.0591
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0018,
            -0.001,
            0.0012,
            -0.0019,
            0.0027,
            -0.0002,
            0.001,
            0.0007,
            0.0014,
            -0.0008,
            0.0001,
            -0.0,
            0.0009,
            0.0008,
            0.0025
          ],
          "after": [
            -0.0758,
            0.0037,
            -0.0993,
            -0.0817,
            0.0834,
            0.1403,
            0.0766,
            -0.1595,
            0.024,
            -0.0015,
            -0.1106,
            -0.0618,
            0.0948,
            -0.0805,
            0.0188,
            0.0772
          ]
        }
      }
    },
    {
      "step": 500,
      "word": "soraia",
      "loss": 2.1818,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1789,
            -0.2185,
            0.0194,
            -0.2389,
            0.0429,
            0.0112,
            -0.2297,
            0.0077,
            -0.0556,
            0.1334,
            0.2096,
            -0.3319,
            -0.0782,
            0.1473,
            -0.0265,
            -0.367
          ],
          "after": [
            0.0178,
            0.0132,
            0.1512,
            -0.0508,
            0.0278,
            -0.0763,
            -0.1181,
            -0.1502,
            -0.0158,
            0.0981,
            0.1603,
            0.0131,
            -0.0891,
            -0.0159,
            -0.0482,
            -0.1983
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0085,
            -0.0012,
            -0.1173,
            -0.0212,
            0.0152,
            -0.094,
            0.148,
            -0.0153,
            -0.1529,
            0.0183,
            0.0001,
            -0.0083,
            0.0589,
            -0.0584,
            -0.1332,
            0.0951
          ],
          "after": [
            0.0247,
            0.1164,
            -0.0588,
            -0.0718,
            0.0336,
            -0.0178,
            0.0986,
            0.0915,
            -0.0903,
            -0.0745,
            0.0205,
            -0.024,
            0.1149,
            0.023,
            -0.0648,
            0.2352
          ]
        },
        "position_0": {
          "grad": [
            0.0114,
            0.0774,
            -0.0224,
            0.054,
            -0.1433,
            0.0483,
            -0.0568,
            0.0345,
            -0.0305,
            0.0213,
            -0.097,
            0.0145,
            0.0838,
            0.0217,
            0.0887,
            -0.0214
          ],
          "after": [
            0.0261,
            -0.1364,
            -0.2846,
            0.0669,
            0.0414,
            -0.048,
            -0.1433,
            0.0675,
            0.097,
            -0.0035,
            0.0215,
            0.0999,
            0.0439,
            -0.0107,
            -0.1386,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0004,
            0.0008,
            -0.0001,
            -0.0009,
            -0.0004,
            -0.0016,
            0.0003,
            0.0005,
            -0.0003,
            0.0001,
            0.0003,
            -0.0001,
            0.0003,
            0.0006,
            -0.0003
          ],
          "after": [
            -0.0755,
            0.0036,
            -0.0994,
            -0.0818,
            0.0832,
            0.1397,
            0.0766,
            -0.1597,
            0.0236,
            -0.0014,
            -0.1103,
            -0.0621,
            0.095,
            -0.0807,
            0.0188,
            0.0767
          ]
        }
      }
    },
    {
      "step": 501,
      "word": "tyson",
      "loss": 2.4069,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0169,
            0.0139,
            0.1517,
            -0.0497,
            0.0281,
            -0.0762,
            -0.1171,
            -0.15,
            -0.0155,
            0.0976,
            0.1595,
            0.0141,
            -0.089,
            -0.0171,
            -0.0487,
            -0.1975
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0645,
            0.0426,
            -0.1447,
            0.0359,
            0.0353,
            -0.13,
            0.2261,
            0.0084,
            -0.1703,
            0.0192,
            -0.0176,
            -0.0142,
            0.1276,
            -0.0939,
            -0.1621,
            0.1225
          ],
          "after": [
            0.0251,
            0.1159,
            -0.058,
            -0.0722,
            0.0333,
            -0.0171,
            0.0977,
            0.0915,
            -0.0895,
            -0.0747,
            0.0209,
            -0.0238,
            0.1142,
            0.0237,
            -0.0641,
            0.2345
          ]
        },
        "position_0": {
          "grad": [
            -0.0226,
            -0.1991,
            0.0835,
            0.1304,
            -0.1673,
            0.0911,
            0.06,
            0.054,
            -0.1046,
            -0.2057,
            -0.1142,
            -0.3092,
            0.1429,
            0.0572,
            0.1667,
            0.186
          ],
          "after": [
            0.0257,
            -0.136,
            -0.2851,
            0.0663,
            0.0422,
            -0.0482,
            -0.1434,
            0.0673,
            0.0973,
            -0.0025,
            0.0219,
            0.1012,
            0.0435,
            -0.0111,
            -0.1394,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0,
            0.0009,
            -0.0001,
            -0.0003,
            0.0,
            -0.0009,
            -0.0002,
            -0.0,
            -0.0001,
            -0.0004,
            -0.0014,
            0.0009,
            0.0002,
            0.0008,
            -0.0005
          ],
          "after": [
            -0.0752,
            0.0036,
            -0.0995,
            -0.082,
            0.083,
            0.1391,
            0.0766,
            -0.1598,
            0.0233,
            -0.0013,
            -0.1101,
            -0.0621,
            0.0951,
            -0.0808,
            0.0189,
            0.0763
          ]
        }
      }
    },
    {
      "step": 502,
      "word": "anaylah",
      "loss": 2.2231,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1199,
            -0.0126,
            -0.008,
            0.0358,
            -0.284,
            0.246,
            -0.0428,
            0.1263,
            0.1123,
            -0.1615,
            -0.3781,
            0.0021,
            0.1417,
            0.1206,
            0.1945,
            0.3422
          ],
          "after": [
            0.0163,
            0.0146,
            0.1522,
            -0.0488,
            0.0287,
            -0.0768,
            -0.1162,
            -0.1501,
            -0.0155,
            0.0974,
            0.1597,
            0.015,
            -0.0891,
            -0.0183,
            -0.0495,
            -0.1972
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0963,
            0.1047,
            -0.0452,
            0.0442,
            0.118,
            -0.1779,
            0.2554,
            -0.0224,
            -0.1527,
            -0.0201,
            -0.0021,
            0.0771,
            0.0467,
            -0.1247,
            -0.1738,
            0.1991
          ],
          "after": [
            0.0255,
            0.1154,
            -0.0573,
            -0.0726,
            0.0329,
            -0.0162,
            0.0968,
            0.0915,
            -0.0886,
            -0.0748,
            0.0211,
            -0.0237,
            0.1135,
            0.0245,
            -0.0633,
            0.2337
          ]
        },
        "position_0": {
          "grad": [
            0.0877,
            -0.0615,
            0.0504,
            -0.1725,
            0.0015,
            -0.0241,
            -0.2652,
            -0.0923,
            0.153,
            0.0365,
            0.1795,
            -0.021,
            -0.1668,
            0.012,
            -0.0349,
            -0.2089
          ],
          "after": [
            0.0251,
            -0.1355,
            -0.2857,
            0.0661,
            0.0429,
            -0.0482,
            -0.1432,
            0.0674,
            0.0974,
            -0.0018,
            0.022,
            0.1023,
            0.0433,
            -0.0114,
            -0.14,
            0.0582
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0001,
            0.0015,
            -0.0011,
            0.0003,
            -0.0012,
            0.0001,
            -0.001,
            -0.0001,
            0.001,
            0.0012,
            0.0004,
            0.0,
            -0.0008,
            -0.0004,
            -0.0017
          ],
          "after": [
            -0.0748,
            0.0036,
            -0.0998,
            -0.082,
            0.0829,
            0.1387,
            0.0767,
            -0.1598,
            0.023,
            -0.0014,
            -0.1101,
            -0.0623,
            0.0951,
            -0.0809,
            0.0189,
            0.0761
          ]
        }
      }
    },
    {
      "step": 503,
      "word": "minerva",
      "loss": 2.559,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1069,
            -0.0177,
            -0.0287,
            -0.069,
            -0.0196,
            -0.0341,
            -0.0235,
            -0.0436,
            -0.0594,
            0.0753,
            0.091,
            -0.0849,
            0.0232,
            0.052,
            -0.007,
            -0.094
          ],
          "after": [
            0.0157,
            0.0152,
            0.1526,
            -0.0479,
            0.0293,
            -0.0773,
            -0.1154,
            -0.15,
            -0.0154,
            0.0971,
            0.1596,
            0.0159,
            -0.0892,
            -0.0194,
            -0.0501,
            -0.1969
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1461,
            0.1063,
            -0.0238,
            -0.0427,
            0.0191,
            -0.1113,
            -0.0475,
            -0.0275,
            -0.2011,
            -0.3004,
            -0.0928,
            -0.065,
            0.0739,
            -0.0052,
            0.0722,
            0.0724
          ],
          "after": [
            0.026,
            0.1149,
            -0.0567,
            -0.0729,
            0.0326,
            -0.0154,
            0.096,
            0.0917,
            -0.0877,
            -0.0744,
            0.0216,
            -0.0235,
            0.1128,
            0.0252,
            -0.0627,
            0.233
          ]
        },
        "position_0": {
          "grad": [
            -0.0424,
            0.027,
            -0.0297,
            0.0187,
            0.113,
            0.0153,
            0.1582,
            0.081,
            0.0461,
            0.0176,
            -0.0331,
            0.1101,
            -0.1103,
            -0.0495,
            0.0025,
            0.0767
          ],
          "after": [
            0.0248,
            -0.1351,
            -0.2861,
            0.0659,
            0.0433,
            -0.0484,
            -0.1432,
            0.0672,
            0.0974,
            -0.0012,
            0.0221,
            0.103,
            0.0432,
            -0.0116,
            -0.1405,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0031,
            0.0012,
            0.0004,
            0.0006,
            -0.0012,
            0.0004,
            -0.0007,
            -0.0011,
            -0.0044,
            -0.0016,
            -0.0005,
            0.0003,
            -0.0024,
            -0.0001,
            -0.0007
          ],
          "after": [
            -0.0745,
            0.0032,
            -0.1002,
            -0.082,
            0.0827,
            0.1385,
            0.0767,
            -0.1596,
            0.0229,
            -0.001,
            -0.1098,
            -0.0623,
            0.0952,
            -0.0806,
            0.0189,
            0.076
          ]
        }
      }
    },
    {
      "step": 504,
      "word": "brelynn",
      "loss": 2.1387,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0151,
            0.0158,
            0.1529,
            -0.0472,
            0.0298,
            -0.0776,
            -0.1147,
            -0.15,
            -0.0152,
            0.0968,
            0.1595,
            0.0167,
            -0.0894,
            -0.0204,
            -0.0507,
            -0.1966
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1401,
            -0.003,
            0.0978,
            0.1387,
            0.173,
            0.0914,
            -0.0206,
            0.0773,
            -0.0534,
            0.0388,
            0.099,
            0.2117,
            -0.1785,
            -0.0892,
            0.0317,
            0.1089
          ],
          "after": [
            0.0267,
            0.1144,
            -0.0562,
            -0.0734,
            0.0322,
            -0.0148,
            0.0954,
            0.0915,
            -0.0869,
            -0.0741,
            0.0217,
            -0.0237,
            0.1125,
            0.0259,
            -0.0622,
            0.2323
          ]
        },
        "position_0": {
          "grad": [
            0.0422,
            0.0102,
            0.0895,
            0.0365,
            -0.0054,
            0.0005,
            0.1084,
            0.0135,
            -0.111,
            -0.0622,
            0.0013,
            0.0029,
            0.1304,
            -0.0968,
            -0.0064,
            0.2405
          ],
          "after": [
            0.0243,
            -0.1349,
            -0.2867,
            0.0657,
            0.0437,
            -0.0485,
            -0.1433,
            0.0671,
            0.0976,
            -0.0005,
            0.0222,
            0.1036,
            0.0431,
            -0.0114,
            -0.1409,
            0.0575
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0001,
            0.0019,
            0.0017,
            -0.0002,
            0.001,
            -0.0004,
            0.0008,
            -0.0028,
            -0.0004,
            0.0007,
            0.0013,
            0.0003,
            0.0008,
            0.0012,
            0.001
          ],
          "after": [
            -0.0741,
            0.0029,
            -0.1008,
            -0.0822,
            0.0825,
            0.1382,
            0.0767,
            -0.1596,
            0.0233,
            -0.0007,
            -0.1097,
            -0.0625,
            0.0952,
            -0.0805,
            0.0189,
            0.0758
          ]
        }
      }
    },
    {
      "step": 505,
      "word": "esha",
      "loss": 2.5235,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5735,
            -0.064,
            -0.449,
            -0.0547,
            -0.354,
            -0.0514,
            -0.1119,
            0.0382,
            -0.1624,
            0.2303,
            0.1107,
            -0.5385,
            0.2434,
            0.2315,
            0.1782,
            -0.2333
          ],
          "after": [
            0.0139,
            0.0163,
            0.1538,
            -0.0464,
            0.0307,
            -0.0778,
            -0.114,
            -0.1501,
            -0.0148,
            0.0962,
            0.1592,
            0.018,
            -0.0898,
            -0.0217,
            -0.0516,
            -0.196
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1771,
            0.3954,
            0.6528,
            -0.036,
            -0.0024,
            -0.0116,
            0.3288,
            -0.0377,
            0.1408,
            -0.2126,
            0.0117,
            -0.1112,
            -0.1438,
            0.1556,
            0.0428,
            -0.1954
          ],
          "after": [
            0.0275,
            0.1136,
            -0.0565,
            -0.0738,
            0.0318,
            -0.0143,
            0.0947,
            0.0915,
            -0.0863,
            -0.0735,
            0.0219,
            -0.0237,
            0.1123,
            0.0263,
            -0.0619,
            0.2319
          ]
        },
        "position_0": {
          "grad": [
            0.0144,
            -0.306,
            0.0561,
            -0.0436,
            -0.0846,
            0.0798,
            -0.3856,
            -0.075,
            0.2686,
            0.1413,
            -0.0163,
            -0.1037,
            -0.2324,
            0.0533,
            0.0796,
            -0.4277
          ],
          "after": [
            0.0239,
            -0.134,
            -0.2874,
            0.0656,
            0.0441,
            -0.0488,
            -0.143,
            0.0672,
            0.0973,
            -0.0003,
            0.0223,
            0.1043,
            0.0432,
            -0.0115,
            -0.1415,
            0.0576
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0001,
            0.0006,
            -0.0009,
            0.0002,
            -0.0012,
            -0.0002,
            -0.0003,
            -0.0004,
            -0.0002,
            0.001,
            -0.0004,
            0.0001,
            -0.001,
            0.0005,
            -0.0012
          ],
          "after": [
            -0.0736,
            0.0027,
            -0.1013,
            -0.0823,
            0.0824,
            0.1381,
            0.0768,
            -0.1595,
            0.0236,
            -0.0003,
            -0.1097,
            -0.0626,
            0.0952,
            -0.0802,
            0.0188,
            0.0758
          ]
        }
      }
    },
    {
      "step": 506,
      "word": "susannah",
      "loss": 2.3997,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.142,
            0.1241,
            0.0112,
            0.1319,
            -0.0375,
            -0.006,
            0.1153,
            0.0726,
            0.1177,
            0.0211,
            -0.1621,
            0.0364,
            0.13,
            -0.0702,
            -0.0333,
            0.1681
          ],
          "after": [
            0.0131,
            0.0166,
            0.1546,
            -0.046,
            0.0316,
            -0.078,
            -0.1135,
            -0.1504,
            -0.0147,
            0.0956,
            0.1593,
            0.019,
            -0.0904,
            -0.0226,
            -0.0522,
            -0.1958
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0525,
            0.0038,
            -0.0965,
            -0.0008,
            0.0755,
            -0.1282,
            0.1616,
            0.0135,
            -0.1052,
            -0.0017,
            -0.0081,
            0.0851,
            0.0073,
            -0.1031,
            -0.1081,
            0.1393
          ],
          "after": [
            0.0282,
            0.1129,
            -0.0566,
            -0.0741,
            0.0315,
            -0.0137,
            0.0939,
            0.0915,
            -0.0858,
            -0.073,
            0.022,
            -0.0238,
            0.1122,
            0.0267,
            -0.0614,
            0.2314
          ]
        },
        "position_0": {
          "grad": [
            -0.002,
            0.081,
            -0.0167,
            0.1037,
            -0.0914,
            0.0432,
            0.0077,
            0.0367,
            -0.0508,
            0.0079,
            -0.079,
            0.0634,
            0.0717,
            0.0171,
            0.089,
            0.0247
          ],
          "after": [
            0.0235,
            -0.1334,
            -0.2879,
            0.0653,
            0.0447,
            -0.0492,
            -0.1427,
            0.0671,
            0.0971,
            -0.0002,
            0.0226,
            0.1048,
            0.0433,
            -0.0115,
            -0.1421,
            0.0577
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0,
            0.0004,
            -0.0,
            0.0014,
            -0.0006,
            0.0014,
            0.0002,
            0.0001,
            0.0008,
            0.0006,
            0.0018,
            -0.0002,
            -0.0012,
            -0.0011,
            0.0001
          ],
          "after": [
            -0.0732,
            0.0024,
            -0.1019,
            -0.0824,
            0.0821,
            0.1381,
            0.0767,
            -0.1595,
            0.0239,
            -0.0001,
            -0.1098,
            -0.0628,
            0.0952,
            -0.0799,
            0.0188,
            0.0758
          ]
        }
      }
    },
    {
      "step": 507,
      "word": "oceane",
      "loss": 2.4369,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0274,
            0.1463,
            -0.2037,
            0.0098,
            -0.1992,
            -0.1446,
            0.0402,
            -0.0598,
            0.0388,
            0.0043,
            -0.0414,
            -0.0269,
            -0.0076,
            -0.0169,
            0.0876,
            0.005
          ],
          "after": [
            0.0123,
            0.0165,
            0.1555,
            -0.0457,
            0.0326,
            -0.0777,
            -0.1132,
            -0.1504,
            -0.0146,
            0.0951,
            0.1595,
            0.02,
            -0.0909,
            -0.0234,
            -0.053,
            -0.1956
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0498,
            -0.1384,
            0.3056,
            0.0198,
            -0.4394,
            0.4022,
            -0.61,
            -0.071,
            0.4006,
            0.0423,
            0.1892,
            -0.0791,
            -0.2864,
            0.3047,
            0.3916,
            -0.4792
          ],
          "after": [
            0.0288,
            0.1125,
            -0.057,
            -0.0744,
            0.0315,
            -0.0136,
            0.0937,
            0.0917,
            -0.0856,
            -0.0727,
            0.0217,
            -0.0238,
            0.1123,
            0.0267,
            -0.0614,
            0.2315
          ]
        },
        "position_0": {
          "grad": [
            0.1263,
            -0.1434,
            0.0919,
            -0.1681,
            0.0795,
            -0.0421,
            -0.3082,
            -0.0882,
            0.0954,
            0.1861,
            0.0825,
            -0.0139,
            -0.1167,
            0.0175,
            -0.0934,
            -0.2636
          ],
          "after": [
            0.0228,
            -0.1327,
            -0.2887,
            0.0653,
            0.045,
            -0.0494,
            -0.1421,
            0.0674,
            0.0968,
            -0.0005,
            0.0226,
            0.1052,
            0.0434,
            -0.0116,
            -0.1425,
            0.0581
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0002,
            -0.0011,
            0.0005,
            0.0004,
            0.0003,
            0.0005,
            0.0,
            0.0001,
            0.0005,
            -0.0008,
            0.0007,
            -0.0006,
            0.0,
            0.0006,
            0.0003
          ],
          "after": [
            -0.0729,
            0.0022,
            -0.1022,
            -0.0825,
            0.0819,
            0.1381,
            0.0766,
            -0.1595,
            0.0241,
            -0.0,
            -0.1098,
            -0.0631,
            0.0952,
            -0.0796,
            0.0188,
            0.0757
          ]
        }
      }
    },
    {
      "step": 508,
      "word": "vihaan",
      "loss": 2.5138,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0952,
            0.1062,
            0.2118,
            0.0198,
            -0.0553,
            -0.0517,
            -0.266,
            -0.1029,
            0.1805,
            -0.323,
            0.1228,
            0.059,
            -0.2496,
            0.1073,
            0.0563,
            -0.1244
          ],
          "after": [
            0.0116,
            0.0163,
            0.156,
            -0.0455,
            0.0335,
            -0.0774,
            -0.1125,
            -0.1502,
            -0.015,
            0.0953,
            0.1594,
            0.0207,
            -0.0909,
            -0.0242,
            -0.0537,
            -0.1953
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0384,
            0.0345,
            -0.1261,
            0.0376,
            0.0166,
            -0.1277,
            0.2177,
            0.018,
            -0.1832,
            0.0512,
            -0.0605,
            -0.0207,
            0.1245,
            -0.0348,
            -0.1857,
            0.1436
          ],
          "after": [
            0.0293,
            0.112,
            -0.0572,
            -0.0748,
            0.0316,
            -0.0135,
            0.0934,
            0.0918,
            -0.0853,
            -0.0724,
            0.0216,
            -0.0237,
            0.1124,
            0.0267,
            -0.0612,
            0.2313
          ]
        },
        "position_0": {
          "grad": [
            -0.0684,
            0.0347,
            0.011,
            0.0617,
            0.0687,
            0.0702,
            -0.0015,
            0.0397,
            -0.0594,
            -0.0337,
            -0.0474,
            -0.0717,
            0.0173,
            -0.0589,
            -0.0272,
            0.1402
          ],
          "after": [
            0.0224,
            -0.1321,
            -0.2893,
            0.0653,
            0.0451,
            -0.0498,
            -0.1415,
            0.0674,
            0.0966,
            -0.0008,
            0.0228,
            0.1057,
            0.0436,
            -0.0116,
            -0.1427,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0027,
            -0.0021,
            0.0008,
            -0.0032,
            0.0014,
            0.0013,
            -0.0021,
            0.0007,
            0.0021,
            -0.0053,
            -0.0031,
            -0.0011,
            -0.0,
            0.0015,
            0.0025,
            0.001
          ],
          "after": [
            -0.0722,
            0.0023,
            -0.1026,
            -0.0822,
            0.0815,
            0.1379,
            0.0767,
            -0.1596,
            0.024,
            0.0005,
            -0.1094,
            -0.0633,
            0.0953,
            -0.0795,
            0.0186,
            0.0756
          ]
        }
      }
    },
    {
      "step": 509,
      "word": "huey",
      "loss": 2.8851,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0109,
            0.0161,
            0.1564,
            -0.0453,
            0.0343,
            -0.0771,
            -0.1119,
            -0.15,
            -0.0153,
            0.0954,
            0.1593,
            0.0213,
            -0.0909,
            -0.025,
            -0.0544,
            -0.195
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.4162,
            0.0401,
            -0.015,
            0.2951,
            -0.163,
            0.5259,
            0.3265,
            0.0654,
            -0.0239,
            -0.2078,
            -0.1548,
            0.2063,
            -0.2421,
            -0.154,
            0.3128,
            0.1094
          ],
          "after": [
            0.0303,
            0.1116,
            -0.0574,
            -0.0757,
            0.0318,
            -0.0139,
            0.0929,
            0.0917,
            -0.0851,
            -0.0719,
            0.0218,
            -0.0239,
            0.1126,
            0.0269,
            -0.0613,
            0.2312
          ]
        },
        "position_0": {
          "grad": [
            0.0141,
            -0.0724,
            0.0643,
            0.0823,
            -0.1323,
            0.1486,
            -0.0692,
            0.1118,
            0.183,
            0.0436,
            -0.1327,
            -0.2406,
            0.2265,
            0.1967,
            0.0882,
            0.1417
          ],
          "after": [
            0.022,
            -0.1315,
            -0.2901,
            0.0651,
            0.0455,
            -0.0505,
            -0.141,
            0.0672,
            0.0962,
            -0.0011,
            0.0231,
            0.1066,
            0.0434,
            -0.012,
            -0.1431,
            0.0583
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0028,
            -0.0009,
            -0.0005,
            0.0017,
            -0.0014,
            0.0031,
            0.0017,
            0.0004,
            -0.0009,
            -0.0004,
            -0.0009,
            0.001,
            -0.0014,
            -0.0014,
            0.0015,
            0.001
          ],
          "after": [
            -0.0714,
            0.0024,
            -0.1028,
            -0.0822,
            0.0814,
            0.1374,
            0.0766,
            -0.1598,
            0.0241,
            0.001,
            -0.109,
            -0.0635,
            0.0954,
            -0.0793,
            0.0183,
            0.0754
          ]
        }
      }
    },
    {
      "step": 510,
      "word": "love",
      "loss": 2.5983,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0104,
            0.0159,
            0.1567,
            -0.0452,
            0.0349,
            -0.0768,
            -0.1114,
            -0.1498,
            -0.0156,
            0.0955,
            0.1592,
            0.0218,
            -0.091,
            -0.0256,
            -0.0549,
            -0.1948
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0397,
            -0.0619,
            0.0615,
            -0.2248,
            0.037,
            0.1315,
            -0.4563,
            -0.0719,
            0.218,
            -0.2079,
            0.2175,
            -0.043,
            -0.2439,
            -0.0986,
            0.3184,
            -0.1121
          ],
          "after": [
            0.0312,
            0.1114,
            -0.0576,
            -0.076,
            0.0319,
            -0.0143,
            0.0928,
            0.0918,
            -0.085,
            -0.0711,
            0.0215,
            -0.0241,
            0.1131,
            0.0273,
            -0.0616,
            0.2311
          ]
        },
        "position_0": {
          "grad": [
            -0.105,
            0.157,
            -0.1443,
            -0.0628,
            0.0161,
            0.0764,
            -0.0652,
            0.1128,
            -0.0769,
            -0.0834,
            -0.1928,
            0.2011,
            -0.0855,
            0.0863,
            0.177,
            -0.1205
          ],
          "after": [
            0.0221,
            -0.1312,
            -0.2903,
            0.065,
            0.0458,
            -0.0513,
            -0.1405,
            0.0666,
            0.096,
            -0.0011,
            0.0238,
            0.107,
            0.0433,
            -0.0127,
            -0.1438,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0006,
            0.0004,
            -0.003,
            -0.0012,
            -0.0002,
            -0.001,
            -0.0006,
            0.0005,
            -0.0037,
            0.0006,
            -0.0013,
            -0.0003,
            0.001,
            0.0013,
            -0.0014
          ],
          "after": [
            -0.0706,
            0.0026,
            -0.1031,
            -0.0818,
            0.0813,
            0.1371,
            0.0766,
            -0.1598,
            0.024,
            0.0018,
            -0.1088,
            -0.0635,
            0.0956,
            -0.0793,
            0.0179,
            0.0753
          ]
        }
      }
    },
    {
      "step": 511,
      "word": "shyenne",
      "loss": 2.3937,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0099,
            0.0158,
            0.157,
            -0.045,
            0.0355,
            -0.0766,
            -0.111,
            -0.1497,
            -0.0158,
            0.0957,
            0.1591,
            0.0223,
            -0.091,
            -0.0261,
            -0.0554,
            -0.1946
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3453,
            -0.011,
            -0.2264,
            -0.2245,
            -0.3815,
            0.1647,
            -0.4854,
            -0.0011,
            0.0854,
            -0.4476,
            -0.0016,
            -0.3845,
            0.0195,
            0.3271,
            0.4835,
            -0.3251
          ],
          "after": [
            0.0315,
            0.1112,
            -0.0575,
            -0.0758,
            0.0324,
            -0.0149,
            0.093,
            0.092,
            -0.0851,
            -0.0698,
            0.0213,
            -0.0237,
            0.1135,
            0.0271,
            -0.0623,
            0.2313
          ]
        },
        "position_0": {
          "grad": [
            -0.0054,
            0.0883,
            -0.0261,
            0.1228,
            -0.1248,
            0.0517,
            0.0103,
            0.06,
            -0.049,
            -0.0026,
            -0.107,
            0.0639,
            0.0669,
            0.0211,
            0.1193,
            0.0302
          ],
          "after": [
            0.0221,
            -0.1312,
            -0.2904,
            0.0647,
            0.0463,
            -0.0522,
            -0.1401,
            0.0659,
            0.0959,
            -0.0012,
            0.0245,
            0.1072,
            0.0432,
            -0.0133,
            -0.1446,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0003,
            -0.0001,
            0.0,
            0.0005,
            0.0002,
            0.0009,
            0.0006,
            0.0001,
            -0.0001,
            -0.0001,
            0.0012,
            -0.0004,
            -0.0011,
            0.0005,
            -0.0002
          ],
          "after": [
            -0.0698,
            0.0027,
            -0.1033,
            -0.0815,
            0.0813,
            0.1367,
            0.0766,
            -0.16,
            0.024,
            0.0024,
            -0.1085,
            -0.0637,
            0.0958,
            -0.0791,
            0.0176,
            0.0753
          ]
        }
      }
    },
    {
      "step": 512,
      "word": "claire",
      "loss": 2.3421,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0276,
            -0.2165,
            0.0572,
            -0.115,
            0.0942,
            0.0917,
            -0.1651,
            0.0473,
            0.0193,
            -0.0141,
            0.1177,
            -0.1711,
            -0.1194,
            0.0145,
            -0.0131,
            -0.1206
          ],
          "after": [
            0.0095,
            0.0161,
            0.1572,
            -0.0447,
            0.0359,
            -0.0767,
            -0.1104,
            -0.1497,
            -0.016,
            0.0958,
            0.1588,
            0.0229,
            -0.0908,
            -0.0266,
            -0.0558,
            -0.1942
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1948,
            -0.2213,
            -0.2061,
            -0.1969,
            -0.2707,
            0.1308,
            -0.329,
            -0.0214,
            0.0116,
            0.0507,
            0.0937,
            -0.3193,
            0.1354,
            0.2088,
            0.2301,
            -0.0349
          ],
          "after": [
            0.0315,
            0.1112,
            -0.0573,
            -0.0752,
            0.0331,
            -0.0155,
            0.0934,
            0.0921,
            -0.0851,
            -0.0688,
            0.0209,
            -0.023,
            0.1137,
            0.0266,
            -0.063,
            0.2315
          ]
        },
        "position_0": {
          "grad": [
            -0.0423,
            0.0547,
            -0.0177,
            0.1141,
            -0.0653,
            -0.013,
            0.134,
            0.0737,
            -0.095,
            0.0319,
            -0.0005,
            0.0641,
            0.0558,
            -0.0241,
            -0.0617,
            0.0733
          ],
          "after": [
            0.0223,
            -0.1313,
            -0.2904,
            0.0643,
            0.0468,
            -0.0529,
            -0.1398,
            0.0651,
            0.0959,
            -0.0013,
            0.0251,
            0.1072,
            0.043,
            -0.0137,
            -0.1452,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0017,
            -0.0002,
            -0.0006,
            -0.0007,
            0.0011,
            -0.0023,
            0.0014,
            0.0006,
            -0.0013,
            -0.0005,
            -0.0003,
            -0.0008,
            0.0006,
            0.0022,
            0.0004
          ],
          "after": [
            -0.069,
            0.0029,
            -0.1035,
            -0.0812,
            0.0813,
            0.1363,
            0.0767,
            -0.1603,
            0.0239,
            0.0031,
            -0.1083,
            -0.0638,
            0.096,
            -0.079,
            0.0172,
            0.0752
          ]
        }
      }
    },
    {
      "step": 513,
      "word": "kaytlynn",
      "loss": 2.4376,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.053,
            -0.1284,
            0.0641,
            -0.0891,
            0.0617,
            0.1043,
            -0.0333,
            0.0109,
            -0.0534,
            0.0911,
            0.0397,
            0.0945,
            -0.02,
            0.0324,
            -0.0103,
            -0.0083
          ],
          "after": [
            0.0091,
            0.0165,
            0.1573,
            -0.0442,
            0.0361,
            -0.077,
            -0.1099,
            -0.1498,
            -0.0161,
            0.0957,
            0.1585,
            0.0233,
            -0.0907,
            -0.0271,
            -0.0561,
            -0.194
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1727,
            0.2147,
            -0.0967,
            0.0786,
            0.0676,
            -0.2178,
            0.3981,
            0.0142,
            -0.2546,
            -0.0512,
            -0.0385,
            0.0085,
            0.117,
            -0.1457,
            -0.1678,
            0.2833
          ],
          "after": [
            0.0317,
            0.111,
            -0.057,
            -0.0749,
            0.0336,
            -0.0158,
            0.0934,
            0.0922,
            -0.0849,
            -0.0678,
            0.0207,
            -0.0224,
            0.1137,
            0.0265,
            -0.0635,
            0.2315
          ]
        },
        "position_0": {
          "grad": [
            -0.089,
            0.0678,
            -0.0758,
            0.0168,
            0.06,
            -0.0622,
            0.1676,
            -0.0192,
            -0.0904,
            0.0187,
            0.0007,
            0.0527,
            0.0574,
            -0.0637,
            -0.1055,
            0.0035
          ],
          "after": [
            0.0228,
            -0.1315,
            -0.2902,
            0.0639,
            0.0471,
            -0.0533,
            -0.1399,
            0.0645,
            0.0961,
            -0.0014,
            0.0256,
            0.1071,
            0.0428,
            -0.0139,
            -0.1454,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            0.0001,
            0.0003,
            0.0026,
            0.0005,
            0.0018,
            0.0029,
            0.0005,
            0.0,
            0.0021,
            0.0004,
            0.0012,
            0.0011,
            -0.0016,
            -0.0014,
            0.0024
          ],
          "after": [
            -0.0681,
            0.0032,
            -0.1036,
            -0.0812,
            0.0812,
            0.1358,
            0.0766,
            -0.1606,
            0.0238,
            0.0035,
            -0.1081,
            -0.064,
            0.0961,
            -0.0787,
            0.0169,
            0.0749
          ]
        }
      }
    },
    {
      "step": 514,
      "word": "cipriano",
      "loss": 2.7242,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1559,
            0.0975,
            -0.032,
            0.071,
            -0.043,
            -0.0224,
            0.0202,
            -0.0339,
            0.0753,
            -0.1868,
            -0.1226,
            0.1441,
            -0.0654,
            -0.0452,
            0.0393,
            0.178
          ],
          "after": [
            0.009,
            0.0167,
            0.1574,
            -0.044,
            0.0363,
            -0.0772,
            -0.1094,
            -0.1497,
            -0.0163,
            0.096,
            0.1584,
            0.0234,
            -0.0904,
            -0.0274,
            -0.0564,
            -0.1939
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.008,
            0.0723,
            -0.1599,
            -0.0035,
            0.0234,
            -0.1429,
            0.2375,
            0.0051,
            -0.2108,
            0.0237,
            -0.0233,
            -0.0479,
            0.1242,
            -0.087,
            -0.1782,
            0.1512
          ],
          "after": [
            0.0319,
            0.1108,
            -0.0566,
            -0.0746,
            0.0339,
            -0.0159,
            0.0933,
            0.0923,
            -0.0846,
            -0.067,
            0.0205,
            -0.0218,
            0.1136,
            0.0264,
            -0.0637,
            0.2313
          ]
        },
        "position_0": {
          "grad": [
            -0.0364,
            0.0449,
            -0.0113,
            0.1019,
            -0.0567,
            -0.0066,
            0.1001,
            0.0508,
            -0.0782,
            0.0154,
            0.0095,
            0.0451,
            0.0439,
            -0.0039,
            -0.0418,
            0.0691
          ],
          "after": [
            0.0233,
            -0.1318,
            -0.29,
            0.0634,
            0.0475,
            -0.0537,
            -0.14,
            0.0638,
            0.0964,
            -0.0016,
            0.0261,
            0.107,
            0.0425,
            -0.0141,
            -0.1456,
            0.0582
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0028,
            0.0001,
            -0.0019,
            -0.0011,
            -0.0016,
            -0.0005,
            -0.0004,
            -0.0002,
            0.0003,
            0.0005,
            -0.0007,
            -0.0036,
            0.0014,
            0.0023,
            -0.0008,
            -0.0009
          ],
          "after": [
            -0.0677,
            0.0034,
            -0.1035,
            -0.0811,
            0.0814,
            0.1354,
            0.0765,
            -0.1609,
            0.0237,
            0.0037,
            -0.1078,
            -0.0638,
            0.096,
            -0.0788,
            0.0167,
            0.0748
          ]
        }
      }
    },
    {
      "step": 515,
      "word": "rudransh",
      "loss": 2.7776,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0644,
            0.1302,
            -0.029,
            0.0449,
            -0.0018,
            -0.0524,
            0.0605,
            -0.0455,
            -0.0016,
            -0.1286,
            -0.0738,
            0.1005,
            -0.0255,
            -0.0452,
            -0.0132,
            0.1051
          ],
          "after": [
            0.009,
            0.0167,
            0.1575,
            -0.0439,
            0.0365,
            -0.0772,
            -0.1091,
            -0.1495,
            -0.0165,
            0.0965,
            0.1586,
            0.0234,
            -0.0902,
            -0.0276,
            -0.0567,
            -0.1941
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0845,
            0.1453,
            -0.1296,
            0.0639,
            0.0155,
            -0.1983,
            0.3201,
            0.0088,
            -0.1968,
            0.0041,
            -0.0429,
            0.0015,
            0.1393,
            -0.1122,
            -0.1892,
            0.1882
          ],
          "after": [
            0.0322,
            0.1104,
            -0.0561,
            -0.0744,
            0.0343,
            -0.0158,
            0.0931,
            0.0923,
            -0.0842,
            -0.0663,
            0.0205,
            -0.0213,
            0.1134,
            0.0266,
            -0.0638,
            0.231
          ]
        },
        "position_0": {
          "grad": [
            0.0089,
            0.0174,
            0.0041,
            -0.008,
            0.0447,
            0.0918,
            0.128,
            -0.0428,
            0.0242,
            -0.0441,
            -0.0498,
            0.0646,
            -0.0873,
            -0.0731,
            0.0684,
            0.1235
          ],
          "after": [
            0.0237,
            -0.1321,
            -0.2899,
            0.0629,
            0.0477,
            -0.0542,
            -0.1403,
            0.0634,
            0.0966,
            -0.0016,
            0.0265,
            0.1067,
            0.0424,
            -0.014,
            -0.1458,
            0.058
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0005,
            0.0006,
            0.0007,
            0.0013,
            -0.0003,
            0.0005,
            0.0,
            0.0001,
            -0.0001,
            0.0,
            0.0007,
            0.0002,
            -0.0002,
            -0.0014,
            0.0005
          ],
          "after": [
            -0.0675,
            0.0035,
            -0.1035,
            -0.0811,
            0.0813,
            0.1351,
            0.0764,
            -0.1612,
            0.0236,
            0.004,
            -0.1076,
            -0.0637,
            0.096,
            -0.0788,
            0.0166,
            0.0746
          ]
        }
      }
    },
    {
      "step": 516,
      "word": "akul",
      "loss": 3.0265,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3201,
            0.1471,
            0.0486,
            -0.082,
            0.1909,
            -0.2957,
            0.2894,
            0.0044,
            0.0125,
            -0.0772,
            -0.091,
            0.0762,
            0.09,
            0.0317,
            -0.2795,
            -0.1065
          ],
          "after": [
            0.0094,
            0.0164,
            0.1575,
            -0.0436,
            0.0364,
            -0.0765,
            -0.1093,
            -0.1494,
            -0.0167,
            0.097,
            0.1589,
            0.0233,
            -0.0901,
            -0.0278,
            -0.0563,
            -0.194
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0306,
            0.2211,
            -0.1671,
            -0.0196,
            -0.0082,
            -0.3268,
            0.3382,
            -0.0108,
            -0.2569,
            -0.0068,
            0.0002,
            -0.0965,
            0.1956,
            -0.0907,
            -0.1873,
            0.2781
          ],
          "after": [
            0.0325,
            0.1099,
            -0.0556,
            -0.0743,
            0.0345,
            -0.0154,
            0.0926,
            0.0924,
            -0.0836,
            -0.0658,
            0.0204,
            -0.0207,
            0.113,
            0.0268,
            -0.0637,
            0.2305
          ]
        },
        "position_0": {
          "grad": [
            0.0772,
            -0.0504,
            0.0986,
            -0.1851,
            0.0748,
            -0.0029,
            -0.3427,
            -0.1518,
            0.1655,
            0.0105,
            0.3504,
            0.0287,
            -0.1779,
            0.0199,
            -0.0509,
            -0.2464
          ],
          "after": [
            0.0238,
            -0.1322,
            -0.29,
            0.0629,
            0.0478,
            -0.0547,
            -0.1401,
            0.0635,
            0.0965,
            -0.0016,
            0.0263,
            0.1064,
            0.0426,
            -0.014,
            -0.146,
            0.0581
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0013,
            -0.0,
            -0.0006,
            0.001,
            0.0006,
            -0.0023,
            0.001,
            0.0013,
            0.0003,
            -0.0011,
            0.0006,
            -0.0005,
            0.0014,
            -0.0005,
            -0.0001
          ],
          "after": [
            -0.0673,
            0.0037,
            -0.1035,
            -0.081,
            0.0812,
            0.1348,
            0.0765,
            -0.1615,
            0.0233,
            0.0042,
            -0.1073,
            -0.0637,
            0.096,
            -0.079,
            0.0166,
            0.0744
          ]
        }
      }
    },
    {
      "step": 517,
      "word": "anahlia",
      "loss": 2.2666,
      "learning_rate": 0.0015,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0897,
            0.1698,
            -0.1875,
            0.0944,
            -0.5437,
            0.1183,
            -0.01,
            0.0159,
            0.0228,
            -0.1102,
            -0.2649,
            -0.3227,
            0.2724,
            0.2477,
            0.2626,
            0.1106
          ],
          "after": [
            0.0097,
            0.0158,
            0.1578,
            -0.0435,
            0.0371,
            -0.0763,
            -0.1094,
            -0.1493,
            -0.0169,
            0.0977,
            0.1597,
            0.0236,
            -0.0905,
            -0.0285,
            -0.0565,
            -0.1941
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1152,
            0.1452,
            -0.0847,
            0.0334,
            0.1398,
            -0.1963,
            0.3334,
            0.0012,
            -0.1705,
            -0.003,
            -0.0196,
            0.0623,
            0.0585,
            -0.1397,
            -0.2163,
            0.2088
          ],
          "after": [
            0.0329,
            0.1093,
            -0.0551,
            -0.0742,
            0.0347,
            -0.0148,
            0.092,
            0.0925,
            -0.083,
            -0.0653,
            0.0205,
            -0.0203,
            0.1126,
            0.0272,
            -0.0635,
            0.2299
          ]
        },
        "position_0": {
          "grad": [
            0.0709,
            -0.061,
            0.0453,
            -0.1615,
            -0.0093,
            -0.0214,
            -0.2593,
            -0.0754,
            0.1513,
            0.0041,
            0.1696,
            -0.0169,
            -0.1673,
            0.0074,
            -0.0095,
            -0.2032
          ],
          "after": [
            0.0236,
            -0.1322,
            -0.2903,
            0.0632,
            0.0479,
            -0.055,
            -0.1396,
            0.0637,
            0.0962,
            -0.0017,
            0.0258,
            0.1062,
            0.0429,
            -0.014,
            -0.146,
            0.0584
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0002,
            -0.0009,
            0.0014,
            -0.0022,
            -0.0016,
            0.0003,
            0.0004,
            -0.0011,
            0.0002,
            0.0006,
            -0.0001,
            0.0002,
            -0.0008,
            0.0016,
            -0.0005
          ],
          "after": [
            -0.0673,
            0.0039,
            -0.1034,
            -0.0811,
            0.0813,
            0.1346,
            0.0766,
            -0.1619,
            0.0232,
            0.0043,
            -0.1072,
            -0.0637,
            0.096,
            -0.079,
            0.0165,
            0.0744
          ]
        }
      }
    },
    {
      "step": 518,
      "word": "aashna",
      "loss": 2.4776,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2403,
            0.1791,
            0.2978,
            -0.2291,
            -0.0131,
            -0.0545,
            -0.438,
            0.0398,
            0.2074,
            0.0472,
            0.3257,
            0.0531,
            -0.1971,
            0.1573,
            -0.0765,
            -0.4626
          ],
          "after": [
            0.0095,
            0.0151,
            0.1576,
            -0.0431,
            0.0377,
            -0.0759,
            -0.1089,
            -0.1493,
            -0.0176,
            0.0982,
            0.1597,
            0.0238,
            -0.0905,
            -0.0294,
            -0.0565,
            -0.1937
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0376,
            0.0346,
            -0.0923,
            0.0205,
            0.0933,
            -0.1265,
            0.1909,
            0.013,
            -0.0974,
            0.0536,
            -0.0003,
            0.0795,
            0.0026,
            -0.1079,
            -0.151,
            0.1056
          ],
          "after": [
            0.0333,
            0.1087,
            -0.0545,
            -0.0742,
            0.0347,
            -0.0142,
            0.0914,
            0.0925,
            -0.0823,
            -0.0649,
            0.0205,
            -0.0201,
            0.1123,
            0.0277,
            -0.0632,
            0.2293
          ]
        },
        "position_0": {
          "grad": [
            0.0694,
            -0.0418,
            0.0469,
            -0.1888,
            0.0361,
            -0.0317,
            -0.267,
            -0.112,
            0.1292,
            0.0452,
            0.2108,
            0.0037,
            -0.1688,
            -0.0221,
            -0.045,
            -0.2247
          ],
          "after": [
            0.0233,
            -0.1321,
            -0.2906,
            0.0637,
            0.0479,
            -0.0552,
            -0.139,
            0.0643,
            0.0957,
            -0.0018,
            0.025,
            0.106,
            0.0435,
            -0.014,
            -0.146,
            0.059
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            0.0001,
            -0.0001,
            0.0006,
            0.0003,
            0.0002,
            0.0012,
            0.0006,
            -0.0005,
            0.0001,
            -0.0003,
            -0.0003,
            0.0006,
            -0.0008,
            0.0002,
            0.0001
          ],
          "after": [
            -0.0672,
            0.0041,
            -0.1033,
            -0.0812,
            0.0814,
            0.1345,
            0.0765,
            -0.1623,
            0.0232,
            0.0044,
            -0.107,
            -0.0637,
            0.0959,
            -0.079,
            0.0164,
            0.0743
          ]
        }
      }
    },
    {
      "step": 519,
      "word": "brodan",
      "loss": 2.0645,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0623,
            0.1382,
            -0.0379,
            0.0574,
            -0.0151,
            -0.0856,
            0.0359,
            -0.0556,
            0.0125,
            -0.1728,
            -0.0765,
            0.1071,
            -0.0662,
            -0.0492,
            -0.0085,
            0.0921
          ],
          "after": [
            0.0095,
            0.0142,
            0.1576,
            -0.0428,
            0.0382,
            -0.0754,
            -0.1086,
            -0.1492,
            -0.0181,
            0.0989,
            0.1598,
            0.0239,
            -0.0904,
            -0.03,
            -0.0566,
            -0.1934
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0685,
            0.1531,
            -0.1082,
            0.0482,
            -0.0133,
            -0.2424,
            0.2932,
            0.0147,
            -0.2586,
            0.0093,
            -0.0381,
            -0.0436,
            0.2074,
            -0.0609,
            -0.2137,
            0.1496
          ],
          "after": [
            0.0337,
            0.108,
            -0.0539,
            -0.0743,
            0.0347,
            -0.0134,
            0.0906,
            0.0924,
            -0.0816,
            -0.0647,
            0.0205,
            -0.0198,
            0.1118,
            0.0282,
            -0.0628,
            0.2286
          ]
        },
        "position_0": {
          "grad": [
            0.0463,
            0.0358,
            0.1062,
            0.0326,
            0.0254,
            -0.0157,
            0.147,
            -0.0039,
            -0.1537,
            -0.0582,
            0.0268,
            0.0134,
            0.1781,
            -0.1036,
            -0.0435,
            0.2958
          ],
          "after": [
            0.0228,
            -0.1321,
            -0.2913,
            0.0641,
            0.0478,
            -0.0553,
            -0.1385,
            0.0648,
            0.0955,
            -0.0018,
            0.0244,
            0.1058,
            0.0437,
            -0.0137,
            -0.1459,
            0.0591
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0004,
            0.0029,
            0.0002,
            -0.0009,
            0.0002,
            0.0001,
            0.0004,
            -0.0013,
            0.001,
            0.0014,
            0.0012,
            0.0016,
            0.0012,
            -0.0018,
            -0.0
          ],
          "after": [
            -0.067,
            0.0043,
            -0.1036,
            -0.0813,
            0.0815,
            0.1344,
            0.0765,
            -0.1627,
            0.0233,
            0.0044,
            -0.107,
            -0.0638,
            0.0957,
            -0.0791,
            0.0164,
            0.0742
          ]
        }
      }
    },
    {
      "step": 520,
      "word": "younis",
      "loss": 2.7354,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0095,
            0.0134,
            0.1575,
            -0.0425,
            0.0386,
            -0.0749,
            -0.1082,
            -0.1491,
            -0.0186,
            0.0995,
            0.16,
            0.0239,
            -0.0903,
            -0.0306,
            -0.0566,
            -0.1931
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0326,
            -0.0313,
            -0.0464,
            -0.011,
            0.1083,
            -0.1112,
            0.1195,
            0.0078,
            -0.1056,
            0.0757,
            0.0106,
            0.0286,
            0.04,
            -0.0806,
            -0.1585,
            0.0771
          ],
          "after": [
            0.0341,
            0.1075,
            -0.0534,
            -0.0744,
            0.0346,
            -0.0127,
            0.09,
            0.0923,
            -0.0809,
            -0.0645,
            0.0206,
            -0.0197,
            0.1113,
            0.0288,
            -0.0623,
            0.228
          ]
        },
        "position_0": {
          "grad": [
            0.0437,
            -0.166,
            0.0856,
            -0.1936,
            0.0382,
            0.1485,
            -0.1472,
            -0.016,
            0.0596,
            0.0699,
            0.0065,
            0.0222,
            -0.0865,
            0.0934,
            0.0417,
            -0.0622
          ],
          "after": [
            0.0222,
            -0.1318,
            -0.292,
            0.0648,
            0.0477,
            -0.0558,
            -0.138,
            0.0653,
            0.0953,
            -0.002,
            0.0238,
            0.1056,
            0.044,
            -0.0137,
            -0.1459,
            0.0592
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0017,
            -0.0009,
            0.0,
            0.0002,
            -0.0007,
            -0.0009,
            0.0003,
            0.0011,
            -0.0007,
            -0.0003,
            0.0022,
            -0.0016,
            0.0001,
            0.0007,
            0.0008
          ],
          "after": [
            -0.067,
            0.0043,
            -0.1037,
            -0.0815,
            0.0816,
            0.1344,
            0.0765,
            -0.1631,
            0.0233,
            0.0045,
            -0.1069,
            -0.0641,
            0.0957,
            -0.0792,
            0.0164,
            0.0741
          ]
        }
      }
    },
    {
      "step": 521,
      "word": "analyn",
      "loss": 1.747,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0069,
            0.2661,
            -0.381,
            0.0449,
            -0.5658,
            0.0615,
            -0.16,
            0.0476,
            0.0051,
            -0.253,
            -0.3151,
            0.013,
            -0.0563,
            0.2069,
            0.3641,
            -0.1517
          ],
          "after": [
            0.0095,
            0.0123,
            0.1579,
            -0.0424,
            0.0397,
            -0.0747,
            -0.1077,
            -0.1492,
            -0.019,
            0.1004,
            0.1607,
            0.0239,
            -0.0901,
            -0.0314,
            -0.0573,
            -0.1927
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0866,
            0.0842,
            -0.0698,
            0.0189,
            0.0788,
            -0.1756,
            0.2529,
            -0.0101,
            -0.1322,
            0.0353,
            0.0115,
            0.0446,
            0.0716,
            -0.1301,
            -0.1527,
            0.1496
          ],
          "after": [
            0.0345,
            0.107,
            -0.0529,
            -0.0745,
            0.0344,
            -0.0118,
            0.0892,
            0.0923,
            -0.0801,
            -0.0645,
            0.0206,
            -0.0196,
            0.1108,
            0.0294,
            -0.0618,
            0.2273
          ]
        },
        "position_0": {
          "grad": [
            0.0913,
            -0.0999,
            0.0487,
            -0.1908,
            -0.0079,
            -0.0274,
            -0.2922,
            -0.107,
            0.1982,
            0.0108,
            0.2135,
            -0.0315,
            -0.1931,
            0.0071,
            -0.0036,
            -0.2582
          ],
          "after": [
            0.0214,
            -0.1313,
            -0.2929,
            0.0658,
            0.0477,
            -0.0562,
            -0.1372,
            0.066,
            0.0948,
            -0.0022,
            0.0229,
            0.1055,
            0.0445,
            -0.0137,
            -0.1459,
            0.0597
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0003,
            0.0009,
            0.0001,
            0.0005,
            -0.0006,
            0.001,
            -0.0008,
            -0.0011,
            0.0011,
            0.0011,
            -0.0004,
            0.0006,
            -0.0008,
            -0.0009,
            -0.0011
          ],
          "after": [
            -0.0669,
            0.0043,
            -0.1039,
            -0.0816,
            0.0817,
            0.1344,
            0.0765,
            -0.1633,
            0.0235,
            0.0044,
            -0.107,
            -0.0643,
            0.0956,
            -0.0792,
            0.0164,
            0.0741
          ]
        }
      }
    },
    {
      "step": 522,
      "word": "anahla",
      "loss": 2.0771,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.134,
            0.1902,
            -0.2041,
            0.0585,
            -0.6341,
            0.1458,
            -0.0444,
            0.0495,
            0.0587,
            -0.0759,
            -0.2291,
            -0.4268,
            0.3474,
            0.3157,
            0.2468,
            0.0148
          ],
          "after": [
            0.0093,
            0.011,
            0.1586,
            -0.0424,
            0.0413,
            -0.0748,
            -0.1073,
            -0.1494,
            -0.0195,
            0.1014,
            0.1617,
            0.0245,
            -0.0905,
            -0.0327,
            -0.0584,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0955,
            0.1334,
            -0.0938,
            0.0433,
            0.1095,
            -0.2001,
            0.3241,
            0.0036,
            -0.167,
            0.0082,
            -0.0124,
            0.0568,
            0.067,
            -0.143,
            -0.2006,
            0.1936
          ],
          "after": [
            0.035,
            0.1064,
            -0.0523,
            -0.0747,
            0.0342,
            -0.0109,
            0.0884,
            0.0923,
            -0.0794,
            -0.0645,
            0.0206,
            -0.0196,
            0.1103,
            0.0302,
            -0.0612,
            0.2266
          ]
        },
        "position_0": {
          "grad": [
            0.0961,
            -0.0924,
            0.0428,
            -0.1864,
            -0.0154,
            -0.0364,
            -0.2901,
            -0.1044,
            0.1987,
            0.0028,
            0.2043,
            -0.0339,
            -0.1868,
            0.0187,
            0.0021,
            -0.2465
          ],
          "after": [
            0.0204,
            -0.1307,
            -0.2937,
            0.0669,
            0.0476,
            -0.0564,
            -0.1363,
            0.0669,
            0.094,
            -0.0023,
            0.0218,
            0.1055,
            0.0452,
            -0.0137,
            -0.1459,
            0.0604
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0004,
            -0.0008,
            -0.0002,
            -0.0005,
            -0.0004,
            0.0012,
            0.0002,
            -0.0003,
            0.0002,
            -0.0001,
            -0.0008,
            0.0006,
            -0.0003,
            0.0006,
            -0.0003
          ],
          "after": [
            -0.0668,
            0.0044,
            -0.1039,
            -0.0816,
            0.0817,
            0.1345,
            0.0763,
            -0.1635,
            0.0236,
            0.0044,
            -0.1071,
            -0.0644,
            0.0955,
            -0.0791,
            0.0164,
            0.0741
          ]
        }
      }
    },
    {
      "step": 523,
      "word": "mirren",
      "loss": 2.1004,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0092,
            0.01,
            0.1591,
            -0.0424,
            0.0427,
            -0.075,
            -0.1068,
            -0.1495,
            -0.0199,
            0.1022,
            0.1625,
            0.0249,
            -0.0908,
            -0.0338,
            -0.0593,
            -0.1922
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0539,
            -0.2591,
            0.012,
            -0.0321,
            0.0824,
            0.1371,
            -0.345,
            0.0333,
            0.0079,
            -0.0661,
            -0.0487,
            -0.0283,
            -0.0456,
            0.1986,
            0.0297,
            -0.0256
          ],
          "after": [
            0.0354,
            0.1061,
            -0.0519,
            -0.0747,
            0.0339,
            -0.0103,
            0.0879,
            0.0921,
            -0.0787,
            -0.0644,
            0.0208,
            -0.0195,
            0.11,
            0.0305,
            -0.0607,
            0.226
          ]
        },
        "position_0": {
          "grad": [
            -0.0497,
            0.0515,
            -0.0313,
            0.0401,
            0.1214,
            -0.0028,
            0.219,
            0.1093,
            0.0125,
            0.0018,
            -0.0474,
            0.1143,
            -0.0901,
            -0.0581,
            -0.0118,
            0.1177
          ],
          "after": [
            0.0198,
            -0.1303,
            -0.2943,
            0.0678,
            0.0474,
            -0.0566,
            -0.1357,
            0.0674,
            0.0934,
            -0.0025,
            0.021,
            0.1052,
            0.0459,
            -0.0136,
            -0.1459,
            0.0608
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.0004,
            0.0013,
            0.0009,
            -0.0001,
            0.0005,
            -0.0011,
            0.0004,
            0.0007,
            0.0005,
            0.0007,
            0.0014,
            -0.001,
            -0.0001,
            -0.0009,
            0.0003
          ],
          "after": [
            -0.0668,
            0.0045,
            -0.1042,
            -0.0818,
            0.0818,
            0.1345,
            0.0763,
            -0.1637,
            0.0237,
            0.0043,
            -0.1072,
            -0.0646,
            0.0955,
            -0.0791,
            0.0165,
            0.0741
          ]
        }
      }
    },
    {
      "step": 524,
      "word": "aarza",
      "loss": 2.5117,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5232,
            -0.0873,
            0.139,
            -0.4516,
            0.251,
            0.0298,
            -0.3783,
            -0.1293,
            0.3914,
            -0.0236,
            0.4694,
            -0.0582,
            -0.3928,
            0.0681,
            -0.1197,
            -0.345
          ],
          "after": [
            0.0084,
            0.0092,
            0.1594,
            -0.0416,
            0.0436,
            -0.0751,
            -0.106,
            -0.1493,
            -0.021,
            0.1029,
            0.1624,
            0.0254,
            -0.0905,
            -0.0349,
            -0.0598,
            -0.1915
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0403,
            0.0732,
            -0.1111,
            0.0409,
            0.0213,
            -0.1425,
            0.1918,
            -0.0087,
            -0.1453,
            0.0529,
            -0.0079,
            0.0003,
            0.0995,
            -0.0732,
            -0.1802,
            0.0843
          ],
          "after": [
            0.0357,
            0.1059,
            -0.0514,
            -0.0749,
            0.0337,
            -0.0096,
            0.0873,
            0.092,
            -0.0781,
            -0.0643,
            0.0209,
            -0.0195,
            0.1096,
            0.0309,
            -0.0602,
            0.2254
          ]
        },
        "position_0": {
          "grad": [
            0.0816,
            -0.0635,
            0.0407,
            -0.2612,
            0.0431,
            -0.0434,
            -0.2981,
            -0.1679,
            0.1478,
            0.0863,
            0.2848,
            0.0069,
            -0.186,
            -0.0414,
            -0.0641,
            -0.2678
          ],
          "after": [
            0.0189,
            -0.1298,
            -0.2949,
            0.0689,
            0.047,
            -0.0566,
            -0.1349,
            0.0683,
            0.0926,
            -0.0028,
            0.0198,
            0.105,
            0.0467,
            -0.0134,
            -0.1457,
            0.0615
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.0007,
            -0.0003,
            0.0005,
            -0.0019,
            -0.0006,
            0.0011,
            -0.0012,
            -0.0003,
            0.0011,
            0.0005,
            -0.0013,
            0.0013,
            0.0008,
            -0.0005,
            -0.0005
          ],
          "after": [
            -0.067,
            0.0045,
            -0.1043,
            -0.082,
            0.0821,
            0.1346,
            0.0762,
            -0.1637,
            0.0238,
            0.0041,
            -0.1073,
            -0.0647,
            0.0954,
            -0.0791,
            0.0166,
            0.0742
          ]
        }
      }
    },
    {
      "step": 525,
      "word": "denni",
      "loss": 2.3126,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0077,
            0.0086,
            0.1597,
            -0.0409,
            0.0443,
            -0.0753,
            -0.1052,
            -0.1491,
            -0.022,
            0.1036,
            0.1623,
            0.0258,
            -0.0903,
            -0.0358,
            -0.0603,
            -0.191
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0835,
            -0.3544,
            0.2616,
            -0.204,
            0.3077,
            0.1092,
            -0.4617,
            -0.0754,
            0.4778,
            0.3075,
            0.1853,
            0.3157,
            -0.3614,
            -0.0296,
            -0.0486,
            -0.1622
          ],
          "after": [
            0.0359,
            0.106,
            -0.0513,
            -0.0746,
            0.0332,
            -0.0091,
            0.0871,
            0.0922,
            -0.0779,
            -0.0648,
            0.0206,
            -0.0199,
            0.1096,
            0.0313,
            -0.0596,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            -0.0586,
            0.0187,
            -0.0254,
            -0.0007,
            0.0475,
            -0.1135,
            0.0859,
            0.0224,
            0.0201,
            -0.1354,
            -0.1763,
            -0.0423,
            0.0579,
            0.1067,
            0.1324,
            0.1886
          ],
          "after": [
            0.0184,
            -0.1294,
            -0.2954,
            0.0699,
            0.0467,
            -0.0563,
            -0.1343,
            0.0689,
            0.0919,
            -0.0028,
            0.0191,
            0.1049,
            0.0473,
            -0.0135,
            -0.1459,
            0.0619
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0008,
            0.0012,
            -0.0024,
            0.0049,
            0.0004,
            0.0014,
            -0.0009,
            0.0003,
            0.0012,
            0.0003,
            -0.0002,
            0.0004,
            -0.0,
            -0.0043,
            -0.0
          ],
          "after": [
            -0.0671,
            0.0046,
            -0.1046,
            -0.0819,
            0.0818,
            0.1346,
            0.076,
            -0.1636,
            0.0238,
            0.0039,
            -0.1075,
            -0.0647,
            0.0953,
            -0.0792,
            0.017,
            0.0743
          ]
        }
      }
    },
    {
      "step": 526,
      "word": "kabeer",
      "loss": 2.4621,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0393,
            0.0288,
            0.3018,
            -0.023,
            0.1616,
            0.01,
            0.1408,
            0.0236,
            -0.1716,
            0.0083,
            0.0615,
            0.0782,
            0.1282,
            -0.2192,
            -0.1568,
            0.266
          ],
          "after": [
            0.0072,
            0.008,
            0.1595,
            -0.0403,
            0.0448,
            -0.0754,
            -0.1048,
            -0.149,
            -0.0224,
            0.1041,
            0.1621,
            0.026,
            -0.0903,
            -0.0362,
            -0.0604,
            -0.1909
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0217,
            -0.0527,
            -0.3189,
            -0.1952,
            -0.2446,
            0.5426,
            -0.0969,
            0.0391,
            0.3756,
            -0.5084,
            -0.2926,
            0.0104,
            -0.2637,
            0.0596,
            0.8673,
            -0.1028
          ],
          "after": [
            0.0361,
            0.1062,
            -0.0508,
            -0.0739,
            0.033,
            -0.0093,
            0.0871,
            0.0922,
            -0.0781,
            -0.0644,
            0.021,
            -0.0202,
            0.1099,
            0.0316,
            -0.0599,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            -0.1009,
            0.1116,
            -0.0948,
            0.0081,
            0.1063,
            -0.1045,
            0.222,
            -0.0205,
            -0.1205,
            0.0246,
            -0.0316,
            0.0503,
            0.107,
            -0.0718,
            -0.1875,
            0.0187
          ],
          "after": [
            0.0183,
            -0.1293,
            -0.2955,
            0.0708,
            0.0462,
            -0.0557,
            -0.134,
            0.0695,
            0.0915,
            -0.0028,
            0.0186,
            0.1047,
            0.0477,
            -0.0134,
            -0.1456,
            0.0622
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0008,
            -0.0009,
            -0.0022,
            0.0003,
            -0.0001,
            -0.0003,
            -0.001,
            0.0008,
            -0.0024,
            -0.0011,
            -0.0002,
            -0.0019,
            -0.0003,
            0.0022,
            -0.0015
          ],
          "after": [
            -0.0672,
            0.0047,
            -0.1047,
            -0.0816,
            0.0815,
            0.1346,
            0.0759,
            -0.1633,
            0.0238,
            0.0039,
            -0.1075,
            -0.0647,
            0.0953,
            -0.0792,
            0.0172,
            0.0745
          ]
        }
      }
    },
    {
      "step": 527,
      "word": "conan",
      "loss": 1.7708,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.004,
            0.1321,
            -0.1935,
            0.0183,
            -0.1707,
            -0.1507,
            0.0735,
            -0.0638,
            0.0493,
            0.0002,
            -0.0192,
            0.0086,
            -0.0369,
            -0.0341,
            0.07,
            0.0268
          ],
          "after": [
            0.0067,
            0.0073,
            0.1596,
            -0.0398,
            0.0453,
            -0.0752,
            -0.1045,
            -0.1487,
            -0.0229,
            0.1045,
            0.162,
            0.0262,
            -0.0902,
            -0.0364,
            -0.0606,
            -0.1908
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0029,
            0.0077,
            -0.1614,
            0.0236,
            0.0124,
            -0.104,
            0.1969,
            0.0336,
            -0.2069,
            0.0591,
            -0.0171,
            -0.0349,
            0.1383,
            -0.0855,
            -0.1472,
            0.142
          ],
          "after": [
            0.0363,
            0.1064,
            -0.0503,
            -0.0734,
            0.0328,
            -0.0093,
            0.0869,
            0.0921,
            -0.0781,
            -0.0642,
            0.0213,
            -0.0205,
            0.11,
            0.0319,
            -0.0599,
            0.2246
          ]
        },
        "position_0": {
          "grad": [
            -0.0317,
            0.0246,
            -0.0175,
            0.1653,
            -0.1593,
            -0.0371,
            0.1053,
            0.1188,
            -0.0441,
            -0.0523,
            -0.044,
            -0.0215,
            0.0508,
            0.0585,
            0.0053,
            0.0803
          ],
          "after": [
            0.0183,
            -0.1293,
            -0.2955,
            0.0712,
            0.0461,
            -0.0552,
            -0.1339,
            0.0697,
            0.0912,
            -0.0026,
            0.0182,
            0.1046,
            0.048,
            -0.0135,
            -0.1454,
            0.0623
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            -0.0002,
            0.0004,
            -0.0004,
            -0.0002,
            -0.0003,
            -0.0004,
            -0.0002,
            0.0002,
            0.0003,
            0.0003,
            -0.0002,
            0.0,
            0.0003,
            -0.0004,
            -0.0006
          ],
          "after": [
            -0.0672,
            0.0049,
            -0.1049,
            -0.0812,
            0.0814,
            0.1347,
            0.0758,
            -0.163,
            0.0237,
            0.0039,
            -0.1076,
            -0.0647,
            0.0953,
            -0.0792,
            0.0174,
            0.0747
          ]
        }
      }
    },
    {
      "step": 528,
      "word": "vaeda",
      "loss": 2.4612,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2493,
            -0.2338,
            0.0011,
            0.0489,
            -0.1204,
            0.1364,
            -0.2133,
            -0.0046,
            -0.0015,
            0.3328,
            0.1411,
            -0.257,
            -0.0948,
            0.0851,
            0.0507,
            -0.5193
          ],
          "after": [
            0.006,
            0.007,
            0.1596,
            -0.0395,
            0.046,
            -0.0753,
            -0.104,
            -0.1485,
            -0.0233,
            0.1043,
            0.1616,
            0.0267,
            -0.09,
            -0.0368,
            -0.0609,
            -0.1901
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1431,
            0.1495,
            -0.2235,
            0.2686,
            -0.1691,
            0.4545,
            0.3479,
            0.2481,
            0.0392,
            -0.2072,
            -0.2279,
            0.0687,
            0.1015,
            -0.0435,
            0.3483,
            0.303
          ],
          "after": [
            0.0366,
            0.1063,
            -0.0496,
            -0.0735,
            0.0328,
            -0.0098,
            0.0865,
            0.0912,
            -0.0781,
            -0.0637,
            0.0221,
            -0.0208,
            0.11,
            0.0323,
            -0.0603,
            0.224
          ]
        },
        "position_0": {
          "grad": [
            -0.0795,
            0.0939,
            0.0154,
            0.0653,
            0.142,
            0.0583,
            0.0434,
            0.0349,
            -0.1154,
            -0.0123,
            -0.0584,
            -0.0425,
            0.0672,
            -0.088,
            -0.1006,
            0.1892
          ],
          "after": [
            0.0186,
            -0.1295,
            -0.2956,
            0.0715,
            0.0457,
            -0.0548,
            -0.1338,
            0.0698,
            0.0912,
            -0.0025,
            0.018,
            0.1046,
            0.0482,
            -0.0134,
            -0.145,
            0.0622
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.001,
            -0.0011,
            0.0018,
            -0.0019,
            0.0035,
            -0.001,
            0.0022,
            0.0012,
            -0.0001,
            -0.0008,
            0.0004,
            -0.0007,
            0.0007,
            0.0009,
            0.0032
          ],
          "after": [
            -0.0672,
            0.0051,
            -0.1049,
            -0.0812,
            0.0814,
            0.1344,
            0.0758,
            -0.1631,
            0.0234,
            0.0039,
            -0.1075,
            -0.0647,
            0.0954,
            -0.0793,
            0.0175,
            0.0746
          ]
        }
      }
    },
    {
      "step": 529,
      "word": "maryjane",
      "loss": 2.3286,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1334,
            0.0877,
            -0.0106,
            -0.0091,
            0.1293,
            0.0312,
            0.1456,
            -0.1693,
            0.116,
            -0.1536,
            -0.0618,
            0.2556,
            -0.1585,
            -0.2036,
            -0.0002,
            0.2586
          ],
          "after": [
            0.0056,
            0.0067,
            0.1597,
            -0.0392,
            0.0463,
            -0.0755,
            -0.1038,
            -0.1478,
            -0.0239,
            0.1044,
            0.1614,
            0.0268,
            -0.0896,
            -0.0367,
            -0.0611,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2784,
            0.0938,
            -0.193,
            -0.0617,
            -0.4216,
            0.1319,
            -0.2201,
            -0.0026,
            -0.0425,
            0.002,
            -0.0559,
            -0.4212,
            0.25,
            0.2407,
            0.1013,
            -0.1675
          ],
          "after": [
            0.0365,
            0.1062,
            -0.0489,
            -0.0735,
            0.0332,
            -0.0103,
            0.0863,
            0.0904,
            -0.078,
            -0.0633,
            0.0228,
            -0.0205,
            0.1097,
            0.0322,
            -0.0606,
            0.2237
          ]
        },
        "position_0": {
          "grad": [
            -0.0559,
            0.0893,
            -0.0226,
            0.033,
            0.1606,
            -0.0157,
            0.2218,
            0.0607,
            -0.0491,
            0.0525,
            -0.0174,
            0.1428,
            -0.0417,
            -0.0907,
            -0.0712,
            0.1187
          ],
          "after": [
            0.019,
            -0.1298,
            -0.2956,
            0.0716,
            0.0451,
            -0.0545,
            -0.134,
            0.0697,
            0.0912,
            -0.0025,
            0.0178,
            0.1043,
            0.0483,
            -0.013,
            -0.1445,
            0.062
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0014,
            0.0001,
            -0.0001,
            0.0001,
            0.0002,
            0.0004,
            -0.0,
            -0.0001,
            0.0045,
            0.0018,
            -0.0013,
            0.0004,
            -0.0024,
            -0.0002,
            -0.0005
          ],
          "after": [
            -0.067,
            0.0055,
            -0.1049,
            -0.0811,
            0.0814,
            0.1341,
            0.0757,
            -0.1632,
            0.0232,
            0.0035,
            -0.1077,
            -0.0646,
            0.0955,
            -0.0792,
            0.0175,
            0.0745
          ]
        }
      }
    },
    {
      "step": 530,
      "word": "melana",
      "loss": 1.5718,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1075,
            0.1287,
            -0.1554,
            -0.0875,
            -0.1286,
            -0.1341,
            0.0554,
            -0.0685,
            -0.0317,
            0.1163,
            0.0442,
            -0.0894,
            0.0392,
            0.0065,
            0.0048,
            -0.0981
          ],
          "after": [
            0.0051,
            0.0062,
            0.16,
            -0.0388,
            0.0468,
            -0.0754,
            -0.1037,
            -0.1471,
            -0.0244,
            0.1043,
            0.1611,
            0.027,
            -0.0893,
            -0.0367,
            -0.0613,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3014,
            -0.2204,
            0.2905,
            -0.0582,
            0.222,
            0.0847,
            -0.3579,
            -0.1086,
            0.3951,
            0.2042,
            0.2555,
            0.1808,
            -0.3308,
            -0.0618,
            -0.0955,
            -0.2153
          ],
          "after": [
            0.0361,
            0.1063,
            -0.0486,
            -0.0734,
            0.0333,
            -0.0108,
            0.0864,
            0.0901,
            -0.0783,
            -0.0632,
            0.0229,
            -0.0205,
            0.1098,
            0.0323,
            -0.0609,
            0.2237
          ]
        },
        "position_0": {
          "grad": [
            -0.0606,
            0.0711,
            -0.0289,
            0.0348,
            0.1655,
            -0.0165,
            0.2217,
            0.0983,
            -0.002,
            0.018,
            -0.044,
            0.1334,
            -0.0686,
            -0.0724,
            -0.0486,
            0.1228
          ],
          "after": [
            0.0196,
            -0.1302,
            -0.2955,
            0.0717,
            0.0443,
            -0.0542,
            -0.1345,
            0.0693,
            0.0912,
            -0.0026,
            0.0177,
            0.1037,
            0.0486,
            -0.0124,
            -0.144,
            0.0617
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.0003,
            0.0012,
            -0.0014,
            -0.0005,
            -0.0006,
            -0.0025,
            -0.0012,
            0.0011,
            0.0004,
            0.0012,
            -0.0007,
            -0.0011,
            0.0008,
            -0.0001,
            -0.0014
          ],
          "after": [
            -0.067,
            0.0058,
            -0.1051,
            -0.0809,
            0.0814,
            0.1339,
            0.0759,
            -0.1631,
            0.0229,
            0.0031,
            -0.1079,
            -0.0644,
            0.0956,
            -0.0791,
            0.0176,
            0.0746
          ]
        }
      }
    },
    {
      "step": 531,
      "word": "jeziel",
      "loss": 2.4386,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0047,
            0.0058,
            0.1602,
            -0.0384,
            0.0472,
            -0.0752,
            -0.1035,
            -0.1464,
            -0.0247,
            0.1041,
            0.1609,
            0.0272,
            -0.0891,
            -0.0366,
            -0.0615,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3353,
            -0.1907,
            0.2308,
            0.232,
            -0.2838,
            0.1707,
            -0.743,
            -0.0983,
            0.5417,
            -0.0383,
            0.1649,
            0.1975,
            -0.2644,
            0.0903,
            0.287,
            -0.2805
          ],
          "after": [
            0.0353,
            0.1066,
            -0.0485,
            -0.0737,
            0.0337,
            -0.0114,
            0.0869,
            0.0902,
            -0.079,
            -0.0631,
            0.0227,
            -0.0207,
            0.1102,
            0.0322,
            -0.0613,
            0.2239
          ]
        },
        "position_0": {
          "grad": [
            -0.0272,
            0.1239,
            0.0522,
            0.2103,
            0.0793,
            -0.1117,
            0.2359,
            0.0862,
            -0.2808,
            -0.0671,
            0.0003,
            0.0828,
            0.3601,
            -0.1439,
            -0.2015,
            0.1933
          ],
          "after": [
            0.0202,
            -0.1308,
            -0.2956,
            0.0714,
            0.0435,
            -0.0536,
            -0.1351,
            0.0687,
            0.0917,
            -0.0024,
            0.0177,
            0.1031,
            0.0483,
            -0.0116,
            -0.1432,
            0.0611
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0001,
            -0.0003,
            0.0004,
            -0.0013,
            0.0006,
            -0.0012,
            0.0,
            0.0005,
            -0.0002,
            -0.0002,
            0.0011,
            -0.0014,
            0.0002,
            0.0019,
            -0.0002
          ],
          "after": [
            -0.067,
            0.0061,
            -0.1052,
            -0.0807,
            0.0816,
            0.1337,
            0.0761,
            -0.163,
            0.0225,
            0.0028,
            -0.1082,
            -0.0644,
            0.0958,
            -0.0791,
            0.0175,
            0.0747
          ]
        }
      }
    },
    {
      "step": 532,
      "word": "pharrah",
      "loss": 2.4891,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1493,
            -0.1165,
            0.1967,
            0.1281,
            0.2786,
            0.2317,
            0.1741,
            0.0525,
            0.1569,
            0.0136,
            -0.0614,
            0.1442,
            0.0089,
            -0.2012,
            -0.1137,
            0.3277
          ],
          "after": [
            0.0045,
            0.0056,
            0.1602,
            -0.0384,
            0.0472,
            -0.0756,
            -0.1037,
            -0.146,
            -0.0254,
            0.104,
            0.1609,
            0.0271,
            -0.0889,
            -0.0362,
            -0.0614,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0591,
            0.1205,
            -0.1628,
            0.0187,
            0.0078,
            -0.2257,
            0.3456,
            0.0111,
            -0.2327,
            -0.0172,
            -0.0608,
            -0.0117,
            0.1398,
            -0.1075,
            -0.1973,
            0.1799
          ],
          "after": [
            0.0347,
            0.1068,
            -0.0483,
            -0.0741,
            0.034,
            -0.0117,
            0.0871,
            0.0903,
            -0.0794,
            -0.063,
            0.0226,
            -0.0209,
            0.1104,
            0.0323,
            -0.0615,
            0.2239
          ]
        },
        "position_0": {
          "grad": [
            0.0686,
            0.1875,
            -0.0481,
            0.1255,
            -0.0056,
            -0.1308,
            0.348,
            0.0503,
            -0.3007,
            -0.0223,
            -0.0704,
            0.0253,
            0.0738,
            0.0777,
            -0.0444,
            0.2592
          ],
          "after": [
            0.0204,
            -0.1317,
            -0.2955,
            0.0709,
            0.0428,
            -0.0528,
            -0.136,
            0.0681,
            0.0925,
            -0.0023,
            0.0177,
            0.1025,
            0.048,
            -0.0111,
            -0.1424,
            0.0604
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.001,
            -0.0002,
            0.0002,
            -0.0006,
            -0.0013,
            0.0023,
            -0.0002,
            -0.0009,
            0.0003,
            0.0007,
            0.0005,
            0.0004,
            -0.0013,
            -0.0008,
            -0.0003
          ],
          "after": [
            -0.067,
            0.0063,
            -0.1052,
            -0.0807,
            0.0818,
            0.1337,
            0.0761,
            -0.1629,
            0.0224,
            0.0025,
            -0.1084,
            -0.0644,
            0.096,
            -0.0789,
            0.0175,
            0.0748
          ]
        }
      }
    },
    {
      "step": 533,
      "word": "vedh",
      "loss": 3.3772,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0044,
            0.0055,
            0.1601,
            -0.0383,
            0.0473,
            -0.076,
            -0.1038,
            -0.1457,
            -0.0259,
            0.1039,
            0.1608,
            0.0271,
            -0.0887,
            -0.0359,
            -0.0614,
            -0.1894
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2346,
            -0.1669,
            0.226,
            -0.1047,
            0.3944,
            0.1891,
            -0.642,
            -0.1037,
            0.5976,
            0.185,
            0.276,
            0.2068,
            -0.4726,
            -0.0381,
            -0.0955,
            -0.2194
          ],
          "after": [
            0.0338,
            0.1071,
            -0.0483,
            -0.0741,
            0.0339,
            -0.0122,
            0.0877,
            0.0906,
            -0.0802,
            -0.0631,
            0.0221,
            -0.0213,
            0.111,
            0.0324,
            -0.0616,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            -0.1243,
            0.0833,
            0.0335,
            0.1571,
            0.115,
            0.1015,
            0.0595,
            0.0597,
            -0.124,
            -0.0757,
            -0.0374,
            -0.0587,
            0.073,
            -0.0473,
            -0.013,
            0.2709
          ],
          "after": [
            0.021,
            -0.1326,
            -0.2956,
            0.0703,
            0.042,
            -0.0523,
            -0.1368,
            0.0674,
            0.0934,
            -0.0019,
            0.0179,
            0.1021,
            0.0476,
            -0.0106,
            -0.1417,
            0.0594
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0013,
            0.0008,
            0.0006,
            -0.0003,
            0.0015,
            -0.0005,
            -0.0018,
            -0.0003,
            0.0013,
            0.0005,
            0.0011,
            -0.0005,
            -0.0005,
            0.0003,
            -0.0021,
            -0.0005
          ],
          "after": [
            -0.0671,
            0.0063,
            -0.1054,
            -0.0805,
            0.0818,
            0.1337,
            0.0763,
            -0.1628,
            0.022,
            0.0022,
            -0.1088,
            -0.0644,
            0.0962,
            -0.0788,
            0.0177,
            0.0749
          ]
        }
      }
    },
    {
      "step": 534,
      "word": "leola",
      "loss": 2.1898,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.293,
            -0.0557,
            -0.1002,
            -0.0542,
            -0.047,
            -0.0124,
            -0.0275,
            0.0082,
            -0.1119,
            0.1988,
            0.1,
            -0.2632,
            0.1352,
            0.1083,
            -0.0012,
            -0.2032
          ],
          "after": [
            0.0039,
            0.0055,
            0.1602,
            -0.0381,
            0.0473,
            -0.0763,
            -0.1039,
            -0.1454,
            -0.0261,
            0.1035,
            0.1606,
            0.0274,
            -0.0888,
            -0.0358,
            -0.0613,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1718,
            -0.3227,
            0.2544,
            -0.066,
            0.3377,
            0.1045,
            -0.4758,
            -0.0022,
            0.3975,
            0.2427,
            0.0887,
            0.2198,
            -0.2824,
            0.0267,
            -0.0933,
            -0.3179
          ],
          "after": [
            0.0329,
            0.1077,
            -0.0485,
            -0.0741,
            0.0335,
            -0.0127,
            0.0885,
            0.0909,
            -0.0812,
            -0.0636,
            0.0214,
            -0.022,
            0.1118,
            0.0324,
            -0.0616,
            0.2245
          ]
        },
        "position_0": {
          "grad": [
            -0.0871,
            0.1161,
            -0.1294,
            -0.0926,
            0.0214,
            0.0433,
            -0.0762,
            0.0867,
            -0.0493,
            -0.034,
            -0.1801,
            0.1642,
            -0.0846,
            0.034,
            0.1011,
            -0.1439
          ],
          "after": [
            0.0218,
            -0.1336,
            -0.2952,
            0.0699,
            0.0412,
            -0.0521,
            -0.1374,
            0.0666,
            0.0943,
            -0.0015,
            0.0183,
            0.1015,
            0.0474,
            -0.0102,
            -0.1413,
            0.0588
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.001,
            -0.0001,
            -0.0006,
            0.0005,
            0.0009,
            -0.0016,
            0.0005,
            0.0009,
            0.0006,
            -0.0008,
            0.0011,
            -0.0013,
            0.0006,
            0.0005,
            -0.0008
          ],
          "after": [
            -0.0672,
            0.0064,
            -0.1054,
            -0.0804,
            0.0818,
            0.1336,
            0.0766,
            -0.1627,
            0.0216,
            0.0019,
            -0.109,
            -0.0645,
            0.0965,
            -0.0788,
            0.0178,
            0.0751
          ]
        }
      }
    },
    {
      "step": 535,
      "word": "theo",
      "loss": 3.1363,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0035,
            0.0054,
            0.1603,
            -0.038,
            0.0474,
            -0.0765,
            -0.1039,
            -0.1452,
            -0.0263,
            0.1031,
            0.1604,
            0.0276,
            -0.0888,
            -0.0357,
            -0.0613,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.028,
            -0.0164,
            0.2911,
            0.2318,
            0.0168,
            0.2994,
            -0.3203,
            -0.0322,
            0.1101,
            0.1663,
            0.3108,
            0.2141,
            -0.2801,
            -0.1613,
            0.1911,
            -0.2862
          ],
          "after": [
            0.0322,
            0.1082,
            -0.049,
            -0.0745,
            0.0332,
            -0.0134,
            0.0893,
            0.0913,
            -0.0821,
            -0.0643,
            0.0203,
            -0.0228,
            0.1127,
            0.0327,
            -0.0617,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            -0.0424,
            -0.1946,
            0.0833,
            0.1637,
            -0.178,
            0.0771,
            0.0851,
            0.0691,
            -0.1318,
            -0.2514,
            -0.1444,
            -0.3536,
            0.1883,
            0.0859,
            0.1782,
            0.2627
          ],
          "after": [
            0.0226,
            -0.1341,
            -0.2952,
            0.0692,
            0.041,
            -0.0521,
            -0.1381,
            0.0657,
            0.0952,
            -0.0006,
            0.0188,
            0.1016,
            0.047,
            -0.0101,
            -0.1414,
            0.0579
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            0.0012,
            -0.0003,
            0.001,
            -0.0005,
            0.0012,
            0.0003,
            -0.0003,
            -0.0002,
            -0.001,
            -0.0001,
            -0.0,
            -0.0002,
            -0.0003,
            0.002,
            -0.0006
          ],
          "after": [
            -0.0672,
            0.0064,
            -0.1055,
            -0.0803,
            0.0818,
            0.1334,
            0.0768,
            -0.1626,
            0.0213,
            0.0018,
            -0.1091,
            -0.0646,
            0.0967,
            -0.0787,
            0.0177,
            0.0754
          ]
        }
      }
    },
    {
      "step": 536,
      "word": "zaron",
      "loss": 1.8476,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0958,
            -0.123,
            -0.0607,
            -0.1116,
            0.1245,
            0.0219,
            0.0577,
            -0.1879,
            0.1433,
            -0.0666,
            0.0755,
            0.0744,
            -0.1492,
            -0.1268,
            0.0468,
            0.0923
          ],
          "after": [
            0.003,
            0.0056,
            0.1604,
            -0.0377,
            0.0473,
            -0.0768,
            -0.1041,
            -0.1445,
            -0.0268,
            0.1029,
            0.1601,
            0.0277,
            -0.0887,
            -0.0354,
            -0.0613,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0814,
            0.0723,
            -0.1609,
            0.0471,
            -0.0155,
            -0.1551,
            0.2703,
            0.0293,
            -0.2409,
            0.0207,
            -0.0533,
            -0.0294,
            0.1639,
            -0.0723,
            -0.1928,
            0.1394
          ],
          "after": [
            0.0317,
            0.1086,
            -0.0493,
            -0.0749,
            0.0329,
            -0.0138,
            0.0899,
            0.0915,
            -0.0827,
            -0.0649,
            0.0195,
            -0.0234,
            0.1134,
            0.0331,
            -0.0617,
            0.2255
          ]
        },
        "position_0": {
          "grad": [
            0.0211,
            0.0917,
            -0.0182,
            0.2201,
            0.0952,
            -0.0756,
            0.2711,
            0.0258,
            -0.0451,
            0.0506,
            0.0075,
            0.1021,
            0.0453,
            -0.0646,
            -0.0339,
            0.0975
          ],
          "after": [
            0.0233,
            -0.1346,
            -0.2951,
            0.0684,
            0.0405,
            -0.0519,
            -0.1389,
            0.0648,
            0.096,
            0.0001,
            0.0193,
            0.1016,
            0.0465,
            -0.0099,
            -0.1413,
            0.0571
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0004,
            0.0005,
            -0.0003,
            0.0013,
            -0.0002,
            0.0003,
            0.0004,
            0.0002,
            0.0001,
            0.0001,
            0.0009,
            -0.0004,
            -0.0009,
            -0.0002,
            -0.0
          ],
          "after": [
            -0.0671,
            0.0065,
            -0.1056,
            -0.0803,
            0.0817,
            0.1332,
            0.0769,
            -0.1626,
            0.0209,
            0.0016,
            -0.1093,
            -0.0647,
            0.097,
            -0.0786,
            0.0177,
            0.0756
          ]
        }
      }
    },
    {
      "step": 537,
      "word": "kalena",
      "loss": 1.5779,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0065,
            0.128,
            -0.1083,
            -0.1681,
            0.0366,
            -0.0544,
            -0.1345,
            0.0775,
            -0.1109,
            0.0836,
            -0.0291,
            0.1252,
            -0.0518,
            0.0295,
            0.0293,
            -0.3583
          ],
          "after": [
            0.0026,
            0.0056,
            0.1607,
            -0.0372,
            0.0471,
            -0.0769,
            -0.104,
            -0.1441,
            -0.0269,
            0.1026,
            0.1599,
            0.0277,
            -0.0884,
            -0.0352,
            -0.0614,
            -0.1888
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0552,
            -0.2428,
            0.0931,
            -0.1548,
            0.1486,
            0.0356,
            -0.2564,
            0.0903,
            0.1116,
            -0.0128,
            -0.0446,
            0.053,
            -0.1318,
            -0.0283,
            0.1932,
            -0.2038
          ],
          "after": [
            0.0313,
            0.1092,
            -0.0496,
            -0.0749,
            0.0325,
            -0.0142,
            0.0905,
            0.0914,
            -0.0833,
            -0.0654,
            0.0189,
            -0.024,
            0.114,
            0.0334,
            -0.0619,
            0.226
          ]
        },
        "position_0": {
          "grad": [
            -0.0927,
            0.0618,
            -0.0981,
            -0.0096,
            0.0813,
            -0.0959,
            0.175,
            -0.0243,
            -0.0673,
            0.0244,
            -0.0173,
            0.0426,
            0.0639,
            -0.0683,
            -0.1426,
            -0.0264
          ],
          "after": [
            0.0241,
            -0.1352,
            -0.2947,
            0.0676,
            0.04,
            -0.0514,
            -0.1398,
            0.0642,
            0.0969,
            0.0006,
            0.0197,
            0.1014,
            0.0461,
            -0.0095,
            -0.141,
            0.0564
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0009,
            0.0005,
            -0.0011,
            -0.0006,
            0.0001,
            -0.0027,
            -0.0002,
            0.0007,
            -0.0001,
            0.0003,
            -0.001,
            -0.0008,
            0.0007,
            0.0009,
            -0.0013
          ],
          "after": [
            -0.067,
            0.0066,
            -0.1057,
            -0.0801,
            0.0817,
            0.1331,
            0.0772,
            -0.1626,
            0.0206,
            0.0015,
            -0.1094,
            -0.0648,
            0.0973,
            -0.0785,
            0.0176,
            0.0759
          ]
        }
      }
    },
    {
      "step": 538,
      "word": "tamiya",
      "loss": 2.285,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0794,
            0.0899,
            0.1763,
            -0.1052,
            0.2775,
            -0.0484,
            0.22,
            0.0769,
            0.0121,
            0.1222,
            0.1774,
            0.0953,
            -0.1189,
            -0.1195,
            -0.1304,
            -0.1013
          ],
          "after": [
            0.0023,
            0.0054,
            0.1607,
            -0.0365,
            0.0467,
            -0.0768,
            -0.1042,
            -0.144,
            -0.0271,
            0.1021,
            0.1594,
            0.0275,
            -0.0881,
            -0.0348,
            -0.0613,
            -0.1883
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0827,
            0.0783,
            -0.1172,
            0.0517,
            0.0561,
            -0.1264,
            0.2016,
            -0.0004,
            -0.1917,
            0.048,
            -0.0289,
            -0.0379,
            0.1317,
            -0.0441,
            -0.2052,
            0.1141
          ],
          "after": [
            0.0311,
            0.1096,
            -0.0498,
            -0.0751,
            0.0322,
            -0.0144,
            0.0909,
            0.0914,
            -0.0837,
            -0.0658,
            0.0184,
            -0.0245,
            0.1145,
            0.0337,
            -0.0618,
            0.2264
          ]
        },
        "position_0": {
          "grad": [
            -0.0259,
            -0.1069,
            0.0451,
            0.0639,
            -0.0452,
            0.0382,
            0.0932,
            0.0092,
            -0.1366,
            -0.0784,
            -0.0826,
            -0.1792,
            0.1391,
            -0.0273,
            0.0265,
            0.1531
          ],
          "after": [
            0.0249,
            -0.1355,
            -0.2945,
            0.0669,
            0.0397,
            -0.0511,
            -0.1406,
            0.0636,
            0.0978,
            0.0013,
            0.0202,
            0.1017,
            0.0455,
            -0.0091,
            -0.1408,
            0.0556
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            0.001,
            -0.0008,
            -0.0013,
            -0.0023,
            -0.0011,
            -0.0017,
            -0.0006,
            0.0012,
            -0.001,
            0.0002,
            -0.0007,
            -0.0007,
            0.0018,
            0.0015,
            -0.0011
          ],
          "after": [
            -0.067,
            0.0066,
            -0.1057,
            -0.0798,
            0.0819,
            0.1331,
            0.0777,
            -0.1625,
            0.0201,
            0.0015,
            -0.1096,
            -0.0647,
            0.0976,
            -0.0787,
            0.0174,
            0.0763
          ]
        }
      }
    },
    {
      "step": 539,
      "word": "kaylin",
      "loss": 1.7624,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0668,
            -0.1986,
            0.0902,
            -0.1254,
            0.0779,
            0.1321,
            -0.0762,
            0.04,
            -0.057,
            0.1081,
            0.0488,
            0.1199,
            -0.0315,
            0.0478,
            -0.021,
            -0.0291
          ],
          "after": [
            0.0021,
            0.0055,
            0.1605,
            -0.0357,
            0.0462,
            -0.0771,
            -0.1043,
            -0.144,
            -0.0271,
            0.1016,
            0.1589,
            0.0272,
            -0.0877,
            -0.0346,
            -0.0611,
            -0.1879
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1423,
            0.1611,
            -0.1119,
            0.0326,
            0.0536,
            -0.2143,
            0.3856,
            0.0194,
            -0.285,
            0.0034,
            -0.0346,
            -0.0246,
            0.1205,
            -0.1302,
            -0.169,
            0.2648
          ],
          "after": [
            0.0311,
            0.1097,
            -0.0498,
            -0.0753,
            0.0318,
            -0.0144,
            0.0911,
            0.0912,
            -0.0837,
            -0.0662,
            0.0181,
            -0.0249,
            0.1148,
            0.0342,
            -0.0617,
            0.2264
          ]
        },
        "position_0": {
          "grad": [
            -0.0918,
            0.0567,
            -0.0993,
            -0.0062,
            0.0627,
            -0.0879,
            0.1772,
            -0.0287,
            -0.0604,
            0.0177,
            -0.0018,
            0.0191,
            0.0647,
            -0.0605,
            -0.1254,
            -0.0217
          ],
          "after": [
            0.0259,
            -0.1359,
            -0.2941,
            0.0663,
            0.0393,
            -0.0507,
            -0.1416,
            0.0632,
            0.0987,
            0.0018,
            0.0207,
            0.1018,
            0.045,
            -0.0086,
            -0.1403,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0002,
            0.0003,
            -0.0001,
            0.0019,
            0.0015,
            0.0013,
            0.001,
            0.0006,
            -0.0005,
            -0.0007,
            0.0017,
            -0.0003,
            -0.0011,
            -0.0007,
            0.0011
          ],
          "after": [
            -0.0668,
            0.0066,
            -0.1058,
            -0.0795,
            0.0819,
            0.1329,
            0.0779,
            -0.1625,
            0.0196,
            0.0015,
            -0.1096,
            -0.0649,
            0.0979,
            -0.0788,
            0.0172,
            0.0765
          ]
        }
      }
    },
    {
      "step": 540,
      "word": "urie",
      "loss": 2.6321,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0018,
            0.0057,
            0.1604,
            -0.035,
            0.0458,
            -0.0773,
            -0.1044,
            -0.144,
            -0.0271,
            0.101,
            0.1585,
            0.027,
            -0.0874,
            -0.0344,
            -0.0609,
            -0.1876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0978,
            0.0693,
            -0.0808,
            0.0109,
            -0.4153,
            -0.211,
            -0.1538,
            0.0958,
            0.0455,
            -0.3418,
            -0.0472,
            0.0805,
            0.0695,
            -0.1212,
            0.5903,
            -0.0721
          ],
          "after": [
            0.0312,
            0.1098,
            -0.0497,
            -0.0755,
            0.0319,
            -0.0142,
            0.0913,
            0.0908,
            -0.0838,
            -0.0661,
            0.0179,
            -0.0253,
            0.1149,
            0.0348,
            -0.062,
            0.2266
          ]
        },
        "position_0": {
          "grad": [
            0.1189,
            -0.0346,
            0.0707,
            -0.0412,
            0.156,
            -0.0926,
            -0.4112,
            -0.0394,
            0.1634,
            0.0392,
            0.0442,
            -0.1779,
            -0.1218,
            -0.0299,
            -0.2272,
            -0.1539
          ],
          "after": [
            0.0263,
            -0.1362,
            -0.2939,
            0.0658,
            0.0387,
            -0.05,
            -0.1419,
            0.063,
            0.0991,
            0.0021,
            0.021,
            0.1023,
            0.0447,
            -0.0081,
            -0.1395,
            0.0546
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0004,
            0.0009,
            0.0004,
            -0.0003,
            -0.0018,
            -0.0014,
            0.0013,
            -0.0002,
            -0.0,
            0.0017,
            0.0003,
            -0.0001,
            -0.002,
            0.001,
            -0.0002
          ],
          "after": [
            -0.0666,
            0.0066,
            -0.106,
            -0.0793,
            0.0819,
            0.133,
            0.0782,
            -0.1628,
            0.0192,
            0.0016,
            -0.1099,
            -0.065,
            0.0981,
            -0.0786,
            0.0171,
            0.0767
          ]
        }
      }
    },
    {
      "step": 541,
      "word": "makynzee",
      "loss": 2.8191,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2821,
            0.1276,
            0.1076,
            0.0169,
            0.1706,
            0.0217,
            0.2415,
            0.0367,
            -0.0964,
            0.0236,
            0.0342,
            0.1304,
            -0.025,
            -0.1682,
            -0.1697,
            -0.048
          ],
          "after": [
            0.002,
            0.0056,
            0.1602,
            -0.0345,
            0.0453,
            -0.0776,
            -0.1048,
            -0.1441,
            -0.027,
            0.1006,
            0.158,
            0.0266,
            -0.0871,
            -0.0339,
            -0.0605,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.6278,
            -0.0293,
            -0.3673,
            -0.1705,
            -0.8346,
            0.4786,
            -0.3406,
            0.0172,
            0.2135,
            0.1695,
            -0.0564,
            -0.5788,
            0.2107,
            0.4151,
            0.6943,
            -0.4395
          ],
          "after": [
            0.0305,
            0.1099,
            -0.0493,
            -0.0753,
            0.0326,
            -0.0145,
            0.0916,
            0.0904,
            -0.0841,
            -0.0662,
            0.0178,
            -0.025,
            0.1148,
            0.0347,
            -0.0627,
            0.227
          ]
        },
        "position_0": {
          "grad": [
            -0.0403,
            0.0661,
            -0.0304,
            0.0243,
            0.1596,
            -0.0065,
            0.2018,
            0.0393,
            -0.0184,
            0.0619,
            0.0193,
            0.1244,
            -0.047,
            -0.0821,
            -0.0572,
            0.0994
          ],
          "after": [
            0.0268,
            -0.1365,
            -0.2937,
            0.0654,
            0.0378,
            -0.0495,
            -0.1424,
            0.0627,
            0.0996,
            0.0022,
            0.0212,
            0.1025,
            0.0444,
            -0.0074,
            -0.1387,
            0.0542
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            -0.0005,
            -0.0001,
            -0.001,
            0.0024,
            -0.002,
            0.003,
            0.0002,
            -0.0002,
            0.0038,
            0.0006,
            0.0004,
            0.0,
            -0.0019,
            -0.0009,
            -0.002
          ],
          "after": [
            -0.0666,
            0.0067,
            -0.1061,
            -0.079,
            0.0817,
            0.1332,
            0.0783,
            -0.163,
            0.0189,
            0.0012,
            -0.1101,
            -0.0652,
            0.0984,
            -0.0781,
            0.017,
            0.077
          ]
        }
      }
    },
    {
      "step": 542,
      "word": "carlin",
      "loss": 1.851,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0937,
            -0.0976,
            -0.0488,
            -0.0791,
            0.0871,
            0.008,
            0.081,
            -0.1525,
            0.1124,
            -0.0576,
            0.0449,
            0.052,
            -0.0989,
            -0.1049,
            0.0417,
            0.0667
          ],
          "after": [
            0.002,
            0.0057,
            0.1601,
            -0.0339,
            0.0447,
            -0.0778,
            -0.1053,
            -0.1438,
            -0.027,
            0.1003,
            0.1576,
            0.0262,
            -0.0868,
            -0.0333,
            -0.0602,
            -0.187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0432,
            0.1516,
            -0.1906,
            0.0456,
            0.0059,
            -0.1771,
            0.3526,
            -0.0044,
            -0.3042,
            0.0203,
            -0.0239,
            -0.0637,
            0.1861,
            -0.1193,
            -0.2125,
            0.251
          ],
          "after": [
            0.03,
            0.1098,
            -0.0488,
            -0.0752,
            0.0333,
            -0.0145,
            0.0917,
            0.09,
            -0.0841,
            -0.0664,
            0.0178,
            -0.0246,
            0.1146,
            0.0348,
            -0.0632,
            0.2272
          ]
        },
        "position_0": {
          "grad": [
            -0.0259,
            0.0187,
            -0.022,
            0.1062,
            -0.1056,
            -0.0273,
            0.0962,
            0.0728,
            -0.0424,
            0.0162,
            -0.0035,
            0.0153,
            0.027,
            0.0008,
            -0.0234,
            0.0318
          ],
          "after": [
            0.0273,
            -0.1368,
            -0.2934,
            0.0648,
            0.0373,
            -0.0489,
            -0.1429,
            0.0622,
            0.1,
            0.0023,
            0.0214,
            0.1026,
            0.0442,
            -0.0069,
            -0.138,
            0.0538
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0002,
            0.001,
            -0.0003,
            0.0012,
            -0.0006,
            0.001,
            -0.0003,
            0.0006,
            0.001,
            -0.0001,
            0.0013,
            0.0002,
            -0.0006,
            -0.0017,
            -0.0003
          ],
          "after": [
            -0.0665,
            0.0068,
            -0.1063,
            -0.0788,
            0.0814,
            0.1335,
            0.0782,
            -0.1632,
            0.0185,
            0.0009,
            -0.1104,
            -0.0655,
            0.0985,
            -0.0777,
            0.017,
            0.0774
          ]
        }
      }
    },
    {
      "step": 543,
      "word": "vaibhav",
      "loss": 3.1499,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3345,
            -0.3022,
            0.0654,
            0.2937,
            0.4046,
            0.299,
            0.0539,
            0.1085,
            -0.1966,
            -0.0763,
            0.0531,
            0.0786,
            -0.0239,
            -0.2487,
            -0.1119,
            0.4269
          ],
          "after": [
            0.0024,
            0.0062,
            0.1599,
            -0.0339,
            0.0438,
            -0.0786,
            -0.1057,
            -0.1438,
            -0.0267,
            0.1001,
            0.1571,
            0.0258,
            -0.0864,
            -0.0323,
            -0.0597,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1138,
            0.0697,
            -0.0471,
            0.0724,
            0.1143,
            -0.2103,
            0.2855,
            0.0258,
            -0.2671,
            0.0395,
            -0.0532,
            0.068,
            0.1116,
            -0.0716,
            -0.2438,
            0.1841
          ],
          "after": [
            0.0297,
            0.1097,
            -0.0484,
            -0.0753,
            0.0337,
            -0.0144,
            0.0916,
            0.0896,
            -0.0839,
            -0.0665,
            0.0179,
            -0.0244,
            0.1143,
            0.035,
            -0.0634,
            0.2272
          ]
        },
        "position_0": {
          "grad": [
            -0.0782,
            0.0859,
            -0.0008,
            0.0504,
            0.1379,
            0.0544,
            0.0593,
            -0.014,
            -0.1036,
            0.0204,
            0.0191,
            -0.0032,
            0.0541,
            -0.0825,
            -0.0839,
            0.1654
          ],
          "after": [
            0.028,
            -0.1373,
            -0.2932,
            0.0643,
            0.0367,
            -0.0486,
            -0.1434,
            0.0618,
            0.1006,
            0.0023,
            0.0215,
            0.1027,
            0.044,
            -0.0062,
            -0.1373,
            0.0533
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            0.0029,
            0.004,
            -0.0008,
            0.0043,
            0.0011,
            0.0026,
            -0.0027,
            -0.0008,
            0.0049,
            0.0028,
            0.0017,
            0.0017,
            0.0012,
            -0.0063,
            0.0005
          ],
          "after": [
            -0.0663,
            0.0065,
            -0.107,
            -0.0784,
            0.0807,
            0.1336,
            0.078,
            -0.1629,
            0.0184,
            0.0002,
            -0.1109,
            -0.0659,
            0.0985,
            -0.0775,
            0.0176,
            0.0776
          ]
        }
      }
    },
    {
      "step": 544,
      "word": "feyza",
      "loss": 2.5054,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2757,
            -0.0489,
            -0.098,
            -0.0472,
            -0.0382,
            -0.0154,
            -0.031,
            0.0044,
            -0.1075,
            0.1739,
            0.0827,
            -0.2398,
            0.1231,
            0.1039,
            -0.0035,
            -0.1921
          ],
          "after": [
            0.0024,
            0.0068,
            0.1598,
            -0.0338,
            0.043,
            -0.0793,
            -0.1061,
            -0.1438,
            -0.0262,
            0.0997,
            0.1566,
            0.0258,
            -0.0863,
            -0.0317,
            -0.0593,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2817,
            -0.285,
            0.3025,
            -0.1502,
            0.4586,
            0.189,
            -0.5311,
            -0.1004,
            0.5322,
            0.1366,
            0.1938,
            0.1433,
            -0.2899,
            0.0513,
            -0.0926,
            -0.2051
          ],
          "after": [
            0.0292,
            0.1098,
            -0.0483,
            -0.0751,
            0.0337,
            -0.0144,
            0.0919,
            0.0897,
            -0.0841,
            -0.0669,
            0.0176,
            -0.0243,
            0.1143,
            0.035,
            -0.0635,
            0.2274
          ]
        },
        "position_0": {
          "grad": [
            0.0151,
            0.2187,
            -0.0297,
            0.1502,
            0.1359,
            -0.2675,
            0.1852,
            -0.0553,
            -0.2991,
            -0.0944,
            0.1556,
            -0.0063,
            0.2877,
            -0.0138,
            -0.1073,
            0.208
          ],
          "after": [
            0.0285,
            -0.1381,
            -0.2929,
            0.0635,
            0.0358,
            -0.0476,
            -0.144,
            0.0617,
            0.1014,
            0.0025,
            0.0214,
            0.1028,
            0.0434,
            -0.0056,
            -0.1364,
            0.0526
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            -0.0013,
            0.0002,
            -0.0005,
            0.0011,
            0.0022,
            -0.0012,
            -0.0004,
            0.0016,
            0.0021,
            0.0005,
            0.0012,
            -0.0017,
            0.0008,
            0.0002,
            0.0005
          ],
          "after": [
            -0.0662,
            0.0064,
            -0.1076,
            -0.0781,
            0.0801,
            0.1335,
            0.0779,
            -0.1626,
            0.018,
            -0.0006,
            -0.1114,
            -0.0664,
            0.0987,
            -0.0774,
            0.018,
            0.0778
          ]
        }
      }
    },
    {
      "step": 545,
      "word": "ellowynn",
      "loss": 2.8425,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0024,
            0.0072,
            0.1598,
            -0.0338,
            0.0423,
            -0.0799,
            -0.1064,
            -0.1438,
            -0.0258,
            0.0994,
            0.1561,
            0.0257,
            -0.0861,
            -0.0312,
            -0.0589,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2435,
            0.3977,
            0.321,
            0.0607,
            0.0754,
            -0.1942,
            0.5093,
            -0.015,
            -0.1751,
            -0.1682,
            -0.0131,
            -0.0413,
            0.0179,
            -0.0537,
            -0.165,
            0.1466
          ],
          "after": [
            0.029,
            0.1096,
            -0.0485,
            -0.0751,
            0.0337,
            -0.0143,
            0.0918,
            0.0897,
            -0.0841,
            -0.0669,
            0.0174,
            -0.0242,
            0.1143,
            0.0352,
            -0.0635,
            0.2274
          ]
        },
        "position_0": {
          "grad": [
            -0.0379,
            -0.1127,
            0.0399,
            0.0181,
            0.0479,
            0.0877,
            -0.1165,
            -0.0579,
            0.0564,
            0.1238,
            0.0475,
            0.0585,
            -0.0859,
            -0.0501,
            -0.0288,
            -0.1868
          ],
          "after": [
            0.0291,
            -0.1385,
            -0.2928,
            0.0629,
            0.0351,
            -0.047,
            -0.1444,
            0.0617,
            0.1021,
            0.0024,
            0.0211,
            0.1027,
            0.043,
            -0.0049,
            -0.1356,
            0.0523
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0007,
            0.003,
            -0.0003,
            0.0014,
            -0.0022,
            0.0064,
            0.0017,
            -0.0018,
            0.003,
            0.0054,
            -0.0002,
            0.0036,
            -0.0046,
            -0.0073,
            -0.0013
          ],
          "after": [
            -0.0661,
            0.0064,
            -0.1085,
            -0.0778,
            0.0794,
            0.1336,
            0.0773,
            -0.1626,
            0.0179,
            -0.0015,
            -0.1124,
            -0.0667,
            0.0985,
            -0.0768,
            0.0189,
            0.0781
          ]
        }
      }
    },
    {
      "step": 546,
      "word": "anvita",
      "loss": 2.4573,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.13,
            0.2415,
            -0.3193,
            0.1148,
            -0.5673,
            0.0233,
            0.0745,
            -0.1135,
            -0.1901,
            -0.1683,
            -0.1765,
            -0.2542,
            0.2141,
            0.2561,
            0.2898,
            0.1156
          ],
          "after": [
            0.0023,
            0.0073,
            0.1602,
            -0.0339,
            0.0425,
            -0.0805,
            -0.1067,
            -0.1436,
            -0.0251,
            0.0994,
            0.156,
            0.026,
            -0.0864,
            -0.0312,
            -0.0592,
            -0.1875
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.09,
            0.0648,
            -0.0775,
            0.0387,
            0.1242,
            -0.1745,
            0.2871,
            0.0163,
            -0.1805,
            0.0563,
            -0.0451,
            0.0183,
            0.116,
            -0.0813,
            -0.2363,
            0.12
          ],
          "after": [
            0.0289,
            0.1093,
            -0.0485,
            -0.0751,
            0.0335,
            -0.014,
            0.0915,
            0.0897,
            -0.084,
            -0.067,
            0.0173,
            -0.0242,
            0.1141,
            0.0354,
            -0.0634,
            0.2273
          ]
        },
        "position_0": {
          "grad": [
            0.0816,
            -0.0464,
            0.0781,
            -0.1483,
            0.008,
            0.0233,
            -0.2622,
            -0.0961,
            0.1164,
            0.017,
            0.207,
            0.0439,
            -0.1567,
            0.0343,
            -0.0292,
            -0.1904
          ],
          "after": [
            0.0293,
            -0.1388,
            -0.2929,
            0.0626,
            0.0344,
            -0.0466,
            -0.1445,
            0.062,
            0.1025,
            0.0023,
            0.0206,
            0.1026,
            0.0429,
            -0.0045,
            -0.1349,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0013,
            -0.0002,
            -0.004,
            0.0021,
            -0.0002,
            0.0017,
            -0.0014,
            -0.0005,
            0.0026,
            0.0015,
            -0.0018,
            -0.0003,
            -0.0004,
            -0.0012,
            -0.0011
          ],
          "after": [
            -0.0659,
            0.0066,
            -0.1093,
            -0.0771,
            0.0786,
            0.1337,
            0.0767,
            -0.1623,
            0.0179,
            -0.0026,
            -0.1134,
            -0.0669,
            0.0984,
            -0.0762,
            0.0198,
            0.0784
          ]
        }
      }
    },
    {
      "step": 547,
      "word": "vash",
      "loss": 2.49,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0204,
            0.1947,
            0.0946,
            0.204,
            -0.1934,
            -0.0576,
            -0.0244,
            0.0543,
            -0.1093,
            0.0702,
            -0.0575,
            0.0918,
            0.1726,
            0.0598,
            0.085,
            -0.102
          ],
          "after": [
            0.0022,
            0.007,
            0.1604,
            -0.0344,
            0.0428,
            -0.0808,
            -0.107,
            -0.1435,
            -0.0243,
            0.0993,
            0.1561,
            0.0261,
            -0.0868,
            -0.0313,
            -0.0595,
            -0.1875
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0492,
            0.0601,
            -0.1875,
            0.0428,
            0.0004,
            -0.1904,
            0.3378,
            0.0331,
            -0.2442,
            0.0523,
            -0.0529,
            0.0252,
            0.1165,
            -0.1017,
            -0.1818,
            0.1776
          ],
          "after": [
            0.0289,
            0.109,
            -0.0485,
            -0.0752,
            0.0334,
            -0.0136,
            0.0911,
            0.0896,
            -0.0838,
            -0.0672,
            0.0173,
            -0.0242,
            0.1139,
            0.0357,
            -0.0631,
            0.2271
          ]
        },
        "position_0": {
          "grad": [
            -0.11,
            0.0634,
            -0.005,
            0.108,
            0.0957,
            0.0837,
            0.0153,
            0.0275,
            -0.0681,
            -0.0646,
            -0.0316,
            -0.0961,
            0.0364,
            -0.0461,
            0.0079,
            0.2104
          ],
          "after": [
            0.0298,
            -0.1392,
            -0.293,
            0.0622,
            0.0336,
            -0.0464,
            -0.1446,
            0.0622,
            0.103,
            0.0023,
            0.0203,
            0.1027,
            0.0427,
            -0.004,
            -0.1343,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            0.0002,
            -0.0006,
            -0.0022,
            0.0017,
            -0.0026,
            0.0003,
            -0.0013,
            0.001,
            0.0016,
            0.0021,
            0.0009,
            -0.0012,
            -0.0005,
            -0.0026,
            -0.001
          ],
          "after": [
            -0.0659,
            0.0067,
            -0.1098,
            -0.0762,
            0.0777,
            0.1341,
            0.0761,
            -0.1619,
            0.0178,
            -0.0036,
            -0.1145,
            -0.0671,
            0.0984,
            -0.0756,
            0.0207,
            0.0789
          ]
        }
      }
    },
    {
      "step": 548,
      "word": "hifza",
      "loss": 2.9923,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.283,
            -0.0498,
            -0.1041,
            -0.0485,
            -0.0357,
            -0.024,
            -0.031,
            0.0098,
            -0.1092,
            0.1779,
            0.0807,
            -0.2506,
            0.1275,
            0.1084,
            -0.0101,
            -0.195
          ],
          "after": [
            0.0017,
            0.0068,
            0.1607,
            -0.0347,
            0.0431,
            -0.081,
            -0.1072,
            -0.1434,
            -0.0234,
            0.0989,
            0.156,
            0.0265,
            -0.0873,
            -0.0316,
            -0.0598,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1108,
            0.1107,
            -0.2331,
            0.047,
            -0.018,
            -0.1729,
            0.3621,
            0.0453,
            -0.2948,
            0.0271,
            -0.0965,
            -0.0563,
            0.1925,
            -0.0771,
            -0.2237,
            0.208
          ],
          "after": [
            0.029,
            0.1086,
            -0.0482,
            -0.0753,
            0.0332,
            -0.0131,
            0.0906,
            0.0893,
            -0.0833,
            -0.0674,
            0.0174,
            -0.0241,
            0.1136,
            0.0361,
            -0.0627,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            -0.0,
            -0.0311,
            0.0703,
            0.0914,
            -0.0813,
            0.1498,
            -0.0236,
            0.0954,
            0.1208,
            0.0553,
            -0.1083,
            -0.1272,
            0.2254,
            0.1609,
            0.0665,
            0.1361
          ],
          "after": [
            0.0303,
            -0.1394,
            -0.2933,
            0.0617,
            0.0332,
            -0.0467,
            -0.1446,
            0.0621,
            0.1032,
            0.0022,
            0.0201,
            0.1031,
            0.0423,
            -0.0039,
            -0.1339,
            0.0514
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0029,
            0.0014,
            -0.0013,
            -0.0016,
            0.0024,
            -0.001,
            0.0044,
            -0.0026,
            -0.0009,
            0.0036,
            -0.0015,
            -0.0042,
            0.0044,
            0.0037,
            -0.0071,
            -0.0
          ],
          "after": [
            -0.0662,
            0.0066,
            -0.1101,
            -0.0753,
            0.0768,
            0.1345,
            0.0753,
            -0.1612,
            0.0178,
            -0.0047,
            -0.1152,
            -0.0668,
            0.098,
            -0.0756,
            0.022,
            0.0792
          ]
        }
      }
    },
    {
      "step": 549,
      "word": "hannon",
      "loss": 1.9344,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0853,
            0.0846,
            -0.2503,
            -0.0202,
            -0.3177,
            -0.138,
            -0.0768,
            -0.0659,
            0.0821,
            -0.177,
            -0.159,
            -0.1809,
            0.1039,
            0.2017,
            0.109,
            0.103
          ],
          "after": [
            0.0013,
            0.0065,
            0.1613,
            -0.035,
            0.0437,
            -0.0809,
            -0.1072,
            -0.1432,
            -0.0228,
            0.0988,
            0.1561,
            0.0271,
            -0.0879,
            -0.0322,
            -0.0603,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0831,
            0.0243,
            -0.1175,
            0.036,
            0.0272,
            -0.1125,
            0.2586,
            0.0417,
            -0.1717,
            0.0506,
            -0.0435,
            0.0184,
            0.0869,
            -0.0936,
            -0.1366,
            0.1427
          ],
          "after": [
            0.0292,
            0.1082,
            -0.0478,
            -0.0756,
            0.0331,
            -0.0125,
            0.0899,
            0.089,
            -0.0828,
            -0.0676,
            0.0177,
            -0.0241,
            0.1131,
            0.0366,
            -0.0623,
            0.2263
          ]
        },
        "position_0": {
          "grad": [
            0.0049,
            -0.0818,
            0.0576,
            0.0614,
            -0.135,
            0.1175,
            -0.0907,
            0.1095,
            0.1761,
            0.0216,
            -0.1405,
            -0.1638,
            0.1195,
            0.1477,
            0.1164,
            0.0464
          ],
          "after": [
            0.0306,
            -0.1395,
            -0.2937,
            0.0611,
            0.033,
            -0.0472,
            -0.1446,
            0.0617,
            0.1031,
            0.0021,
            0.0202,
            0.1036,
            0.0418,
            -0.0043,
            -0.1338,
            0.051
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0001,
            0.0004,
            0.0001,
            0.0004,
            -0.0002,
            0.0002,
            -0.0001,
            0.0004,
            0.0009,
            0.0003,
            0.0007,
            0.0002,
            0.0001,
            -0.001,
            -0.0
          ],
          "after": [
            -0.0666,
            0.0066,
            -0.1104,
            -0.0745,
            0.0759,
            0.1349,
            0.0746,
            -0.1606,
            0.0178,
            -0.0058,
            -0.1159,
            -0.0667,
            0.0976,
            -0.0756,
            0.0232,
            0.0795
          ]
        }
      }
    },
    {
      "step": 550,
      "word": "maclin",
      "loss": 1.8796,
      "learning_rate": 0.0014,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1084,
            0.0611,
            0.1087,
            0.1583,
            -0.0331,
            -0.0601,
            0.1587,
            0.1015,
            -0.1018,
            0.0529,
            0.0989,
            0.0609,
            0.0324,
            -0.0284,
            -0.0947,
            -0.0318
          ],
          "after": [
            0.001,
            0.0061,
            0.1616,
            -0.0354,
            0.0442,
            -0.0807,
            -0.1075,
            -0.1433,
            -0.022,
            0.0987,
            0.1561,
            0.0275,
            -0.0885,
            -0.0327,
            -0.0605,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0919,
            0.1647,
            -0.1919,
            0.0532,
            0.04,
            -0.1936,
            0.3519,
            0.0156,
            -0.31,
            0.0172,
            -0.0486,
            -0.0333,
            0.1773,
            -0.1003,
            -0.2257,
            0.2685
          ],
          "after": [
            0.0295,
            0.1077,
            -0.0473,
            -0.0759,
            0.033,
            -0.0119,
            0.0892,
            0.0887,
            -0.0821,
            -0.0678,
            0.018,
            -0.024,
            0.1126,
            0.0371,
            -0.0617,
            0.2257
          ]
        },
        "position_0": {
          "grad": [
            -0.0319,
            0.0415,
            -0.035,
            0.0276,
            0.1423,
            -0.0019,
            0.1978,
            0.0727,
            0.035,
            0.0272,
            -0.0046,
            0.1108,
            -0.0819,
            -0.0522,
            -0.0096,
            0.105
          ],
          "after": [
            0.031,
            -0.1396,
            -0.2939,
            0.0606,
            0.0326,
            -0.0476,
            -0.1447,
            0.0611,
            0.1029,
            0.0019,
            0.0203,
            0.1039,
            0.0414,
            -0.0045,
            -0.1337,
            0.0505
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.0002,
            0.001,
            -0.0016,
            0.0022,
            -0.0013,
            0.001,
            -0.001,
            0.0006,
            0.0012,
            0.0012,
            0.0003,
            0.0004,
            -0.0004,
            -0.0024,
            -0.0006
          ],
          "after": [
            -0.067,
            0.0065,
            -0.1108,
            -0.0737,
            0.075,
            0.1354,
            0.074,
            -0.1599,
            0.0177,
            -0.0068,
            -0.1166,
            -0.0666,
            0.0973,
            -0.0755,
            0.0243,
            0.0798
          ]
        }
      }
    },
    {
      "step": 551,
      "word": "yazid",
      "loss": 2.8382,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0376,
            0.0665,
            0.0404,
            0.3009,
            0.1619,
            -0.1374,
            0.3222,
            -0.0065,
            0.0099,
            0.0963,
            0.1707,
            0.1816,
            0.0155,
            -0.1203,
            -0.0375,
            -0.0078
          ],
          "after": [
            0.0008,
            0.0057,
            0.1619,
            -0.0363,
            0.0445,
            -0.0802,
            -0.1082,
            -0.1433,
            -0.0214,
            0.0984,
            0.1558,
            0.0276,
            -0.089,
            -0.0328,
            -0.0606,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0562,
            0.0496,
            -0.1108,
            0.0634,
            0.0943,
            -0.1348,
            0.2412,
            0.0204,
            -0.2217,
            0.0521,
            -0.054,
            -0.009,
            0.1147,
            -0.0674,
            -0.2485,
            0.1306
          ],
          "after": [
            0.0298,
            0.1073,
            -0.0468,
            -0.0762,
            0.0328,
            -0.0112,
            0.0884,
            0.0883,
            -0.0814,
            -0.0681,
            0.0183,
            -0.0239,
            0.1121,
            0.0377,
            -0.0611,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            0.0401,
            -0.1549,
            0.1008,
            -0.1949,
            0.0174,
            0.1639,
            -0.1518,
            0.0289,
            0.0344,
            0.0865,
            -0.0472,
            0.0593,
            -0.0976,
            0.0794,
            0.0443,
            -0.0672
          ],
          "after": [
            0.0313,
            -0.1395,
            -0.2944,
            0.0605,
            0.0322,
            -0.0484,
            -0.1447,
            0.0606,
            0.1028,
            0.0015,
            0.0205,
            0.104,
            0.0413,
            -0.0048,
            -0.1337,
            0.0502
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0002,
            -0.0002,
            -0.0024,
            0.0003,
            -0.0001,
            -0.001,
            -0.0006,
            0.0016,
            0.0007,
            0.0006,
            0.0,
            -0.0011,
            0.0006,
            0.0003,
            -0.0005
          ],
          "after": [
            -0.0673,
            0.0065,
            -0.1111,
            -0.0727,
            0.0742,
            0.1358,
            0.0735,
            -0.1592,
            0.0173,
            -0.0077,
            -0.1173,
            -0.0666,
            0.0971,
            -0.0755,
            0.0253,
            0.0802
          ]
        }
      }
    },
    {
      "step": 552,
      "word": "iyanni",
      "loss": 2.3283,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1592,
            0.0883,
            -0.2319,
            0.1179,
            -0.295,
            -0.0194,
            -0.0432,
            -0.0768,
            0.0556,
            0.0178,
            -0.107,
            -0.1061,
            0.0348,
            0.1698,
            0.0304,
            0.0665
          ],
          "after": [
            0.0005,
            0.0053,
            0.1624,
            -0.0373,
            0.0451,
            -0.0797,
            -0.1087,
            -0.1432,
            -0.0211,
            0.0982,
            0.1557,
            0.0278,
            -0.0895,
            -0.0333,
            -0.0608,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0507,
            -0.0394,
            -0.026,
            -0.018,
            0.1212,
            -0.0694,
            0.1147,
            -0.0056,
            -0.0847,
            0.0699,
            -0.0126,
            0.0172,
            0.0278,
            -0.0678,
            -0.139,
            0.0577
          ],
          "after": [
            0.0302,
            0.1069,
            -0.0464,
            -0.0765,
            0.0326,
            -0.0106,
            0.0877,
            0.088,
            -0.0807,
            -0.0684,
            0.0186,
            -0.0239,
            0.1116,
            0.0382,
            -0.0605,
            0.2246
          ]
        },
        "position_0": {
          "grad": [
            -0.0483,
            -0.242,
            0.0584,
            -0.1533,
            0.0193,
            0.1214,
            -0.2575,
            0.0402,
            0.1126,
            -0.0001,
            -0.0011,
            -0.1682,
            -0.2109,
            0.0335,
            0.0256,
            -0.1817
          ],
          "after": [
            0.0316,
            -0.1389,
            -0.295,
            0.0607,
            0.0319,
            -0.0493,
            -0.1444,
            0.06,
            0.1024,
            0.0012,
            0.0206,
            0.1045,
            0.0414,
            -0.0052,
            -0.1337,
            0.0501
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0009,
            0.0004,
            -0.0005,
            0.0013,
            -0.0004,
            0.0014,
            -0.0002,
            -0.0003,
            0.001,
            -0.0,
            -0.0004,
            0.0011,
            -0.0005,
            -0.0012,
            0.0001
          ],
          "after": [
            -0.0675,
            0.0065,
            -0.1114,
            -0.0718,
            0.0734,
            0.1362,
            0.0729,
            -0.1585,
            0.0171,
            -0.0086,
            -0.1178,
            -0.0665,
            0.0969,
            -0.0755,
            0.0262,
            0.0804
          ]
        }
      }
    },
    {
      "step": 553,
      "word": "rosaleah",
      "loss": 2.5685,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3213,
            0.1026,
            0.2267,
            0.0697,
            0.2426,
            0.1724,
            -0.016,
            0.2219,
            0.093,
            -0.0774,
            -0.2451,
            0.2875,
            0.0533,
            -0.1047,
            -0.0945,
            0.0349
          ],
          "after": [
            0.0006,
            0.0047,
            0.1625,
            -0.0382,
            0.0453,
            -0.0797,
            -0.1091,
            -0.1436,
            -0.0209,
            0.0981,
            0.1561,
            0.0277,
            -0.09,
            -0.0335,
            -0.0607,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1452,
            -0.2686,
            -0.0176,
            -0.1305,
            -0.179,
            0.1519,
            -0.3375,
            0.0096,
            0.1862,
            0.1267,
            0.0319,
            -0.0984,
            -0.002,
            0.1492,
            0.1511,
            -0.2632
          ],
          "after": [
            0.0303,
            0.1069,
            -0.046,
            -0.0765,
            0.0325,
            -0.0102,
            0.0873,
            0.0877,
            -0.0803,
            -0.0689,
            0.0188,
            -0.0237,
            0.1112,
            0.0385,
            -0.06,
            0.2243
          ]
        },
        "position_0": {
          "grad": [
            0.0085,
            -0.012,
            0.0041,
            -0.056,
            0.0363,
            0.0933,
            0.0583,
            -0.0494,
            0.075,
            -0.0201,
            -0.0478,
            0.0409,
            -0.129,
            -0.0829,
            0.0554,
            0.0617
          ],
          "after": [
            0.0319,
            -0.1383,
            -0.2956,
            0.0609,
            0.0315,
            -0.0503,
            -0.1442,
            0.0597,
            0.1021,
            0.0009,
            0.0208,
            0.1047,
            0.0417,
            -0.0053,
            -0.1339,
            0.05
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0007,
            0.0004,
            -0.0005,
            -0.0003,
            0.0003,
            -0.0012,
            -0.0004,
            -0.0003,
            -0.0003,
            -0.0007,
            -0.0006,
            -0.0001,
            0.001,
            0.0017,
            -0.0013
          ],
          "after": [
            -0.0676,
            0.0066,
            -0.1117,
            -0.071,
            0.0727,
            0.1365,
            0.0726,
            -0.158,
            0.0169,
            -0.0093,
            -0.1182,
            -0.0663,
            0.0966,
            -0.0756,
            0.0268,
            0.0808
          ]
        }
      }
    },
    {
      "step": 554,
      "word": "libbie",
      "loss": 2.6813,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0007,
            0.0042,
            0.1627,
            -0.039,
            0.0455,
            -0.0797,
            -0.1094,
            -0.144,
            -0.0208,
            0.098,
            0.1564,
            0.0276,
            -0.0904,
            -0.0336,
            -0.0607,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1027,
            0.1426,
            -0.1588,
            0.0794,
            -0.7121,
            -0.0611,
            -0.0986,
            -0.0063,
            0.0047,
            -0.0683,
            0.0484,
            -0.1204,
            0.2331,
            0.121,
            0.3961,
            0.0178
          ],
          "after": [
            0.0302,
            0.1068,
            -0.0455,
            -0.0766,
            0.033,
            -0.0097,
            0.087,
            0.0875,
            -0.0799,
            -0.0692,
            0.0189,
            -0.0234,
            0.1106,
            0.0386,
            -0.06,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            -0.0784,
            0.1229,
            -0.0998,
            -0.0586,
            0.0469,
            0.0572,
            -0.0339,
            0.0724,
            -0.0757,
            -0.0192,
            -0.105,
            0.1717,
            -0.0566,
            0.0207,
            0.0988,
            -0.0999
          ],
          "after": [
            0.0323,
            -0.1381,
            -0.2957,
            0.0612,
            0.0311,
            -0.0514,
            -0.144,
            0.0592,
            0.1019,
            0.0008,
            0.0211,
            0.1047,
            0.042,
            -0.0055,
            -0.1342,
            0.05
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0047,
            0.0029,
            0.0025,
            -0.0001,
            -0.002,
            -0.0034,
            -0.0002,
            -0.0026,
            -0.0013,
            0.0038,
            0.0,
            -0.0032,
            0.0071,
            0.0081,
            -0.0057,
            0.0009
          ],
          "after": [
            -0.0682,
            0.0064,
            -0.1123,
            -0.0703,
            0.0724,
            0.1371,
            0.0723,
            -0.1571,
            0.017,
            -0.0103,
            -0.1186,
            -0.0659,
            0.0959,
            -0.0765,
            0.0278,
            0.0811
          ]
        }
      }
    },
    {
      "step": 555,
      "word": "kynzleigh",
      "loss": 3.2338,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0007,
            0.0038,
            0.1628,
            -0.0397,
            0.0457,
            -0.0797,
            -0.1097,
            -0.1443,
            -0.0207,
            0.0979,
            0.1566,
            0.0275,
            -0.0907,
            -0.0338,
            -0.0607,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0113,
            -0.1915,
            -0.033,
            -0.0984,
            -0.065,
            0.0677,
            -0.1455,
            0.0342,
            0.0983,
            0.1156,
            -0.02,
            -0.0593,
            0.0248,
            0.0677,
            0.0791,
            -0.0999
          ],
          "after": [
            0.0302,
            0.1068,
            -0.045,
            -0.0766,
            0.0335,
            -0.0095,
            0.0868,
            0.0872,
            -0.0796,
            -0.0696,
            0.019,
            -0.0231,
            0.11,
            0.0385,
            -0.0599,
            0.2239
          ]
        },
        "position_0": {
          "grad": [
            -0.0705,
            0.0506,
            -0.0762,
            0.0243,
            0.0421,
            -0.0382,
            0.1545,
            -0.0104,
            -0.0559,
            0.0035,
            0.0069,
            0.0398,
            0.0477,
            -0.0408,
            -0.0648,
            0.0075
          ],
          "after": [
            0.033,
            -0.138,
            -0.2956,
            0.0614,
            0.0307,
            -0.0521,
            -0.144,
            0.0588,
            0.1018,
            0.0006,
            0.0214,
            0.1045,
            0.0422,
            -0.0055,
            -0.1343,
            0.05
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            -0.0008,
            -0.0017,
            0.0003,
            0.0018,
            0.0004,
            0.0057,
            0.0004,
            -0.0006,
            0.0029,
            -0.0005,
            0.0009,
            0.002,
            -0.0007,
            -0.0023,
            0.0026
          ],
          "after": [
            -0.0687,
            0.0063,
            -0.1126,
            -0.0698,
            0.0719,
            0.1376,
            0.0716,
            -0.1564,
            0.0171,
            -0.0113,
            -0.1188,
            -0.0656,
            0.095,
            -0.0773,
            0.0288,
            0.081
          ]
        }
      }
    },
    {
      "step": 556,
      "word": "azrah",
      "loss": 2.3912,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1566,
            0.0595,
            0.3322,
            0.4345,
            0.3689,
            -0.0844,
            0.4197,
            0.1358,
            0.2056,
            0.2465,
            0.0627,
            0.2167,
            0.2334,
            -0.1536,
            -0.2564,
            0.1443
          ],
          "after": [
            0.001,
            0.0033,
            0.1624,
            -0.0409,
            0.0454,
            -0.0795,
            -0.1105,
            -0.1449,
            -0.021,
            0.0975,
            0.1568,
            0.0271,
            -0.0914,
            -0.0336,
            -0.0602,
            -0.1874
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0431,
            0.0687,
            -0.1392,
            0.0224,
            0.0288,
            -0.1694,
            0.2255,
            0.0033,
            -0.1938,
            0.0016,
            -0.073,
            -0.0123,
            0.1356,
            -0.0445,
            -0.2122,
            0.1318
          ],
          "after": [
            0.0302,
            0.1068,
            -0.0445,
            -0.0765,
            0.0339,
            -0.0091,
            0.0865,
            0.087,
            -0.0793,
            -0.0699,
            0.0192,
            -0.0228,
            0.1095,
            0.0386,
            -0.0598,
            0.2237
          ]
        },
        "position_0": {
          "grad": [
            0.1101,
            -0.03,
            0.0803,
            -0.1857,
            0.0295,
            0.0017,
            -0.2784,
            -0.1365,
            0.1152,
            0.0705,
            0.2495,
            0.0542,
            -0.1744,
            0.0218,
            -0.07,
            -0.2365
          ],
          "after": [
            0.0331,
            -0.1379,
            -0.2958,
            0.0619,
            0.0303,
            -0.0528,
            -0.1437,
            0.0588,
            0.1015,
            0.0004,
            0.0213,
            0.1043,
            0.0426,
            -0.0056,
            -0.1343,
            0.0503
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0001,
            -0.0006,
            -0.0014,
            0.0,
            -0.0016,
            -0.0003,
            -0.0004,
            -0.0001,
            0.0001,
            0.0003,
            -0.0009,
            0.001,
            0.0002,
            0.0003,
            -0.001
          ],
          "after": [
            -0.069,
            0.0062,
            -0.1127,
            -0.0691,
            0.0715,
            0.1381,
            0.0711,
            -0.1557,
            0.0172,
            -0.0122,
            -0.119,
            -0.0653,
            0.0943,
            -0.0779,
            0.0296,
            0.081
          ]
        }
      }
    },
    {
      "step": 557,
      "word": "kenzo",
      "loss": 2.6113,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0012,
            0.003,
            0.1622,
            -0.042,
            0.0452,
            -0.0794,
            -0.1112,
            -0.1454,
            -0.0212,
            0.0971,
            0.1568,
            0.0268,
            -0.0919,
            -0.0335,
            -0.0597,
            -0.1875
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2441,
            -0.3492,
            0.1838,
            0.0129,
            0.2938,
            0.2317,
            -0.5683,
            -0.1106,
            0.471,
            0.2395,
            0.2074,
            0.2345,
            -0.2314,
            0.0189,
            -0.2246,
            -0.3843
          ],
          "after": [
            0.0299,
            0.1072,
            -0.0443,
            -0.0765,
            0.034,
            -0.009,
            0.0866,
            0.0871,
            -0.0793,
            -0.0706,
            0.0191,
            -0.0229,
            0.1092,
            0.0386,
            -0.0595,
            0.2239
          ]
        },
        "position_0": {
          "grad": [
            -0.1005,
            0.0525,
            -0.1157,
            0.0163,
            0.0389,
            -0.0877,
            0.179,
            -0.01,
            -0.0381,
            -0.0362,
            -0.0119,
            0.0105,
            0.0682,
            -0.016,
            -0.0845,
            -0.0162
          ],
          "after": [
            0.0336,
            -0.1378,
            -0.2956,
            0.0623,
            0.0299,
            -0.0531,
            -0.1436,
            0.0589,
            0.1014,
            0.0002,
            0.0211,
            0.1041,
            0.0429,
            -0.0056,
            -0.1341,
            0.0505
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            -0.0011,
            -0.0001,
            0.0006,
            0.0009,
            0.0004,
            0.0004,
            0.0,
            0.0012,
            0.0028,
            0.0011,
            0.0011,
            0.0008,
            -0.0002,
            -0.0031,
            -0.0003
          ],
          "after": [
            -0.0694,
            0.0063,
            -0.1128,
            -0.0686,
            0.071,
            0.1386,
            0.0706,
            -0.1552,
            0.0172,
            -0.0132,
            -0.1194,
            -0.0651,
            0.0935,
            -0.0784,
            0.0305,
            0.081
          ]
        }
      }
    },
    {
      "step": 558,
      "word": "tashi",
      "loss": 2.2248,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0213,
            0.1741,
            0.0836,
            0.1427,
            -0.126,
            -0.0273,
            -0.0158,
            0.0304,
            -0.1006,
            0.0899,
            -0.0454,
            0.0921,
            0.1385,
            0.0144,
            0.0339,
            -0.0823
          ],
          "after": [
            0.0014,
            0.0024,
            0.1618,
            -0.0431,
            0.0451,
            -0.0792,
            -0.1118,
            -0.1459,
            -0.0212,
            0.0966,
            0.157,
            0.0264,
            -0.0925,
            -0.0334,
            -0.0594,
            -0.1876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.054,
            0.0612,
            -0.1277,
            0.028,
            0.0746,
            -0.1252,
            0.2259,
            -0.0085,
            -0.1546,
            0.0353,
            -0.0127,
            0.0021,
            0.0646,
            -0.0879,
            -0.1821,
            0.1032
          ],
          "after": [
            0.0298,
            0.1074,
            -0.0439,
            -0.0766,
            0.034,
            -0.0087,
            0.0866,
            0.0872,
            -0.0793,
            -0.0712,
            0.0189,
            -0.0229,
            0.1089,
            0.0387,
            -0.0591,
            0.2239
          ]
        },
        "position_0": {
          "grad": [
            -0.0213,
            -0.169,
            0.057,
            0.0962,
            -0.1299,
            0.0413,
            0.0485,
            0.0215,
            -0.0993,
            -0.1609,
            -0.1126,
            -0.2561,
            0.123,
            0.0407,
            0.1234,
            0.1617
          ],
          "after": [
            0.0341,
            -0.1375,
            -0.2956,
            0.0624,
            0.0298,
            -0.0535,
            -0.1436,
            0.0589,
            0.1014,
            0.0005,
            0.0212,
            0.1044,
            0.043,
            -0.0057,
            -0.1342,
            0.0505
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0004,
            0.0007,
            -0.001,
            0.0019,
            0.0004,
            -0.0001,
            -0.0001,
            -0.0001,
            0.0,
            -0.0004,
            -0.0002,
            -0.0001,
            0.0001,
            -0.001,
            -0.0001
          ],
          "after": [
            -0.0698,
            0.0063,
            -0.113,
            -0.0681,
            0.0705,
            0.1389,
            0.0702,
            -0.1547,
            0.0171,
            -0.014,
            -0.1196,
            -0.0649,
            0.0929,
            -0.0789,
            0.0313,
            0.0811
          ]
        }
      }
    },
    {
      "step": 559,
      "word": "kenzington",
      "loss": 2.8595,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0016,
            0.0018,
            0.1615,
            -0.044,
            0.0451,
            -0.079,
            -0.1122,
            -0.1464,
            -0.0213,
            0.0961,
            0.1571,
            0.0261,
            -0.0931,
            -0.0334,
            -0.0592,
            -0.1876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0432,
            -0.1621,
            0.1029,
            0.0189,
            0.2705,
            0.0802,
            -0.2211,
            -0.0612,
            0.2178,
            0.1236,
            0.1058,
            0.202,
            -0.1492,
            -0.0483,
            -0.1918,
            -0.1288
          ],
          "after": [
            0.0296,
            0.1078,
            -0.0437,
            -0.0767,
            0.0338,
            -0.0086,
            0.0867,
            0.0876,
            -0.0794,
            -0.0718,
            0.0186,
            -0.0232,
            0.1088,
            0.0388,
            -0.0586,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            -0.0613,
            0.0451,
            -0.0699,
            0.0194,
            0.0402,
            -0.0454,
            0.127,
            -0.0101,
            -0.0447,
            -0.0004,
            -0.0013,
            0.0315,
            0.0407,
            -0.0309,
            -0.0583,
            -0.0002
          ],
          "after": [
            0.0346,
            -0.1373,
            -0.2954,
            0.0626,
            0.0296,
            -0.0537,
            -0.1438,
            0.0589,
            0.1015,
            0.0007,
            0.0213,
            0.1046,
            0.043,
            -0.0058,
            -0.1342,
            0.0506
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0044,
            -0.0024,
            -0.0019,
            -0.0011,
            -0.0005,
            -0.0009,
            -0.0017,
            0.0006,
            -0.001,
            0.0028,
            0.0001,
            -0.0014,
            -0.0006,
            0.0018,
            -0.0005,
            -0.0021
          ],
          "after": [
            -0.0706,
            0.0067,
            -0.113,
            -0.0675,
            0.07,
            0.1393,
            0.0699,
            -0.1543,
            0.0172,
            -0.015,
            -0.1198,
            -0.0646,
            0.0924,
            -0.0795,
            0.0321,
            0.0814
          ]
        }
      }
    },
    {
      "step": 560,
      "word": "red",
      "loss": 3.1122,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0017,
            0.0014,
            0.1613,
            -0.0448,
            0.045,
            -0.0789,
            -0.1126,
            -0.1467,
            -0.0213,
            0.0958,
            0.1573,
            0.0258,
            -0.0936,
            -0.0333,
            -0.0589,
            -0.1876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2932,
            -0.4617,
            0.476,
            -0.0544,
            0.6726,
            0.2742,
            -0.9385,
            0.0122,
            0.7134,
            0.237,
            0.2324,
            0.3339,
            -0.5842,
            0.156,
            -0.0714,
            -0.3348
          ],
          "after": [
            0.0291,
            0.1085,
            -0.044,
            -0.0767,
            0.0331,
            -0.0088,
            0.0873,
            0.0878,
            -0.08,
            -0.0727,
            0.018,
            -0.0239,
            0.1093,
            0.0388,
            -0.0582,
            0.2245
          ]
        },
        "position_0": {
          "grad": [
            0.0319,
            -0.1244,
            0.0297,
            -0.0582,
            -0.0821,
            0.2115,
            0.0526,
            -0.037,
            0.2792,
            -0.1861,
            -0.1806,
            -0.0199,
            -0.3463,
            -0.0761,
            0.2791,
            0.1336
          ],
          "after": [
            0.035,
            -0.1369,
            -0.2953,
            0.0628,
            0.0296,
            -0.0544,
            -0.1439,
            0.059,
            0.1011,
            0.0014,
            0.0216,
            0.1048,
            0.0434,
            -0.0056,
            -0.1347,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0043,
            -0.004,
            0.0013,
            -0.006,
            0.0011,
            -0.0008,
            -0.0056,
            0.0001,
            0.0025,
            -0.001,
            0.0008,
            0.0017,
            -0.0039,
            -0.0002,
            0.0038,
            -0.0011
          ],
          "after": [
            -0.0708,
            0.0074,
            -0.1131,
            -0.0664,
            0.0696,
            0.1397,
            0.0702,
            -0.1541,
            0.017,
            -0.0157,
            -0.1201,
            -0.0645,
            0.0923,
            -0.08,
            0.0324,
            0.0818
          ]
        }
      }
    },
    {
      "step": 561,
      "word": "karis",
      "loss": 1.8154,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.102,
            -0.1183,
            -0.0492,
            -0.1219,
            0.1031,
            0.034,
            0.0578,
            -0.1781,
            0.1599,
            -0.0549,
            0.0513,
            0.0658,
            -0.163,
            -0.1229,
            0.0402,
            0.073
          ],
          "after": [
            0.0017,
            0.0012,
            0.1611,
            -0.0453,
            0.0449,
            -0.0788,
            -0.1131,
            -0.1466,
            -0.0216,
            0.0956,
            0.1573,
            0.0255,
            -0.0937,
            -0.033,
            -0.0588,
            -0.1877
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0667,
            0.1128,
            -0.1264,
            0.001,
            -0.0131,
            -0.177,
            0.297,
            0.0178,
            -0.2332,
            0.008,
            -0.055,
            -0.0452,
            0.1122,
            -0.0671,
            -0.1243,
            0.1736
          ],
          "after": [
            0.0287,
            0.1091,
            -0.0441,
            -0.0766,
            0.0325,
            -0.0088,
            0.0876,
            0.088,
            -0.0803,
            -0.0735,
            0.0175,
            -0.0244,
            0.1096,
            0.0388,
            -0.0577,
            0.2246
          ]
        },
        "position_0": {
          "grad": [
            -0.0899,
            0.0653,
            -0.1273,
            -0.0239,
            0.0514,
            -0.1058,
            0.1866,
            -0.0258,
            -0.0563,
            0.0135,
            -0.0025,
            0.0183,
            0.0633,
            -0.0585,
            -0.125,
            -0.0485
          ],
          "after": [
            0.0356,
            -0.1367,
            -0.2948,
            0.063,
            0.0295,
            -0.0548,
            -0.1443,
            0.0592,
            0.1009,
            0.0019,
            0.0219,
            0.1049,
            0.0437,
            -0.0053,
            -0.1349,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0016,
            -0.0003,
            -0.0032,
            0.0049,
            -0.0011,
            0.001,
            0.0003,
            -0.0005,
            -0.0019,
            0.0007,
            0.002,
            -0.0022,
            -0.0023,
            -0.0007,
            -0.0006
          ],
          "after": [
            -0.0708,
            0.0082,
            -0.1131,
            -0.0651,
            0.0687,
            0.1401,
            0.0703,
            -0.1539,
            0.0168,
            -0.0162,
            -0.1204,
            -0.0647,
            0.0925,
            -0.0801,
            0.0328,
            0.0821
          ]
        }
      }
    },
    {
      "step": 562,
      "word": "barkot",
      "loss": 2.8084,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0607,
            -0.0608,
            -0.022,
            -0.0705,
            0.1508,
            0.0291,
            0.1094,
            -0.1656,
            0.0765,
            -0.0305,
            0.0736,
            0.0984,
            -0.0828,
            -0.1235,
            0.0023,
            0.0936
          ],
          "after": [
            0.0016,
            0.0012,
            0.161,
            -0.0456,
            0.0446,
            -0.0788,
            -0.1136,
            -0.1461,
            -0.022,
            0.0955,
            0.1571,
            0.0252,
            -0.0938,
            -0.0326,
            -0.0587,
            -0.1879
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0835,
            0.1367,
            -0.0441,
            0.0444,
            0.005,
            -0.2039,
            0.2338,
            0.0069,
            -0.2042,
            0.0416,
            -0.017,
            0.0035,
            0.1235,
            -0.0492,
            -0.1702,
            0.1345
          ],
          "after": [
            0.0285,
            0.1094,
            -0.0442,
            -0.0767,
            0.032,
            -0.0086,
            0.0878,
            0.0881,
            -0.0805,
            -0.0742,
            0.0171,
            -0.0248,
            0.1097,
            0.0389,
            -0.0572,
            0.2247
          ]
        },
        "position_0": {
          "grad": [
            0.0479,
            0.032,
            0.0881,
            0.0016,
            0.0427,
            -0.0119,
            0.1112,
            -0.024,
            -0.1423,
            -0.0312,
            0.0242,
            0.0322,
            0.1573,
            -0.1342,
            -0.0619,
            0.2696
          ],
          "after": [
            0.036,
            -0.1366,
            -0.2947,
            0.0631,
            0.0294,
            -0.055,
            -0.1447,
            0.0594,
            0.1009,
            0.0024,
            0.0221,
            0.105,
            0.0437,
            -0.0047,
            -0.1349,
            0.05
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            0.0021,
            -0.0004,
            0.0029,
            0.001,
            0.002,
            0.0051,
            0.0002,
            0.0022,
            0.0021,
            0.0007,
            0.0015,
            0.0008,
            -0.0029,
            -0.0053,
            0.0009
          ],
          "after": [
            -0.0709,
            0.0086,
            -0.1131,
            -0.0644,
            0.0679,
            0.1403,
            0.07,
            -0.1538,
            0.0164,
            -0.0167,
            -0.1207,
            -0.065,
            0.0925,
            -0.0799,
            0.0335,
            0.0824
          ]
        }
      }
    },
    {
      "step": 563,
      "word": "eiden",
      "loss": 2.2517,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0016,
            0.0011,
            0.1609,
            -0.0459,
            0.0443,
            -0.0789,
            -0.114,
            -0.1457,
            -0.0224,
            0.0953,
            0.157,
            0.0248,
            -0.0938,
            -0.0322,
            -0.0587,
            -0.1881
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3365,
            0.0629,
            0.763,
            -0.2699,
            0.1093,
            0.2222,
            -0.1824,
            -0.0565,
            0.4329,
            -0.2558,
            0.0935,
            0.0165,
            -0.3588,
            0.0636,
            0.4608,
            -0.3141
          ],
          "after": [
            0.0287,
            0.1096,
            -0.0449,
            -0.0763,
            0.0315,
            -0.0086,
            0.088,
            0.0883,
            -0.0809,
            -0.0745,
            0.0167,
            -0.0252,
            0.1102,
            0.0389,
            -0.0571,
            0.225
          ]
        },
        "position_0": {
          "grad": [
            -0.0043,
            -0.2234,
            0.0596,
            -0.0909,
            0.0107,
            0.0792,
            -0.3171,
            -0.1074,
            0.1801,
            0.1746,
            0.0298,
            -0.0444,
            -0.1822,
            -0.0098,
            -0.0303,
            -0.3695
          ],
          "after": [
            0.0363,
            -0.1361,
            -0.2948,
            0.0634,
            0.0292,
            -0.0554,
            -0.1447,
            0.0599,
            0.1007,
            0.0025,
            0.0223,
            0.1051,
            0.044,
            -0.0042,
            -0.1349,
            0.0501
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0007,
            -0.0004,
            -0.0025,
            0.0012,
            -0.0002,
            -0.0004,
            -0.0001,
            0.0012,
            -0.0,
            -0.0007,
            0.0014,
            -0.0012,
            0.0001,
            0.0012,
            -0.0003
          ],
          "after": [
            -0.0708,
            0.0091,
            -0.1131,
            -0.0635,
            0.0671,
            0.1405,
            0.0698,
            -0.1537,
            0.0159,
            -0.0172,
            -0.1209,
            -0.0653,
            0.0926,
            -0.0798,
            0.034,
            0.0826
          ]
        }
      }
    },
    {
      "step": 564,
      "word": "janiylah",
      "loss": 2.2259,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1225,
            0.1267,
            0.0356,
            0.0552,
            -0.0829,
            0.0388,
            0.0028,
            0.0429,
            0.1658,
            -0.0517,
            -0.2207,
            -0.0375,
            0.1496,
            0.0276,
            -0.0792,
            0.1828
          ],
          "after": [
            0.0017,
            0.0009,
            0.1608,
            -0.0462,
            0.0442,
            -0.079,
            -0.1144,
            -0.1454,
            -0.023,
            0.0953,
            0.1573,
            0.0246,
            -0.094,
            -0.032,
            -0.0584,
            -0.1885
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1234,
            0.0952,
            -0.0732,
            0.0088,
            0.0692,
            -0.1358,
            0.2538,
            0.0173,
            -0.1797,
            -0.017,
            -0.0433,
            0.0443,
            0.0644,
            -0.0842,
            -0.135,
            0.1801
          ],
          "after": [
            0.029,
            0.1097,
            -0.0454,
            -0.0759,
            0.031,
            -0.0085,
            0.0881,
            0.0885,
            -0.0811,
            -0.0747,
            0.0163,
            -0.0255,
            0.1105,
            0.039,
            -0.0569,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            -0.0028,
            0.0518,
            0.005,
            0.1013,
            0.0164,
            -0.0895,
            0.1197,
            0.0394,
            -0.1635,
            -0.0312,
            0.0248,
            0.0389,
            0.213,
            -0.1209,
            -0.1005,
            0.0732
          ],
          "after": [
            0.0366,
            -0.1357,
            -0.2948,
            0.0635,
            0.029,
            -0.0555,
            -0.1448,
            0.0602,
            0.1007,
            0.0025,
            0.0223,
            0.1051,
            0.044,
            -0.0034,
            -0.1347,
            0.0501
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            -0.0004,
            -0.0004,
            -0.0006,
            -0.0009,
            -0.0004,
            -0.0019,
            -0.0003,
            0.0005,
            -0.0003,
            -0.0001,
            -0.0006,
            -0.0006,
            0.0013,
            0.0008,
            -0.0014
          ],
          "after": [
            -0.0708,
            0.0095,
            -0.113,
            -0.0626,
            0.0665,
            0.1407,
            0.0698,
            -0.1535,
            0.0153,
            -0.0176,
            -0.1211,
            -0.0656,
            0.0928,
            -0.0798,
            0.0343,
            0.0829
          ]
        }
      }
    },
    {
      "step": 565,
      "word": "pinchos",
      "loss": 2.9745,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0018,
            0.0007,
            0.1607,
            -0.0465,
            0.0441,
            -0.079,
            -0.1147,
            -0.1452,
            -0.0235,
            0.0953,
            0.1576,
            0.0244,
            -0.0942,
            -0.0317,
            -0.0583,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0001,
            0.068,
            -0.1606,
            -0.0106,
            0.0536,
            -0.1477,
            0.2539,
            0.0291,
            -0.1932,
            0.0172,
            -0.0072,
            0.0135,
            0.0706,
            -0.1197,
            -0.1542,
            0.1581
          ],
          "after": [
            0.0293,
            0.1097,
            -0.0457,
            -0.0756,
            0.0306,
            -0.0082,
            0.088,
            0.0885,
            -0.0812,
            -0.0749,
            0.0161,
            -0.0259,
            0.1107,
            0.0393,
            -0.0566,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            0.055,
            0.1301,
            -0.059,
            0.0814,
            -0.0362,
            -0.1083,
            0.257,
            0.0193,
            -0.2261,
            -0.0371,
            -0.0168,
            -0.01,
            0.0326,
            0.0974,
            0.0229,
            0.1997
          ],
          "after": [
            0.0367,
            -0.1357,
            -0.2947,
            0.0634,
            0.029,
            -0.0554,
            -0.1452,
            0.0604,
            0.1011,
            0.0027,
            0.0224,
            0.1052,
            0.0439,
            -0.003,
            -0.1345,
            0.0499
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0001,
            0.0005,
            -0.0009,
            0.0016,
            0.0003,
            0.0006,
            -0.0002,
            0.0003,
            0.0003,
            0.0004,
            -0.0005,
            0.0009,
            -0.0002,
            -0.0014,
            0.0003
          ],
          "after": [
            -0.0709,
            0.0099,
            -0.113,
            -0.0618,
            0.0658,
            0.1408,
            0.0697,
            -0.1534,
            0.0148,
            -0.018,
            -0.1213,
            -0.0657,
            0.0929,
            -0.0798,
            0.0347,
            0.0832
          ]
        }
      }
    },
    {
      "step": 566,
      "word": "vishan",
      "loss": 2.02,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1027,
            0.1237,
            -0.02,
            0.0569,
            0.001,
            -0.0689,
            0.0169,
            -0.0505,
            0.0326,
            -0.1918,
            -0.094,
            0.1532,
            -0.0953,
            -0.0578,
            -0.0068,
            0.1111
          ],
          "after": [
            0.002,
            0.0003,
            0.1606,
            -0.0468,
            0.044,
            -0.079,
            -0.115,
            -0.1448,
            -0.024,
            0.0957,
            0.158,
            0.024,
            -0.0943,
            -0.0315,
            -0.0581,
            -0.1891
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0511,
            0.0397,
            -0.1145,
            0.0107,
            0.0393,
            -0.1346,
            0.237,
            0.0154,
            -0.1624,
            0.0373,
            -0.0433,
            0.0323,
            0.0705,
            -0.079,
            -0.15,
            0.1243
          ],
          "after": [
            0.0296,
            0.1097,
            -0.0459,
            -0.0753,
            0.0301,
            -0.0079,
            0.0878,
            0.0885,
            -0.0811,
            -0.0751,
            0.0159,
            -0.0262,
            0.1108,
            0.0396,
            -0.0563,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            -0.0826,
            0.0454,
            -0.008,
            0.0631,
            0.0776,
            0.0777,
            -0.0002,
            0.0111,
            -0.0459,
            -0.0292,
            -0.0099,
            -0.0496,
            0.0126,
            -0.0546,
            -0.0179,
            0.137
          ],
          "after": [
            0.037,
            -0.1357,
            -0.2946,
            0.0633,
            0.0288,
            -0.0555,
            -0.1455,
            0.0605,
            0.1015,
            0.0029,
            0.0225,
            0.1053,
            0.0438,
            -0.0026,
            -0.1344,
            0.0496
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0007,
            0.0007,
            -0.0033,
            0.0021,
            -0.0002,
            -0.0024,
            -0.0017,
            0.0009,
            -0.0007,
            0.001,
            -0.0012,
            -0.001,
            0.0022,
            -0.0015,
            -0.002
          ],
          "after": [
            -0.0711,
            0.0103,
            -0.113,
            -0.0608,
            0.0651,
            0.1409,
            0.0698,
            -0.153,
            0.0142,
            -0.0182,
            -0.1216,
            -0.0658,
            0.093,
            -0.08,
            0.0352,
            0.0836
          ]
        }
      }
    },
    {
      "step": 567,
      "word": "iyona",
      "loss": 2.3915,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2784,
            -0.061,
            -0.0965,
            -0.0774,
            -0.0415,
            -0.0239,
            -0.0402,
            -0.0084,
            -0.0964,
            0.1948,
            0.1007,
            -0.249,
            0.115,
            0.0959,
            0.0036,
            -0.2089
          ],
          "after": [
            0.0018,
            0.0001,
            0.1607,
            -0.0469,
            0.044,
            -0.0788,
            -0.1152,
            -0.1446,
            -0.0242,
            0.0956,
            0.1581,
            0.024,
            -0.0944,
            -0.0314,
            -0.058,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0039,
            -0.0553,
            -0.0374,
            -0.0271,
            0.054,
            -0.0482,
            0.0627,
            -0.0082,
            -0.0704,
            0.0697,
            0.0027,
            -0.0207,
            0.0413,
            -0.0391,
            -0.1061,
            0.0213
          ],
          "after": [
            0.0299,
            0.1097,
            -0.046,
            -0.075,
            0.0297,
            -0.0076,
            0.0876,
            0.0886,
            -0.081,
            -0.0753,
            0.0158,
            -0.0265,
            0.1109,
            0.0399,
            -0.0559,
            0.2248
          ]
        },
        "position_0": {
          "grad": [
            -0.0472,
            -0.2852,
            0.0682,
            -0.2107,
            0.0468,
            0.1311,
            -0.319,
            0.0169,
            0.1296,
            0.0058,
            0.037,
            -0.2105,
            -0.2287,
            0.0691,
            0.0087,
            -0.206
          ],
          "after": [
            0.0374,
            -0.1352,
            -0.2947,
            0.0635,
            0.0285,
            -0.0558,
            -0.1455,
            0.0606,
            0.1016,
            0.0031,
            0.0226,
            0.1058,
            0.044,
            -0.0023,
            -0.1343,
            0.0495
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0001,
            -0.0008,
            0.0003,
            -0.0007,
            0.0009,
            0.001,
            0.0007,
            0.0005,
            0.0004,
            0.0002,
            0.0002,
            0.0001,
            0.0,
            0.0,
            0.0005
          ],
          "after": [
            -0.0712,
            0.0106,
            -0.113,
            -0.0599,
            0.0645,
            0.141,
            0.0699,
            -0.1528,
            0.0137,
            -0.0185,
            -0.1218,
            -0.0658,
            0.0931,
            -0.0802,
            0.0356,
            0.0839
          ]
        }
      }
    },
    {
      "step": 568,
      "word": "bobby",
      "loss": 2.9194,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0016,
            -0.0001,
            0.1608,
            -0.0471,
            0.044,
            -0.0787,
            -0.1153,
            -0.1443,
            -0.0245,
            0.0956,
            0.1582,
            0.024,
            -0.0946,
            -0.0313,
            -0.0578,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0375,
            0.0816,
            -0.0148,
            0.0448,
            -0.0551,
            -0.2093,
            0.1078,
            0.02,
            -0.2575,
            0.0785,
            0.0028,
            -0.0049,
            0.1804,
            0.0436,
            -0.1748,
            0.0984
          ],
          "after": [
            0.0302,
            0.1096,
            -0.0461,
            -0.0749,
            0.0294,
            -0.0071,
            0.0873,
            0.0885,
            -0.0808,
            -0.0757,
            0.0157,
            -0.0267,
            0.1107,
            0.0402,
            -0.0555,
            0.2246
          ]
        },
        "position_0": {
          "grad": [
            0.0417,
            0.0378,
            0.1135,
            0.0317,
            0.0313,
            -0.0001,
            0.1341,
            -0.0259,
            -0.1755,
            -0.0894,
            0.0767,
            0.0195,
            0.2048,
            -0.105,
            -0.0231,
            0.3558
          ],
          "after": [
            0.0376,
            -0.1349,
            -0.2951,
            0.0636,
            0.0282,
            -0.0561,
            -0.1456,
            0.0607,
            0.1019,
            0.0034,
            0.0224,
            0.1062,
            0.044,
            -0.0019,
            -0.1341,
            0.0491
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0058,
            0.002,
            0.0064,
            0.0014,
            -0.0018,
            -0.0047,
            0.0013,
            -0.0005,
            -0.0044,
            0.0055,
            0.0026,
            -0.001,
            0.0096,
            0.007,
            -0.01,
            -0.001
          ],
          "after": [
            -0.072,
            0.0106,
            -0.1137,
            -0.0593,
            0.0642,
            0.1415,
            0.0698,
            -0.1525,
            0.0138,
            -0.0191,
            -0.1223,
            -0.0657,
            0.0924,
            -0.0811,
            0.0365,
            0.0843
          ]
        }
      }
    },
    {
      "step": 569,
      "word": "diem",
      "loss": 2.6065,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0015,
            -0.0003,
            0.1608,
            -0.0472,
            0.0439,
            -0.0787,
            -0.1155,
            -0.1441,
            -0.0246,
            0.0955,
            0.1583,
            0.024,
            -0.0947,
            -0.0312,
            -0.0577,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1449,
            0.3037,
            -0.0819,
            0.4599,
            -0.3081,
            0.0309,
            0.1918,
            0.1369,
            -0.1126,
            -0.1531,
            -0.0173,
            0.3494,
            -0.0351,
            -0.2405,
            0.3908,
            0.1767
          ],
          "after": [
            0.0305,
            0.1093,
            -0.0461,
            -0.0755,
            0.0294,
            -0.0068,
            0.087,
            0.088,
            -0.0805,
            -0.0758,
            0.0156,
            -0.0273,
            0.1107,
            0.0407,
            -0.0554,
            0.2243
          ]
        },
        "position_0": {
          "grad": [
            -0.0878,
            0.0288,
            -0.0401,
            -0.0818,
            0.0884,
            -0.1011,
            0.0582,
            -0.0529,
            0.037,
            -0.0673,
            -0.1467,
            -0.0086,
            0.032,
            0.0615,
            0.1207,
            0.177
          ],
          "after": [
            0.0381,
            -0.1347,
            -0.2953,
            0.0639,
            0.0278,
            -0.0561,
            -0.1457,
            0.061,
            0.1022,
            0.0039,
            0.0226,
            0.1066,
            0.0439,
            -0.0016,
            -0.1343,
            0.0485
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0009,
            0.002,
            0.0045,
            -0.0051,
            0.0006,
            -0.0051,
            0.0021,
            0.0008,
            0.0019,
            0.0029,
            0.0057,
            -0.0041,
            -0.0025,
            0.0025,
            0.0012
          ],
          "after": [
            -0.0726,
            0.0107,
            -0.1145,
            -0.0593,
            0.0644,
            0.1418,
            0.0701,
            -0.1526,
            0.0139,
            -0.0198,
            -0.1231,
            -0.0662,
            0.0922,
            -0.0816,
            0.0372,
            0.0845
          ]
        }
      }
    },
    {
      "step": 570,
      "word": "zhander",
      "loss": 2.6895,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0963,
            0.1051,
            -0.1809,
            0.0614,
            -0.1981,
            0.0065,
            -0.0039,
            -0.0668,
            0.0079,
            0.0283,
            -0.1004,
            -0.0308,
            0.0353,
            0.0952,
            -0.0059,
            0.0704
          ],
          "after": [
            0.0013,
            -0.0006,
            0.1611,
            -0.0474,
            0.0441,
            -0.0786,
            -0.1156,
            -0.1437,
            -0.0248,
            0.0954,
            0.1586,
            0.0241,
            -0.0949,
            -0.0314,
            -0.0576,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0491,
            -0.1856,
            -0.1252,
            -0.2029,
            -0.3419,
            0.0653,
            -0.1815,
            -0.0565,
            0.1183,
            0.0722,
            0.0314,
            -0.1615,
            0.1099,
            0.1376,
            0.202,
            -0.0478
          ],
          "after": [
            0.0308,
            0.1091,
            -0.046,
            -0.0758,
            0.0297,
            -0.0065,
            0.0869,
            0.0878,
            -0.0803,
            -0.0759,
            0.0155,
            -0.0276,
            0.1105,
            0.0409,
            -0.0555,
            0.2241
          ]
        },
        "position_0": {
          "grad": [
            0.0233,
            0.0536,
            -0.0274,
            0.1424,
            0.0349,
            -0.0459,
            0.1732,
            0.0103,
            0.0298,
            0.0261,
            0.012,
            0.0659,
            -0.0028,
            -0.0404,
            0.0225,
            0.0323
          ],
          "after": [
            0.0384,
            -0.1345,
            -0.2954,
            0.0638,
            0.0274,
            -0.056,
            -0.146,
            0.0612,
            0.1024,
            0.0043,
            0.0227,
            0.1067,
            0.0438,
            -0.0014,
            -0.1344,
            0.048
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            0.0003,
            -0.0011,
            0.0008,
            -0.0001,
            0.0004,
            0.0022,
            0.0011,
            -0.001,
            0.0006,
            0.0001,
            0.0002,
            0.0013,
            -0.0015,
            0.0001,
            0.0001
          ],
          "after": [
            -0.073,
            0.0108,
            -0.1151,
            -0.0594,
            0.0646,
            0.1421,
            0.0702,
            -0.1529,
            0.014,
            -0.0205,
            -0.1237,
            -0.0667,
            0.0919,
            -0.0818,
            0.0378,
            0.0847
          ]
        }
      }
    },
    {
      "step": 571,
      "word": "kaylynn",
      "loss": 1.933,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0522,
            -0.1558,
            0.0791,
            -0.0996,
            0.0687,
            0.1214,
            -0.0507,
            0.0371,
            -0.0601,
            0.1032,
            0.0348,
            0.1204,
            -0.0277,
            0.0227,
            -0.0231,
            -0.0346
          ],
          "after": [
            0.001,
            -0.0006,
            0.1612,
            -0.0474,
            0.0442,
            -0.0788,
            -0.1156,
            -0.1435,
            -0.0248,
            0.0952,
            0.1588,
            0.024,
            -0.095,
            -0.0315,
            -0.0575,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1105,
            0.1376,
            -0.094,
            0.0262,
            0.0072,
            -0.1745,
            0.3023,
            0.011,
            -0.2099,
            -0.0067,
            -0.0289,
            0.0041,
            0.09,
            -0.1135,
            -0.0926,
            0.2086
          ],
          "after": [
            0.0312,
            0.1089,
            -0.0458,
            -0.076,
            0.0299,
            -0.0061,
            0.0865,
            0.0876,
            -0.08,
            -0.0761,
            0.0154,
            -0.0279,
            0.1103,
            0.0413,
            -0.0555,
            0.2237
          ]
        },
        "position_0": {
          "grad": [
            -0.0778,
            0.051,
            -0.096,
            -0.0091,
            0.0645,
            -0.0684,
            0.1448,
            -0.0218,
            -0.0481,
            0.0207,
            0.0129,
            0.0392,
            0.0405,
            -0.0598,
            -0.0982,
            -0.0409
          ],
          "after": [
            0.0389,
            -0.1346,
            -0.2953,
            0.0638,
            0.027,
            -0.0558,
            -0.1464,
            0.0614,
            0.1026,
            0.0045,
            0.0228,
            0.1068,
            0.0437,
            -0.0009,
            -0.1343,
            0.0476
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0002,
            0.0001,
            0.0008,
            0.0007,
            0.0008,
            0.0014,
            0.0008,
            -0.0003,
            -0.0004,
            -0.0002,
            0.0014,
            -0.0002,
            -0.0014,
            0.0002,
            0.0016
          ],
          "after": [
            -0.0732,
            0.0109,
            -0.1156,
            -0.0596,
            0.0647,
            0.1422,
            0.0702,
            -0.1532,
            0.0142,
            -0.0211,
            -0.1243,
            -0.0672,
            0.0916,
            -0.0819,
            0.0382,
            0.0846
          ]
        }
      }
    },
    {
      "step": 572,
      "word": "zigmund",
      "loss": 3.2076,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0008,
            -0.0006,
            0.1613,
            -0.0474,
            0.0443,
            -0.079,
            -0.1156,
            -0.1434,
            -0.0249,
            0.095,
            0.1589,
            0.0239,
            -0.0951,
            -0.0316,
            -0.0574,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0985,
            0.1323,
            -0.166,
            0.0225,
            0.0781,
            -0.1727,
            0.2512,
            0.0274,
            -0.2315,
            0.0201,
            -0.048,
            0.0269,
            0.1348,
            -0.0827,
            -0.2076,
            0.1885
          ],
          "after": [
            0.0316,
            0.1086,
            -0.0455,
            -0.0762,
            0.03,
            -0.0057,
            0.0861,
            0.0873,
            -0.0795,
            -0.0762,
            0.0155,
            -0.0281,
            0.11,
            0.0418,
            -0.0553,
            0.2232
          ]
        },
        "position_0": {
          "grad": [
            0.008,
            0.0653,
            -0.0228,
            0.1609,
            0.0626,
            -0.0292,
            0.1843,
            -0.0005,
            0.0033,
            0.0145,
            0.0339,
            0.0823,
            0.0224,
            -0.0297,
            0.0146,
            0.0641
          ],
          "after": [
            0.0393,
            -0.1347,
            -0.2951,
            0.0636,
            0.0265,
            -0.0555,
            -0.1469,
            0.0616,
            0.1028,
            0.0047,
            0.0228,
            0.1067,
            0.0435,
            -0.0005,
            -0.1343,
            0.0472
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0019,
            -0.0022,
            -0.0025,
            0.0032,
            -0.0006,
            0.0032,
            0.0001,
            0.0004,
            0.004,
            -0.0,
            0.0005,
            0.0027,
            -0.0008,
            -0.003,
            0.0017
          ],
          "after": [
            -0.0732,
            0.0108,
            -0.1158,
            -0.0594,
            0.0644,
            0.1424,
            0.0699,
            -0.1535,
            0.0143,
            -0.0219,
            -0.1247,
            -0.0676,
            0.0912,
            -0.0819,
            0.0388,
            0.0844
          ]
        }
      }
    },
    {
      "step": 573,
      "word": "akbar",
      "loss": 3.0252,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3904,
            0.0624,
            0.2418,
            -0.0518,
            0.4212,
            -0.0899,
            0.4659,
            -0.0227,
            0.1929,
            -0.1429,
            -0.1201,
            0.3817,
            -0.2312,
            -0.2418,
            -0.2205,
            0.075
          ],
          "after": [
            0.0011,
            -0.0007,
            0.1611,
            -0.0473,
            0.0439,
            -0.079,
            -0.1163,
            -0.1432,
            -0.0252,
            0.095,
            0.1592,
            0.0233,
            -0.0949,
            -0.0313,
            -0.0569,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0402,
            0.131,
            -0.0558,
            0.0075,
            0.0254,
            -0.2339,
            0.2254,
            0.0106,
            -0.205,
            0.0056,
            -0.05,
            -0.0143,
            0.1186,
            -0.032,
            -0.1772,
            0.1482
          ],
          "after": [
            0.032,
            0.1081,
            -0.0452,
            -0.0765,
            0.0301,
            -0.005,
            0.0857,
            0.087,
            -0.079,
            -0.0763,
            0.0156,
            -0.0283,
            0.1096,
            0.0422,
            -0.0551,
            0.2227
          ]
        },
        "position_0": {
          "grad": [
            0.1078,
            -0.0137,
            0.0972,
            -0.1561,
            0.0851,
            0.0034,
            -0.2646,
            -0.1317,
            0.0844,
            0.0818,
            0.2739,
            0.0895,
            -0.1462,
            0.0058,
            -0.1082,
            -0.2142
          ],
          "after": [
            0.0393,
            -0.1347,
            -0.2952,
            0.0636,
            0.0259,
            -0.0553,
            -0.1471,
            0.0621,
            0.1028,
            0.0046,
            0.0223,
            0.1065,
            0.0436,
            -0.0002,
            -0.1341,
            0.0471
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0001,
            -0.001,
            -0.0033,
            0.001,
            -0.0042,
            0.0023,
            0.0016,
            0.0013,
            -0.0023,
            -0.0031,
            -0.0026,
            0.0006,
            0.0005,
            0.002,
            0.0024
          ],
          "after": [
            -0.0732,
            0.0106,
            -0.1158,
            -0.059,
            0.0642,
            0.143,
            0.0695,
            -0.154,
            0.0142,
            -0.0223,
            -0.1248,
            -0.0678,
            0.0908,
            -0.0819,
            0.0392,
            0.0839
          ]
        }
      }
    },
    {
      "step": 574,
      "word": "jacobi",
      "loss": 2.5948,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1024,
            0.0817,
            0.1048,
            0.1349,
            -0.0035,
            -0.0389,
            0.1336,
            0.0764,
            -0.1226,
            0.0783,
            0.1164,
            0.0651,
            0.0171,
            -0.0531,
            -0.1344,
            -0.0062
          ],
          "after": [
            0.0015,
            -0.0009,
            0.1608,
            -0.0474,
            0.0436,
            -0.0788,
            -0.117,
            -0.1432,
            -0.0253,
            0.095,
            0.1593,
            0.0228,
            -0.0947,
            -0.0309,
            -0.0562,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0607,
            0.1106,
            -0.1085,
            0.0074,
            0.0356,
            -0.1524,
            0.2228,
            0.0396,
            -0.2592,
            -0.0076,
            -0.0532,
            0.0027,
            0.1322,
            -0.0194,
            -0.1655,
            0.1761
          ],
          "after": [
            0.0324,
            0.1077,
            -0.0448,
            -0.0767,
            0.0302,
            -0.0043,
            0.0851,
            0.0866,
            -0.0784,
            -0.0764,
            0.0158,
            -0.0285,
            0.1091,
            0.0426,
            -0.0547,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0031,
            0.0818,
            0.0081,
            0.1347,
            0.038,
            -0.1129,
            0.1568,
            0.0313,
            -0.2295,
            -0.0483,
            0.0615,
            0.049,
            0.3084,
            -0.1368,
            -0.1371,
            0.1222
          ],
          "after": [
            0.0394,
            -0.135,
            -0.2953,
            0.0634,
            0.0253,
            -0.0549,
            -0.1474,
            0.0625,
            0.1032,
            0.0047,
            0.0219,
            0.1062,
            0.0433,
            0.0004,
            -0.1336,
            0.0468
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0016,
            0.0005,
            -0.0003,
            -0.0011,
            -0.001,
            -0.0002,
            0.0008,
            0.0002,
            0.0009,
            0.0014,
            0.0002,
            -0.0004,
            -0.0016,
            0.0001,
            -0.002
          ],
          "after": [
            -0.073,
            0.0107,
            -0.1159,
            -0.0586,
            0.064,
            0.1436,
            0.0692,
            -0.1545,
            0.0141,
            -0.0228,
            -0.125,
            -0.0679,
            0.0905,
            -0.0818,
            0.0395,
            0.0837
          ]
        }
      }
    },
    {
      "step": 575,
      "word": "mazzy",
      "loss": 2.4361,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0177,
            0.056,
            0.0243,
            0.2721,
            0.1412,
            -0.1429,
            0.2979,
            -0.0172,
            0.0154,
            0.0718,
            0.1584,
            0.1545,
            0.0205,
            -0.0877,
            -0.0191,
            -0.0296
          ],
          "after": [
            0.0018,
            -0.0012,
            0.1605,
            -0.048,
            0.0432,
            -0.0784,
            -0.118,
            -0.1432,
            -0.0254,
            0.0948,
            0.1591,
            0.0222,
            -0.0945,
            -0.0305,
            -0.0556,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1247,
            0.151,
            -0.2202,
            0.0602,
            -0.0158,
            -0.1823,
            0.3115,
            0.0319,
            -0.2712,
            0.0033,
            -0.0919,
            -0.0145,
            0.202,
            -0.0669,
            -0.2032,
            0.1859
          ],
          "after": [
            0.033,
            0.1071,
            -0.0444,
            -0.0769,
            0.0302,
            -0.0036,
            0.0845,
            0.0862,
            -0.0777,
            -0.0765,
            0.0162,
            -0.0286,
            0.1085,
            0.043,
            -0.0543,
            0.2214
          ]
        },
        "position_0": {
          "grad": [
            -0.0173,
            0.0649,
            -0.0489,
            0.0319,
            0.1539,
            -0.0227,
            0.2339,
            0.097,
            0.0226,
            0.0282,
            0.0086,
            0.123,
            -0.0733,
            -0.0569,
            -0.0059,
            0.1176
          ],
          "after": [
            0.0394,
            -0.1353,
            -0.2952,
            0.0632,
            0.0246,
            -0.0544,
            -0.1479,
            0.0625,
            0.1034,
            0.0047,
            0.0214,
            0.1057,
            0.0431,
            0.0011,
            -0.1332,
            0.0465
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0022,
            0.002,
            -0.0027,
            0.0015,
            -0.0009,
            -0.0005,
            0.0021,
            -0.0004,
            -0.0008,
            0.0013,
            -0.0004,
            -0.0006,
            0.0031,
            0.0017,
            -0.0039,
            0.0007
          ],
          "after": [
            -0.0731,
            0.0105,
            -0.1156,
            -0.0584,
            0.064,
            0.1442,
            0.0688,
            -0.155,
            0.0141,
            -0.0233,
            -0.1251,
            -0.068,
            0.09,
            -0.0819,
            0.04,
            0.0835
          ]
        }
      }
    },
    {
      "step": 576,
      "word": "zelie",
      "loss": 2.0814,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0021,
            -0.0015,
            0.1603,
            -0.0484,
            0.0428,
            -0.0781,
            -0.1188,
            -0.1431,
            -0.0255,
            0.0946,
            0.1589,
            0.0216,
            -0.0944,
            -0.0301,
            -0.0551,
            -0.19
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3833,
            -0.2151,
            0.4495,
            0.1781,
            -0.2544,
            0.2728,
            -0.7924,
            -0.0275,
            0.6423,
            -0.0476,
            0.1746,
            0.2288,
            -0.3654,
            0.176,
            0.4997,
            -0.272
          ],
          "after": [
            0.0329,
            0.1069,
            -0.0443,
            -0.0775,
            0.0305,
            -0.0032,
            0.0844,
            0.086,
            -0.0775,
            -0.0765,
            0.0161,
            -0.029,
            0.1084,
            0.0431,
            -0.0543,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            0.0271,
            0.0409,
            -0.0266,
            0.1393,
            0.0644,
            -0.0591,
            0.1547,
            -0.0097,
            0.0716,
            0.0306,
            0.029,
            0.0518,
            -0.0136,
            -0.0318,
            0.0097,
            0.0107
          ],
          "after": [
            0.0394,
            -0.1356,
            -0.2951,
            0.0628,
            0.0238,
            -0.0539,
            -0.1485,
            0.0626,
            0.1036,
            0.0046,
            0.021,
            0.1052,
            0.043,
            0.0018,
            -0.1329,
            0.0462
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0011,
            0.0024,
            0.0006,
            -0.0005,
            -0.0016,
            -0.0035,
            0.0007,
            0.0007,
            -0.0007,
            0.0004,
            0.0005,
            -0.0007,
            -0.0001,
            -0.0001,
            -0.0003
          ],
          "after": [
            -0.0733,
            0.0105,
            -0.1157,
            -0.0583,
            0.064,
            0.1448,
            0.0687,
            -0.1554,
            0.014,
            -0.0237,
            -0.1252,
            -0.0681,
            0.0896,
            -0.0819,
            0.0405,
            0.0833
          ]
        }
      }
    },
    {
      "step": 577,
      "word": "delany",
      "loss": 1.9826,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0192,
            0.116,
            -0.1462,
            -0.0136,
            -0.106,
            -0.098,
            0.0481,
            -0.0655,
            0.0409,
            -0.0042,
            -0.0301,
            0.0266,
            -0.0283,
            -0.0293,
            0.0396,
            0.031
          ],
          "after": [
            0.0024,
            -0.0019,
            0.1602,
            -0.0488,
            0.0426,
            -0.0775,
            -0.1196,
            -0.1429,
            -0.0257,
            0.0945,
            0.1588,
            0.0211,
            -0.0943,
            -0.0297,
            -0.0547,
            -0.1901
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1077,
            -0.3085,
            0.2266,
            -0.1464,
            0.1373,
            0.1443,
            -0.4346,
            -0.0717,
            0.4239,
            0.2218,
            0.1699,
            0.149,
            -0.2438,
            0.0537,
            0.0328,
            -0.1505
          ],
          "after": [
            0.0328,
            0.1069,
            -0.0445,
            -0.0777,
            0.0306,
            -0.003,
            0.0846,
            0.086,
            -0.0777,
            -0.0768,
            0.0158,
            -0.0295,
            0.1085,
            0.0431,
            -0.0543,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            -0.066,
            0.0359,
            -0.0317,
            -0.0659,
            0.1013,
            -0.0592,
            0.0592,
            -0.0525,
            0.0023,
            -0.0234,
            -0.0807,
            0.0406,
            0.0309,
            0.0228,
            0.0464,
            0.1278
          ],
          "after": [
            0.0396,
            -0.1359,
            -0.2949,
            0.0625,
            0.023,
            -0.0534,
            -0.149,
            0.0628,
            0.1037,
            0.0046,
            0.0208,
            0.1047,
            0.0428,
            0.0023,
            -0.1327,
            0.0458
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0012,
            0.0011,
            -0.0017,
            0.0004,
            -0.001,
            -0.0016,
            -0.0003,
            0.001,
            0.0008,
            0.0007,
            -0.0003,
            0.0001,
            0.0002,
            -0.0009,
            -0.0002
          ],
          "after": [
            -0.0734,
            0.0106,
            -0.1159,
            -0.058,
            0.064,
            0.1455,
            0.0688,
            -0.1558,
            0.0138,
            -0.0241,
            -0.1254,
            -0.0682,
            0.0893,
            -0.082,
            0.0409,
            0.0832
          ]
        }
      }
    },
    {
      "step": 578,
      "word": "bernadine",
      "loss": 2.5357,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2483,
            0.0283,
            0.0598,
            -0.0403,
            0.1365,
            0.0142,
            0.09,
            -0.0337,
            0.0352,
            -0.0762,
            -0.1017,
            0.139,
            0.0417,
            -0.0318,
            0.0033,
            0.2394
          ],
          "after": [
            0.0029,
            -0.0022,
            0.1601,
            -0.0491,
            0.0423,
            -0.0771,
            -0.1203,
            -0.1427,
            -0.0259,
            0.0945,
            0.1589,
            0.0205,
            -0.0942,
            -0.0293,
            -0.0544,
            -0.1904
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.446,
            -0.1514,
            -0.0429,
            -0.1148,
            -0.0521,
            0.3216,
            -0.4041,
            -0.0623,
            0.2981,
            0.0576,
            0.1069,
            -0.2313,
            -0.1304,
            0.123,
            0.1489,
            -0.3684
          ],
          "after": [
            0.0322,
            0.1072,
            -0.0446,
            -0.0777,
            0.0307,
            -0.0032,
            0.0849,
            0.0862,
            -0.0781,
            -0.0772,
            0.0153,
            -0.0297,
            0.1087,
            0.0429,
            -0.0544,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            0.034,
            -0.0001,
            0.0536,
            -0.0047,
            -0.0076,
            0.0045,
            0.0468,
            -0.0091,
            -0.0568,
            -0.029,
            -0.004,
            0.0161,
            0.0602,
            -0.0943,
            -0.0208,
            0.1562
          ],
          "after": [
            0.0396,
            -0.1362,
            -0.2949,
            0.0623,
            0.0223,
            -0.0529,
            -0.1496,
            0.063,
            0.1038,
            0.0046,
            0.0206,
            0.1043,
            0.0426,
            0.0029,
            -0.1325,
            0.0453
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0033,
            -0.0004,
            -0.0013,
            -0.0002,
            -0.0006,
            0.0002,
            -0.0032,
            -0.0007,
            0.0001,
            0.0001,
            0.0009,
            -0.0021,
            -0.0018,
            0.0006,
            -0.0003,
            -0.002
          ],
          "after": [
            -0.0738,
            0.0108,
            -0.1159,
            -0.0578,
            0.064,
            0.146,
            0.069,
            -0.156,
            0.0136,
            -0.0245,
            -0.1257,
            -0.068,
            0.0891,
            -0.0821,
            0.0413,
            0.0834
          ]
        }
      }
    },
    {
      "step": 579,
      "word": "zala",
      "loss": 2.0735,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3312,
            0.0364,
            -0.5394,
            -0.2938,
            -0.2772,
            -0.1021,
            -0.2918,
            0.0966,
            -0.1648,
            0.2312,
            -0.0621,
            -0.2276,
            0.0383,
            0.3161,
            0.2741,
            -0.564
          ],
          "after": [
            0.0029,
            -0.0026,
            0.1607,
            -0.0488,
            0.0423,
            -0.0766,
            -0.1206,
            -0.1427,
            -0.0258,
            0.0941,
            0.1591,
            0.0203,
            -0.0942,
            -0.0295,
            -0.0547,
            -0.1901
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0406,
            0.1539,
            -0.2138,
            0.0508,
            -0.1172,
            -0.1629,
            0.2713,
            -0.015,
            -0.2717,
            -0.0187,
            -0.0355,
            -0.0745,
            0.1971,
            -0.0417,
            -0.1558,
            0.1844
          ],
          "after": [
            0.0317,
            0.1072,
            -0.0445,
            -0.0777,
            0.0309,
            -0.0031,
            0.0851,
            0.0864,
            -0.0782,
            -0.0775,
            0.015,
            -0.0297,
            0.1087,
            0.0429,
            -0.0544,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            0.033,
            0.0624,
            -0.0418,
            0.1807,
            0.0754,
            -0.0757,
            0.2157,
            0.0006,
            0.0612,
            0.0386,
            0.0278,
            0.091,
            0.0005,
            -0.054,
            0.0234,
            0.017
          ],
          "after": [
            0.0396,
            -0.1366,
            -0.2948,
            0.0619,
            0.0216,
            -0.0523,
            -0.1502,
            0.0631,
            0.1039,
            0.0046,
            0.0205,
            0.1037,
            0.0424,
            0.0036,
            -0.1324,
            0.0449
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0009,
            0.0002,
            -0.0007,
            0.0001,
            0.0002,
            -0.0013,
            0.0005,
            0.0006,
            -0.001,
            -0.001,
            -0.0002,
            -0.0003,
            0.0009,
            0.0007,
            -0.0002
          ],
          "after": [
            -0.0742,
            0.011,
            -0.116,
            -0.0575,
            0.064,
            0.1465,
            0.0693,
            -0.1562,
            0.0133,
            -0.0247,
            -0.1258,
            -0.0678,
            0.089,
            -0.0823,
            0.0416,
            0.0835
          ]
        }
      }
    },
    {
      "step": 580,
      "word": "lyna",
      "loss": 2.3822,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.455,
            -0.0706,
            -0.3624,
            -0.1486,
            -0.248,
            -0.0398,
            -0.0682,
            -0.0069,
            -0.1366,
            0.275,
            0.1267,
            -0.4615,
            0.2095,
            0.2248,
            0.1397,
            -0.2217
          ],
          "after": [
            0.0024,
            -0.0028,
            0.1616,
            -0.0484,
            0.0426,
            -0.076,
            -0.1208,
            -0.1428,
            -0.0254,
            0.0934,
            0.159,
            0.0206,
            -0.0945,
            -0.0301,
            -0.0551,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0405,
            0.0935,
            -0.1634,
            0.0237,
            -0.0957,
            -0.141,
            0.2517,
            -0.0079,
            -0.2213,
            0.0211,
            -0.019,
            -0.0629,
            0.1586,
            -0.0716,
            -0.1028,
            0.1728
          ],
          "after": [
            0.0313,
            0.1072,
            -0.0443,
            -0.0778,
            0.0311,
            -0.003,
            0.0851,
            0.0866,
            -0.0781,
            -0.0777,
            0.0148,
            -0.0297,
            0.1086,
            0.0429,
            -0.0543,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            -0.0938,
            0.0647,
            -0.1209,
            -0.1097,
            -0.0796,
            0.0811,
            -0.1472,
            0.1423,
            0.0224,
            -0.0939,
            -0.251,
            0.146,
            -0.1841,
            0.0801,
            0.2386,
            -0.2152
          ],
          "after": [
            0.0398,
            -0.137,
            -0.2944,
            0.0616,
            0.0211,
            -0.052,
            -0.1506,
            0.0629,
            0.1039,
            0.0048,
            0.0207,
            0.103,
            0.0425,
            0.004,
            -0.1327,
            0.0447
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0013,
            0.0005,
            -0.0004,
            -0.0009,
            -0.0007,
            -0.0013,
            -0.001,
            -0.0011,
            -0.0033,
            0.0,
            -0.0014,
            -0.0004,
            -0.0001,
            0.002,
            -0.0002
          ],
          "after": [
            -0.0744,
            0.0111,
            -0.116,
            -0.0572,
            0.0641,
            0.1469,
            0.0697,
            -0.1562,
            0.0133,
            -0.0246,
            -0.1259,
            -0.0676,
            0.089,
            -0.0824,
            0.0417,
            0.0836
          ]
        }
      }
    },
    {
      "step": 581,
      "word": "camdyn",
      "loss": 2.2259,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1127,
            0.0789,
            0.1481,
            -0.0029,
            0.2076,
            -0.0584,
            0.2673,
            0.0809,
            0.0424,
            0.0034,
            0.0812,
            0.0966,
            -0.1177,
            -0.0868,
            -0.0675,
            -0.0171
          ],
          "after": [
            0.0022,
            -0.0031,
            0.1622,
            -0.048,
            0.0426,
            -0.0754,
            -0.1212,
            -0.143,
            -0.0251,
            0.0927,
            0.1588,
            0.0208,
            -0.0946,
            -0.0304,
            -0.0554,
            -0.1891
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0113,
            0.1103,
            -0.1683,
            0.0599,
            -0.0008,
            -0.1252,
            0.2251,
            0.0193,
            -0.235,
            0.0219,
            -0.0465,
            -0.0359,
            0.1512,
            -0.0742,
            -0.1528,
            0.1661
          ],
          "after": [
            0.031,
            0.107,
            -0.044,
            -0.078,
            0.0314,
            -0.0027,
            0.085,
            0.0868,
            -0.0779,
            -0.078,
            0.0146,
            -0.0296,
            0.1083,
            0.043,
            -0.0542,
            0.2207
          ]
        },
        "position_0": {
          "grad": [
            -0.0294,
            0.0455,
            -0.0252,
            0.1097,
            -0.0683,
            -0.014,
            0.1164,
            0.0576,
            -0.0761,
            0.0313,
            0.023,
            0.0523,
            0.0482,
            -0.0105,
            -0.0396,
            0.0474
          ],
          "after": [
            0.0401,
            -0.1374,
            -0.2939,
            0.0613,
            0.0208,
            -0.0517,
            -0.1511,
            0.0625,
            0.104,
            0.0048,
            0.0209,
            0.1023,
            0.0425,
            0.0043,
            -0.133,
            0.0446
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0006,
            0.0011,
            0.0009,
            0.0021,
            -0.0003,
            0.0004,
            -0.0002,
            -0.0009,
            0.0017,
            -0.0001,
            0.0006,
            0.0006,
            -0.0008,
            -0.0023,
            0.0
          ],
          "after": [
            -0.0746,
            0.0112,
            -0.1162,
            -0.0571,
            0.064,
            0.1473,
            0.07,
            -0.1563,
            0.0133,
            -0.0246,
            -0.126,
            -0.0674,
            0.0889,
            -0.0824,
            0.042,
            0.0838
          ]
        }
      }
    },
    {
      "step": 582,
      "word": "emersen",
      "loss": 2.3947,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0019,
            -0.0034,
            0.1627,
            -0.0477,
            0.0427,
            -0.0749,
            -0.1216,
            -0.1432,
            -0.0249,
            0.0922,
            0.1587,
            0.021,
            -0.0947,
            -0.0307,
            -0.0556,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0772,
            0.0954,
            0.7565,
            0.0646,
            -0.3382,
            0.547,
            -0.3205,
            -0.1025,
            0.4995,
            0.0817,
            0.2682,
            -0.2833,
            -0.1602,
            0.2389,
            0.3796,
            -0.4227
          ],
          "after": [
            0.0307,
            0.1068,
            -0.0443,
            -0.0783,
            0.0318,
            -0.003,
            0.0851,
            0.0872,
            -0.0781,
            -0.0783,
            0.0141,
            -0.0292,
            0.1082,
            0.0428,
            -0.0543,
            0.2208
          ]
        },
        "position_0": {
          "grad": [
            -0.008,
            -0.1232,
            0.0449,
            -0.0236,
            0.0375,
            0.0671,
            -0.1694,
            -0.0754,
            0.087,
            0.1622,
            0.0346,
            0.0265,
            -0.106,
            -0.041,
            -0.0426,
            -0.2477
          ],
          "after": [
            0.0404,
            -0.1376,
            -0.2937,
            0.061,
            0.0205,
            -0.0516,
            -0.1513,
            0.0624,
            0.104,
            0.0045,
            0.0209,
            0.1016,
            0.0426,
            0.0048,
            -0.1331,
            0.0447
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0002,
            0.0014,
            0.0011,
            -0.0012,
            0.0,
            0.0008,
            -0.0001,
            -0.0005,
            0.0007,
            0.0016,
            -0.0015,
            0.0012,
            -0.0013,
            -0.0009,
            -0.0
          ],
          "after": [
            -0.0747,
            0.0113,
            -0.1166,
            -0.0571,
            0.0641,
            0.1476,
            0.0701,
            -0.1562,
            0.0135,
            -0.0247,
            -0.1262,
            -0.0671,
            0.0887,
            -0.0824,
            0.0423,
            0.0839
          ]
        }
      }
    },
    {
      "step": 583,
      "word": "elysabeth",
      "loss": 2.9976,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1828,
            0.0165,
            0.182,
            0.0404,
            0.0245,
            0.0815,
            0.1402,
            0.0072,
            -0.0371,
            -0.07,
            -0.0409,
            0.1032,
            0.1293,
            -0.1368,
            -0.0477,
            0.3598
          ],
          "after": [
            0.0019,
            -0.0037,
            0.1629,
            -0.0475,
            0.0427,
            -0.0746,
            -0.1221,
            -0.1433,
            -0.0247,
            0.0918,
            0.1586,
            0.021,
            -0.0949,
            -0.0307,
            -0.0557,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0456,
            0.0774,
            0.1497,
            -0.0517,
            -0.0896,
            0.2153,
            0.1488,
            -0.0593,
            0.1797,
            -0.1363,
            0.0294,
            -0.2831,
            -0.0936,
            -0.0226,
            0.1802,
            -0.1785
          ],
          "after": [
            0.0304,
            0.1065,
            -0.0448,
            -0.0784,
            0.0323,
            -0.0034,
            0.085,
            0.0878,
            -0.0783,
            -0.0784,
            0.0135,
            -0.0286,
            0.1082,
            0.0427,
            -0.0545,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            -0.0126,
            -0.105,
            0.0379,
            -0.012,
            0.0188,
            0.055,
            -0.1347,
            -0.0605,
            0.07,
            0.1223,
            0.0339,
            0.0205,
            -0.0981,
            -0.0236,
            -0.0234,
            -0.1934
          ],
          "after": [
            0.0406,
            -0.1375,
            -0.2936,
            0.0608,
            0.0202,
            -0.0517,
            -0.1514,
            0.0625,
            0.1039,
            0.004,
            0.021,
            0.101,
            0.0429,
            0.0052,
            -0.1331,
            0.045
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0003,
            0.0032,
            0.0019,
            0.0016,
            0.0008,
            -0.001,
            -0.0013,
            -0.0007,
            0.0006,
            0.0013,
            -0.0016,
            0.0008,
            -0.0007,
            -0.0029,
            -0.0009
          ],
          "after": [
            -0.0748,
            0.0114,
            -0.1172,
            -0.0573,
            0.0639,
            0.1478,
            0.0704,
            -0.156,
            0.0137,
            -0.0249,
            -0.1266,
            -0.0667,
            0.0885,
            -0.0822,
            0.0427,
            0.084
          ]
        }
      }
    },
    {
      "step": 584,
      "word": "niaomi",
      "loss": 2.9992,
      "learning_rate": 0.0013,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0416,
            -0.1098,
            0.1517,
            -0.1699,
            0.1579,
            -0.099,
            -0.1458,
            0.0347,
            -0.0294,
            -0.0046,
            0.0435,
            0.0719,
            -0.0765,
            -0.1375,
            0.076,
            -0.1384
          ],
          "after": [
            0.002,
            -0.0037,
            0.1629,
            -0.0471,
            0.0425,
            -0.0741,
            -0.1224,
            -0.1436,
            -0.0244,
            0.0915,
            0.1585,
            0.0209,
            -0.095,
            -0.0305,
            -0.056,
            -0.1886
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0804,
            0.0194,
            -0.0648,
            0.0019,
            0.0897,
            -0.0864,
            0.1606,
            0.0206,
            -0.1173,
            0.023,
            -0.04,
            0.0244,
            0.0508,
            -0.0627,
            -0.1278,
            0.0842
          ],
          "after": [
            0.0302,
            0.1063,
            -0.0451,
            -0.0785,
            0.0326,
            -0.0037,
            0.0849,
            0.0882,
            -0.0785,
            -0.0785,
            0.0132,
            -0.028,
            0.1082,
            0.0426,
            -0.0546,
            0.2212
          ]
        },
        "position_0": {
          "grad": [
            -0.0259,
            0.0995,
            -0.0801,
            -0.0038,
            -0.3288,
            0.028,
            -0.0125,
            -0.0108,
            0.0887,
            -0.1485,
            -0.2457,
            -0.0906,
            -0.035,
            0.1054,
            0.1712,
            0.0847
          ],
          "after": [
            0.0409,
            -0.1377,
            -0.2933,
            0.0606,
            0.0205,
            -0.0518,
            -0.1514,
            0.0626,
            0.1037,
            0.0038,
            0.0214,
            0.1007,
            0.0431,
            0.0052,
            -0.1335,
            0.0452
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0003,
            -0.0006,
            0.0022,
            -0.0013,
            -0.0009,
            -0.0013,
            0.0011,
            -0.0009,
            -0.0003,
            -0.0003,
            0.0011,
            -0.0001,
            -0.0006,
            0.0009,
            0.0004
          ],
          "after": [
            -0.075,
            0.0114,
            -0.1177,
            -0.0577,
            0.0639,
            0.1481,
            0.0706,
            -0.156,
            0.014,
            -0.0249,
            -0.1269,
            -0.0664,
            0.0883,
            -0.082,
            0.043,
            0.0841
          ]
        }
      }
    },
    {
      "step": 585,
      "word": "jaicere",
      "loss": 2.2878,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0579,
            -0.1989,
            0.083,
            -0.079,
            0.1338,
            0.1965,
            -0.1602,
            0.06,
            -0.0147,
            0.0256,
            0.1007,
            -0.086,
            -0.1514,
            -0.0058,
            -0.0388,
            -0.0729
          ],
          "after": [
            0.0021,
            -0.0034,
            0.1628,
            -0.0466,
            0.0422,
            -0.0742,
            -0.1224,
            -0.1439,
            -0.0242,
            0.0913,
            0.1582,
            0.021,
            -0.0949,
            -0.0303,
            -0.0561,
            -0.1885
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.214,
            -0.4373,
            0.0303,
            -0.0789,
            -0.2407,
            0.3413,
            -0.5962,
            0.0477,
            0.1475,
            0.0093,
            -0.0553,
            -0.298,
            -0.0098,
            0.4585,
            0.3584,
            -0.431
          ],
          "after": [
            0.0298,
            0.1065,
            -0.0453,
            -0.0785,
            0.0331,
            -0.0043,
            0.0852,
            0.0884,
            -0.0787,
            -0.0786,
            0.0129,
            -0.0272,
            0.1082,
            0.042,
            -0.0549,
            0.2217
          ]
        },
        "position_0": {
          "grad": [
            0.0033,
            0.0601,
            -0.006,
            0.0892,
            0.0224,
            -0.0988,
            0.1201,
            0.0276,
            -0.1883,
            -0.0235,
            0.0409,
            0.0355,
            0.2512,
            -0.1317,
            -0.1339,
            0.0805
          ],
          "after": [
            0.0412,
            -0.1379,
            -0.293,
            0.0603,
            0.0208,
            -0.0517,
            -0.1516,
            0.0626,
            0.1037,
            0.0038,
            0.0217,
            0.1003,
            0.043,
            0.0056,
            -0.1335,
            0.0453
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            0.0013,
            0.001,
            0.001,
            -0.0004,
            0.0007,
            0.0014,
            0.0005,
            0.0005,
            -0.0006,
            -0.0011,
            0.0013,
            0.0005,
            -0.0009,
            -0.001,
            0.0008
          ],
          "after": [
            -0.075,
            0.0113,
            -0.1182,
            -0.0581,
            0.064,
            0.1483,
            0.0708,
            -0.1561,
            0.0142,
            -0.025,
            -0.127,
            -0.0664,
            0.0881,
            -0.0818,
            0.0433,
            0.0842
          ]
        }
      }
    },
    {
      "step": 586,
      "word": "iyland",
      "loss": 2.4,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0286,
            0.1237,
            -0.1306,
            0.0136,
            -0.1226,
            -0.1092,
            0.072,
            -0.0577,
            0.0392,
            -0.0162,
            -0.0267,
            0.046,
            -0.0398,
            -0.0309,
            0.0519,
            0.0241
          ],
          "after": [
            0.0022,
            -0.0033,
            0.1629,
            -0.0462,
            0.0421,
            -0.074,
            -0.1225,
            -0.1441,
            -0.024,
            0.091,
            0.158,
            0.0209,
            -0.0947,
            -0.0301,
            -0.0563,
            -0.1883
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0422,
            0.0728,
            -0.0664,
            0.0351,
            0.0611,
            -0.1114,
            0.2032,
            -0.0188,
            -0.1529,
            0.0256,
            -0.018,
            -0.0034,
            0.0778,
            -0.071,
            -0.1841,
            0.1277
          ],
          "after": [
            0.0295,
            0.1066,
            -0.0455,
            -0.0785,
            0.0334,
            -0.0047,
            0.0852,
            0.0886,
            -0.0788,
            -0.0788,
            0.0128,
            -0.0265,
            0.1081,
            0.0415,
            -0.0551,
            0.222
          ]
        },
        "position_0": {
          "grad": [
            -0.047,
            -0.221,
            0.0725,
            -0.1614,
            0.0962,
            0.1239,
            -0.2339,
            0.0053,
            0.077,
            0.045,
            0.0563,
            -0.1214,
            -0.1772,
            0.021,
            -0.0273,
            -0.1698
          ],
          "after": [
            0.0416,
            -0.1377,
            -0.2929,
            0.0604,
            0.0208,
            -0.0518,
            -0.1514,
            0.0626,
            0.1037,
            0.0036,
            0.0218,
            0.1002,
            0.0432,
            0.0059,
            -0.1335,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.002,
            -0.0002,
            -0.0012,
            0.0006,
            -0.0001,
            -0.0001,
            0.0006,
            0.0002,
            0.0009,
            -0.0003,
            -0.0001,
            0.0,
            -0.0003,
            0.0004,
            -0.0005
          ],
          "after": [
            -0.075,
            0.0115,
            -0.1186,
            -0.0583,
            0.064,
            0.1484,
            0.0709,
            -0.1562,
            0.0143,
            -0.0251,
            -0.127,
            -0.0663,
            0.088,
            -0.0815,
            0.0436,
            0.0842
          ]
        }
      }
    },
    {
      "step": 587,
      "word": "haniya",
      "loss": 2.0805,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1887,
            0.0702,
            -0.2578,
            -0.217,
            -0.3223,
            -0.1087,
            -0.1757,
            -0.0793,
            0.0702,
            -0.0146,
            -0.0753,
            -0.2545,
            0.0875,
            0.2105,
            0.0526,
            -0.0626
          ],
          "after": [
            0.0021,
            -0.0034,
            0.1633,
            -0.0455,
            0.0423,
            -0.0736,
            -0.1223,
            -0.144,
            -0.0241,
            0.0909,
            0.1579,
            0.0212,
            -0.0947,
            -0.0302,
            -0.0566,
            -0.1882
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0412,
            0.0215,
            -0.1069,
            0.0165,
            0.0006,
            -0.0723,
            0.1699,
            0.0128,
            -0.1426,
            0.0323,
            -0.0364,
            -0.0211,
            0.0716,
            -0.052,
            -0.1018,
            0.1
          ],
          "after": [
            0.0293,
            0.1067,
            -0.0456,
            -0.0786,
            0.0337,
            -0.0049,
            0.0852,
            0.0887,
            -0.0787,
            -0.0789,
            0.0127,
            -0.0259,
            0.108,
            0.0412,
            -0.0552,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0094,
            -0.0859,
            0.0694,
            -0.0105,
            -0.1137,
            0.1103,
            -0.1426,
            0.0819,
            0.1907,
            0.0762,
            -0.1302,
            -0.1843,
            0.1137,
            0.1336,
            0.0638,
            -0.0036
          ],
          "after": [
            0.0418,
            -0.1373,
            -0.2931,
            0.0604,
            0.0211,
            -0.0522,
            -0.1512,
            0.0623,
            0.1034,
            0.0033,
            0.0221,
            0.1005,
            0.0431,
            0.0058,
            -0.1336,
            0.0457
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0016,
            -0.0003,
            -0.001,
            -0.0026,
            -0.0012,
            -0.0028,
            -0.0009,
            0.0009,
            -0.0023,
            0.0001,
            -0.0007,
            -0.0008,
            0.0019,
            0.0021,
            -0.0012
          ],
          "after": [
            -0.075,
            0.0114,
            -0.1189,
            -0.0584,
            0.0642,
            0.1487,
            0.0712,
            -0.1562,
            0.0143,
            -0.025,
            -0.1271,
            -0.0661,
            0.0879,
            -0.0815,
            0.0437,
            0.0844
          ]
        }
      }
    },
    {
      "step": 588,
      "word": "corrina",
      "loss": 2.0788,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1029,
            -0.0428,
            -0.0407,
            -0.083,
            -0.0118,
            -0.0268,
            -0.0263,
            -0.0343,
            -0.0599,
            0.0798,
            0.0885,
            -0.0822,
            0.0162,
            0.0558,
            0.0052,
            -0.0932
          ],
          "after": [
            0.0019,
            -0.0034,
            0.1636,
            -0.0448,
            0.0425,
            -0.0732,
            -0.1222,
            -0.1438,
            -0.024,
            0.0906,
            0.1578,
            0.0215,
            -0.0947,
            -0.0304,
            -0.0568,
            -0.1879
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0023,
            0.0718,
            -0.1299,
            0.0128,
            -0.0173,
            -0.0979,
            0.1999,
            -0.0027,
            -0.1794,
            0.0283,
            -0.0233,
            -0.0439,
            0.1223,
            -0.073,
            -0.1335,
            0.1204
          ],
          "after": [
            0.0291,
            0.1067,
            -0.0456,
            -0.0787,
            0.034,
            -0.005,
            0.0851,
            0.0889,
            -0.0786,
            -0.0791,
            0.0126,
            -0.0254,
            0.1077,
            0.041,
            -0.0551,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            -0.0255,
            0.0172,
            -0.0185,
            0.072,
            -0.0926,
            -0.0139,
            0.0656,
            0.0531,
            -0.0329,
            0.0111,
            0.0111,
            0.0024,
            0.0181,
            0.0033,
            -0.0154,
            0.0234
          ],
          "after": [
            0.0421,
            -0.1371,
            -0.2932,
            0.0603,
            0.0214,
            -0.0525,
            -0.1511,
            0.062,
            0.1032,
            0.003,
            0.0224,
            0.1007,
            0.0431,
            0.0057,
            -0.1337,
            0.0459
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.002,
            0.0,
            -0.0003,
            0.0003,
            -0.0003,
            0.0003,
            -0.0017,
            -0.0013,
            -0.0012,
            -0.0006,
            0.0004,
            -0.0019,
            -0.0003,
            0.0016,
            0.0,
            -0.0007
          ],
          "after": [
            -0.0752,
            0.0114,
            -0.1192,
            -0.0585,
            0.0644,
            0.1488,
            0.0716,
            -0.156,
            0.0144,
            -0.0248,
            -0.1272,
            -0.0658,
            0.0879,
            -0.0817,
            0.0437,
            0.0846
          ]
        }
      }
    },
    {
      "step": 589,
      "word": "kayden",
      "loss": 1.8451,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0854,
            -0.2001,
            0.0786,
            -0.1162,
            0.0479,
            0.1468,
            -0.1093,
            0.0487,
            -0.0436,
            0.1115,
            0.044,
            0.1138,
            -0.0331,
            0.0636,
            -0.0098,
            -0.0356
          ],
          "after": [
            0.0016,
            -0.0031,
            0.1638,
            -0.0441,
            0.0427,
            -0.0732,
            -0.1219,
            -0.1438,
            -0.0238,
            0.0902,
            0.1575,
            0.0217,
            -0.0947,
            -0.0307,
            -0.057,
            -0.1877
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0171,
            -0.2775,
            0.0187,
            -0.1752,
            -0.1983,
            0.1383,
            -0.3165,
            -0.0036,
            0.0932,
            -0.0045,
            -0.0144,
            -0.0751,
            -0.0231,
            0.1791,
            0.2377,
            -0.0275
          ],
          "after": [
            0.0289,
            0.1069,
            -0.0455,
            -0.0784,
            0.0344,
            -0.0053,
            0.0852,
            0.089,
            -0.0785,
            -0.0792,
            0.0126,
            -0.0248,
            0.1075,
            0.0407,
            -0.0552,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0806,
            0.0497,
            -0.1136,
            -0.0246,
            0.0627,
            -0.0703,
            0.1463,
            -0.0307,
            -0.0438,
            0.0228,
            -0.0012,
            0.0271,
            0.0493,
            -0.043,
            -0.1164,
            -0.0417
          ],
          "after": [
            0.0426,
            -0.137,
            -0.2929,
            0.0603,
            0.0216,
            -0.0526,
            -0.1511,
            0.0618,
            0.103,
            0.0027,
            0.0226,
            0.1009,
            0.043,
            0.0058,
            -0.1335,
            0.046
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0017,
            0.001,
            0.0011,
            0.0,
            0.002,
            -0.0012,
            0.0008,
            -0.0001,
            0.0008,
            0.0004,
            0.0003,
            -0.0003,
            -0.0001,
            -0.0012,
            0.0016
          ],
          "after": [
            -0.0754,
            0.0115,
            -0.1195,
            -0.0587,
            0.0646,
            0.1488,
            0.072,
            -0.1559,
            0.0146,
            -0.0248,
            -0.1273,
            -0.0656,
            0.0879,
            -0.0818,
            0.0438,
            0.0847
          ]
        }
      }
    },
    {
      "step": 590,
      "word": "jaylarose",
      "loss": 2.4501,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0948,
            -0.1386,
            0.1116,
            -0.1072,
            0.2262,
            0.2918,
            0.0784,
            -0.0388,
            0.0329,
            0.0611,
            -0.0017,
            0.3176,
            -0.1472,
            -0.1559,
            -0.0531,
            0.2171
          ],
          "after": [
            0.0015,
            -0.0026,
            0.1639,
            -0.0432,
            0.0426,
            -0.0738,
            -0.1218,
            -0.1438,
            -0.0237,
            0.0897,
            0.1573,
            0.0215,
            -0.0945,
            -0.0307,
            -0.057,
            -0.1877
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1986,
            0.1085,
            -0.1174,
            0.0168,
            -0.2997,
            0.0593,
            0.0067,
            0.0286,
            -0.1543,
            0.1315,
            -0.015,
            -0.3569,
            0.3395,
            0.1683,
            0.0283,
            -0.1212
          ],
          "after": [
            0.0285,
            0.1071,
            -0.0454,
            -0.0783,
            0.0349,
            -0.0055,
            0.0852,
            0.089,
            -0.0784,
            -0.0795,
            0.0127,
            -0.0239,
            0.1071,
            0.0401,
            -0.0554,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            -0.0025,
            0.0545,
            -0.0082,
            0.0831,
            0.0303,
            -0.0782,
            0.1164,
            0.0214,
            -0.1646,
            -0.0095,
            0.0413,
            0.0456,
            0.197,
            -0.1206,
            -0.1108,
            0.0707
          ],
          "after": [
            0.0431,
            -0.137,
            -0.2927,
            0.0601,
            0.0217,
            -0.0525,
            -0.1513,
            0.0615,
            0.1032,
            0.0025,
            0.0228,
            0.1009,
            0.0427,
            0.0061,
            -0.1332,
            0.0461
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0003,
            0.0009,
            0.0002,
            0.0006,
            0.0005,
            0.0004,
            0.0003,
            -0.0007,
            0.0004,
            0.0004,
            0.0009,
            0.0,
            -0.0003,
            -0.001,
            -0.0004
          ],
          "after": [
            -0.0755,
            0.0117,
            -0.1199,
            -0.0589,
            0.0647,
            0.1487,
            0.0723,
            -0.1559,
            0.0148,
            -0.0248,
            -0.1275,
            -0.0655,
            0.0879,
            -0.0819,
            0.044,
            0.0847
          ]
        }
      }
    },
    {
      "step": 591,
      "word": "kennedii",
      "loss": 2.448,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0014,
            -0.0022,
            0.1639,
            -0.0425,
            0.0424,
            -0.0743,
            -0.1217,
            -0.1437,
            -0.0236,
            0.0893,
            0.1572,
            0.0213,
            -0.0942,
            -0.0307,
            -0.0571,
            -0.1877
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2765,
            -0.3117,
            0.2885,
            0.0108,
            0.1296,
            0.3288,
            -0.7665,
            -0.1506,
            0.3881,
            -0.0356,
            0.1387,
            0.0471,
            -0.2355,
            0.268,
            -0.0575,
            -0.3308
          ],
          "after": [
            0.0279,
            0.1075,
            -0.0456,
            -0.0781,
            0.0353,
            -0.006,
            0.0857,
            0.0895,
            -0.0785,
            -0.0797,
            0.0125,
            -0.0232,
            0.1069,
            0.0393,
            -0.0554,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0647,
            0.0206,
            -0.0879,
            -0.0004,
            0.0144,
            -0.0502,
            0.1031,
            -0.005,
            -0.0067,
            -0.0042,
            -0.0232,
            0.0067,
            0.0137,
            -0.0188,
            -0.0584,
            -0.0424
          ],
          "after": [
            0.0436,
            -0.137,
            -0.2923,
            0.06,
            0.0218,
            -0.0523,
            -0.1515,
            0.0613,
            0.1033,
            0.0023,
            0.0229,
            0.1009,
            0.0424,
            0.0064,
            -0.1328,
            0.0462
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0001,
            0.0009,
            0.0007,
            -0.0001,
            0.0002,
            -0.0023,
            -0.0003,
            0.0006,
            0.0,
            0.0004,
            -0.0006,
            0.0005,
            0.0005,
            -0.0018,
            -0.0006
          ],
          "after": [
            -0.0758,
            0.0118,
            -0.1203,
            -0.0592,
            0.0648,
            0.1486,
            0.0728,
            -0.1559,
            0.0149,
            -0.0248,
            -0.1276,
            -0.0654,
            0.0878,
            -0.082,
            0.0443,
            0.0849
          ]
        }
      }
    },
    {
      "step": 592,
      "word": "jovianne",
      "loss": 2.2646,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0752,
            0.1081,
            -0.0195,
            0.0267,
            -0.0026,
            -0.0453,
            0.0215,
            -0.0389,
            0.023,
            -0.1391,
            -0.0764,
            0.1107,
            -0.0627,
            -0.046,
            -0.0131,
            0.0763
          ],
          "after": [
            0.0014,
            -0.002,
            0.164,
            -0.042,
            0.0424,
            -0.0746,
            -0.1216,
            -0.1435,
            -0.0236,
            0.0892,
            0.1572,
            0.021,
            -0.094,
            -0.0307,
            -0.0571,
            -0.1878
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2545,
            0.1052,
            -0.1652,
            -0.0374,
            -0.4045,
            0.1047,
            -0.1523,
            -0.0045,
            -0.0722,
            0.0002,
            -0.0634,
            -0.4285,
            0.264,
            0.2124,
            0.0738,
            -0.131
          ],
          "after": [
            0.0271,
            0.1077,
            -0.0455,
            -0.0779,
            0.0359,
            -0.0065,
            0.0862,
            0.09,
            -0.0786,
            -0.0799,
            0.0124,
            -0.0221,
            0.1065,
            0.0384,
            -0.0556,
            0.2233
          ]
        },
        "position_0": {
          "grad": [
            -0.0014,
            0.0473,
            0.0036,
            0.0985,
            0.033,
            -0.0769,
            0.1095,
            0.0254,
            -0.1683,
            -0.04,
            0.0525,
            0.0304,
            0.2305,
            -0.1054,
            -0.1076,
            0.0884
          ],
          "after": [
            0.0441,
            -0.1371,
            -0.2919,
            0.0597,
            0.0218,
            -0.052,
            -0.1518,
            0.0611,
            0.1036,
            0.0023,
            0.0229,
            0.1009,
            0.0418,
            0.0069,
            -0.1323,
            0.0462
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            0.0001,
            0.0006,
            0.0007,
            -0.0043,
            -0.0005,
            -0.0053,
            0.0009,
            0.0005,
            -0.0029,
            -0.0006,
            -0.0012,
            -0.0007,
            0.0015,
            0.0029,
            -0.0009
          ],
          "after": [
            -0.0762,
            0.0119,
            -0.1207,
            -0.0595,
            0.0652,
            0.1485,
            0.0735,
            -0.156,
            0.0149,
            -0.0245,
            -0.1277,
            -0.0651,
            0.0878,
            -0.0823,
            0.0443,
            0.0851
          ]
        }
      }
    },
    {
      "step": 593,
      "word": "manasvini",
      "loss": 2.3504,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0463,
            0.2216,
            0.0087,
            0.0624,
            -0.1143,
            0.0128,
            -0.0044,
            -0.0098,
            -0.053,
            -0.075,
            -0.1631,
            0.0842,
            0.1327,
            0.0367,
            -0.0301,
            0.0854
          ],
          "after": [
            0.0014,
            -0.0022,
            0.164,
            -0.0416,
            0.0424,
            -0.0749,
            -0.1216,
            -0.1433,
            -0.0235,
            0.0893,
            0.1574,
            0.0206,
            -0.0939,
            -0.0307,
            -0.0571,
            -0.188
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1116,
            0.0545,
            -0.0626,
            0.0301,
            0.087,
            -0.1266,
            0.1989,
            0.0133,
            -0.1477,
            0.002,
            -0.0487,
            0.0703,
            0.0444,
            -0.0797,
            -0.1378,
            0.1309
          ],
          "after": [
            0.0265,
            0.1079,
            -0.0455,
            -0.0778,
            0.0364,
            -0.0069,
            0.0864,
            0.0903,
            -0.0785,
            -0.0801,
            0.0124,
            -0.0213,
            0.1061,
            0.0377,
            -0.0556,
            0.2235
          ]
        },
        "position_0": {
          "grad": [
            -0.0124,
            0.0235,
            -0.036,
            0.0026,
            0.0924,
            -0.0055,
            0.1178,
            0.0462,
            0.0332,
            0.0352,
            -0.0176,
            0.078,
            -0.0818,
            -0.0479,
            -0.0101,
            0.0534
          ],
          "after": [
            0.0446,
            -0.1372,
            -0.2915,
            0.0594,
            0.0216,
            -0.0517,
            -0.1522,
            0.0608,
            0.1039,
            0.0021,
            0.023,
            0.1007,
            0.0415,
            0.0075,
            -0.1318,
            0.0462
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0013,
            0.0002,
            -0.0003,
            -0.002,
            0.0017,
            -0.0017,
            -0.002,
            -0.001,
            -0.0017,
            -0.0024,
            0.0014,
            -0.0009,
            -0.0022,
            -0.0002,
            0.0004,
            -0.0024
          ],
          "after": [
            -0.0766,
            0.012,
            -0.1211,
            -0.0595,
            0.0655,
            0.1487,
            0.0743,
            -0.1559,
            0.0152,
            -0.0241,
            -0.128,
            -0.0648,
            0.088,
            -0.0824,
            0.0443,
            0.0855
          ]
        }
      }
    },
    {
      "step": 594,
      "word": "virgilio",
      "loss": 2.8316,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0015,
            -0.0023,
            0.1641,
            -0.0413,
            0.0424,
            -0.0751,
            -0.1215,
            -0.1432,
            -0.0234,
            0.0893,
            0.1577,
            0.0203,
            -0.0939,
            -0.0307,
            -0.057,
            -0.1882
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1046,
            0.1465,
            -0.1074,
            0.0102,
            0.069,
            -0.159,
            0.2909,
            -0.008,
            -0.2128,
            0.0154,
            -0.0283,
            0.0214,
            0.0993,
            -0.1027,
            -0.2065,
            0.1894
          ],
          "after": [
            0.0262,
            0.1079,
            -0.0453,
            -0.0778,
            0.0368,
            -0.007,
            0.0865,
            0.0906,
            -0.0783,
            -0.0802,
            0.0125,
            -0.0207,
            0.1057,
            0.0372,
            -0.0554,
            0.2236
          ]
        },
        "position_0": {
          "grad": [
            -0.0692,
            0.0434,
            -0.0131,
            0.0379,
            0.0862,
            0.0689,
            0.0127,
            -0.0094,
            -0.0614,
            -0.0071,
            0.0136,
            -0.023,
            0.0235,
            -0.0575,
            -0.0447,
            0.119
          ],
          "after": [
            0.0452,
            -0.1374,
            -0.2912,
            0.0592,
            0.0214,
            -0.0516,
            -0.1525,
            0.0605,
            0.1042,
            0.002,
            0.023,
            0.1006,
            0.0411,
            0.0081,
            -0.1313,
            0.046
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0053,
            0.0004,
            0.0004,
            0.0043,
            -0.0054,
            0.0006,
            -0.001,
            -0.0003,
            0.0021,
            0.0037,
            -0.0002,
            -0.0006,
            0.0007,
            0.0022,
            -0.0029,
            -0.0016
          ],
          "after": [
            -0.0775,
            0.012,
            -0.1214,
            -0.06,
            0.0661,
            0.1487,
            0.0751,
            -0.1558,
            0.0151,
            -0.0241,
            -0.1281,
            -0.0645,
            0.0881,
            -0.0828,
            0.0445,
            0.0861
          ]
        }
      }
    },
    {
      "step": 595,
      "word": "rhyann",
      "loss": 2.4391,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0102,
            0.1145,
            -0.1803,
            0.0141,
            -0.163,
            -0.1329,
            0.0623,
            -0.0548,
            0.0541,
            0.0018,
            -0.0103,
            0.0248,
            -0.0531,
            -0.0343,
            0.0789,
            0.013
          ],
          "after": [
            0.0015,
            -0.0026,
            0.1643,
            -0.0411,
            0.0427,
            -0.0751,
            -0.1215,
            -0.1429,
            -0.0234,
            0.0893,
            0.1579,
            0.02,
            -0.0938,
            -0.0306,
            -0.0572,
            -0.1883
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0574,
            0.0655,
            -0.1503,
            0.0049,
            -0.009,
            -0.1586,
            0.2754,
            0.0245,
            -0.1833,
            0.0128,
            -0.0818,
            -0.0263,
            0.1516,
            -0.077,
            -0.149,
            0.1498
          ],
          "after": [
            0.0259,
            0.1078,
            -0.0451,
            -0.0777,
            0.0371,
            -0.007,
            0.0865,
            0.0908,
            -0.0781,
            -0.0804,
            0.0127,
            -0.0201,
            0.1052,
            0.0369,
            -0.0552,
            0.2235
          ]
        },
        "position_0": {
          "grad": [
            0.0187,
            -0.0612,
            0.0108,
            -0.0362,
            -0.0467,
            0.1391,
            0.0596,
            -0.0236,
            0.143,
            -0.0833,
            -0.1147,
            0.0031,
            -0.1852,
            -0.0603,
            0.1561,
            0.0806
          ],
          "after": [
            0.0456,
            -0.1375,
            -0.2909,
            0.059,
            0.0212,
            -0.0518,
            -0.1529,
            0.0604,
            0.1042,
            0.0021,
            0.0233,
            0.1005,
            0.0411,
            0.0087,
            -0.1312,
            0.0457
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0024,
            0.002,
            -0.0001,
            0.0003,
            -0.0005,
            -0.0007,
            0.0043,
            0.0014,
            -0.0003,
            -0.0026,
            -0.0011,
            0.0018,
            0.0012,
            -0.0028,
            0.0012,
            0.0004
          ],
          "after": [
            -0.078,
            0.0118,
            -0.1217,
            -0.0604,
            0.0668,
            0.1489,
            0.0754,
            -0.1559,
            0.015,
            -0.0238,
            -0.1282,
            -0.0644,
            0.0881,
            -0.0829,
            0.0446,
            0.0865
          ]
        }
      }
    },
    {
      "step": 596,
      "word": "keldon",
      "loss": 2.1305,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0016,
            -0.0029,
            0.1645,
            -0.0408,
            0.0428,
            -0.0751,
            -0.1216,
            -0.1427,
            -0.0234,
            0.0894,
            0.1581,
            0.0198,
            -0.0937,
            -0.0305,
            -0.0573,
            -0.1884
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2009,
            -0.2316,
            0.2101,
            0.0469,
            0.2512,
            0.1813,
            -0.4088,
            -0.1066,
            0.3634,
            0.185,
            0.203,
            0.1974,
            -0.1952,
            0.0026,
            -0.225,
            -0.2806
          ],
          "after": [
            0.0255,
            0.1079,
            -0.0451,
            -0.0778,
            0.0371,
            -0.0071,
            0.0866,
            0.0913,
            -0.0781,
            -0.0808,
            0.0125,
            -0.0198,
            0.105,
            0.0366,
            -0.0548,
            0.2237
          ]
        },
        "position_0": {
          "grad": [
            -0.0917,
            0.0422,
            -0.1081,
            0.0004,
            0.0768,
            -0.0468,
            0.1597,
            -0.0229,
            -0.047,
            -0.003,
            0.0157,
            0.0433,
            0.0603,
            -0.0343,
            -0.1007,
            -0.0157
          ],
          "after": [
            0.0463,
            -0.1376,
            -0.2903,
            0.0589,
            0.0209,
            -0.0519,
            -0.1533,
            0.0603,
            0.1043,
            0.0022,
            0.0234,
            0.1003,
            0.041,
            0.0094,
            -0.1309,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0016,
            0.0,
            0.0018,
            -0.0002,
            0.0002,
            0.0028,
            -0.0008,
            0.0005,
            0.0016,
            0.0018,
            0.0013,
            0.0019,
            -0.0009,
            -0.0036,
            0.0023
          ],
          "after": [
            -0.0785,
            0.0114,
            -0.1219,
            -0.0609,
            0.0673,
            0.149,
            0.0754,
            -0.1559,
            0.0149,
            -0.0237,
            -0.1284,
            -0.0645,
            0.088,
            -0.0828,
            0.045,
            0.0866
          ]
        }
      }
    },
    {
      "step": 597,
      "word": "slade",
      "loss": 2.4608,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0901,
            0.0238,
            0.0654,
            -0.1,
            0.1731,
            -0.1397,
            0.035,
            -0.1044,
            0.0032,
            -0.0094,
            0.0052,
            0.0423,
            0.043,
            0.0336,
            -0.0258,
            0.0906
          ],
          "after": [
            0.0017,
            -0.0032,
            0.1646,
            -0.0405,
            0.0428,
            -0.0747,
            -0.1216,
            -0.1423,
            -0.0235,
            0.0894,
            0.1582,
            0.0195,
            -0.0937,
            -0.0306,
            -0.0573,
            -0.1886
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0698,
            -0.3184,
            -0.0437,
            -0.2153,
            -0.2446,
            0.1245,
            -0.3548,
            -0.0702,
            0.0813,
            -0.0757,
            0.0277,
            -0.086,
            -0.0289,
            0.1823,
            0.2538,
            -0.023
          ],
          "after": [
            0.0251,
            0.1084,
            -0.045,
            -0.0774,
            0.0374,
            -0.0073,
            0.0869,
            0.0919,
            -0.0781,
            -0.081,
            0.0123,
            -0.0194,
            0.1048,
            0.0362,
            -0.0547,
            0.2239
          ]
        },
        "position_0": {
          "grad": [
            0.0073,
            0.1151,
            -0.0137,
            0.1376,
            -0.1475,
            0.0868,
            0.005,
            0.0658,
            -0.0728,
            0.0142,
            -0.1262,
            0.1037,
            0.1006,
            0.0253,
            0.1355,
            0.0296
          ],
          "after": [
            0.0468,
            -0.1379,
            -0.2899,
            0.0585,
            0.021,
            -0.0522,
            -0.1537,
            0.0601,
            0.1045,
            0.0023,
            0.0237,
            0.1,
            0.0408,
            0.0099,
            -0.1309,
            0.0453
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0012,
            -0.0032,
            -0.0019,
            -0.0036,
            0.002,
            -0.001,
            -0.0014,
            0.0007,
            0.0009,
            -0.0006,
            0.0015,
            -0.003,
            0.0026,
            0.0046,
            -0.0012
          ],
          "after": [
            -0.0789,
            0.0112,
            -0.1217,
            -0.0612,
            0.0681,
            0.1489,
            0.0756,
            -0.1556,
            0.0148,
            -0.0238,
            -0.1285,
            -0.0647,
            0.0881,
            -0.083,
            0.0449,
            0.0868
          ]
        }
      }
    },
    {
      "step": 598,
      "word": "aviyana",
      "loss": 2.1312,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0868,
            0.108,
            -0.0783,
            -0.0246,
            0.2868,
            -0.0956,
            -0.0574,
            -0.1557,
            -0.1624,
            0.1038,
            0.1126,
            0.0277,
            0.0809,
            -0.181,
            -0.2279,
            0.1567
          ],
          "after": [
            0.0019,
            -0.0036,
            0.1648,
            -0.0402,
            0.0425,
            -0.0743,
            -0.1216,
            -0.1415,
            -0.0232,
            0.0893,
            0.1581,
            0.0193,
            -0.0938,
            -0.0303,
            -0.0569,
            -0.189
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.025,
            -0.0012,
            -0.0342,
            0.0043,
            0.0481,
            -0.1039,
            0.1178,
            -0.0022,
            -0.108,
            0.0501,
            -0.0184,
            0.008,
            0.0616,
            -0.0399,
            -0.1258,
            0.0505
          ],
          "after": [
            0.0247,
            0.1088,
            -0.0449,
            -0.0772,
            0.0375,
            -0.0074,
            0.0872,
            0.0924,
            -0.0781,
            -0.0813,
            0.0121,
            -0.0191,
            0.1046,
            0.0359,
            -0.0545,
            0.224
          ]
        },
        "position_0": {
          "grad": [
            0.0874,
            -0.0325,
            0.0916,
            -0.1635,
            0.0426,
            0.0042,
            -0.2797,
            -0.1148,
            0.1187,
            0.0744,
            0.2016,
            0.0233,
            -0.1633,
            0.0195,
            -0.0847,
            -0.2205
          ],
          "after": [
            0.047,
            -0.1381,
            -0.2897,
            0.0585,
            0.0209,
            -0.0524,
            -0.1538,
            0.0602,
            0.1045,
            0.0022,
            0.0237,
            0.0997,
            0.0408,
            0.0103,
            -0.1308,
            0.0454
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.001,
            -0.0003,
            -0.0001,
            -0.0017,
            0.0008,
            -0.0063,
            0.0005,
            0.0015,
            -0.0043,
            -0.0004,
            0.0004,
            -0.0036,
            0.0021,
            0.0031,
            -0.0007
          ],
          "after": [
            -0.0794,
            0.011,
            -0.1216,
            -0.0614,
            0.0689,
            0.1487,
            0.0761,
            -0.1555,
            0.0144,
            -0.0234,
            -0.1286,
            -0.0649,
            0.0884,
            -0.0834,
            0.0447,
            0.0871
          ]
        }
      }
    },
    {
      "step": 599,
      "word": "kento",
      "loss": 2.328,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0021,
            -0.0039,
            0.1649,
            -0.0399,
            0.0422,
            -0.0739,
            -0.1216,
            -0.1408,
            -0.0229,
            0.0891,
            0.1581,
            0.0191,
            -0.0939,
            -0.03,
            -0.0566,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2401,
            -0.382,
            0.2853,
            0.0152,
            0.2947,
            0.301,
            -0.6197,
            -0.1189,
            0.5382,
            0.2438,
            0.2434,
            0.216,
            -0.2701,
            0.054,
            -0.1742,
            -0.4451
          ],
          "after": [
            0.0242,
            0.1094,
            -0.0451,
            -0.0769,
            0.0375,
            -0.0078,
            0.0877,
            0.0932,
            -0.0784,
            -0.0818,
            0.0116,
            -0.0191,
            0.1047,
            0.0355,
            -0.0542,
            0.2244
          ]
        },
        "position_0": {
          "grad": [
            -0.0923,
            0.0161,
            -0.1168,
            -0.0136,
            0.0126,
            -0.0806,
            0.1308,
            -0.0078,
            0.0015,
            -0.0506,
            -0.0239,
            -0.0175,
            0.0267,
            0.0187,
            -0.0504,
            -0.0394
          ],
          "after": [
            0.0474,
            -0.1383,
            -0.2892,
            0.0585,
            0.0208,
            -0.0524,
            -0.1539,
            0.0603,
            0.1045,
            0.0022,
            0.0237,
            0.0994,
            0.0407,
            0.0105,
            -0.1306,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            -0.0016,
            -0.0003,
            0.0005,
            0.001,
            0.0007,
            0.0006,
            0.0004,
            0.0013,
            0.0038,
            0.0014,
            0.0012,
            0.0012,
            -0.0002,
            -0.0039,
            -0.0005
          ],
          "after": [
            -0.08,
            0.0109,
            -0.1214,
            -0.0617,
            0.0695,
            0.1484,
            0.0765,
            -0.1554,
            0.0139,
            -0.0234,
            -0.1288,
            -0.0652,
            0.0887,
            -0.0837,
            0.0447,
            0.0874
          ]
        }
      }
    },
    {
      "step": 600,
      "word": "brayven",
      "loss": 2.3963,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0141,
            -0.1554,
            0.0822,
            -0.1219,
            0.0974,
            0.1126,
            -0.0664,
            0.0601,
            -0.0313,
            0.0317,
            0.0371,
            0.1196,
            -0.0343,
            -0.0094,
            -0.0003,
            0.0057
          ],
          "after": [
            0.0023,
            -0.004,
            0.165,
            -0.0395,
            0.0418,
            -0.0738,
            -0.1215,
            -0.1404,
            -0.0227,
            0.089,
            0.158,
            0.0188,
            -0.0939,
            -0.0298,
            -0.0563,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0895,
            -0.1335,
            -0.0229,
            -0.1396,
            -0.2083,
            0.1084,
            -0.3496,
            -0.066,
            0.177,
            0.0631,
            0.1241,
            -0.1872,
            -0.0083,
            0.1349,
            0.1771,
            -0.1249
          ],
          "after": [
            0.0236,
            0.1101,
            -0.0452,
            -0.0765,
            0.0375,
            -0.0082,
            0.0883,
            0.0941,
            -0.0789,
            -0.0824,
            0.011,
            -0.0189,
            0.1048,
            0.035,
            -0.0541,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            0.0485,
            0.0111,
            0.0678,
            0.0132,
            -0.005,
            0.0138,
            0.0807,
            -0.0125,
            -0.098,
            -0.0396,
            0.0119,
            0.0201,
            0.1093,
            -0.1165,
            -0.0162,
            0.2147
          ],
          "after": [
            0.0476,
            -0.1385,
            -0.289,
            0.0585,
            0.0208,
            -0.0525,
            -0.1542,
            0.0605,
            0.1047,
            0.0023,
            0.0237,
            0.0992,
            0.0406,
            0.011,
            -0.1303,
            0.0454
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0045,
            -0.0012,
            0.0024,
            0.0008,
            -0.0001,
            0.0003,
            0.0008,
            0.0014,
            -0.0023,
            0.002,
            0.0018,
            0.001,
            0.0012,
            -0.0016,
            0.0015,
            0.002
          ],
          "after": [
            -0.08,
            0.011,
            -0.1215,
            -0.062,
            0.07,
            0.1482,
            0.0768,
            -0.1556,
            0.0138,
            -0.0236,
            -0.1292,
            -0.0656,
            0.0888,
            -0.0838,
            0.0447,
            0.0874
          ]
        }
      }
    },
    {
      "step": 601,
      "word": "aleen",
      "loss": 1.9805,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1341,
            0.134,
            -0.1979,
            -0.1324,
            -0.0744,
            -0.0417,
            -0.2166,
            0.0962,
            -0.1158,
            -0.1028,
            -0.2228,
            0.2844,
            -0.1885,
            0.0537,
            0.1724,
            -0.407
          ],
          "after": [
            0.0025,
            -0.0042,
            0.1652,
            -0.0389,
            0.0416,
            -0.0736,
            -0.1212,
            -0.1403,
            -0.0222,
            0.089,
            0.1582,
            0.0182,
            -0.0937,
            -0.0297,
            -0.0564,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1835,
            -0.2665,
            0.13,
            -0.1389,
            0.2179,
            0.5285,
            -0.4044,
            0.1302,
            0.5371,
            -0.1386,
            -0.1315,
            0.454,
            -0.521,
            0.006,
            0.7866,
            -0.4136
          ],
          "after": [
            0.0233,
            0.111,
            -0.0454,
            -0.0759,
            0.0374,
            -0.009,
            0.089,
            0.0945,
            -0.0795,
            -0.0827,
            0.0106,
            -0.0192,
            0.1053,
            0.0346,
            -0.0546,
            0.2256
          ]
        },
        "position_0": {
          "grad": [
            0.0992,
            -0.0665,
            0.1278,
            -0.208,
            0.0346,
            0.0171,
            -0.3887,
            -0.1242,
            0.1764,
            0.0502,
            0.2579,
            0.0284,
            -0.213,
            0.0378,
            -0.0827,
            -0.3045
          ],
          "after": [
            0.0475,
            -0.1385,
            -0.2892,
            0.0588,
            0.0207,
            -0.0526,
            -0.154,
            0.0609,
            0.1045,
            0.0022,
            0.0233,
            0.0989,
            0.0407,
            0.0114,
            -0.13,
            0.0456
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0019,
            -0.0008,
            -0.0001,
            -0.001,
            0.0012,
            0.0006,
            0.0003,
            0.001,
            -0.0,
            -0.0006,
            -0.0009,
            0.0022,
            -0.0016,
            -0.0014,
            0.0021,
            -0.0011
          ],
          "after": [
            -0.0799,
            0.0112,
            -0.1216,
            -0.0621,
            0.0704,
            0.1479,
            0.0771,
            -0.1559,
            0.0137,
            -0.0237,
            -0.1294,
            -0.0661,
            0.089,
            -0.0838,
            0.0445,
            0.0875
          ]
        }
      }
    },
    {
      "step": 602,
      "word": "christion",
      "loss": 2.6354,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0028,
            -0.0044,
            0.1654,
            -0.0385,
            0.0415,
            -0.0735,
            -0.1209,
            -0.1403,
            -0.0218,
            0.089,
            0.1585,
            0.0177,
            -0.0935,
            -0.0296,
            -0.0564,
            -0.1891
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0431,
            0.0868,
            -0.1501,
            0.0048,
            0.0548,
            -0.1436,
            0.2604,
            0.0049,
            -0.1939,
            -0.0149,
            -0.0542,
            -0.0357,
            0.1277,
            -0.0776,
            -0.1847,
            0.1744
          ],
          "after": [
            0.0231,
            0.1116,
            -0.0454,
            -0.0754,
            0.0373,
            -0.0095,
            0.0895,
            0.0948,
            -0.08,
            -0.0829,
            0.0104,
            -0.0195,
            0.1056,
            0.0344,
            -0.0548,
            0.2261
          ]
        },
        "position_0": {
          "grad": [
            -0.0165,
            0.0226,
            -0.0157,
            0.0757,
            -0.0639,
            -0.0071,
            0.0784,
            0.0407,
            -0.0389,
            0.0072,
            0.0082,
            0.0183,
            0.0262,
            0.0095,
            -0.0046,
            0.0441
          ],
          "after": [
            0.0475,
            -0.1386,
            -0.2893,
            0.0589,
            0.0207,
            -0.0526,
            -0.1539,
            0.0612,
            0.1045,
            0.0022,
            0.0229,
            0.0987,
            0.0408,
            0.0116,
            -0.1297,
            0.0457
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0023,
            -0.0011,
            -0.0042,
            -0.0005,
            0.0004,
            -0.0018,
            0.0022,
            0.0016,
            -0.0014,
            -0.0012,
            -0.0014,
            -0.0013,
            0.0001,
            -0.0009,
            0.002,
            0.0019
          ],
          "after": [
            -0.0799,
            0.0115,
            -0.1212,
            -0.0622,
            0.0706,
            0.1479,
            0.0771,
            -0.1564,
            0.0139,
            -0.0237,
            -0.1295,
            -0.0664,
            0.0891,
            -0.0837,
            0.0442,
            0.0874
          ]
        }
      }
    },
    {
      "step": 603,
      "word": "elgin",
      "loss": 2.6784,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.003,
            -0.0046,
            0.1656,
            -0.0381,
            0.0413,
            -0.0733,
            -0.1206,
            -0.1402,
            -0.0215,
            0.0891,
            0.1587,
            0.0173,
            -0.0933,
            -0.0296,
            -0.0565,
            -0.189
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3129,
            0.4998,
            0.4646,
            -0.0232,
            0.1191,
            -0.1033,
            0.5073,
            -0.0276,
            -0.0699,
            -0.1867,
            -0.0031,
            -0.0731,
            -0.0208,
            -0.0302,
            -0.1293,
            0.0286
          ],
          "after": [
            0.0233,
            0.1117,
            -0.0459,
            -0.075,
            0.0371,
            -0.0099,
            0.0896,
            0.0951,
            -0.0804,
            -0.0828,
            0.0103,
            -0.0196,
            0.1059,
            0.0342,
            -0.0549,
            0.2265
          ]
        },
        "position_0": {
          "grad": [
            -0.0121,
            -0.2224,
            0.0945,
            -0.0178,
            -0.0323,
            0.1038,
            -0.2941,
            -0.0741,
            0.1744,
            0.1328,
            0.0312,
            -0.0262,
            -0.1657,
            0.0263,
            0.0323,
            -0.3459
          ],
          "after": [
            0.0475,
            -0.1383,
            -0.2896,
            0.0591,
            0.0208,
            -0.0529,
            -0.1535,
            0.0616,
            0.1042,
            0.0018,
            0.0226,
            0.0985,
            0.0411,
            0.0118,
            -0.1295,
            0.0462
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            0.0003,
            0.0017,
            0.0021,
            0.0021,
            -0.0046,
            0.0078,
            0.0016,
            -0.0008,
            -0.0038,
            -0.0012,
            -0.0002,
            0.0036,
            -0.0041,
            -0.0013,
            0.0023
          ],
          "after": [
            -0.0799,
            0.0117,
            -0.1211,
            -0.0624,
            0.0706,
            0.1483,
            0.0766,
            -0.1571,
            0.0141,
            -0.0234,
            -0.1293,
            -0.0667,
            0.089,
            -0.0831,
            0.0441,
            0.0871
          ]
        }
      }
    },
    {
      "step": 604,
      "word": "annaliah",
      "loss": 2.2305,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2193,
            0.2299,
            -0.0441,
            0.1538,
            -0.2941,
            0.1443,
            -0.0466,
            0.1506,
            0.0555,
            -0.2402,
            -0.4483,
            0.0849,
            0.0638,
            0.0886,
            0.207,
            0.023
          ],
          "after": [
            0.0034,
            -0.0051,
            0.1658,
            -0.0379,
            0.0415,
            -0.0735,
            -0.1203,
            -0.1405,
            -0.0213,
            0.0895,
            0.1596,
            0.0168,
            -0.0932,
            -0.0296,
            -0.0569,
            -0.1889
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0789,
            0.0859,
            -0.0359,
            0.029,
            0.0993,
            -0.1314,
            0.2236,
            0.0014,
            -0.1151,
            0.0036,
            -0.015,
            0.0589,
            0.034,
            -0.0965,
            -0.1608,
            0.137
          ],
          "after": [
            0.0235,
            0.1117,
            -0.0462,
            -0.0746,
            0.0369,
            -0.0101,
            0.0896,
            0.0954,
            -0.0806,
            -0.0828,
            0.0102,
            -0.0198,
            0.1061,
            0.0342,
            -0.0549,
            0.2267
          ]
        },
        "position_0": {
          "grad": [
            0.0726,
            -0.0498,
            0.0777,
            -0.1173,
            -0.0189,
            0.0075,
            -0.2449,
            -0.0547,
            0.1256,
            0.0184,
            0.1355,
            0.0267,
            -0.1663,
            0.0243,
            -0.023,
            -0.2018
          ],
          "after": [
            0.0473,
            -0.1379,
            -0.2901,
            0.0594,
            0.0209,
            -0.0532,
            -0.153,
            0.0621,
            0.1037,
            0.0015,
            0.022,
            0.0983,
            0.0415,
            0.0119,
            -0.1293,
            0.0468
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.001,
            -0.0009,
            0.0017,
            -0.0027,
            -0.0014,
            -0.0001,
            0.0005,
            -0.0004,
            0.001,
            0.0004,
            0.0002,
            0.0003,
            -0.0006,
            0.0015,
            -0.0004
          ],
          "after": [
            -0.0799,
            0.012,
            -0.1208,
            -0.0628,
            0.0709,
            0.1488,
            0.0762,
            -0.1577,
            0.0143,
            -0.0232,
            -0.1293,
            -0.0669,
            0.0889,
            -0.0826,
            0.0438,
            0.0868
          ]
        }
      }
    },
    {
      "step": 605,
      "word": "juliet",
      "loss": 2.6441,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0038,
            -0.0055,
            0.166,
            -0.0379,
            0.0416,
            -0.0737,
            -0.1201,
            -0.1407,
            -0.0212,
            0.0898,
            0.1603,
            0.0164,
            -0.0932,
            -0.0297,
            -0.0573,
            -0.1888
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.011,
            0.1839,
            -0.1272,
            0.2073,
            -0.4179,
            -0.0931,
            0.0347,
            0.0664,
            -0.1431,
            -0.2763,
            -0.1123,
            0.0536,
            0.1602,
            0.0622,
            0.2441,
            0.1705
          ],
          "after": [
            0.0237,
            0.1115,
            -0.0464,
            -0.0747,
            0.037,
            -0.0102,
            0.0896,
            0.0954,
            -0.0807,
            -0.0824,
            0.0103,
            -0.02,
            0.1062,
            0.0341,
            -0.0551,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            0.0057,
            0.0393,
            0.0166,
            0.1396,
            0.0462,
            -0.0957,
            0.1313,
            0.0358,
            -0.1957,
            -0.0685,
            0.0657,
            0.0167,
            0.3111,
            -0.1111,
            -0.1142,
            0.1248
          ],
          "after": [
            0.0471,
            -0.1377,
            -0.2905,
            0.0594,
            0.0209,
            -0.0532,
            -0.1526,
            0.0625,
            0.1036,
            0.0014,
            0.0215,
            0.0981,
            0.0415,
            0.0122,
            -0.1289,
            0.0472
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0007,
            0.0017,
            0.002,
            -0.0013,
            -0.0016,
            -0.0009,
            0.0025,
            -0.0015,
            -0.0041,
            -0.0017,
            0.0006,
            0.0021,
            -0.0001,
            0.0011,
            0.0008
          ],
          "after": [
            -0.0799,
            0.0121,
            -0.1208,
            -0.0634,
            0.0712,
            0.1494,
            0.076,
            -0.1586,
            0.0148,
            -0.0227,
            -0.1291,
            -0.0671,
            0.0886,
            -0.0822,
            0.0436,
            0.0865
          ]
        }
      }
    },
    {
      "step": 606,
      "word": "liliann",
      "loss": 1.8603,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0528,
            0.1038,
            -0.0258,
            0.0486,
            -0.0043,
            -0.0547,
            0.0163,
            -0.0365,
            0.0172,
            -0.1416,
            -0.0653,
            0.1136,
            -0.0776,
            -0.0443,
            -0.0093,
            0.0618
          ],
          "after": [
            0.0041,
            -0.006,
            0.1662,
            -0.0379,
            0.0418,
            -0.0737,
            -0.1199,
            -0.1409,
            -0.0211,
            0.0904,
            0.161,
            0.0159,
            -0.093,
            -0.0297,
            -0.0576,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.065,
            0.1492,
            -0.1422,
            0.025,
            -0.001,
            -0.1652,
            0.3122,
            -0.0168,
            -0.2367,
            0.0097,
            -0.0113,
            -0.0338,
            0.136,
            -0.0926,
            -0.1874,
            0.2071
          ],
          "after": [
            0.024,
            0.1112,
            -0.0464,
            -0.0748,
            0.0371,
            -0.0101,
            0.0894,
            0.0955,
            -0.0806,
            -0.082,
            0.0104,
            -0.0201,
            0.1061,
            0.0341,
            -0.0551,
            0.2266
          ]
        },
        "position_0": {
          "grad": [
            -0.0715,
            0.0846,
            -0.0915,
            -0.0562,
            0.0158,
            0.0697,
            -0.0576,
            0.0734,
            -0.0287,
            -0.0165,
            -0.1355,
            0.1738,
            -0.0967,
            0.003,
            0.0716,
            -0.1093
          ],
          "after": [
            0.0471,
            -0.1376,
            -0.2907,
            0.0596,
            0.0209,
            -0.0533,
            -0.1523,
            0.0626,
            0.1036,
            0.0013,
            0.0213,
            0.0976,
            0.0416,
            0.0125,
            -0.1288,
            0.0477
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.003,
            0.0005,
            0.0017,
            -0.0043,
            -0.0001,
            -0.0066,
            0.0018,
            0.001,
            -0.0024,
            -0.0014,
            0.003,
            -0.0026,
            0.0018,
            0.0045,
            0.0003
          ],
          "after": [
            -0.0799,
            0.0126,
            -0.1209,
            -0.064,
            0.0719,
            0.15,
            0.0762,
            -0.1597,
            0.015,
            -0.0221,
            -0.1287,
            -0.0676,
            0.0885,
            -0.082,
            0.043,
            0.0862
          ]
        }
      }
    },
    {
      "step": 607,
      "word": "indi",
      "loss": 2.8627,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0044,
            -0.0065,
            0.1664,
            -0.0378,
            0.0419,
            -0.0737,
            -0.1198,
            -0.141,
            -0.021,
            0.0908,
            0.1617,
            0.0155,
            -0.0929,
            -0.0297,
            -0.0578,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0148,
            0.0333,
            -0.13,
            -0.0179,
            0.0695,
            -0.1078,
            0.2377,
            0.0091,
            -0.1609,
            0.0804,
            -0.0381,
            -0.0996,
            0.1466,
            -0.0346,
            -0.2327,
            0.076
          ],
          "after": [
            0.0242,
            0.1109,
            -0.0463,
            -0.0748,
            0.0371,
            -0.01,
            0.0892,
            0.0955,
            -0.0804,
            -0.0818,
            0.0105,
            -0.0201,
            0.1059,
            0.0342,
            -0.0549,
            0.2265
          ]
        },
        "position_0": {
          "grad": [
            -0.0603,
            -0.3661,
            0.1216,
            -0.1552,
            -0.0389,
            0.1798,
            -0.343,
            0.0905,
            0.1791,
            -0.0653,
            -0.0032,
            -0.2417,
            -0.2974,
            0.1215,
            0.148,
            -0.2353
          ],
          "after": [
            0.0474,
            -0.137,
            -0.2911,
            0.0599,
            0.0209,
            -0.0538,
            -0.1517,
            0.0624,
            0.1033,
            0.0014,
            0.021,
            0.0977,
            0.042,
            0.0124,
            -0.1289,
            0.0483
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0013,
            0.0002,
            -0.0005,
            -0.0016,
            -0.0008,
            -0.0016,
            0.0006,
            0.0005,
            -0.0009,
            -0.0004,
            0.0007,
            -0.0004,
            0.0001,
            0.002,
            0.0
          ],
          "after": [
            -0.0798,
            0.0132,
            -0.1209,
            -0.0645,
            0.0726,
            0.1505,
            0.0764,
            -0.1607,
            0.0151,
            -0.0216,
            -0.1283,
            -0.0681,
            0.0885,
            -0.0819,
            0.0425,
            0.086
          ]
        }
      }
    },
    {
      "step": 608,
      "word": "jordi",
      "loss": 2.2359,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0047,
            -0.0069,
            0.1665,
            -0.0378,
            0.042,
            -0.0737,
            -0.1196,
            -0.1411,
            -0.0209,
            0.0912,
            0.1622,
            0.0152,
            -0.0928,
            -0.0297,
            -0.058,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0709,
            0.2033,
            -0.1911,
            0.0357,
            0.0086,
            -0.1779,
            0.407,
            0.0549,
            -0.3154,
            -0.0236,
            -0.0909,
            -0.0513,
            0.1956,
            -0.0797,
            -0.203,
            0.1958
          ],
          "after": [
            0.0244,
            0.1105,
            -0.0461,
            -0.075,
            0.0371,
            -0.0097,
            0.0887,
            0.0954,
            -0.0801,
            -0.0816,
            0.0108,
            -0.02,
            0.1056,
            0.0343,
            -0.0547,
            0.2262
          ]
        },
        "position_0": {
          "grad": [
            0.0095,
            0.0341,
            0.0155,
            0.1642,
            0.0031,
            -0.1248,
            0.1622,
            0.0579,
            -0.219,
            -0.0874,
            0.0625,
            -0.0029,
            0.3375,
            -0.1254,
            -0.0969,
            0.129
          ],
          "after": [
            0.0475,
            -0.1365,
            -0.2915,
            0.0599,
            0.021,
            -0.054,
            -0.1513,
            0.0621,
            0.1034,
            0.0017,
            0.0208,
            0.0977,
            0.042,
            0.0127,
            -0.1288,
            0.0487
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.002,
            0.0013,
            -0.0008,
            0.0017,
            -0.0027,
            0.0012,
            0.0015,
            -0.0013,
            0.0003,
            0.0023,
            -0.0002,
            0.0001,
            0.0011,
            0.0013,
            -0.0017,
            0.0011
          ],
          "after": [
            -0.0799,
            0.0135,
            -0.1209,
            -0.065,
            0.0734,
            0.1508,
            0.0766,
            -0.1613,
            0.0152,
            -0.0212,
            -0.128,
            -0.0686,
            0.0884,
            -0.0819,
            0.0421,
            0.0856
          ]
        }
      }
    },
    {
      "step": 609,
      "word": "aiven",
      "loss": 2.214,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0223,
            -0.4919,
            0.0676,
            -0.2397,
            0.2074,
            0.111,
            -0.4282,
            0.1254,
            0.1074,
            -0.0505,
            0.2064,
            -0.2763,
            -0.2811,
            0.1344,
            -0.0527,
            -0.1818
          ],
          "after": [
            0.0049,
            -0.0065,
            0.1665,
            -0.0375,
            0.0418,
            -0.074,
            -0.119,
            -0.1415,
            -0.0211,
            0.0916,
            0.1623,
            0.0152,
            -0.0923,
            -0.0299,
            -0.0581,
            -0.1885
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0603,
            -0.182,
            0.1365,
            -0.2016,
            0.2005,
            0.172,
            -0.4613,
            -0.0179,
            0.3093,
            -0.1133,
            0.156,
            0.0055,
            -0.2922,
            -0.0596,
            0.278,
            -0.1782
          ],
          "after": [
            0.0247,
            0.1103,
            -0.046,
            -0.0747,
            0.037,
            -0.0096,
            0.0886,
            0.0954,
            -0.08,
            -0.0813,
            0.0108,
            -0.02,
            0.1055,
            0.0346,
            -0.0546,
            0.2261
          ]
        },
        "position_0": {
          "grad": [
            0.1093,
            -0.0455,
            0.1091,
            -0.2131,
            0.0286,
            0.0049,
            -0.3745,
            -0.1388,
            0.1489,
            0.0454,
            0.256,
            0.0094,
            -0.1911,
            0.0443,
            -0.0934,
            -0.2672
          ],
          "after": [
            0.0473,
            -0.136,
            -0.2921,
            0.0603,
            0.0209,
            -0.0542,
            -0.1507,
            0.0622,
            0.1032,
            0.0018,
            0.0202,
            0.0977,
            0.0422,
            0.0128,
            -0.1285,
            0.0493
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0001,
            0.0005,
            -0.0007,
            -0.0004,
            -0.0001,
            -0.0021,
            0.0001,
            0.0007,
            -0.0008,
            0.001,
            -0.0001,
            -0.0014,
            -0.0006,
            0.0013,
            -0.0005
          ],
          "after": [
            -0.0799,
            0.0138,
            -0.1209,
            -0.0654,
            0.0741,
            0.1511,
            0.0768,
            -0.1619,
            0.0151,
            -0.0209,
            -0.1279,
            -0.0689,
            0.0885,
            -0.0818,
            0.0417,
            0.0854
          ]
        }
      }
    },
    {
      "step": 610,
      "word": "kawai",
      "loss": 2.4293,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0392,
            -0.2392,
            0.0325,
            0.1285,
            0.2891,
            0.0342,
            -0.009,
            0.1639,
            -0.1473,
            0.0178,
            0.166,
            -0.14,
            -0.0249,
            -0.0403,
            -0.2491,
            -0.0841
          ],
          "after": [
            0.005,
            -0.0058,
            0.1665,
            -0.0374,
            0.0414,
            -0.0742,
            -0.1184,
            -0.1422,
            -0.0209,
            0.0919,
            0.1622,
            0.0154,
            -0.0918,
            -0.03,
            -0.0577,
            -0.1882
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0395,
            0.1421,
            -0.1272,
            0.0299,
            -0.01,
            -0.1907,
            0.3334,
            0.0423,
            -0.2899,
            0.0088,
            -0.0483,
            -0.0852,
            0.1208,
            -0.0577,
            -0.1439,
            0.1946
          ],
          "after": [
            0.025,
            0.11,
            -0.0459,
            -0.0745,
            0.0369,
            -0.0094,
            0.0883,
            0.0952,
            -0.0797,
            -0.081,
            0.0108,
            -0.0199,
            0.1054,
            0.0348,
            -0.0545,
            0.2258
          ]
        },
        "position_0": {
          "grad": [
            -0.0906,
            0.0654,
            -0.1306,
            -0.0175,
            0.0755,
            -0.0944,
            0.1647,
            -0.0393,
            -0.049,
            0.0037,
            0.0034,
            0.0251,
            0.0642,
            -0.0223,
            -0.1246,
            -0.0207
          ],
          "after": [
            0.0474,
            -0.1357,
            -0.2923,
            0.0606,
            0.0208,
            -0.0541,
            -0.1502,
            0.0624,
            0.1032,
            0.0019,
            0.0196,
            0.0977,
            0.0423,
            0.0129,
            -0.1281,
            0.0499
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.001,
            0.0007,
            -0.0032,
            0.0033,
            0.0007,
            -0.0013,
            -0.0008,
            0.0009,
            -0.0041,
            -0.003,
            -0.0023,
            0.0015,
            0.0027,
            -0.001,
            -0.0003
          ],
          "after": [
            -0.0799,
            0.0141,
            -0.121,
            -0.0655,
            0.0744,
            0.1513,
            0.0771,
            -0.1622,
            0.015,
            -0.0203,
            -0.1274,
            -0.069,
            0.0884,
            -0.082,
            0.0414,
            0.0853
          ]
        }
      }
    },
    {
      "step": 611,
      "word": "sahmir",
      "loss": 2.5075,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0113,
            -0.0305,
            0.1501,
            0.09,
            -0.0187,
            0.121,
            -0.001,
            0.1394,
            0.0942,
            0.1528,
            -0.06,
            -0.2119,
            0.2695,
            0.1245,
            -0.0355,
            0.0509
          ],
          "after": [
            0.0051,
            -0.0052,
            0.1663,
            -0.0374,
            0.0411,
            -0.0747,
            -0.118,
            -0.1431,
            -0.021,
            0.092,
            0.1621,
            0.0158,
            -0.0918,
            -0.0303,
            -0.0573,
            -0.1881
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0712,
            0.1127,
            -0.1323,
            0.0168,
            0.0537,
            -0.1525,
            0.277,
            0.0069,
            -0.1819,
            0.0109,
            -0.0398,
            0.0589,
            0.0656,
            -0.1102,
            -0.1789,
            0.1624
          ],
          "after": [
            0.0253,
            0.1097,
            -0.0456,
            -0.0744,
            0.0367,
            -0.009,
            0.0879,
            0.095,
            -0.0794,
            -0.0808,
            0.0109,
            -0.0198,
            0.1052,
            0.0352,
            -0.0543,
            0.2255
          ]
        },
        "position_0": {
          "grad": [
            0.012,
            0.0956,
            -0.0095,
            0.1075,
            -0.1441,
            0.0664,
            -0.019,
            0.0551,
            -0.0409,
            0.0205,
            -0.1228,
            0.0546,
            0.0802,
            0.0313,
            0.1087,
            0.009
          ],
          "after": [
            0.0475,
            -0.1356,
            -0.2924,
            0.0608,
            0.0209,
            -0.0542,
            -0.1499,
            0.0625,
            0.1032,
            0.002,
            0.0194,
            0.0975,
            0.0423,
            0.013,
            -0.1279,
            0.0503
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0001,
            -0.0003,
            0.0007,
            0.0001,
            0.0014,
            0.0001,
            0.0005,
            0.0002,
            0.0001,
            -0.0004,
            0.0003,
            -0.0004,
            0.0002,
            -0.0002,
            0.0009
          ],
          "after": [
            -0.0799,
            0.0145,
            -0.1211,
            -0.0656,
            0.0747,
            0.1513,
            0.0774,
            -0.1626,
            0.0148,
            -0.0198,
            -0.1269,
            -0.0691,
            0.0884,
            -0.0822,
            0.0412,
            0.085
          ]
        }
      }
    },
    {
      "step": 612,
      "word": "abel",
      "loss": 2.6046,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0473,
            0.0031,
            0.4481,
            0.0097,
            0.1352,
            -0.1001,
            0.1113,
            0.0014,
            -0.1369,
            -0.0532,
            0.0925,
            -0.0196,
            0.334,
            -0.259,
            -0.1606,
            0.5107
          ],
          "after": [
            0.0052,
            -0.0047,
            0.1657,
            -0.0375,
            0.0407,
            -0.0749,
            -0.1177,
            -0.1439,
            -0.0208,
            0.0921,
            0.1619,
            0.0162,
            -0.0923,
            -0.0301,
            -0.0567,
            -0.1885
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0593,
            -0.0755,
            -0.0746,
            0.063,
            0.1772,
            0.4749,
            0.0976,
            -0.0483,
            0.3303,
            -0.04,
            0.119,
            0.236,
            -0.4618,
            -0.4067,
            0.256,
            -0.006
          ],
          "after": [
            0.0256,
            0.1094,
            -0.0453,
            -0.0744,
            0.0365,
            -0.0092,
            0.0876,
            0.095,
            -0.0793,
            -0.0806,
            0.0108,
            -0.0201,
            0.1055,
            0.036,
            -0.0543,
            0.2252
          ]
        },
        "position_0": {
          "grad": [
            0.1221,
            -0.0492,
            0.1263,
            -0.2035,
            0.0248,
            0.0061,
            -0.4206,
            -0.1462,
            0.1743,
            0.0329,
            0.3042,
            0.0197,
            -0.2136,
            0.0687,
            -0.0745,
            -0.2874
          ],
          "after": [
            0.0472,
            -0.1354,
            -0.2929,
            0.0612,
            0.021,
            -0.0543,
            -0.1492,
            0.0629,
            0.103,
            0.0019,
            0.0187,
            0.0974,
            0.0425,
            0.0129,
            -0.1277,
            0.051
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.003,
            -0.0006,
            -0.0002,
            0.0017,
            0.0033,
            -0.003,
            0.0017,
            0.0028,
            0.0007,
            -0.0008,
            0.0012,
            -0.0028,
            -0.0002,
            -0.0012,
            0.0003
          ],
          "after": [
            -0.08,
            0.015,
            -0.1211,
            -0.0656,
            0.0748,
            0.1509,
            0.0778,
            -0.1632,
            0.0143,
            -0.0195,
            -0.1265,
            -0.0693,
            0.0886,
            -0.0824,
            0.0411,
            0.0848
          ]
        }
      }
    },
    {
      "step": 613,
      "word": "chyann",
      "loss": 2.2338,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0042,
            0.1187,
            -0.1927,
            0.0192,
            -0.1698,
            -0.1257,
            0.0745,
            -0.0599,
            0.0508,
            0.0167,
            -0.0121,
            0.0236,
            -0.051,
            -0.0321,
            0.0829,
            0.0156
          ],
          "after": [
            0.0052,
            -0.0044,
            0.1653,
            -0.0375,
            0.0405,
            -0.0748,
            -0.1176,
            -0.1444,
            -0.0207,
            0.0921,
            0.1618,
            0.0165,
            -0.0926,
            -0.03,
            -0.0564,
            -0.1888
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0165,
            0.032,
            -0.1749,
            0.0003,
            -0.0069,
            -0.1313,
            0.2138,
            0.0276,
            -0.2017,
            0.0267,
            -0.0589,
            -0.0619,
            0.1627,
            -0.0681,
            -0.1365,
            0.1632
          ],
          "after": [
            0.0259,
            0.1092,
            -0.045,
            -0.0745,
            0.0363,
            -0.0091,
            0.0871,
            0.0949,
            -0.0791,
            -0.0804,
            0.0109,
            -0.0202,
            0.1056,
            0.0368,
            -0.0541,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            -0.03,
            0.0057,
            -0.0119,
            0.1114,
            -0.1158,
            -0.0209,
            0.0795,
            0.072,
            -0.0221,
            -0.0117,
            -0.0006,
            -0.0094,
            0.0252,
            0.0315,
            0.0233,
            0.0379
          ],
          "after": [
            0.047,
            -0.1352,
            -0.2932,
            0.0614,
            0.0212,
            -0.0543,
            -0.1487,
            0.063,
            0.1028,
            0.0019,
            0.0181,
            0.0973,
            0.0427,
            0.0127,
            -0.1275,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0005,
            -0.0004,
            0.0004,
            0.0003,
            0.0008,
            0.0016,
            0.0009,
            -0.0008,
            -0.0008,
            0.0005,
            0.0008,
            0.0002,
            -0.0016,
            0.001,
            0.0007
          ],
          "after": [
            -0.08,
            0.0155,
            -0.121,
            -0.0657,
            0.0749,
            0.1506,
            0.078,
            -0.1638,
            0.0139,
            -0.0191,
            -0.1261,
            -0.0696,
            0.0887,
            -0.0823,
            0.0409,
            0.0845
          ]
        }
      }
    },
    {
      "step": 614,
      "word": "declynn",
      "loss": 2.2138,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0052,
            -0.0041,
            0.165,
            -0.0376,
            0.0403,
            -0.0747,
            -0.1175,
            -0.1448,
            -0.0206,
            0.0922,
            0.1617,
            0.0167,
            -0.0928,
            -0.0298,
            -0.0561,
            -0.1891
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0433,
            -0.2535,
            0.161,
            -0.1096,
            0.1662,
            0.0929,
            -0.2884,
            -0.037,
            0.3168,
            0.2036,
            0.1321,
            0.1548,
            -0.1963,
            -0.0071,
            -0.0067,
            -0.0645
          ],
          "after": [
            0.0261,
            0.1092,
            -0.0448,
            -0.0743,
            0.036,
            -0.0092,
            0.0869,
            0.095,
            -0.0791,
            -0.0806,
            0.0106,
            -0.0204,
            0.1058,
            0.0374,
            -0.054,
            0.2246
          ]
        },
        "position_0": {
          "grad": [
            -0.0631,
            0.0352,
            -0.0245,
            -0.0461,
            0.0717,
            -0.0345,
            0.0585,
            -0.0398,
            -0.0028,
            -0.0423,
            -0.0697,
            0.0413,
            0.0318,
            0.0358,
            0.0654,
            0.1237
          ],
          "after": [
            0.047,
            -0.1352,
            -0.2934,
            0.0616,
            0.0213,
            -0.0542,
            -0.1483,
            0.0632,
            0.1027,
            0.002,
            0.0178,
            0.0971,
            0.0428,
            0.0124,
            -0.1274,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            -0.001,
            0.0002,
            -0.0002,
            0.0014,
            -0.0003,
            0.0004,
            0.0001,
            0.0001,
            0.0019,
            0.0013,
            -0.0003,
            0.0012,
            -0.0003,
            -0.0025,
            0.0016
          ],
          "after": [
            -0.0802,
            0.016,
            -0.121,
            -0.0658,
            0.0748,
            0.1503,
            0.0782,
            -0.1643,
            0.0136,
            -0.0189,
            -0.126,
            -0.0698,
            0.0887,
            -0.0823,
            0.041,
            0.0841
          ]
        }
      }
    },
    {
      "step": 615,
      "word": "nahzir",
      "loss": 2.5419,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0023,
            -0.0192,
            0.1586,
            0.0804,
            0.0088,
            0.1272,
            0.0027,
            0.1411,
            0.0887,
            0.1571,
            -0.0572,
            -0.1977,
            0.2706,
            0.1125,
            -0.0545,
            0.0473
          ],
          "after": [
            0.0052,
            -0.0039,
            0.1646,
            -0.0378,
            0.0402,
            -0.0749,
            -0.1174,
            -0.1455,
            -0.0207,
            0.092,
            0.1617,
            0.0171,
            -0.0934,
            -0.0298,
            -0.0557,
            -0.1894
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0908,
            0.1166,
            -0.1193,
            0.0377,
            0.0505,
            -0.1406,
            0.3106,
            0.025,
            -0.1627,
            0.041,
            -0.0453,
            0.0445,
            0.1023,
            -0.1136,
            -0.2056,
            0.144
          ],
          "after": [
            0.0263,
            0.1092,
            -0.0445,
            -0.0742,
            0.0356,
            -0.0092,
            0.0866,
            0.0949,
            -0.079,
            -0.0808,
            0.0105,
            -0.0207,
            0.1059,
            0.0381,
            -0.0538,
            0.2243
          ]
        },
        "position_0": {
          "grad": [
            -0.0305,
            0.1133,
            -0.0696,
            0.0298,
            -0.3427,
            0.0431,
            0.0151,
            0.0132,
            0.0832,
            -0.1943,
            -0.2675,
            -0.06,
            -0.0376,
            0.1268,
            0.2165,
            0.1184
          ],
          "after": [
            0.0472,
            -0.1353,
            -0.2934,
            0.0617,
            0.022,
            -0.0543,
            -0.148,
            0.0634,
            0.1024,
            0.0025,
            0.0178,
            0.0971,
            0.0429,
            0.012,
            -0.1278,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0002,
            -0.0001,
            0.0007,
            -0.0003,
            0.0011,
            0.0002,
            0.0005,
            0.0003,
            -0.0003,
            -0.0005,
            0.0008,
            -0.0004,
            -0.0001,
            0.0002,
            0.001
          ],
          "after": [
            -0.0803,
            0.0164,
            -0.121,
            -0.0659,
            0.0747,
            0.1499,
            0.0784,
            -0.1649,
            0.0133,
            -0.0188,
            -0.1258,
            -0.07,
            0.0888,
            -0.0822,
            0.041,
            0.0836
          ]
        }
      }
    },
    {
      "step": 616,
      "word": "wilber",
      "loss": 2.8902,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0052,
            -0.0037,
            0.1642,
            -0.0379,
            0.0401,
            -0.0751,
            -0.1173,
            -0.1461,
            -0.0208,
            0.0918,
            0.1617,
            0.0175,
            -0.0939,
            -0.0299,
            -0.0554,
            -0.1897
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0855,
            -0.1552,
            -0.1699,
            -0.0341,
            0.0154,
            0.2445,
            0.0679,
            -0.0391,
            0.1124,
            -0.171,
            -0.0656,
            -0.1097,
            -0.13,
            -0.1187,
            0.0875,
            0.0442
          ],
          "after": [
            0.0265,
            0.1092,
            -0.0442,
            -0.0741,
            0.0354,
            -0.0093,
            0.0863,
            0.095,
            -0.0791,
            -0.0807,
            0.0106,
            -0.0208,
            0.1061,
            0.0389,
            -0.0537,
            0.224
          ]
        },
        "position_0": {
          "grad": [
            0.0946,
            0.1352,
            -0.0695,
            0.1385,
            -0.0366,
            -0.0665,
            0.212,
            0.0198,
            -0.0745,
            -0.0285,
            -0.007,
            -0.1099,
            0.1479,
            0.0138,
            -0.0932,
            0.0719
          ],
          "after": [
            0.047,
            -0.1357,
            -0.2933,
            0.0616,
            0.0226,
            -0.0542,
            -0.1479,
            0.0635,
            0.1023,
            0.0031,
            0.0179,
            0.0973,
            0.0429,
            0.0115,
            -0.1279,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0036,
            0.0045,
            -0.0031,
            0.0043,
            -0.0032,
            0.0022,
            0.0039,
            -0.004,
            0.0017,
            0.0013,
            0.002,
            0.0,
            0.0014,
            -0.0004,
            -0.0041,
            0.0053
          ],
          "after": [
            -0.0807,
            0.0162,
            -0.1206,
            -0.0664,
            0.075,
            0.1494,
            0.0782,
            -0.1647,
            0.0129,
            -0.0187,
            -0.1259,
            -0.0702,
            0.0887,
            -0.0822,
            0.0413,
            0.0827
          ]
        }
      }
    },
    {
      "step": 617,
      "word": "baer",
      "loss": 2.665,
      "learning_rate": 0.0012,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0136,
            -0.3201,
            0.0812,
            0.1251,
            -0.192,
            0.1855,
            -0.3057,
            0.0186,
            0.1683,
            0.1418,
            0.0821,
            -0.0593,
            -0.291,
            0.0016,
            0.1403,
            -0.3818
          ],
          "after": [
            0.0052,
            -0.0031,
            0.1638,
            -0.0382,
            0.0402,
            -0.0756,
            -0.1169,
            -0.1467,
            -0.0212,
            0.0914,
            0.1616,
            0.0179,
            -0.094,
            -0.0299,
            -0.0554,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2375,
            0.273,
            -0.2377,
            0.2144,
            -0.2411,
            0.5444,
            0.3562,
            0.2412,
            0.0979,
            -0.2783,
            -0.2803,
            0.2336,
            0.0679,
            -0.017,
            0.512,
            0.3799
          ],
          "after": [
            0.0268,
            0.1091,
            -0.0437,
            -0.0743,
            0.0353,
            -0.0099,
            0.0858,
            0.0944,
            -0.0791,
            -0.0802,
            0.011,
            -0.0212,
            0.1062,
            0.0396,
            -0.0539,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            0.0852,
            -0.0271,
            0.1278,
            -0.0323,
            -0.0404,
            -0.0165,
            0.0227,
            -0.0088,
            -0.0771,
            -0.0845,
            -0.0233,
            -0.0614,
            0.1462,
            -0.1304,
            0.0019,
            0.2822
          ],
          "after": [
            0.0466,
            -0.1359,
            -0.2935,
            0.0616,
            0.0232,
            -0.0541,
            -0.1479,
            0.0636,
            0.1024,
            0.0037,
            0.018,
            0.0975,
            0.0426,
            0.0114,
            -0.128,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0024,
            -0.0011,
            -0.0031,
            0.0016,
            0.0003,
            0.0055,
            0.0009,
            0.0031,
            0.0025,
            -0.001,
            -0.002,
            0.0035,
            -0.0035,
            -0.0027,
            0.0023,
            0.0038
          ],
          "after": [
            -0.0809,
            0.0162,
            -0.12,
            -0.067,
            0.0752,
            0.1485,
            0.0781,
            -0.165,
            0.0121,
            -0.0186,
            -0.1257,
            -0.0707,
            0.0889,
            -0.0818,
            0.0414,
            0.0815
          ]
        }
      }
    },
    {
      "step": 618,
      "word": "abishai",
      "loss": 2.4502,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0744,
            -0.2407,
            0.4168,
            -0.0212,
            0.3254,
            0.1029,
            -0.0185,
            0.0974,
            -0.0492,
            -0.0382,
            0.0696,
            -0.0553,
            0.0593,
            -0.2103,
            -0.1414,
            0.3155
          ],
          "after": [
            0.0053,
            -0.0023,
            0.163,
            -0.0385,
            0.0399,
            -0.0762,
            -0.1165,
            -0.1474,
            -0.0214,
            0.0911,
            0.1613,
            0.0182,
            -0.0941,
            -0.0296,
            -0.0551,
            -0.1897
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.021,
            0.016,
            -0.0255,
            0.0027,
            0.0932,
            -0.1166,
            0.0912,
            -0.003,
            -0.1034,
            0.0298,
            -0.0018,
            0.0353,
            0.0199,
            -0.033,
            -0.1487,
            0.0599
          ],
          "after": [
            0.0271,
            0.1089,
            -0.0432,
            -0.0745,
            0.0352,
            -0.0103,
            0.0854,
            0.0939,
            -0.0791,
            -0.0799,
            0.0114,
            -0.0215,
            0.1063,
            0.0402,
            -0.054,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            0.0798,
            -0.0034,
            0.061,
            -0.1318,
            0.0387,
            -0.0037,
            -0.2354,
            -0.1115,
            0.0822,
            0.0533,
            0.1948,
            0.0333,
            -0.1327,
            0.0154,
            -0.0823,
            -0.1636
          ],
          "after": [
            0.0461,
            -0.1361,
            -0.2938,
            0.0618,
            0.0236,
            -0.0539,
            -0.1476,
            0.064,
            0.1023,
            0.0041,
            0.0178,
            0.0977,
            0.0426,
            0.0113,
            -0.128,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0014,
            -0.0023,
            -0.0006,
            0.0015,
            0.0027,
            0.0009,
            0.0006,
            0.0017,
            0.0004,
            -0.0003,
            0.0003,
            -0.0017,
            -0.0006,
            -0.0004,
            0.0013
          ],
          "after": [
            -0.081,
            0.0164,
            -0.1192,
            -0.0674,
            0.0752,
            0.1474,
            0.0779,
            -0.1654,
            0.0113,
            -0.0185,
            -0.1256,
            -0.0712,
            0.0893,
            -0.0815,
            0.0415,
            0.0804
          ]
        }
      }
    },
    {
      "step": 619,
      "word": "carlota",
      "loss": 2.3424,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1633,
            -0.095,
            -0.0521,
            -0.1537,
            0.1178,
            0.013,
            0.0546,
            -0.1659,
            0.0039,
            0.0873,
            0.1504,
            -0.0362,
            -0.0572,
            -0.0685,
            -0.0132,
            -0.0366
          ],
          "after": [
            0.0052,
            -0.0015,
            0.1624,
            -0.0384,
            0.0396,
            -0.0767,
            -0.1162,
            -0.1476,
            -0.0216,
            0.0908,
            0.1609,
            0.0186,
            -0.0941,
            -0.0292,
            -0.0549,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0114,
            0.1419,
            -0.1349,
            0.0531,
            -0.0311,
            -0.1315,
            0.2351,
            -0.0206,
            -0.2015,
            -0.0013,
            0.0046,
            -0.0386,
            0.1301,
            -0.0938,
            -0.1313,
            0.1545
          ],
          "after": [
            0.0274,
            0.1086,
            -0.0428,
            -0.0748,
            0.0352,
            -0.0105,
            0.0849,
            0.0935,
            -0.079,
            -0.0796,
            0.0118,
            -0.0218,
            0.1063,
            0.0408,
            -0.054,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0254,
            0.0378,
            -0.0246,
            0.0829,
            -0.064,
            -0.0251,
            0.1011,
            0.0461,
            -0.0631,
            0.0292,
            0.0189,
            0.0299,
            0.0359,
            -0.0101,
            -0.0368,
            0.0439
          ],
          "after": [
            0.0457,
            -0.1363,
            -0.294,
            0.0618,
            0.0241,
            -0.0538,
            -0.1475,
            0.0641,
            0.1023,
            0.0044,
            0.0176,
            0.0978,
            0.0425,
            0.0113,
            -0.1278,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            0.0001,
            0.0003,
            0.0001,
            -0.0001,
            -0.0004,
            0.0004,
            -0.0,
            0.0002,
            0.001,
            0.0001,
            0.0,
            0.0005,
            0.0,
            -0.0013,
            -0.0006
          ],
          "after": [
            -0.0811,
            0.0165,
            -0.1186,
            -0.0678,
            0.0752,
            0.1465,
            0.0777,
            -0.1657,
            0.0105,
            -0.0186,
            -0.1254,
            -0.0716,
            0.0895,
            -0.0812,
            0.0416,
            0.0795
          ]
        }
      }
    },
    {
      "step": 620,
      "word": "fortune",
      "loss": 2.7159,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0052,
            -0.0008,
            0.1619,
            -0.0384,
            0.0393,
            -0.0772,
            -0.116,
            -0.1477,
            -0.0218,
            0.0905,
            0.1606,
            0.0189,
            -0.0941,
            -0.0289,
            -0.0547,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3599,
            0.0277,
            -0.1944,
            0.0238,
            -0.4593,
            0.1074,
            -0.2592,
            -0.105,
            -0.0877,
            -0.0483,
            0.0309,
            -0.4549,
            0.1954,
            0.2838,
            0.1408,
            -0.1592
          ],
          "after": [
            0.0273,
            0.1084,
            -0.0422,
            -0.0751,
            0.0355,
            -0.0108,
            0.0846,
            0.0935,
            -0.0788,
            -0.0793,
            0.012,
            -0.0215,
            0.1061,
            0.041,
            -0.0541,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            0.0069,
            0.1436,
            -0.021,
            0.1313,
            0.0803,
            -0.1879,
            0.1336,
            -0.0298,
            -0.2281,
            -0.0807,
            0.1388,
            -0.0067,
            0.1984,
            -0.0147,
            -0.0354,
            0.1482
          ],
          "after": [
            0.0453,
            -0.1368,
            -0.2941,
            0.0616,
            0.0244,
            -0.0533,
            -0.1475,
            0.0644,
            0.1026,
            0.0048,
            0.0172,
            0.0978,
            0.0423,
            0.0112,
            -0.1277,
            0.0515
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0028,
            0.0034,
            -0.0015,
            0.0031,
            -0.0016,
            0.0005,
            0.0026,
            -0.0018,
            -0.0002,
            0.002,
            0.0006,
            -0.0015,
            0.0021,
            0.0006,
            -0.0022,
            0.0001
          ],
          "after": [
            -0.0815,
            0.0162,
            -0.1179,
            -0.0684,
            0.0754,
            0.1457,
            0.0773,
            -0.1657,
            0.0099,
            -0.0188,
            -0.1254,
            -0.0718,
            0.0896,
            -0.081,
            0.0419,
            0.0787
          ]
        }
      }
    },
    {
      "step": 621,
      "word": "lennyn",
      "loss": 1.9292,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0051,
            -0.0002,
            0.1614,
            -0.0384,
            0.039,
            -0.0776,
            -0.1158,
            -0.1479,
            -0.022,
            0.0902,
            0.1602,
            0.0192,
            -0.0941,
            -0.0287,
            -0.0545,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0997,
            -0.4035,
            0.2797,
            -0.0798,
            0.3432,
            0.1874,
            -0.4615,
            0.0632,
            0.4248,
            0.2422,
            0.0499,
            0.1925,
            -0.2674,
            0.0669,
            -0.0348,
            -0.3172
          ],
          "after": [
            0.0271,
            0.1085,
            -0.0419,
            -0.0752,
            0.0354,
            -0.0112,
            0.0846,
            0.0933,
            -0.0789,
            -0.0794,
            0.0121,
            -0.0215,
            0.1061,
            0.041,
            -0.0542,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            -0.0921,
            0.081,
            -0.1037,
            -0.0401,
            -0.0177,
            0.0741,
            -0.0568,
            0.1094,
            -0.0152,
            -0.0623,
            -0.1867,
            0.1898,
            -0.1294,
            0.0229,
            0.145,
            -0.1136
          ],
          "after": [
            0.0453,
            -0.1373,
            -0.2939,
            0.0615,
            0.0246,
            -0.053,
            -0.1475,
            0.0643,
            0.1029,
            0.0053,
            0.0172,
            0.0976,
            0.0422,
            0.0112,
            -0.1278,
            0.0515
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0005,
            0.0004,
            -0.0009,
            0.0008,
            -0.0001,
            -0.0008,
            0.0004,
            0.0009,
            0.0003,
            -0.0002,
            0.0008,
            -0.0005,
            -0.0,
            -0.0001,
            -0.0005
          ],
          "after": [
            -0.0818,
            0.016,
            -0.1173,
            -0.0688,
            0.0754,
            0.1451,
            0.0771,
            -0.1658,
            0.0093,
            -0.0189,
            -0.1253,
            -0.072,
            0.0896,
            -0.0808,
            0.0422,
            0.0781
          ]
        }
      }
    },
    {
      "step": 622,
      "word": "calayah",
      "loss": 2.2555,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2773,
            -0.0645,
            0.3441,
            -0.0233,
            0.3896,
            0.305,
            -0.0748,
            0.2346,
            -0.0193,
            0.1086,
            -0.2429,
            0.4174,
            0.0559,
            -0.0553,
            -0.2033,
            -0.0111
          ],
          "after": [
            0.0054,
            0.0004,
            0.1607,
            -0.0383,
            0.0385,
            -0.0785,
            -0.1155,
            -0.1485,
            -0.0221,
            0.0898,
            0.1604,
            0.0189,
            -0.0942,
            -0.0283,
            -0.054,
            -0.19
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0228,
            0.0653,
            -0.1199,
            0.0376,
            -0.0293,
            -0.0991,
            0.152,
            -0.0193,
            -0.1837,
            0.0145,
            0.0033,
            -0.0378,
            0.0982,
            -0.0562,
            -0.1068,
            0.1347
          ],
          "after": [
            0.0268,
            0.1086,
            -0.0416,
            -0.0753,
            0.0355,
            -0.0114,
            0.0845,
            0.0932,
            -0.0789,
            -0.0794,
            0.0122,
            -0.0214,
            0.1061,
            0.0412,
            -0.0541,
            0.2217
          ]
        },
        "position_0": {
          "grad": [
            -0.0265,
            0.0461,
            -0.0285,
            0.0745,
            -0.0294,
            -0.029,
            0.0925,
            0.031,
            -0.0657,
            0.055,
            0.022,
            0.0453,
            0.0339,
            -0.0267,
            -0.071,
            0.0378
          ],
          "after": [
            0.0453,
            -0.1378,
            -0.2937,
            0.0614,
            0.0249,
            -0.0527,
            -0.1476,
            0.0642,
            0.1032,
            0.0056,
            0.0171,
            0.0973,
            0.0421,
            0.0112,
            -0.1278,
            0.0514
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0002,
            0.001,
            -0.0007,
            0.0004,
            -0.0009,
            -0.0007,
            -0.0006,
            0.0005,
            0.0003,
            0.0004,
            0.0004,
            -0.0003,
            0.0,
            -0.0006,
            -0.0011
          ],
          "after": [
            -0.082,
            0.0158,
            -0.117,
            -0.0691,
            0.0755,
            0.1446,
            0.0769,
            -0.1657,
            0.0087,
            -0.0191,
            -0.1253,
            -0.0722,
            0.0897,
            -0.0807,
            0.0424,
            0.0777
          ]
        }
      }
    },
    {
      "step": 623,
      "word": "carisa",
      "loss": 2.0419,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2084,
            -0.0882,
            -0.061,
            -0.2192,
            0.0668,
            -0.0021,
            0.0229,
            -0.1592,
            0.0563,
            0.1381,
            0.146,
            -0.0957,
            -0.0689,
            -0.0771,
            -0.0181,
            -0.0996
          ],
          "after": [
            0.0053,
            0.0011,
            0.1601,
            -0.0379,
            0.0379,
            -0.0792,
            -0.1153,
            -0.1487,
            -0.0222,
            0.0892,
            0.1602,
            0.0188,
            -0.0942,
            -0.0279,
            -0.0536,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0413,
            0.0348,
            -0.1348,
            0.0038,
            -0.0048,
            -0.0878,
            0.14,
            -0.0083,
            -0.1716,
            0.027,
            -0.0054,
            -0.0477,
            0.0908,
            -0.057,
            -0.112,
            0.0959
          ],
          "after": [
            0.0266,
            0.1086,
            -0.0412,
            -0.0754,
            0.0355,
            -0.0115,
            0.0844,
            0.0931,
            -0.0788,
            -0.0795,
            0.0123,
            -0.0213,
            0.106,
            0.0413,
            -0.054,
            0.2215
          ]
        },
        "position_0": {
          "grad": [
            -0.025,
            0.0296,
            -0.0268,
            0.0727,
            -0.0694,
            -0.0325,
            0.08,
            0.051,
            -0.045,
            0.0404,
            0.0087,
            0.0191,
            0.0262,
            -0.0126,
            -0.0518,
            0.0242
          ],
          "after": [
            0.0454,
            -0.1383,
            -0.2935,
            0.0611,
            0.0252,
            -0.0523,
            -0.1477,
            0.0639,
            0.1036,
            0.0058,
            0.0171,
            0.097,
            0.0419,
            0.0112,
            -0.1277,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0004,
            0.0021,
            -0.0003,
            -0.0003,
            -0.0003,
            -0.0018,
            -0.001,
            0.0008,
            0.0015,
            0.0004,
            -0.0008,
            0.0004,
            0.0008,
            -0.0002,
            -0.0014
          ],
          "after": [
            -0.0821,
            0.0157,
            -0.1169,
            -0.0693,
            0.0755,
            0.1442,
            0.0769,
            -0.1656,
            0.0081,
            -0.0194,
            -0.1253,
            -0.0724,
            0.0898,
            -0.0806,
            0.0427,
            0.0775
          ]
        }
      }
    },
    {
      "step": 624,
      "word": "atziri",
      "loss": 2.5367,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0357,
            -0.1787,
            0.1974,
            0.1159,
            -0.0054,
            0.0225,
            0.1349,
            0.0797,
            -0.187,
            -0.0839,
            -0.145,
            -0.3679,
            0.3201,
            0.0928,
            0.0099,
            0.2662
          ],
          "after": [
            0.0054,
            0.0018,
            0.1594,
            -0.0378,
            0.0374,
            -0.0799,
            -0.1153,
            -0.149,
            -0.0221,
            0.0889,
            0.1604,
            0.0192,
            -0.0946,
            -0.0278,
            -0.0532,
            -0.1902
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0447,
            0.0924,
            -0.0778,
            0.0328,
            0.0887,
            -0.1273,
            0.1986,
            -0.016,
            -0.1396,
            0.0359,
            -0.0194,
            -0.0149,
            0.0995,
            -0.0505,
            -0.2397,
            0.0699
          ],
          "after": [
            0.0265,
            0.1085,
            -0.0409,
            -0.0756,
            0.0354,
            -0.0116,
            0.0841,
            0.0931,
            -0.0786,
            -0.0797,
            0.0124,
            -0.0212,
            0.1058,
            0.0415,
            -0.0538,
            0.2213
          ]
        },
        "position_0": {
          "grad": [
            0.0903,
            -0.0169,
            0.0765,
            -0.1337,
            0.0136,
            0.0103,
            -0.2529,
            -0.1013,
            0.1014,
            0.0256,
            0.2093,
            0.0451,
            -0.1484,
            0.0294,
            -0.0628,
            -0.1739
          ],
          "after": [
            0.0453,
            -0.1386,
            -0.2935,
            0.0611,
            0.0255,
            -0.0521,
            -0.1476,
            0.064,
            0.1037,
            0.0059,
            0.0167,
            0.0966,
            0.042,
            0.0112,
            -0.1275,
            0.0514
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0004,
            0.0003,
            -0.0005,
            0.0007,
            -0.0003,
            -0.0003,
            0.0002,
            0.0017,
            -0.0014,
            -0.0007,
            -0.0007,
            0.0004,
            0.0006,
            0.0011,
            -0.0
          ],
          "after": [
            -0.0821,
            0.0155,
            -0.1169,
            -0.0695,
            0.0755,
            0.1439,
            0.0769,
            -0.1655,
            0.0073,
            -0.0195,
            -0.1253,
            -0.0724,
            0.0898,
            -0.0807,
            0.0428,
            0.0773
          ]
        }
      }
    },
    {
      "step": 625,
      "word": "arianna",
      "loss": 1.8608,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1135,
            0.0083,
            -0.2111,
            -0.1286,
            -0.1126,
            0.0347,
            0.131,
            -0.2491,
            0.0803,
            -0.0032,
            0.0885,
            0.0116,
            -0.2672,
            -0.2169,
            0.1022,
            -0.0214
          ],
          "after": [
            0.0052,
            0.0025,
            0.159,
            -0.0375,
            0.0371,
            -0.0805,
            -0.1155,
            -0.1488,
            -0.022,
            0.0886,
            0.1603,
            0.0194,
            -0.0946,
            -0.0273,
            -0.0531,
            -0.1903
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0057,
            0.0034,
            -0.0301,
            -0.003,
            0.0436,
            -0.0908,
            0.0882,
            -0.0129,
            -0.068,
            0.0448,
            0.0015,
            0.0029,
            0.0409,
            -0.0415,
            -0.119,
            0.0265
          ],
          "after": [
            0.0263,
            0.1085,
            -0.0405,
            -0.0757,
            0.0353,
            -0.0115,
            0.0839,
            0.0931,
            -0.0784,
            -0.0798,
            0.0125,
            -0.0211,
            0.1056,
            0.0418,
            -0.0535,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            0.0826,
            -0.0318,
            0.058,
            -0.1581,
            0.0239,
            -0.001,
            -0.2551,
            -0.0922,
            0.1118,
            0.0611,
            0.1694,
            0.0295,
            -0.1591,
            -0.0107,
            -0.0834,
            -0.2109
          ],
          "after": [
            0.0449,
            -0.1389,
            -0.2936,
            0.0613,
            0.0257,
            -0.0518,
            -0.1472,
            0.0642,
            0.1037,
            0.0058,
            0.0162,
            0.0963,
            0.0422,
            0.0112,
            -0.1272,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0002,
            0.0004,
            0.0005,
            -0.0006,
            -0.0,
            -0.0015,
            -0.0009,
            -0.0016,
            -0.0003,
            0.0013,
            -0.0007,
            -0.0011,
            0.0,
            0.0011,
            -0.0015
          ],
          "after": [
            -0.0821,
            0.0153,
            -0.1169,
            -0.0697,
            0.0755,
            0.1437,
            0.077,
            -0.1652,
            0.0069,
            -0.0196,
            -0.1254,
            -0.0724,
            0.0899,
            -0.0807,
            0.0428,
            0.0773
          ]
        }
      }
    },
    {
      "step": 626,
      "word": "saveah",
      "loss": 2.4137,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2704,
            -0.0201,
            0.2352,
            0.136,
            0.355,
            0.1478,
            -0.0059,
            0.1061,
            0.0093,
            0.2346,
            0.0298,
            -0.0089,
            0.2595,
            -0.143,
            -0.2591,
            0.2262
          ],
          "after": [
            0.0055,
            0.0031,
            0.1585,
            -0.0374,
            0.0365,
            -0.0813,
            -0.1156,
            -0.1488,
            -0.022,
            0.088,
            0.1602,
            0.0196,
            -0.0949,
            -0.0266,
            -0.0526,
            -0.1907
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0808,
            -0.1343,
            0.0786,
            -0.1786,
            0.1447,
            0.1082,
            -0.3563,
            -0.0135,
            0.2221,
            -0.1622,
            0.1364,
            0.0403,
            -0.288,
            -0.0791,
            0.2726,
            -0.1151
          ],
          "after": [
            0.0263,
            0.1086,
            -0.0403,
            -0.0756,
            0.0351,
            -0.0115,
            0.0839,
            0.0932,
            -0.0784,
            -0.0798,
            0.0124,
            -0.0211,
            0.1057,
            0.0421,
            -0.0535,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            0.0116,
            0.1092,
            -0.015,
            0.09,
            -0.1116,
            0.052,
            -0.0103,
            0.035,
            -0.0659,
            0.0383,
            -0.1043,
            0.0665,
            0.0888,
            0.0163,
            0.0749,
            0.019
          ],
          "after": [
            0.0446,
            -0.1393,
            -0.2937,
            0.0613,
            0.0261,
            -0.0518,
            -0.1469,
            0.0644,
            0.1038,
            0.0057,
            0.0159,
            0.0959,
            0.0423,
            0.0111,
            -0.127,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0002,
            -0.0002,
            0.0003,
            0.0001,
            0.001,
            -0.0007,
            0.0007,
            0.0,
            0.0001,
            0.0004,
            -0.0002,
            -0.0005,
            -0.0005,
            0.0006,
            0.0
          ],
          "after": [
            -0.0821,
            0.0152,
            -0.1169,
            -0.0698,
            0.0756,
            0.1434,
            0.0771,
            -0.1651,
            0.0066,
            -0.0196,
            -0.1255,
            -0.0724,
            0.09,
            -0.0806,
            0.0428,
            0.0773
          ]
        }
      }
    },
    {
      "step": 627,
      "word": "elize",
      "loss": 2.1732,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0057,
            0.0036,
            0.158,
            -0.0374,
            0.036,
            -0.082,
            -0.1157,
            -0.1488,
            -0.022,
            0.0875,
            0.1602,
            0.0198,
            -0.0951,
            -0.0261,
            -0.0521,
            -0.191
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0286,
            0.0767,
            0.8511,
            -0.0799,
            -0.0153,
            0.2457,
            -0.2502,
            -0.1161,
            0.3109,
            -0.2576,
            0.1225,
            -0.1565,
            -0.3292,
            0.2797,
            0.2737,
            -0.3023
          ],
          "after": [
            0.0264,
            0.1086,
            -0.0407,
            -0.0753,
            0.035,
            -0.0118,
            0.084,
            0.0935,
            -0.0785,
            -0.0794,
            0.0121,
            -0.0209,
            0.1061,
            0.042,
            -0.0536,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            -0.0049,
            -0.2032,
            0.0663,
            -0.0513,
            -0.001,
            0.0942,
            -0.2754,
            -0.0912,
            0.1626,
            0.1611,
            0.0303,
            -0.0274,
            -0.1535,
            -0.0053,
            -0.0196,
            -0.3282
          ],
          "after": [
            0.0443,
            -0.1393,
            -0.2939,
            0.0614,
            0.0264,
            -0.0519,
            -0.1464,
            0.0647,
            0.1036,
            0.0052,
            0.0156,
            0.0956,
            0.0426,
            0.0111,
            -0.1269,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0016,
            0.0012,
            -0.0004,
            0.0007,
            0.0024,
            -0.0013,
            -0.0007,
            0.0001,
            0.0018,
            0.0008,
            -0.0014,
            -0.0008,
            0.0015,
            -0.0014,
            -0.0013
          ],
          "after": [
            -0.0822,
            0.0153,
            -0.117,
            -0.07,
            0.0755,
            0.1429,
            0.0773,
            -0.165,
            0.0063,
            -0.0198,
            -0.1257,
            -0.0722,
            0.0902,
            -0.0808,
            0.0429,
            0.0775
          ]
        }
      }
    },
    {
      "step": 628,
      "word": "lavera",
      "loss": 2.1377,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0027,
            0.0285,
            -0.0058,
            -0.032,
            0.2414,
            -0.0308,
            -0.0269,
            -0.0343,
            -0.1881,
            0.2443,
            0.1856,
            -0.0721,
            0.1288,
            -0.1219,
            -0.194,
            -0.0267
          ],
          "after": [
            0.0058,
            0.0039,
            0.1576,
            -0.0373,
            0.0353,
            -0.0825,
            -0.1158,
            -0.1487,
            -0.0217,
            0.0867,
            0.1598,
            0.0201,
            -0.0955,
            -0.0254,
            -0.0514,
            -0.1912
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0951,
            -0.0271,
            0.0541,
            -0.1231,
            0.0947,
            0.0591,
            -0.2436,
            -0.0332,
            0.1409,
            -0.1615,
            0.1169,
            -0.0068,
            -0.1946,
            -0.0963,
            0.2137,
            -0.0571
          ],
          "after": [
            0.0265,
            0.1086,
            -0.0412,
            -0.0748,
            0.0348,
            -0.012,
            0.0842,
            0.0939,
            -0.0788,
            -0.0788,
            0.0116,
            -0.0207,
            0.1065,
            0.042,
            -0.0538,
            0.2213
          ]
        },
        "position_0": {
          "grad": [
            -0.0876,
            0.1423,
            -0.1208,
            -0.0586,
            0.0584,
            0.067,
            -0.0077,
            0.0679,
            -0.0827,
            -0.0006,
            -0.1426,
            0.2345,
            -0.0742,
            -0.0178,
            0.0485,
            -0.0758
          ],
          "after": [
            0.0443,
            -0.1396,
            -0.2938,
            0.0616,
            0.0265,
            -0.0522,
            -0.146,
            0.0648,
            0.1036,
            0.0048,
            0.0155,
            0.0949,
            0.0429,
            0.0111,
            -0.1268,
            0.0529
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0002,
            -0.0002,
            -0.001,
            0.0004,
            -0.0009,
            -0.0005,
            0.0001,
            0.0004,
            -0.0004,
            0.0003,
            -0.0013,
            0.0001,
            -0.0002,
            0.0001,
            -0.0009
          ],
          "after": [
            -0.0823,
            0.0154,
            -0.1171,
            -0.07,
            0.0755,
            0.1425,
            0.0775,
            -0.1648,
            0.0059,
            -0.02,
            -0.1259,
            -0.0719,
            0.0903,
            -0.0808,
            0.043,
            0.0777
          ]
        }
      }
    },
    {
      "step": 629,
      "word": "althea",
      "loss": 2.7923,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0381,
            0.1759,
            -0.1598,
            -0.1886,
            -0.0454,
            -0.0785,
            -0.1005,
            0.0648,
            -0.1944,
            0.0617,
            -0.1005,
            0.2171,
            -0.1071,
            0.0614,
            0.1178,
            -0.4621
          ],
          "after": [
            0.006,
            0.004,
            0.1574,
            -0.0369,
            0.0348,
            -0.0828,
            -0.1157,
            -0.1488,
            -0.0211,
            0.0859,
            0.1597,
            0.02,
            -0.0957,
            -0.025,
            -0.051,
            -0.191
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1844,
            -0.1442,
            0.0689,
            -0.0467,
            -0.21,
            0.0811,
            -0.4091,
            -0.0797,
            -0.0221,
            -0.1313,
            0.0565,
            -0.2234,
            0.1103,
            0.2724,
            0.1424,
            -0.1675
          ],
          "after": [
            0.0264,
            0.1087,
            -0.0416,
            -0.0744,
            0.0348,
            -0.0123,
            0.0846,
            0.0945,
            -0.079,
            -0.0782,
            0.0111,
            -0.0203,
            0.1068,
            0.0417,
            -0.0541,
            0.2216
          ]
        },
        "position_0": {
          "grad": [
            0.0848,
            -0.0303,
            0.0722,
            -0.1438,
            0.048,
            0.0132,
            -0.2668,
            -0.1043,
            0.1143,
            0.0438,
            0.2049,
            0.0526,
            -0.1489,
            0.0075,
            -0.0804,
            -0.1922
          ],
          "after": [
            0.0441,
            -0.1397,
            -0.2939,
            0.062,
            0.0266,
            -0.0524,
            -0.1454,
            0.0652,
            0.1034,
            0.0043,
            0.0152,
            0.0943,
            0.0433,
            0.0111,
            -0.1267,
            0.0536
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.0018,
            -0.0008,
            0.0012,
            -0.0015,
            -0.0009,
            0.0018,
            -0.0012,
            -0.0008,
            -0.0011,
            0.0003,
            -0.0024,
            0.0023,
            0.0008,
            0.0001,
            0.0005
          ],
          "after": [
            -0.0825,
            0.0153,
            -0.1171,
            -0.0701,
            0.0755,
            0.1423,
            0.0776,
            -0.1646,
            0.0058,
            -0.02,
            -0.1262,
            -0.0714,
            0.0902,
            -0.081,
            0.043,
            0.0778
          ]
        }
      }
    },
    {
      "step": 630,
      "word": "theory",
      "loss": 2.7704,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0061,
            0.0041,
            0.1573,
            -0.0366,
            0.0344,
            -0.0831,
            -0.1157,
            -0.1489,
            -0.0205,
            0.0852,
            0.1596,
            0.02,
            -0.0959,
            -0.0246,
            -0.0506,
            -0.1908
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0959,
            0.0524,
            0.1955,
            0.1697,
            0.0598,
            0.2129,
            -0.1154,
            -0.0146,
            0.0336,
            0.0633,
            0.1899,
            0.239,
            -0.216,
            -0.1597,
            0.1418,
            -0.1232
          ],
          "after": [
            0.0264,
            0.1088,
            -0.0421,
            -0.0743,
            0.0348,
            -0.0127,
            0.085,
            0.095,
            -0.0792,
            -0.0777,
            0.0104,
            -0.0203,
            0.1073,
            0.0417,
            -0.0545,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            -0.0199,
            -0.1579,
            0.063,
            0.0545,
            -0.1558,
            0.0468,
            0.0172,
            0.0099,
            -0.0592,
            -0.1267,
            -0.0897,
            -0.2969,
            0.1014,
            0.0748,
            0.1388,
            0.1115
          ],
          "after": [
            0.0439,
            -0.1396,
            -0.2941,
            0.0622,
            0.0269,
            -0.0527,
            -0.1449,
            0.0655,
            0.1034,
            0.0042,
            0.015,
            0.0942,
            0.0435,
            0.0109,
            -0.1268,
            0.054
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0014,
            -0.0002,
            0.001,
            -0.0002,
            0.0012,
            0.0008,
            -0.0001,
            -0.0005,
            -0.0012,
            -0.0002,
            0.001,
            -0.0006,
            -0.001,
            0.0019,
            -0.0006
          ],
          "after": [
            -0.0825,
            0.0151,
            -0.117,
            -0.0703,
            0.0756,
            0.1421,
            0.0776,
            -0.1643,
            0.0057,
            -0.02,
            -0.1263,
            -0.0711,
            0.0902,
            -0.081,
            0.0429,
            0.078
          ]
        }
      }
    },
    {
      "step": 631,
      "word": "tabassum",
      "loss": 3.0402,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1304,
            0.2182,
            0.4223,
            0.1642,
            0.2231,
            0.0589,
            0.2098,
            0.0698,
            -0.2624,
            0.0068,
            -0.0174,
            0.2433,
            0.264,
            -0.2386,
            -0.2052,
            0.2578
          ],
          "after": [
            0.0064,
            0.0039,
            0.1567,
            -0.0366,
            0.0338,
            -0.0834,
            -0.1159,
            -0.1491,
            -0.0197,
            0.0847,
            0.1595,
            0.0197,
            -0.0964,
            -0.0239,
            -0.05,
            -0.1908
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.069,
            0.0791,
            -0.0603,
            0.0401,
            0.0601,
            -0.1542,
            0.1407,
            0.0028,
            -0.1383,
            -0.0008,
            -0.0067,
            0.0505,
            0.0531,
            -0.0533,
            -0.1319,
            0.0944
          ],
          "after": [
            0.0265,
            0.1088,
            -0.0425,
            -0.0742,
            0.0347,
            -0.0129,
            0.0853,
            0.0954,
            -0.0792,
            -0.0773,
            0.0098,
            -0.0203,
            0.1076,
            0.0417,
            -0.0547,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0044,
            -0.0737,
            0.0398,
            0.0627,
            -0.032,
            0.0243,
            0.0778,
            -0.0123,
            -0.1186,
            -0.0476,
            -0.0556,
            -0.146,
            0.1237,
            0.0099,
            0.0368,
            0.125
          ],
          "after": [
            0.0438,
            -0.1394,
            -0.2944,
            0.0623,
            0.0272,
            -0.053,
            -0.1445,
            0.0657,
            0.1035,
            0.0042,
            0.015,
            0.0944,
            0.0436,
            0.0107,
            -0.1269,
            0.0542
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0,
            0.0001,
            -0.001,
            0.0024,
            -0.0017,
            0.0016,
            -0.0,
            -0.0004,
            -0.0008,
            0.0014,
            0.003,
            -0.0016,
            -0.0027,
            -0.0009,
            0.0002
          ],
          "after": [
            -0.0825,
            0.0148,
            -0.117,
            -0.0703,
            0.0755,
            0.142,
            0.0775,
            -0.1641,
            0.0057,
            -0.0198,
            -0.1266,
            -0.0711,
            0.0903,
            -0.0808,
            0.0429,
            0.0781
          ]
        }
      }
    },
    {
      "step": 632,
      "word": "arena",
      "loss": 1.7853,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.418,
            -0.2421,
            -0.3003,
            -0.1427,
            -0.1745,
            0.1532,
            0.0431,
            -0.1692,
            0.0308,
            0.0894,
            0.1857,
            -0.3051,
            -0.1582,
            -0.0923,
            0.1868,
            -0.1886
          ],
          "after": [
            0.0062,
            0.004,
            0.1566,
            -0.0364,
            0.0334,
            -0.0839,
            -0.1161,
            -0.1489,
            -0.0189,
            0.084,
            0.1592,
            0.0198,
            -0.0966,
            -0.0232,
            -0.0498,
            -0.1907
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0624,
            -0.2083,
            0.1944,
            0.0926,
            0.2706,
            0.3547,
            -0.379,
            0.0858,
            0.1876,
            0.0892,
            0.1455,
            0.2939,
            -0.4082,
            -0.0405,
            0.2084,
            -0.0802
          ],
          "after": [
            0.0266,
            0.109,
            -0.0429,
            -0.0744,
            0.0344,
            -0.0134,
            0.0857,
            0.0956,
            -0.0794,
            -0.0771,
            0.0091,
            -0.0205,
            0.1082,
            0.0417,
            -0.055,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            0.1089,
            -0.0679,
            0.0765,
            -0.2213,
            -0.0005,
            -0.0098,
            -0.3645,
            -0.1117,
            0.183,
            0.039,
            0.2059,
            0.0017,
            -0.213,
            0.0272,
            -0.0691,
            -0.2737
          ],
          "after": [
            0.0435,
            -0.1391,
            -0.2948,
            0.0627,
            0.0275,
            -0.0533,
            -0.1439,
            0.0662,
            0.1033,
            0.0042,
            0.0146,
            0.0946,
            0.0439,
            0.0105,
            -0.1269,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0002,
            0.0002,
            0.0005,
            -0.0011,
            0.0003,
            -0.0001,
            -0.0007,
            -0.0011,
            0.0006,
            0.0013,
            -0.0009,
            -0.0001,
            -0.0001,
            0.0004,
            -0.0009
          ],
          "after": [
            -0.0824,
            0.0147,
            -0.117,
            -0.0704,
            0.0754,
            0.1419,
            0.0774,
            -0.1638,
            0.0058,
            -0.0198,
            -0.127,
            -0.0711,
            0.0904,
            -0.0806,
            0.0429,
            0.0783
          ]
        }
      }
    },
    {
      "step": 633,
      "word": "magdalene",
      "loss": 2.4933,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2727,
            0.1384,
            0.0272,
            0.1269,
            0.1495,
            0.0326,
            0.0663,
            0.1568,
            -0.1458,
            0.1065,
            0.0026,
            0.2776,
            -0.0185,
            -0.1448,
            -0.0792,
            0.0552
          ],
          "after": [
            0.0063,
            0.0039,
            0.1564,
            -0.0364,
            0.033,
            -0.0845,
            -0.1164,
            -0.1491,
            -0.0181,
            0.0833,
            0.1589,
            0.0196,
            -0.0967,
            -0.0224,
            -0.0495,
            -0.1907
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.5742,
            -0.256,
            -0.218,
            -0.1446,
            -0.4428,
            0.3263,
            -0.5797,
            -0.0164,
            0.0252,
            0.1346,
            0.02,
            -0.551,
            0.2122,
            0.446,
            0.2845,
            -0.5092
          ],
          "after": [
            0.0262,
            0.1094,
            -0.0432,
            -0.0742,
            0.0345,
            -0.014,
            0.0863,
            0.0957,
            -0.0796,
            -0.0771,
            0.0084,
            -0.0202,
            0.1086,
            0.0413,
            -0.0555,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.0145,
            0.0527,
            -0.0361,
            0.0314,
            0.1074,
            -0.0138,
            0.1636,
            0.0394,
            -0.0096,
            0.0334,
            0.001,
            0.0758,
            -0.0371,
            -0.0451,
            -0.0226,
            0.0946
          ],
          "after": [
            0.0432,
            -0.1389,
            -0.2951,
            0.063,
            0.0275,
            -0.0535,
            -0.1435,
            0.0666,
            0.1032,
            0.004,
            0.0143,
            0.0947,
            0.0442,
            0.0105,
            -0.1269,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0017,
            0.0019,
            -0.0004,
            0.0027,
            -0.0019,
            0.001,
            -0.0017,
            -0.0018,
            0.0014,
            0.0027,
            0.0016,
            0.0008,
            -0.0016,
            -0.0037,
            -0.0003
          ],
          "after": [
            -0.0824,
            0.0144,
            -0.1172,
            -0.0705,
            0.0752,
            0.142,
            0.0772,
            -0.1634,
            0.0062,
            -0.0198,
            -0.1276,
            -0.0712,
            0.0905,
            -0.0802,
            0.0431,
            0.0785
          ]
        }
      }
    },
    {
      "step": 634,
      "word": "lyonel",
      "loss": 2.5038,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0064,
            0.0039,
            0.1563,
            -0.0364,
            0.0326,
            -0.0849,
            -0.1166,
            -0.1492,
            -0.0174,
            0.0827,
            0.1587,
            0.0194,
            -0.0968,
            -0.0217,
            -0.0492,
            -0.1906
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0993,
            0.0799,
            -0.0288,
            0.0479,
            -0.2493,
            0.0287,
            -0.2108,
            -0.1096,
            -0.131,
            -0.2675,
            -0.0197,
            -0.2076,
            0.0875,
            0.2264,
            0.0433,
            0.0955
          ],
          "after": [
            0.0257,
            0.1096,
            -0.0433,
            -0.0742,
            0.0348,
            -0.0146,
            0.087,
            0.0961,
            -0.0796,
            -0.0768,
            0.0079,
            -0.0198,
            0.1088,
            0.0406,
            -0.0559,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            -0.1054,
            0.1014,
            -0.1079,
            -0.0445,
            0.0291,
            0.0826,
            -0.0367,
            0.0861,
            -0.0427,
            -0.0416,
            -0.151,
            0.205,
            -0.0946,
            0.002,
            0.092,
            -0.0846
          ],
          "after": [
            0.0432,
            -0.1389,
            -0.295,
            0.0634,
            0.0275,
            -0.0538,
            -0.1431,
            0.0666,
            0.1031,
            0.004,
            0.0143,
            0.0943,
            0.0445,
            0.0104,
            -0.127,
            0.0554
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0015,
            0.0008,
            0.0012,
            -0.0004,
            0.0006,
            -0.0025,
            -0.0006,
            -0.0009,
            -0.0028,
            0.0,
            -0.0007,
            -0.0009,
            0.0009,
            0.0001,
            -0.0001
          ],
          "after": [
            -0.0825,
            0.014,
            -0.1175,
            -0.0706,
            0.075,
            0.142,
            0.0773,
            -0.1629,
            0.0066,
            -0.0197,
            -0.1282,
            -0.0712,
            0.0906,
            -0.08,
            0.0433,
            0.0787
          ]
        }
      }
    },
    {
      "step": 635,
      "word": "trygve",
      "loss": 3.1485,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0065,
            0.0038,
            0.1561,
            -0.0364,
            0.0323,
            -0.0853,
            -0.1168,
            -0.1493,
            -0.0168,
            0.0822,
            0.1585,
            0.0193,
            -0.0969,
            -0.0211,
            -0.049,
            -0.1906
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.112,
            -0.1081,
            -0.1194,
            -0.1556,
            -0.2458,
            0.1784,
            -0.3386,
            -0.1006,
            0.2247,
            0.083,
            0.1474,
            -0.2717,
            0.0193,
            0.0966,
            0.2063,
            -0.1329
          ],
          "after": [
            0.0251,
            0.1099,
            -0.0434,
            -0.0739,
            0.0352,
            -0.0152,
            0.0877,
            0.0968,
            -0.0798,
            -0.0766,
            0.0072,
            -0.0192,
            0.109,
            0.04,
            -0.0564,
            0.2239
          ]
        },
        "position_0": {
          "grad": [
            -0.0276,
            -0.1385,
            0.0639,
            0.0989,
            -0.115,
            0.0696,
            0.105,
            0.0206,
            -0.128,
            -0.1315,
            -0.0803,
            -0.2195,
            0.1429,
            0.0393,
            0.1297,
            0.1668
          ],
          "after": [
            0.0433,
            -0.1387,
            -0.2952,
            0.0635,
            0.0277,
            -0.0542,
            -0.1429,
            0.0666,
            0.1033,
            0.0043,
            0.0144,
            0.0944,
            0.0447,
            0.0102,
            -0.1273,
            0.0555
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0023,
            -0.0037,
            0.002,
            -0.0013,
            0.0018,
            0.0031,
            -0.0003,
            -0.0012,
            -0.0012,
            -0.001,
            -0.0012,
            0.0011,
            0.0005,
            0.0031,
            -0.0007
          ],
          "after": [
            -0.0825,
            0.0134,
            -0.1173,
            -0.071,
            0.075,
            0.1419,
            0.0771,
            -0.1624,
            0.0072,
            -0.0194,
            -0.1285,
            -0.0711,
            0.0906,
            -0.0799,
            0.0432,
            0.079
          ]
        }
      }
    },
    {
      "step": 636,
      "word": "tyke",
      "loss": 2.6721,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0065,
            0.0038,
            0.156,
            -0.0363,
            0.0321,
            -0.0856,
            -0.117,
            -0.1494,
            -0.0162,
            0.0818,
            0.1583,
            0.0191,
            -0.097,
            -0.0206,
            -0.0488,
            -0.1906
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0698,
            -0.3684,
            0.0464,
            -0.0697,
            0.0397,
            0.2432,
            -0.4949,
            -0.1269,
            0.1752,
            -0.1841,
            0.1472,
            -0.0084,
            -0.0563,
            -0.1037,
            0.2859,
            -0.3757
          ],
          "after": [
            0.0247,
            0.1105,
            -0.0435,
            -0.0736,
            0.0355,
            -0.016,
            0.0885,
            0.0976,
            -0.0801,
            -0.0762,
            0.0064,
            -0.0186,
            0.1091,
            0.0395,
            -0.057,
            0.2245
          ]
        },
        "position_0": {
          "grad": [
            -0.0291,
            -0.2353,
            0.1008,
            0.0997,
            -0.1838,
            0.0693,
            0.0527,
            0.0282,
            -0.1064,
            -0.1959,
            -0.1242,
            -0.367,
            0.1622,
            0.0953,
            0.1984,
            0.1734
          ],
          "after": [
            0.0435,
            -0.1382,
            -0.2955,
            0.0635,
            0.0281,
            -0.0547,
            -0.1428,
            0.0666,
            0.1035,
            0.0049,
            0.0146,
            0.0951,
            0.0446,
            0.0099,
            -0.1279,
            0.0554
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.002,
            -0.002,
            -0.0013,
            -0.0018,
            0.0023,
            -0.0019,
            -0.0009,
            0.0003,
            -0.0031,
            -0.0036,
            0.0008,
            -0.0013,
            0.0032,
            0.0048,
            -0.0008
          ],
          "after": [
            -0.0826,
            0.0131,
            -0.1169,
            -0.0711,
            0.0751,
            0.1415,
            0.0771,
            -0.1619,
            0.0076,
            -0.019,
            -0.1284,
            -0.0711,
            0.0907,
            -0.0801,
            0.0429,
            0.0792
          ]
        }
      }
    },
    {
      "step": 637,
      "word": "moorea",
      "loss": 2.5005,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1204,
            -0.0041,
            -0.0321,
            -0.1144,
            -0.0322,
            -0.0571,
            -0.0312,
            -0.0117,
            -0.0548,
            0.1225,
            0.0975,
            -0.1027,
            0.0461,
            0.0502,
            -0.0155,
            -0.1581
          ],
          "after": [
            0.0064,
            0.0037,
            0.156,
            -0.0362,
            0.0319,
            -0.0858,
            -0.1171,
            -0.1495,
            -0.0157,
            0.0812,
            0.158,
            0.0191,
            -0.0972,
            -0.0202,
            -0.0486,
            -0.1904
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0659,
            -0.2953,
            0.0008,
            -0.029,
            0.023,
            0.15,
            -0.4198,
            0.0651,
            -0.0219,
            -0.0956,
            -0.0493,
            -0.0848,
            -0.0208,
            0.2443,
            0.1062,
            -0.0496
          ],
          "after": [
            0.0243,
            0.1113,
            -0.0436,
            -0.0733,
            0.0358,
            -0.0167,
            0.0894,
            0.0982,
            -0.0803,
            -0.0757,
            0.0058,
            -0.0181,
            0.1093,
            0.0389,
            -0.0576,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            -0.0192,
            0.0303,
            -0.0361,
            -0.0183,
            0.1245,
            -0.0145,
            0.1569,
            0.0506,
            0.0358,
            0.0288,
            0.0097,
            0.026,
            -0.0669,
            -0.0368,
            -0.0234,
            0.0936
          ],
          "after": [
            0.0437,
            -0.1378,
            -0.2957,
            0.0635,
            0.0283,
            -0.0551,
            -0.1428,
            0.0664,
            0.1037,
            0.0054,
            0.0148,
            0.0956,
            0.0446,
            0.0097,
            -0.1284,
            0.0553
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            0.0005,
            -0.004,
            0.0018,
            -0.0015,
            0.0026,
            -0.0013,
            0.0002,
            -0.001,
            -0.0026,
            -0.004,
            -0.0002,
            -0.0012,
            0.0021,
            0.0059,
            0.003
          ],
          "after": [
            -0.0825,
            0.0128,
            -0.1162,
            -0.0714,
            0.0753,
            0.141,
            0.0772,
            -0.1615,
            0.008,
            -0.0184,
            -0.1279,
            -0.0711,
            0.0908,
            -0.0805,
            0.0422,
            0.0792
          ]
        }
      }
    },
    {
      "step": 638,
      "word": "alexiah",
      "loss": 2.4574,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3044,
            0.1534,
            0.0893,
            0.0699,
            0.1588,
            0.1151,
            -0.0285,
            0.1733,
            0.005,
            -0.0019,
            -0.3017,
            0.3052,
            0.0218,
            -0.013,
            -0.034,
            -0.0967
          ],
          "after": [
            0.0067,
            0.0035,
            0.1559,
            -0.0361,
            0.0316,
            -0.0861,
            -0.1171,
            -0.1499,
            -0.0153,
            0.0808,
            0.1582,
            0.0188,
            -0.0973,
            -0.0199,
            -0.0484,
            -0.1901
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.018,
            -0.2547,
            0.1931,
            0.0192,
            0.2449,
            0.2079,
            -0.2962,
            0.0524,
            0.1969,
            0.1746,
            0.0385,
            0.1995,
            -0.2519,
            -0.0194,
            0.0105,
            -0.2118
          ],
          "after": [
            0.024,
            0.1121,
            -0.0438,
            -0.073,
            0.0358,
            -0.0175,
            0.0904,
            0.0985,
            -0.0806,
            -0.0756,
            0.0052,
            -0.0178,
            0.1097,
            0.0384,
            -0.0581,
            0.2258
          ]
        },
        "position_0": {
          "grad": [
            0.064,
            -0.0005,
            0.0503,
            -0.1247,
            0.0723,
            0.0065,
            -0.2093,
            -0.1144,
            0.0749,
            0.0537,
            0.2038,
            0.0606,
            -0.1035,
            0.0072,
            -0.1078,
            -0.1385
          ],
          "after": [
            0.0437,
            -0.1374,
            -0.296,
            0.0636,
            0.0284,
            -0.0555,
            -0.1427,
            0.0665,
            0.1037,
            0.0057,
            0.0147,
            0.0959,
            0.0448,
            0.0095,
            -0.1286,
            0.0553
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0011,
            0.0002,
            0.0012,
            -0.0026,
            0.0004,
            -0.002,
            0.0001,
            0.0001,
            0.0001,
            0.0006,
            0.0007,
            -0.0009,
            0.0007,
            0.0013,
            -0.0
          ],
          "after": [
            -0.0824,
            0.0126,
            -0.1157,
            -0.0718,
            0.0757,
            0.1405,
            0.0774,
            -0.1612,
            0.0084,
            -0.0179,
            -0.1275,
            -0.0712,
            0.0911,
            -0.0809,
            0.0416,
            0.0791
          ]
        }
      }
    },
    {
      "step": 639,
      "word": "nekoda",
      "loss": 2.5926,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1105,
            0.0029,
            -0.0311,
            -0.1074,
            -0.0307,
            -0.055,
            -0.0266,
            -0.0131,
            -0.0529,
            0.1111,
            0.0904,
            -0.0932,
            0.0451,
            0.0476,
            -0.0145,
            -0.1465
          ],
          "after": [
            0.0068,
            0.0033,
            0.1558,
            -0.0359,
            0.0313,
            -0.0863,
            -0.1172,
            -0.1502,
            -0.0148,
            0.0802,
            0.1582,
            0.0186,
            -0.0975,
            -0.0197,
            -0.0482,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2063,
            0.0117,
            0.225,
            0.0169,
            0.2216,
            -0.0291,
            -0.4437,
            -0.1063,
            0.1617,
            -0.0063,
            0.084,
            0.0414,
            -0.1917,
            0.1717,
            -0.1979,
            -0.0825
          ],
          "after": [
            0.0235,
            0.1128,
            -0.0441,
            -0.0728,
            0.0357,
            -0.0181,
            0.0914,
            0.0991,
            -0.081,
            -0.0754,
            0.0046,
            -0.0176,
            0.1102,
            0.0377,
            -0.0584,
            0.2264
          ]
        },
        "position_0": {
          "grad": [
            -0.0478,
            0.1312,
            -0.0797,
            0.0482,
            -0.2903,
            0.0308,
            0.0486,
            -0.0175,
            0.0385,
            -0.1793,
            -0.2136,
            -0.0308,
            -0.0015,
            0.1076,
            0.1686,
            0.1429
          ],
          "after": [
            0.0438,
            -0.1374,
            -0.2961,
            0.0637,
            0.0288,
            -0.0559,
            -0.1426,
            0.0667,
            0.1037,
            0.0064,
            0.0149,
            0.0962,
            0.0449,
            0.0091,
            -0.1291,
            0.0551
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0008,
            -0.0007,
            -0.0003,
            -0.0003,
            0.0009,
            -0.0002,
            0.0007,
            0.0001,
            0.0003,
            0.0004,
            -0.0,
            -0.0002,
            -0.0,
            0.0001,
            -0.0007
          ],
          "after": [
            -0.0824,
            0.0126,
            -0.1151,
            -0.0721,
            0.0761,
            0.14,
            0.0776,
            -0.161,
            0.0087,
            -0.0175,
            -0.1272,
            -0.0712,
            0.0913,
            -0.0813,
            0.041,
            0.0791
          ]
        }
      }
    },
    {
      "step": 640,
      "word": "kalli",
      "loss": 1.9565,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1462,
            0.1534,
            -0.1025,
            -0.1063,
            0.0332,
            -0.0603,
            -0.0853,
            0.0666,
            -0.0532,
            -0.0504,
            -0.1472,
            0.2658,
            -0.0962,
            0.0302,
            0.0502,
            -0.2656
          ],
          "after": [
            0.0071,
            0.0029,
            0.1558,
            -0.0356,
            0.0311,
            -0.0864,
            -0.1171,
            -0.1506,
            -0.0143,
            0.0798,
            0.1584,
            0.0182,
            -0.0975,
            -0.0195,
            -0.0481,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.107,
            0.3281,
            -0.2321,
            0.0597,
            -0.0496,
            -0.2894,
            0.5357,
            0.0128,
            -0.3788,
            -0.0285,
            -0.0491,
            -0.0822,
            0.2058,
            -0.1299,
            -0.1971,
            0.3445
          ],
          "after": [
            0.0232,
            0.1131,
            -0.0442,
            -0.0727,
            0.0356,
            -0.0184,
            0.0919,
            0.0995,
            -0.0811,
            -0.0753,
            0.0042,
            -0.0174,
            0.1104,
            0.0373,
            -0.0585,
            0.2267
          ]
        },
        "position_0": {
          "grad": [
            -0.0978,
            0.0693,
            -0.1301,
            0.0279,
            0.0963,
            -0.0908,
            0.2277,
            -0.0116,
            -0.0737,
            0.0194,
            -0.0084,
            0.0431,
            0.0892,
            -0.0617,
            -0.1361,
            -0.0089
          ],
          "after": [
            0.0442,
            -0.1374,
            -0.2958,
            0.0638,
            0.0291,
            -0.056,
            -0.1427,
            0.0668,
            0.1038,
            0.0069,
            0.0151,
            0.0964,
            0.0449,
            0.0088,
            -0.1292,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            0.0026,
            -0.0002,
            0.0012,
            0.0017,
            -0.0,
            0.0043,
            -0.0016,
            -0.0016,
            0.0006,
            0.0006,
            -0.0017,
            0.0021,
            0.0005,
            -0.004,
            0.0017
          ],
          "after": [
            -0.0826,
            0.0123,
            -0.1146,
            -0.0724,
            0.0763,
            0.1396,
            0.0774,
            -0.1607,
            0.0092,
            -0.0173,
            -0.1271,
            -0.0711,
            0.0913,
            -0.0816,
            0.0408,
            0.079
          ]
        }
      }
    },
    {
      "step": 641,
      "word": "zandra",
      "loss": 2.2097,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1343,
            0.1571,
            -0.2118,
            -0.1196,
            -0.3101,
            -0.1439,
            -0.0612,
            -0.0862,
            0.0124,
            -0.0569,
            -0.0416,
            -0.185,
            0.104,
            0.2068,
            0.0749,
            -0.0723
          ],
          "after": [
            0.0071,
            0.0024,
            0.1561,
            -0.0352,
            0.0312,
            -0.0862,
            -0.1169,
            -0.1507,
            -0.0139,
            0.0795,
            0.1587,
            0.018,
            -0.0977,
            -0.0197,
            -0.0482,
            -0.1887
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.084,
            0.2082,
            -0.1743,
            0.0844,
            0.027,
            -0.2099,
            0.376,
            0.0407,
            -0.2769,
            -0.011,
            -0.0681,
            0.0172,
            0.156,
            -0.129,
            -0.2265,
            0.1659
          ],
          "after": [
            0.0231,
            0.1132,
            -0.0442,
            -0.0728,
            0.0355,
            -0.0186,
            0.0922,
            0.0998,
            -0.081,
            -0.0751,
            0.0039,
            -0.0172,
            0.1105,
            0.0372,
            -0.0585,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            0.0208,
            0.0456,
            -0.0284,
            0.1605,
            0.042,
            -0.0567,
            0.206,
            0.0196,
            0.0215,
            0.0224,
            0.0131,
            0.0702,
            0.0061,
            -0.057,
            0.0387,
            0.0378
          ],
          "after": [
            0.0444,
            -0.1375,
            -0.2956,
            0.0635,
            0.0293,
            -0.056,
            -0.143,
            0.0669,
            0.1038,
            0.0072,
            0.0152,
            0.0965,
            0.0449,
            0.0088,
            -0.1295,
            0.0549
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            0.001,
            -0.0014,
            0.0017,
            0.0004,
            -0.0,
            0.003,
            0.0009,
            -0.0006,
            0.0008,
            -0.0006,
            -0.0002,
            0.0016,
            -0.0008,
            -0.0012,
            0.0008
          ],
          "after": [
            -0.0829,
            0.0119,
            -0.114,
            -0.0729,
            0.0764,
            0.1392,
            0.0771,
            -0.1605,
            0.0097,
            -0.0171,
            -0.1269,
            -0.0709,
            0.0912,
            -0.0818,
            0.0407,
            0.0787
          ]
        }
      }
    },
    {
      "step": 642,
      "word": "zayna",
      "loss": 1.9791,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.4935,
            -0.366,
            -0.1197,
            -0.2353,
            -0.0821,
            0.0563,
            -0.2244,
            0.1345,
            -0.1448,
            0.3122,
            0.1161,
            -0.2394,
            0.1118,
            0.2457,
            0.0295,
            -0.3267
          ],
          "after": [
            0.0067,
            0.0024,
            0.1564,
            -0.0345,
            0.0313,
            -0.0861,
            -0.1165,
            -0.1511,
            -0.0133,
            0.0788,
            0.1588,
            0.0181,
            -0.0979,
            -0.0203,
            -0.0482,
            -0.1879
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0503,
            0.0539,
            -0.1676,
            0.0215,
            -0.0271,
            -0.1333,
            0.207,
            0.0267,
            -0.2044,
            0.0333,
            -0.0433,
            -0.0303,
            0.1395,
            -0.0692,
            -0.1238,
            0.1008
          ],
          "after": [
            0.023,
            0.1133,
            -0.0441,
            -0.0729,
            0.0354,
            -0.0185,
            0.0924,
            0.0999,
            -0.0807,
            -0.075,
            0.0037,
            -0.017,
            0.1104,
            0.0371,
            -0.0584,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            0.0325,
            0.0268,
            -0.0249,
            0.1359,
            0.0266,
            -0.0785,
            0.168,
            0.0101,
            0.069,
            0.0168,
            0.0141,
            0.0233,
            -0.0167,
            -0.0392,
            0.0512,
            0.0108
          ],
          "after": [
            0.0445,
            -0.1376,
            -0.2953,
            0.0632,
            0.0294,
            -0.0558,
            -0.1434,
            0.067,
            0.1038,
            0.0075,
            0.0153,
            0.0965,
            0.0449,
            0.0089,
            -0.1297,
            0.0548
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0,
            -0.0004,
            0.0006,
            -0.001,
            0.0014,
            -0.0011,
            0.0002,
            0.0,
            -0.0011,
            -0.0008,
            -0.0002,
            -0.0004,
            0.0009,
            0.0009,
            0.0009
          ],
          "after": [
            -0.0831,
            0.0116,
            -0.1135,
            -0.0734,
            0.0766,
            0.1388,
            0.077,
            -0.1603,
            0.0101,
            -0.0168,
            -0.1266,
            -0.0708,
            0.0911,
            -0.0821,
            0.0405,
            0.0784
          ]
        }
      }
    },
    {
      "step": 643,
      "word": "darious",
      "loss": 2.5424,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0788,
            -0.053,
            -0.0512,
            -0.0828,
            0.0719,
            0.0244,
            0.0456,
            -0.1487,
            0.1086,
            -0.0319,
            0.0406,
            0.0401,
            -0.1002,
            -0.078,
            0.0101,
            0.0811
          ],
          "after": [
            0.0062,
            0.0025,
            0.1567,
            -0.0338,
            0.0314,
            -0.0861,
            -0.1162,
            -0.1512,
            -0.013,
            0.0783,
            0.1588,
            0.0182,
            -0.098,
            -0.0206,
            -0.0483,
            -0.1874
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0252,
            0.0999,
            -0.0952,
            0.0317,
            0.0514,
            -0.1612,
            0.2046,
            0.0159,
            -0.192,
            0.0145,
            -0.0154,
            -0.0085,
            0.0879,
            -0.0615,
            -0.1862,
            0.099
          ],
          "after": [
            0.0229,
            0.1132,
            -0.0439,
            -0.0731,
            0.0354,
            -0.0184,
            0.0925,
            0.1,
            -0.0804,
            -0.075,
            0.0036,
            -0.0168,
            0.1103,
            0.0371,
            -0.0581,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            -0.0551,
            0.0452,
            -0.0282,
            -0.0751,
            0.0869,
            -0.0588,
            0.0544,
            -0.0569,
            -0.019,
            -0.0181,
            -0.0553,
            0.0169,
            0.0426,
            0.0208,
            0.0227,
            0.1149
          ],
          "after": [
            0.0448,
            -0.1378,
            -0.295,
            0.063,
            0.0293,
            -0.0555,
            -0.1437,
            0.0672,
            0.1038,
            0.0078,
            0.0154,
            0.0965,
            0.0449,
            0.0088,
            -0.13,
            0.0546
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0011,
            0.0004,
            -0.0007,
            0.0009,
            -0.0004,
            -0.0015,
            0.001,
            -0.0004,
            -0.0025,
            -0.0002,
            0.0009,
            -0.0012,
            -0.0005,
            0.002,
            0.0002
          ],
          "after": [
            -0.0832,
            0.0115,
            -0.1131,
            -0.0737,
            0.0767,
            0.1384,
            0.0769,
            -0.1604,
            0.0105,
            -0.0165,
            -0.1264,
            -0.0708,
            0.0912,
            -0.0823,
            0.0403,
            0.0782
          ]
        }
      }
    },
    {
      "step": 644,
      "word": "dandre",
      "loss": 2.2325,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0305,
            0.146,
            -0.1811,
            -0.0095,
            -0.2755,
            -0.0978,
            -0.0355,
            -0.0761,
            0.0598,
            -0.1559,
            -0.1228,
            -0.1021,
            0.0569,
            0.1448,
            0.0853,
            0.0769
          ],
          "after": [
            0.0058,
            0.0024,
            0.1572,
            -0.0332,
            0.0317,
            -0.0859,
            -0.1159,
            -0.1511,
            -0.0128,
            0.0781,
            0.1589,
            0.0183,
            -0.0982,
            -0.0211,
            -0.0485,
            -0.187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1492,
            -0.2019,
            -0.1303,
            -0.0793,
            -0.206,
            0.1307,
            -0.278,
            -0.0022,
            0.0683,
            0.063,
            0.0583,
            -0.2461,
            0.0756,
            0.1766,
            0.2327,
            -0.1019
          ],
          "after": [
            0.0227,
            0.1133,
            -0.0436,
            -0.073,
            0.0354,
            -0.0184,
            0.0926,
            0.1001,
            -0.0802,
            -0.075,
            0.0034,
            -0.0164,
            0.1101,
            0.0369,
            -0.0581,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            -0.0675,
            0.0389,
            -0.0333,
            -0.0539,
            0.0734,
            -0.0631,
            0.0821,
            -0.0378,
            -0.0151,
            -0.0455,
            -0.0913,
            0.039,
            0.0346,
            0.0246,
            0.0741,
            0.1321
          ],
          "after": [
            0.0452,
            -0.138,
            -0.2946,
            0.0629,
            0.0291,
            -0.0551,
            -0.1441,
            0.0674,
            0.1038,
            0.0081,
            0.0157,
            0.0965,
            0.0448,
            0.0088,
            -0.1303,
            0.0543
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            0.0017,
            -0.0024,
            0.0024,
            0.0005,
            0.0,
            0.0048,
            0.0007,
            -0.0012,
            0.0018,
            -0.0005,
            0.0002,
            0.0017,
            -0.0009,
            -0.0012,
            0.0002
          ],
          "after": [
            -0.0834,
            0.0112,
            -0.1125,
            -0.0742,
            0.0767,
            0.1381,
            0.0765,
            -0.1605,
            0.011,
            -0.0163,
            -0.1261,
            -0.0708,
            0.091,
            -0.0823,
            0.0401,
            0.0779
          ]
        }
      }
    },
    {
      "step": 645,
      "word": "renly",
      "loss": 2.2361,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0054,
            0.0023,
            0.1576,
            -0.0326,
            0.032,
            -0.0858,
            -0.1157,
            -0.151,
            -0.0127,
            0.0779,
            0.1591,
            0.0184,
            -0.0983,
            -0.0215,
            -0.0487,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0968,
            -0.272,
            0.3167,
            -0.0496,
            0.4579,
            0.1279,
            -0.4719,
            0.0078,
            0.4047,
            0.1758,
            0.1555,
            0.2278,
            -0.3401,
            0.0377,
            -0.0679,
            -0.1253
          ],
          "after": [
            0.0225,
            0.1137,
            -0.0436,
            -0.073,
            0.0352,
            -0.0185,
            0.093,
            0.1002,
            -0.0803,
            -0.0752,
            0.003,
            -0.0163,
            0.1102,
            0.0366,
            -0.058,
            0.2269
          ]
        },
        "position_0": {
          "grad": [
            0.0087,
            -0.0508,
            0.0047,
            -0.0292,
            -0.0616,
            0.1512,
            0.0875,
            -0.0283,
            0.1545,
            -0.1202,
            -0.1404,
            0.0258,
            -0.2015,
            -0.074,
            0.1826,
            0.126
          ],
          "after": [
            0.0455,
            -0.1381,
            -0.2944,
            0.0628,
            0.0291,
            -0.0551,
            -0.1445,
            0.0677,
            0.1036,
            0.0087,
            0.0161,
            0.0964,
            0.0449,
            0.0089,
            -0.1309,
            0.0539
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0001,
            0.0001,
            -0.0009,
            0.0005,
            -0.0009,
            -0.0013,
            0.0,
            0.001,
            0.0003,
            0.0005,
            -0.0004,
            -0.0001,
            0.0009,
            -0.0006,
            -0.0002
          ],
          "after": [
            -0.0837,
            0.011,
            -0.1121,
            -0.0745,
            0.0766,
            0.138,
            0.0763,
            -0.1606,
            0.0113,
            -0.0161,
            -0.1259,
            -0.0707,
            0.091,
            -0.0825,
            0.0401,
            0.0777
          ]
        }
      }
    },
    {
      "step": 646,
      "word": "nayah",
      "loss": 2.1646,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0447,
            -0.2887,
            0.2844,
            -0.0763,
            0.112,
            0.2295,
            -0.1685,
            0.2193,
            0.0883,
            0.2832,
            0.0073,
            -0.0227,
            0.1982,
            0.141,
            -0.1375,
            -0.0215
          ],
          "after": [
            0.005,
            0.0026,
            0.1576,
            -0.0321,
            0.0321,
            -0.086,
            -0.1152,
            -0.1513,
            -0.0127,
            0.0774,
            0.1592,
            0.0186,
            -0.0987,
            -0.0221,
            -0.0486,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0579,
            0.0189,
            -0.0877,
            0.0407,
            0.0648,
            -0.1103,
            0.2084,
            0.0458,
            -0.1444,
            0.0515,
            -0.0411,
            0.0448,
            0.0526,
            -0.0981,
            -0.1372,
            0.1031
          ],
          "after": [
            0.0223,
            0.1139,
            -0.0435,
            -0.0729,
            0.0349,
            -0.0185,
            0.0932,
            0.1001,
            -0.0802,
            -0.0755,
            0.0027,
            -0.0163,
            0.1103,
            0.0366,
            -0.0579,
            0.227
          ]
        },
        "position_0": {
          "grad": [
            -0.0373,
            0.1181,
            -0.0994,
            0.0081,
            -0.3713,
            0.0067,
            0.0023,
            -0.0092,
            0.0844,
            -0.2048,
            -0.2855,
            -0.0813,
            -0.0458,
            0.1252,
            0.2148,
            0.1177
          ],
          "after": [
            0.0458,
            -0.1384,
            -0.2939,
            0.0628,
            0.0296,
            -0.0552,
            -0.1449,
            0.0679,
            0.1033,
            0.0095,
            0.0168,
            0.0964,
            0.0451,
            0.0087,
            -0.1317,
            0.0534
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0015,
            -0.0004,
            -0.0008,
            -0.004,
            0.0013,
            -0.0025,
            -0.0015,
            0.0009,
            0.0032,
            0.0018,
            -0.0026,
            0.0006,
            0.0024,
            0.0025,
            -0.0012
          ],
          "after": [
            -0.084,
            0.0109,
            -0.1116,
            -0.0747,
            0.077,
            0.1377,
            0.0763,
            -0.1605,
            0.0114,
            -0.0162,
            -0.126,
            -0.0705,
            0.0908,
            -0.0828,
            0.0398,
            0.0777
          ]
        }
      }
    },
    {
      "step": 647,
      "word": "aethan",
      "loss": 2.3442,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0688,
            -0.1382,
            0.0417,
            0.0048,
            0.0208,
            -0.0218,
            -0.3787,
            -0.0539,
            0.3613,
            -0.098,
            -0.0061,
            0.1448,
            -0.3721,
            0.0279,
            -0.1087,
            -0.2975
          ],
          "after": [
            0.0048,
            0.003,
            0.1576,
            -0.0316,
            0.0322,
            -0.0862,
            -0.1144,
            -0.1515,
            -0.0133,
            0.077,
            0.1593,
            0.0185,
            -0.0986,
            -0.0226,
            -0.0484,
            -0.1858
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1425,
            0.1374,
            -0.104,
            -0.0031,
            0.2274,
            0.1997,
            0.1734,
            0.2209,
            0.3437,
            -0.1104,
            -0.2386,
            0.2457,
            -0.0302,
            -0.0179,
            0.0859,
            0.3093
          ],
          "after": [
            0.0223,
            0.114,
            -0.0434,
            -0.0729,
            0.0345,
            -0.0186,
            0.0933,
            0.0995,
            -0.0804,
            -0.0756,
            0.0029,
            -0.0165,
            0.1104,
            0.0365,
            -0.0578,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            0.0885,
            -0.0416,
            0.0688,
            -0.1766,
            0.0207,
            -0.0112,
            -0.3142,
            -0.1219,
            0.1509,
            0.052,
            0.1973,
            0.0017,
            -0.1697,
            0.0349,
            -0.0785,
            -0.2224
          ],
          "after": [
            0.0459,
            -0.1385,
            -0.2937,
            0.063,
            0.03,
            -0.0552,
            -0.1449,
            0.0684,
            0.1028,
            0.0101,
            0.0171,
            0.0965,
            0.0455,
            0.0085,
            -0.1323,
            0.0533
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0003,
            -0.0001,
            -0.0006,
            0.0006,
            0.0,
            0.0003,
            -0.0,
            -0.0013,
            0.0001,
            0.0002,
            0.0024,
            -0.0025,
            -0.0016,
            0.0012,
            -0.0005
          ],
          "after": [
            -0.0841,
            0.0109,
            -0.1112,
            -0.0748,
            0.0772,
            0.1375,
            0.0762,
            -0.1604,
            0.0117,
            -0.0164,
            -0.1261,
            -0.0705,
            0.0909,
            -0.083,
            0.0396,
            0.0777
          ]
        }
      }
    },
    {
      "step": 648,
      "word": "brooklyne",
      "loss": 3.0985,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0046,
            0.0034,
            0.1576,
            -0.0312,
            0.0323,
            -0.0864,
            -0.1138,
            -0.1517,
            -0.0138,
            0.0767,
            0.1594,
            0.0185,
            -0.0984,
            -0.023,
            -0.0482,
            -0.1853
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2486,
            0.1125,
            -0.1431,
            -0.0381,
            -0.2856,
            -0.0754,
            -0.0791,
            -0.0313,
            -0.2169,
            -0.0319,
            0.0128,
            -0.3176,
            0.2146,
            0.2113,
            0.0194,
            -0.122
          ],
          "after": [
            0.0221,
            0.114,
            -0.0432,
            -0.0729,
            0.0343,
            -0.0187,
            0.0934,
            0.0991,
            -0.0804,
            -0.0757,
            0.003,
            -0.0163,
            0.1103,
            0.0363,
            -0.0577,
            0.2267
          ]
        },
        "position_0": {
          "grad": [
            0.0217,
            0.0214,
            0.0569,
            0.0352,
            0.0347,
            0.0198,
            0.1067,
            0.0001,
            -0.1013,
            -0.016,
            0.0199,
            0.0442,
            0.1048,
            -0.1132,
            -0.032,
            0.1989
          ],
          "after": [
            0.0459,
            -0.1387,
            -0.2936,
            0.0631,
            0.0303,
            -0.0552,
            -0.145,
            0.0689,
            0.1026,
            0.0107,
            0.0173,
            0.0964,
            0.0457,
            0.0085,
            -0.1327,
            0.0529
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0024,
            0.0014,
            0.0005,
            0.0002,
            -0.0032,
            0.0017,
            0.0012,
            -0.0049,
            -0.0008,
            0.0003,
            -0.0015,
            0.0025,
            0.0011,
            -0.0001,
            -0.0006
          ],
          "after": [
            -0.0843,
            0.0106,
            -0.111,
            -0.075,
            0.0773,
            0.1376,
            0.0761,
            -0.1604,
            0.0125,
            -0.0164,
            -0.1261,
            -0.0703,
            0.0908,
            -0.0832,
            0.0393,
            0.0778
          ]
        }
      }
    },
    {
      "step": 649,
      "word": "lillienne",
      "loss": 2.1967,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0044,
            0.0037,
            0.1576,
            -0.0309,
            0.0324,
            -0.0865,
            -0.1132,
            -0.1518,
            -0.0143,
            0.0765,
            0.1594,
            0.0185,
            -0.0983,
            -0.0234,
            -0.048,
            -0.1849
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.4212,
            0.0967,
            -0.2502,
            0.01,
            -0.7736,
            0.0992,
            -0.2829,
            -0.1178,
            -0.0176,
            -0.0496,
            0.1128,
            -0.4279,
            0.2615,
            0.2723,
            0.4264,
            -0.2018
          ],
          "after": [
            0.0214,
            0.114,
            -0.0428,
            -0.0728,
            0.0348,
            -0.0188,
            0.0937,
            0.099,
            -0.0804,
            -0.0756,
            0.0029,
            -0.0158,
            0.11,
            0.0357,
            -0.058,
            0.2267
          ]
        },
        "position_0": {
          "grad": [
            -0.0759,
            0.0939,
            -0.0876,
            -0.0226,
            0.0662,
            0.0594,
            0.0182,
            0.0585,
            -0.0591,
            0.0163,
            -0.1029,
            0.1896,
            -0.0496,
            -0.0415,
            0.0189,
            -0.0504
          ],
          "after": [
            0.0461,
            -0.139,
            -0.2933,
            0.0633,
            0.0305,
            -0.0553,
            -0.1451,
            0.0691,
            0.1025,
            0.0111,
            0.0177,
            0.0961,
            0.0459,
            0.0087,
            -0.1331,
            0.0527
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0045,
            0.0011,
            -0.0026,
            0.0016,
            -0.0053,
            -0.001,
            -0.0039,
            -0.0007,
            -0.001,
            0.0007,
            0.0005,
            -0.0027,
            -0.0003,
            0.003,
            0.0028,
            -0.0019
          ],
          "after": [
            -0.0849,
            0.0103,
            -0.1106,
            -0.0753,
            0.0779,
            0.1378,
            0.0762,
            -0.1604,
            0.0133,
            -0.0165,
            -0.1263,
            -0.0699,
            0.0908,
            -0.0837,
            0.039,
            0.078
          ]
        }
      }
    },
    {
      "step": 650,
      "word": "micheala",
      "loss": 2.5912,
      "learning_rate": 0.0011,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1751,
            0.0733,
            -0.056,
            0.077,
            0.1128,
            0.1178,
            -0.0166,
            0.1194,
            0.0269,
            -0.1478,
            -0.2152,
            0.2476,
            -0.0949,
            -0.0502,
            0.104,
            0.0312
          ],
          "after": [
            0.0045,
            0.0038,
            0.1577,
            -0.0307,
            0.0323,
            -0.0868,
            -0.1127,
            -0.1521,
            -0.0147,
            0.0765,
            0.1598,
            0.0182,
            -0.0981,
            -0.0237,
            -0.0481,
            -0.1846
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0666,
            -0.103,
            0.0371,
            -0.0048,
            -0.0482,
            0.0195,
            -0.2314,
            -0.0571,
            -0.0946,
            -0.0912,
            0.0257,
            -0.0843,
            0.0596,
            0.1554,
            0.0263,
            -0.0316
          ],
          "after": [
            0.0209,
            0.114,
            -0.0426,
            -0.0728,
            0.0352,
            -0.0189,
            0.094,
            0.0991,
            -0.0804,
            -0.0755,
            0.0027,
            -0.0153,
            0.1097,
            0.0351,
            -0.0582,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            -0.0101,
            0.034,
            -0.0342,
            0.0207,
            0.0868,
            -0.023,
            0.1503,
            0.0469,
            0.0188,
            0.0315,
            -0.0089,
            0.0374,
            -0.0589,
            -0.0338,
            -0.0088,
            0.081
          ],
          "after": [
            0.0463,
            -0.1393,
            -0.2931,
            0.0634,
            0.0305,
            -0.0554,
            -0.1454,
            0.0692,
            0.1023,
            0.0115,
            0.018,
            0.0958,
            0.0461,
            0.0089,
            -0.1335,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.002,
            -0.0007,
            0.0005,
            -0.0041,
            0.0022,
            -0.0001,
            -0.0008,
            -0.002,
            0.0014,
            0.0004,
            0.0002,
            -0.0029,
            0.0003,
            0.0028,
            -0.0014,
            -0.0024
          ],
          "after": [
            -0.0856,
            0.0101,
            -0.1103,
            -0.0751,
            0.0782,
            0.1379,
            0.0764,
            -0.1601,
            0.0138,
            -0.0166,
            -0.1264,
            -0.0693,
            0.0907,
            -0.0843,
            0.0388,
            0.0785
          ]
        }
      }
    },
    {
      "step": 651,
      "word": "cinthya",
      "loss": 2.6744,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0843,
            -0.0316,
            -0.0437,
            -0.0691,
            -0.0151,
            -0.0398,
            -0.0176,
            -0.0305,
            -0.059,
            0.0538,
            0.0752,
            -0.0559,
            0.0082,
            0.0456,
            0.0043,
            -0.0762
          ],
          "after": [
            0.0044,
            0.004,
            0.1578,
            -0.0304,
            0.0323,
            -0.087,
            -0.1122,
            -0.1523,
            -0.015,
            0.0764,
            0.16,
            0.018,
            -0.098,
            -0.0239,
            -0.0481,
            -0.1843
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0414,
            0.0797,
            -0.1524,
            0.0473,
            0.0705,
            -0.1253,
            0.2539,
            0.0368,
            -0.2077,
            0.0274,
            -0.0486,
            -0.0199,
            0.1185,
            -0.0993,
            -0.184,
            0.1584
          ],
          "after": [
            0.0204,
            0.1139,
            -0.0422,
            -0.0728,
            0.0355,
            -0.019,
            0.0941,
            0.099,
            -0.0802,
            -0.0754,
            0.0027,
            -0.0149,
            0.1093,
            0.0347,
            -0.0583,
            0.2268
          ]
        },
        "position_0": {
          "grad": [
            -0.0308,
            0.0176,
            -0.0197,
            0.1097,
            -0.0837,
            -0.0273,
            0.0965,
            0.0624,
            -0.0414,
            0.0016,
            0.0025,
            0.0062,
            0.0223,
            0.0048,
            0.0005,
            0.0448
          ],
          "after": [
            0.0466,
            -0.1396,
            -0.2928,
            0.0633,
            0.0306,
            -0.0554,
            -0.1456,
            0.0691,
            0.1023,
            0.0117,
            0.0183,
            0.0955,
            0.0463,
            0.009,
            -0.1338,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0006,
            -0.0005,
            0.0015,
            0.0011,
            0.0004,
            0.0026,
            0.0011,
            0.0007,
            0.002,
            -0.0004,
            0.0012,
            0.001,
            -0.001,
            -0.0022,
            0.0003
          ],
          "after": [
            -0.0862,
            0.0099,
            -0.11,
            -0.0751,
            0.0783,
            0.138,
            0.0764,
            -0.16,
            0.0142,
            -0.0169,
            -0.1265,
            -0.0689,
            0.0905,
            -0.0848,
            0.0387,
            0.0789
          ]
        }
      }
    },
    {
      "step": 652,
      "word": "rosealee",
      "loss": 2.5768,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2722,
            0.121,
            -0.0022,
            -0.0543,
            0.0832,
            0.087,
            -0.0346,
            0.092,
            0.052,
            -0.1059,
            -0.1585,
            0.3094,
            -0.1148,
            -0.0672,
            0.0754,
            -0.0225
          ],
          "after": [
            0.0047,
            0.004,
            0.1579,
            -0.0301,
            0.0322,
            -0.0873,
            -0.1118,
            -0.1527,
            -0.0153,
            0.0765,
            0.1604,
            0.0176,
            -0.0977,
            -0.0241,
            -0.0482,
            -0.184
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3043,
            -0.1808,
            -0.0934,
            -0.148,
            -0.607,
            0.633,
            -0.6148,
            0.0643,
            0.3467,
            0.1168,
            -0.2164,
            -0.6373,
            0.3388,
            0.436,
            0.7181,
            -0.597
          ],
          "after": [
            0.0197,
            0.114,
            -0.0418,
            -0.0726,
            0.0361,
            -0.0194,
            0.0945,
            0.0988,
            -0.0803,
            -0.0755,
            0.003,
            -0.014,
            0.1088,
            0.0339,
            -0.0588,
            0.2272
          ]
        },
        "position_0": {
          "grad": [
            0.0048,
            -0.0193,
            -0.0062,
            -0.0439,
            0.0043,
            0.0956,
            0.0511,
            -0.0463,
            0.086,
            -0.027,
            -0.0705,
            0.0316,
            -0.1341,
            -0.0774,
            0.0615,
            0.072
          ],
          "after": [
            0.0468,
            -0.1398,
            -0.2925,
            0.0633,
            0.0307,
            -0.0557,
            -0.1459,
            0.0691,
            0.1021,
            0.012,
            0.0186,
            0.0952,
            0.0466,
            0.0093,
            -0.1341,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0004,
            0.0023,
            -0.0002,
            0.0014,
            -0.0004,
            -0.0013,
            -0.0002,
            -0.0005,
            -0.0005,
            -0.0004,
            -0.0007,
            0.0005,
            0.0001,
            0.0002,
            -0.0019
          ],
          "after": [
            -0.0866,
            0.0096,
            -0.11,
            -0.0751,
            0.0784,
            0.1382,
            0.0764,
            -0.1599,
            0.0145,
            -0.017,
            -0.1265,
            -0.0685,
            0.0904,
            -0.0852,
            0.0386,
            0.0794
          ]
        }
      }
    },
    {
      "step": 653,
      "word": "anuhea",
      "loss": 2.3536,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1114,
            0.2103,
            -0.3353,
            0.0146,
            -0.6017,
            -0.059,
            -0.0643,
            -0.133,
            -0.0607,
            -0.2202,
            -0.172,
            -0.2406,
            0.1065,
            0.275,
            0.3055,
            0.0459
          ],
          "after": [
            0.0047,
            0.0038,
            0.1583,
            -0.0299,
            0.0326,
            -0.0875,
            -0.1114,
            -0.1528,
            -0.0154,
            0.0769,
            0.161,
            0.0174,
            -0.0976,
            -0.0246,
            -0.0488,
            -0.1837
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1908,
            -0.2845,
            0.1598,
            -0.0793,
            -0.0827,
            0.116,
            -0.5166,
            -0.0598,
            0.0821,
            -0.0205,
            0.0856,
            -0.1329,
            0.0175,
            0.2363,
            0.1245,
            -0.2554
          ],
          "after": [
            0.019,
            0.1144,
            -0.0416,
            -0.0723,
            0.0367,
            -0.0199,
            0.0951,
            0.0988,
            -0.0804,
            -0.0755,
            0.0031,
            -0.0131,
            0.1083,
            0.033,
            -0.0593,
            0.2277
          ]
        },
        "position_0": {
          "grad": [
            0.0838,
            -0.0525,
            0.0739,
            -0.1548,
            -0.0044,
            -0.0083,
            -0.3118,
            -0.1022,
            0.1644,
            0.0154,
            0.1861,
            -0.0033,
            -0.1676,
            0.0588,
            -0.04,
            -0.2194
          ],
          "after": [
            0.0467,
            -0.1399,
            -0.2924,
            0.0635,
            0.0308,
            -0.0558,
            -0.1459,
            0.0694,
            0.1018,
            0.0122,
            0.0186,
            0.095,
            0.047,
            0.0095,
            -0.1343,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0004,
            0.0014,
            0.0017,
            -0.0002,
            0.0007,
            -0.0012,
            -0.0007,
            -0.002,
            -0.0007,
            0.0013,
            -0.0011,
            -0.0006,
            -0.0007,
            -0.0008,
            -0.0007
          ],
          "after": [
            -0.0869,
            0.0094,
            -0.1101,
            -0.0752,
            0.0784,
            0.1382,
            0.0765,
            -0.1597,
            0.0151,
            -0.0171,
            -0.1266,
            -0.0681,
            0.0903,
            -0.0855,
            0.0386,
            0.0799
          ]
        }
      }
    },
    {
      "step": 654,
      "word": "leora",
      "loss": 2.1684,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3436,
            -0.0566,
            -0.1602,
            -0.0772,
            -0.054,
            -0.0745,
            -0.0109,
            0.042,
            -0.1459,
            0.2053,
            0.0928,
            -0.2853,
            0.1634,
            0.1243,
            -0.0098,
            -0.2449
          ],
          "after": [
            0.0045,
            0.0036,
            0.1588,
            -0.0296,
            0.0331,
            -0.0875,
            -0.111,
            -0.1529,
            -0.0153,
            0.077,
            0.1613,
            0.0176,
            -0.0977,
            -0.0252,
            -0.0493,
            -0.1833
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1487,
            -0.3799,
            0.3346,
            -0.0915,
            0.3193,
            0.2073,
            -0.5244,
            0.0177,
            0.4731,
            0.22,
            0.0772,
            0.1894,
            -0.293,
            0.0835,
            -0.0228,
            -0.3759
          ],
          "after": [
            0.0182,
            0.115,
            -0.0417,
            -0.0719,
            0.037,
            -0.0205,
            0.0958,
            0.0988,
            -0.0808,
            -0.0758,
            0.0031,
            -0.0126,
            0.1081,
            0.0322,
            -0.0597,
            0.2284
          ]
        },
        "position_0": {
          "grad": [
            -0.1081,
            0.1135,
            -0.1345,
            -0.1002,
            0.0375,
            0.0696,
            -0.0777,
            0.0976,
            -0.0324,
            -0.0237,
            -0.1859,
            0.2076,
            -0.1076,
            0.0109,
            0.0809,
            -0.1262
          ],
          "after": [
            0.0469,
            -0.1401,
            -0.2921,
            0.0638,
            0.0308,
            -0.0561,
            -0.1458,
            0.0694,
            0.1015,
            0.0124,
            0.0189,
            0.0944,
            0.0475,
            0.0096,
            -0.1346,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0012,
            -0.0012,
            -0.0008,
            -0.0001,
            0.0022,
            -0.0024,
            0.0008,
            0.0012,
            -0.0003,
            -0.0018,
            0.0017,
            -0.0024,
            0.0013,
            0.0028,
            0.0002
          ],
          "after": [
            -0.0872,
            0.0093,
            -0.1101,
            -0.0753,
            0.0784,
            0.138,
            0.0768,
            -0.1596,
            0.0154,
            -0.0172,
            -0.1266,
            -0.0679,
            0.0904,
            -0.0859,
            0.0385,
            0.0803
          ]
        }
      }
    },
    {
      "step": 655,
      "word": "vaughn",
      "loss": 3.1509,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0993,
            -0.0632,
            -0.0991,
            0.2081,
            0.1357,
            -0.0301,
            -0.2706,
            0.0308,
            -0.0309,
            0.092,
            0.1242,
            -0.0229,
            -0.0108,
            -0.1048,
            -0.1109,
            0.0632
          ],
          "after": [
            0.0041,
            0.0036,
            0.1593,
            -0.0297,
            0.0333,
            -0.0875,
            -0.1103,
            -0.1531,
            -0.0152,
            0.0769,
            0.1615,
            0.0177,
            -0.0977,
            -0.0255,
            -0.0495,
            -0.183
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0919,
            0.1429,
            -0.1587,
            0.0367,
            0.073,
            -0.2049,
            0.3255,
            0.0679,
            -0.2573,
            0.0424,
            -0.0581,
            0.0626,
            0.127,
            -0.1353,
            -0.1965,
            0.1986
          ],
          "after": [
            0.0176,
            0.1153,
            -0.0417,
            -0.0716,
            0.0372,
            -0.0208,
            0.0963,
            0.0986,
            -0.0809,
            -0.0761,
            0.0032,
            -0.0121,
            0.1078,
            0.0316,
            -0.0599,
            0.2288
          ]
        },
        "position_0": {
          "grad": [
            -0.0783,
            0.0505,
            -0.0008,
            0.0998,
            0.071,
            0.076,
            0.0196,
            0.008,
            -0.0703,
            -0.052,
            -0.0093,
            -0.0621,
            0.0498,
            -0.0215,
            0.0059,
            0.1775
          ],
          "after": [
            0.0473,
            -0.1404,
            -0.2918,
            0.064,
            0.0307,
            -0.0565,
            -0.1457,
            0.0694,
            0.1014,
            0.0127,
            0.0191,
            0.0941,
            0.0479,
            0.0097,
            -0.1349,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            0.0009,
            -0.0015,
            0.0018,
            0.0018,
            0.0024,
            0.0023,
            0.0022,
            -0.0012,
            -0.0014,
            -0.002,
            0.0025,
            -0.0007,
            -0.0022,
            -0.0009,
            0.0033
          ],
          "after": [
            -0.0872,
            0.0091,
            -0.1099,
            -0.0755,
            0.0783,
            0.1376,
            0.0769,
            -0.1598,
            0.0158,
            -0.0171,
            -0.1263,
            -0.0679,
            0.0905,
            -0.086,
            0.0384,
            0.0803
          ]
        }
      }
    },
    {
      "step": 656,
      "word": "branston",
      "loss": 2.4025,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0594,
            0.1206,
            -0.1334,
            0.0613,
            -0.1737,
            -0.0063,
            -0.0013,
            -0.0692,
            -0.0003,
            -0.0076,
            -0.079,
            -0.0046,
            0.033,
            0.0773,
            0.0013,
            0.0671
          ],
          "after": [
            0.0038,
            0.0034,
            0.1599,
            -0.0298,
            0.0337,
            -0.0874,
            -0.1098,
            -0.1531,
            -0.0151,
            0.0768,
            0.1617,
            0.0179,
            -0.0979,
            -0.026,
            -0.0497,
            -0.1829
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1187,
            0.1352,
            -0.0976,
            0.0352,
            0.0442,
            -0.2299,
            0.2987,
            0.0407,
            -0.2142,
            -0.0043,
            -0.0399,
            0.0414,
            0.1287,
            -0.0956,
            -0.1789,
            0.1501
          ],
          "after": [
            0.0172,
            0.1155,
            -0.0416,
            -0.0714,
            0.0374,
            -0.021,
            0.0966,
            0.0983,
            -0.0809,
            -0.0764,
            0.0034,
            -0.0118,
            0.1075,
            0.0312,
            -0.06,
            0.2291
          ]
        },
        "position_0": {
          "grad": [
            0.0348,
            0.0104,
            0.0631,
            0.0428,
            0.0083,
            0.0065,
            0.1219,
            0.0175,
            -0.1061,
            -0.0363,
            -0.0065,
            0.0361,
            0.1107,
            -0.1125,
            -0.0034,
            0.216
          ],
          "after": [
            0.0476,
            -0.1407,
            -0.2917,
            0.064,
            0.0306,
            -0.0569,
            -0.1458,
            0.0693,
            0.1014,
            0.0131,
            0.0193,
            0.0937,
            0.0481,
            0.01,
            -0.1352,
            0.0515
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0002,
            -0.001,
            0.001,
            0.0004,
            -0.0007,
            0.0039,
            0.0025,
            -0.0021,
            0.0016,
            -0.0005,
            0.0033,
            0.0006,
            -0.0022,
            0.0011,
            0.0026
          ],
          "after": [
            -0.0871,
            0.009,
            -0.1097,
            -0.0758,
            0.0782,
            0.1374,
            0.0767,
            -0.1604,
            0.0164,
            -0.0172,
            -0.1261,
            -0.0683,
            0.0906,
            -0.0858,
            0.0382,
            0.0801
          ]
        }
      }
    },
    {
      "step": 657,
      "word": "shyne",
      "loss": 2.3863,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0035,
            0.0032,
            0.1604,
            -0.0299,
            0.034,
            -0.0874,
            -0.1093,
            -0.1531,
            -0.015,
            0.0767,
            0.1619,
            0.018,
            -0.0979,
            -0.0263,
            -0.0499,
            -0.1827
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1953,
            -0.1178,
            -0.0458,
            -0.0139,
            -0.1807,
            0.0532,
            -0.3812,
            -0.1224,
            -0.0801,
            -0.2621,
            -0.0441,
            -0.2282,
            0.0308,
            0.2926,
            0.0445,
            -0.009
          ],
          "after": [
            0.0167,
            0.1158,
            -0.0414,
            -0.0712,
            0.0376,
            -0.0211,
            0.097,
            0.0984,
            -0.0809,
            -0.0763,
            0.0035,
            -0.0113,
            0.1072,
            0.0306,
            -0.0601,
            0.2293
          ]
        },
        "position_0": {
          "grad": [
            0.0114,
            0.074,
            -0.0072,
            0.1668,
            -0.2331,
            0.0602,
            -0.012,
            0.085,
            -0.0385,
            -0.0425,
            -0.1553,
            0.0134,
            0.0896,
            0.0845,
            0.2097,
            0.0332
          ],
          "after": [
            0.0478,
            -0.1411,
            -0.2916,
            0.0638,
            0.0309,
            -0.0573,
            -0.1458,
            0.0691,
            0.1015,
            0.0134,
            0.0197,
            0.0934,
            0.0482,
            0.0101,
            -0.1357,
            0.0512
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0,
            -0.0018,
            -0.0003,
            -0.0027,
            0.002,
            -0.0009,
            -0.001,
            0.0003,
            0.0005,
            -0.0002,
            0.0008,
            -0.0018,
            0.0015,
            0.0037,
            -0.0005
          ],
          "after": [
            -0.087,
            0.0089,
            -0.1093,
            -0.076,
            0.0783,
            0.137,
            0.0766,
            -0.1607,
            0.0168,
            -0.0173,
            -0.1258,
            -0.0686,
            0.0908,
            -0.0859,
            0.0379,
            0.0799
          ]
        }
      }
    },
    {
      "step": 658,
      "word": "eoin",
      "loss": 2.7905,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0033,
            0.0031,
            0.1608,
            -0.03,
            0.0343,
            -0.0874,
            -0.1089,
            -0.1531,
            -0.0149,
            0.0767,
            0.162,
            0.0181,
            -0.098,
            -0.0266,
            -0.0501,
            -0.1826
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.349,
            0.4267,
            0.7387,
            -0.0347,
            0.1263,
            0.0838,
            0.3203,
            -0.0442,
            0.1818,
            -0.2932,
            0.037,
            0.0048,
            -0.2241,
            0.0535,
            0.0696,
            -0.1865
          ],
          "after": [
            0.0166,
            0.1157,
            -0.0418,
            -0.071,
            0.0378,
            -0.0213,
            0.0972,
            0.0985,
            -0.081,
            -0.0759,
            0.0036,
            -0.0109,
            0.1071,
            0.03,
            -0.0602,
            0.2297
          ]
        },
        "position_0": {
          "grad": [
            0.0037,
            -0.2934,
            0.0831,
            -0.1084,
            -0.089,
            0.0944,
            -0.4418,
            -0.1105,
            0.2673,
            0.1512,
            0.024,
            -0.1545,
            -0.2375,
            0.0702,
            0.0336,
            -0.4646
          ],
          "after": [
            0.0479,
            -0.1409,
            -0.2917,
            0.0638,
            0.0313,
            -0.0578,
            -0.1455,
            0.0691,
            0.1012,
            0.0134,
            0.02,
            0.0934,
            0.0485,
            0.0101,
            -0.1362,
            0.0514
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0018,
            0.0008,
            -0.0025,
            -0.0004,
            0.0015,
            0.0003,
            0.0026,
            0.0011,
            0.0015,
            0.0019,
            -0.0013,
            0.0044,
            -0.0019,
            -0.002,
            0.0023,
            0.0013
          ],
          "after": [
            -0.0867,
            0.0087,
            -0.1087,
            -0.0761,
            0.0783,
            0.1366,
            0.0764,
            -0.1611,
            0.017,
            -0.0175,
            -0.1254,
            -0.0693,
            0.0911,
            -0.0857,
            0.0374,
            0.0796
          ]
        }
      }
    },
    {
      "step": 659,
      "word": "odell",
      "loss": 2.66,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.003,
            0.003,
            0.1612,
            -0.03,
            0.0345,
            -0.0874,
            -0.1086,
            -0.1531,
            -0.0148,
            0.0766,
            0.1622,
            0.0182,
            -0.0981,
            -0.0269,
            -0.0502,
            -0.1824
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1252,
            -0.0056,
            0.1093,
            0.0415,
            0.0052,
            0.158,
            -0.0329,
            -0.071,
            0.1476,
            0.0897,
            0.1655,
            0.2459,
            -0.2364,
            -0.1717,
            0.1158,
            0.0961
          ],
          "after": [
            0.0166,
            0.1156,
            -0.0423,
            -0.0709,
            0.0379,
            -0.0215,
            0.0973,
            0.0988,
            -0.0811,
            -0.0756,
            0.0035,
            -0.0108,
            0.1072,
            0.0296,
            -0.0604,
            0.2299
          ]
        },
        "position_0": {
          "grad": [
            0.1058,
            -0.1105,
            0.111,
            -0.0816,
            0.0807,
            -0.0119,
            -0.2606,
            -0.0428,
            0.0387,
            0.1507,
            0.0914,
            0.0315,
            -0.0933,
            0.0028,
            -0.0551,
            -0.2321
          ],
          "after": [
            0.0478,
            -0.1406,
            -0.292,
            0.0639,
            0.0314,
            -0.0583,
            -0.145,
            0.0693,
            0.1009,
            0.0131,
            0.0201,
            0.0933,
            0.0488,
            0.01,
            -0.1365,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.002,
            0.0,
            -0.001,
            -0.0022,
            0.0014,
            -0.0023,
            -0.0007,
            0.0018,
            0.0054,
            0.0032,
            0.0,
            -0.0014,
            -0.0,
            -0.0004,
            -0.0003
          ],
          "after": [
            -0.0864,
            0.0088,
            -0.1083,
            -0.0761,
            0.0784,
            0.1362,
            0.0763,
            -0.1614,
            0.017,
            -0.0181,
            -0.1255,
            -0.0699,
            0.0915,
            -0.0856,
            0.0371,
            0.0794
          ]
        }
      }
    },
    {
      "step": 660,
      "word": "elisandra",
      "loss": 2.5042,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0333,
            0.1365,
            -0.0253,
            -0.0036,
            -0.0153,
            -0.0451,
            0.0383,
            -0.0766,
            -0.0063,
            -0.0913,
            -0.0298,
            0.0485,
            -0.036,
            -0.0489,
            -0.0331,
            0.0677
          ],
          "after": [
            0.0029,
            0.0027,
            0.1615,
            -0.0301,
            0.0347,
            -0.0873,
            -0.1083,
            -0.153,
            -0.0148,
            0.0767,
            0.1623,
            0.0182,
            -0.0981,
            -0.027,
            -0.0502,
            -0.1824
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.29,
            0.3679,
            0.3232,
            0.0255,
            0.1543,
            -0.1472,
            0.4154,
            -0.0299,
            -0.0666,
            -0.2334,
            -0.0068,
            0.0865,
            -0.0904,
            -0.085,
            -0.1103,
            0.1036
          ],
          "after": [
            0.0169,
            0.1153,
            -0.0428,
            -0.0708,
            0.0379,
            -0.0217,
            0.0973,
            0.0992,
            -0.0812,
            -0.0751,
            0.0033,
            -0.0107,
            0.1074,
            0.0295,
            -0.0605,
            0.23
          ]
        },
        "position_0": {
          "grad": [
            -0.0072,
            -0.1082,
            0.0264,
            -0.0101,
            0.0102,
            0.0541,
            -0.1354,
            -0.0527,
            0.0778,
            0.1231,
            0.026,
            0.0078,
            -0.0979,
            -0.0321,
            -0.0234,
            -0.2024
          ],
          "after": [
            0.0477,
            -0.1402,
            -0.2923,
            0.064,
            0.0316,
            -0.0588,
            -0.1444,
            0.0695,
            0.1006,
            0.0126,
            0.0202,
            0.0932,
            0.0492,
            0.01,
            -0.1368,
            0.0523
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0026,
            -0.0016,
            -0.0004,
            -0.0003,
            -0.0013,
            0.0018,
            -0.0038,
            -0.0003,
            -0.0006,
            0.0025,
            0.0003,
            -0.0051,
            0.001,
            0.0035,
            -0.0003,
            -0.0026
          ],
          "after": [
            -0.0864,
            0.009,
            -0.1078,
            -0.0761,
            0.0787,
            0.1356,
            0.0765,
            -0.1615,
            0.017,
            -0.0188,
            -0.1255,
            -0.07,
            0.0918,
            -0.0858,
            0.0368,
            0.0795
          ]
        }
      }
    },
    {
      "step": 661,
      "word": "renner",
      "loss": 2.1774,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0028,
            0.0025,
            0.1618,
            -0.0301,
            0.0349,
            -0.0872,
            -0.1081,
            -0.1529,
            -0.0147,
            0.0768,
            0.1625,
            0.0182,
            -0.0981,
            -0.0271,
            -0.0503,
            -0.1824
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.252,
            -0.3742,
            0.4415,
            -0.0389,
            0.227,
            0.3183,
            -0.9659,
            -0.1225,
            0.51,
            -0.0596,
            0.1151,
            0.0679,
            -0.3712,
            0.3448,
            0.0865,
            -0.2779
          ],
          "after": [
            0.017,
            0.1152,
            -0.0436,
            -0.0707,
            0.0377,
            -0.022,
            0.0977,
            0.0997,
            -0.0816,
            -0.0746,
            0.003,
            -0.0108,
            0.1079,
            0.029,
            -0.0606,
            0.2303
          ]
        },
        "position_0": {
          "grad": [
            0.0118,
            -0.0542,
            -0.0076,
            -0.0339,
            -0.0355,
            0.1225,
            0.0686,
            -0.0277,
            0.1377,
            -0.0794,
            -0.1184,
            0.0238,
            -0.1713,
            -0.0838,
            0.1284,
            0.0899
          ],
          "after": [
            0.0476,
            -0.1398,
            -0.2926,
            0.0642,
            0.0318,
            -0.0594,
            -0.144,
            0.0698,
            0.1001,
            0.0124,
            0.0204,
            0.0931,
            0.0498,
            0.0103,
            -0.1372,
            0.0527
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0015,
            0.0002,
            -0.0009,
            0.0016,
            -0.0007,
            -0.002,
            0.0003,
            0.0003,
            0.0008,
            0.0007,
            -0.0011,
            -0.0002,
            0.0008,
            -0.0009,
            -0.0007
          ],
          "after": [
            -0.0865,
            0.0093,
            -0.1075,
            -0.076,
            0.0787,
            0.1352,
            0.0767,
            -0.1617,
            0.017,
            -0.0195,
            -0.1256,
            -0.0699,
            0.092,
            -0.0861,
            0.0366,
            0.0797
          ]
        }
      }
    },
    {
      "step": 662,
      "word": "kourtni",
      "loss": 2.8398,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0027,
            0.0023,
            0.162,
            -0.0302,
            0.0351,
            -0.0871,
            -0.1079,
            -0.1528,
            -0.0147,
            0.0769,
            0.1626,
            0.0182,
            -0.0981,
            -0.0273,
            -0.0503,
            -0.1824
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1419,
            0.2004,
            -0.127,
            0.0394,
            0.0713,
            -0.2332,
            0.4079,
            0.0583,
            -0.2622,
            -0.012,
            -0.0506,
            -0.0119,
            0.1365,
            -0.1237,
            -0.1858,
            0.212
          ],
          "after": [
            0.0171,
            0.1151,
            -0.0442,
            -0.0707,
            0.0375,
            -0.0221,
            0.0978,
            0.1001,
            -0.0818,
            -0.0742,
            0.0029,
            -0.0108,
            0.1081,
            0.0287,
            -0.0606,
            0.2304
          ]
        },
        "position_0": {
          "grad": [
            -0.074,
            0.0441,
            -0.0839,
            0.0268,
            0.0557,
            -0.0541,
            0.154,
            -0.0122,
            -0.055,
            -0.0135,
            0.0159,
            0.0063,
            0.0622,
            -0.0254,
            -0.0827,
            0.0076
          ],
          "after": [
            0.0476,
            -0.1395,
            -0.2926,
            0.0643,
            0.0318,
            -0.0599,
            -0.1438,
            0.07,
            0.0997,
            0.0122,
            0.0205,
            0.093,
            0.0502,
            0.0105,
            -0.1374,
            0.0529
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0055,
            -0.0016,
            0.0033,
            0.0021,
            0.0005,
            0.0056,
            -0.0024,
            -0.001,
            -0.0003,
            0.0001,
            -0.0002,
            0.0014,
            -0.0025,
            -0.0013,
            0.0009
          ],
          "after": [
            -0.0865,
            0.009,
            -0.107,
            -0.0763,
            0.0786,
            0.1348,
            0.0766,
            -0.1616,
            0.0172,
            -0.02,
            -0.1258,
            -0.0698,
            0.0921,
            -0.086,
            0.0365,
            0.0797
          ]
        }
      }
    },
    {
      "step": 663,
      "word": "tavarious",
      "loss": 2.6187,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1472,
            0.0203,
            0.0775,
            0.0635,
            0.3782,
            0.1576,
            0.1207,
            -0.1313,
            -0.0453,
            0.0541,
            0.0546,
            0.2111,
            -0.0472,
            -0.2778,
            -0.1631,
            0.2625
          ],
          "after": [
            0.0027,
            0.0021,
            0.1622,
            -0.0303,
            0.0348,
            -0.0873,
            -0.1079,
            -0.1525,
            -0.0145,
            0.0769,
            0.1626,
            0.018,
            -0.0981,
            -0.027,
            -0.0501,
            -0.1826
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0888,
            0.1168,
            -0.0572,
            0.0542,
            0.1068,
            -0.1873,
            0.2526,
            0.0136,
            -0.186,
            0.0021,
            -0.0174,
            0.0454,
            0.0746,
            -0.0968,
            -0.2276,
            0.1303
          ],
          "after": [
            0.0173,
            0.1148,
            -0.0447,
            -0.0708,
            0.0373,
            -0.0221,
            0.0979,
            0.1003,
            -0.0818,
            -0.0738,
            0.0028,
            -0.0108,
            0.1083,
            0.0285,
            -0.0605,
            0.2304
          ]
        },
        "position_0": {
          "grad": [
            -0.0101,
            -0.0718,
            0.034,
            0.0403,
            -0.0238,
            0.0195,
            0.067,
            -0.0094,
            -0.1069,
            -0.0394,
            -0.0359,
            -0.1317,
            0.1039,
            -0.0032,
            0.0173,
            0.1028
          ],
          "after": [
            0.0477,
            -0.1391,
            -0.2927,
            0.0643,
            0.0319,
            -0.0603,
            -0.1437,
            0.0703,
            0.0996,
            0.0121,
            0.0207,
            0.0932,
            0.0504,
            0.0107,
            -0.1376,
            0.0531
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0003,
            -0.0002,
            0.0004,
            -0.0002,
            -0.0003,
            -0.0005,
            0.0003,
            0.0005,
            -0.0001,
            0.0002,
            0.0005,
            -0.0002,
            -0.0005,
            0.0007,
            0.0007
          ],
          "after": [
            -0.0865,
            0.0088,
            -0.1066,
            -0.0765,
            0.0786,
            0.1345,
            0.0766,
            -0.1615,
            0.0172,
            -0.0204,
            -0.1259,
            -0.0698,
            0.0922,
            -0.086,
            0.0364,
            0.0797
          ]
        }
      }
    },
    {
      "step": 664,
      "word": "paarth",
      "loss": 2.8368,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1262,
            -0.0352,
            0.111,
            -0.0062,
            -0.047,
            0.3236,
            -0.1582,
            -0.1106,
            0.227,
            -0.0634,
            0.3319,
            0.1085,
            -0.4153,
            -0.0854,
            0.0994,
            0.0176
          ],
          "after": [
            0.0026,
            0.002,
            0.1622,
            -0.0304,
            0.0347,
            -0.088,
            -0.1077,
            -0.152,
            -0.0148,
            0.0769,
            0.1622,
            0.0178,
            -0.0975,
            -0.0266,
            -0.0501,
            -0.1828
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0797,
            0.1661,
            -0.1329,
            0.0526,
            0.0351,
            -0.2173,
            0.3255,
            0.0232,
            -0.2124,
            -0.0175,
            -0.0143,
            0.0309,
            0.0831,
            -0.1406,
            -0.1842,
            0.1263
          ],
          "after": [
            0.0176,
            0.1145,
            -0.045,
            -0.0709,
            0.037,
            -0.0219,
            0.0977,
            0.1005,
            -0.0817,
            -0.0735,
            0.0027,
            -0.0109,
            0.1084,
            0.0285,
            -0.0602,
            0.2303
          ]
        },
        "position_0": {
          "grad": [
            0.0981,
            0.1378,
            -0.044,
            0.0564,
            -0.0243,
            -0.1649,
            0.286,
            0.0137,
            -0.2922,
            -0.009,
            -0.005,
            -0.0637,
            0.0601,
            0.0933,
            -0.0076,
            0.2002
          ],
          "after": [
            0.0476,
            -0.139,
            -0.2927,
            0.0642,
            0.032,
            -0.0604,
            -0.1438,
            0.0705,
            0.0999,
            0.012,
            0.0209,
            0.0933,
            0.0505,
            0.0107,
            -0.1378,
            0.053
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            0.0003,
            0.0006,
            -0.0002,
            0.0001,
            -0.0009,
            -0.0001,
            -0.0012,
            -0.0001,
            0.0016,
            0.0012,
            -0.0004,
            0.0002,
            0.0003,
            -0.0013,
            -0.0011
          ],
          "after": [
            -0.0866,
            0.0085,
            -0.1063,
            -0.0767,
            0.0785,
            0.1344,
            0.0765,
            -0.1613,
            0.0173,
            -0.0209,
            -0.1261,
            -0.0698,
            0.0922,
            -0.086,
            0.0364,
            0.0797
          ]
        }
      }
    },
    {
      "step": 665,
      "word": "aabir",
      "loss": 2.5884,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1845,
            0.0617,
            0.5896,
            -0.3391,
            0.2766,
            -0.0268,
            -0.4321,
            0.0395,
            0.2833,
            -0.1342,
            0.5351,
            0.077,
            -0.2138,
            -0.0812,
            -0.3367,
            -0.0521
          ],
          "after": [
            0.0024,
            0.0018,
            0.1616,
            -0.03,
            0.0343,
            -0.0885,
            -0.1071,
            -0.1517,
            -0.0154,
            0.0772,
            0.1612,
            0.0175,
            -0.0969,
            -0.0262,
            -0.0496,
            -0.1829
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0268,
            0.0851,
            -0.0346,
            0.0438,
            0.0885,
            -0.1665,
            0.1851,
            0.0094,
            -0.1798,
            0.0632,
            -0.0037,
            0.023,
            0.0757,
            -0.0568,
            -0.2387,
            0.0814
          ],
          "after": [
            0.0178,
            0.1142,
            -0.0452,
            -0.0711,
            0.0368,
            -0.0216,
            0.0976,
            0.1006,
            -0.0815,
            -0.0733,
            0.0026,
            -0.0109,
            0.1084,
            0.0286,
            -0.0599,
            0.2302
          ]
        },
        "position_0": {
          "grad": [
            0.1073,
            -0.0126,
            0.0716,
            -0.2193,
            0.0758,
            -0.024,
            -0.3175,
            -0.149,
            0.1216,
            0.0952,
            0.2478,
            0.0287,
            -0.1431,
            0.0125,
            -0.1545,
            -0.2372
          ],
          "after": [
            0.0472,
            -0.1389,
            -0.2929,
            0.0644,
            0.032,
            -0.0603,
            -0.1436,
            0.0709,
            0.0999,
            0.0118,
            0.0207,
            0.0935,
            0.0508,
            0.0106,
            -0.1377,
            0.0532
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            -0.0001,
            -0.0003,
            0.0023,
            -0.0014,
            0.002,
            0.0006,
            0.0012,
            -0.0006,
            -0.0017,
            -0.0006,
            0.0016,
            -0.0012,
            -0.0023,
            0.0015,
            0.001
          ],
          "after": [
            -0.0866,
            0.0083,
            -0.106,
            -0.077,
            0.0786,
            0.134,
            0.0764,
            -0.1612,
            0.0174,
            -0.0212,
            -0.1262,
            -0.0699,
            0.0924,
            -0.0857,
            0.0363,
            0.0797
          ]
        }
      }
    },
    {
      "step": 666,
      "word": "alexianna",
      "loss": 2.2824,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2114,
            0.2749,
            -0.0951,
            -0.0258,
            0.0026,
            -0.0502,
            -0.023,
            -0.0225,
            -0.0345,
            -0.1804,
            -0.2168,
            0.3012,
            -0.1678,
            -0.0358,
            0.0661,
            -0.1
          ],
          "after": [
            0.0024,
            0.0013,
            0.1612,
            -0.0297,
            0.034,
            -0.0889,
            -0.1065,
            -0.1513,
            -0.0159,
            0.0776,
            0.1606,
            0.0169,
            -0.0961,
            -0.0258,
            -0.0492,
            -0.1829
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0579,
            -0.1866,
            0.1485,
            0.0229,
            0.2358,
            0.1347,
            -0.1686,
            0.0521,
            0.1304,
            0.1442,
            0.0162,
            0.1994,
            -0.1996,
            -0.0574,
            -0.0191,
            -0.129
          ],
          "after": [
            0.018,
            0.114,
            -0.0455,
            -0.0713,
            0.0364,
            -0.0214,
            0.0975,
            0.1006,
            -0.0814,
            -0.0733,
            0.0026,
            -0.0112,
            0.1085,
            0.0287,
            -0.0595,
            0.2301
          ]
        },
        "position_0": {
          "grad": [
            0.0509,
            0.0043,
            0.0397,
            -0.093,
            0.0612,
            -0.0031,
            -0.1565,
            -0.0852,
            0.0589,
            0.06,
            0.1615,
            0.0546,
            -0.0829,
            -0.0058,
            -0.0954,
            -0.1223
          ],
          "after": [
            0.0467,
            -0.1388,
            -0.2931,
            0.0648,
            0.0319,
            -0.0603,
            -0.1434,
            0.0715,
            0.0999,
            0.0115,
            0.0204,
            0.0935,
            0.0511,
            0.0106,
            -0.1375,
            0.0535
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.004,
            0.0001,
            -0.0021,
            0.001,
            -0.0034,
            0.0,
            -0.0037,
            -0.0003,
            -0.0014,
            0.0002,
            0.0001,
            -0.0038,
            -0.0002,
            0.0028,
            0.0019,
            -0.0025
          ],
          "after": [
            -0.087,
            0.0081,
            -0.1056,
            -0.0774,
            0.0789,
            0.1337,
            0.0766,
            -0.1612,
            0.0176,
            -0.0215,
            -0.1263,
            -0.0697,
            0.0925,
            -0.0858,
            0.0361,
            0.0799
          ]
        }
      }
    },
    {
      "step": 667,
      "word": "cruzito",
      "loss": 3.007,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0024,
            0.0009,
            0.1609,
            -0.0294,
            0.0338,
            -0.0893,
            -0.106,
            -0.1511,
            -0.0164,
            0.078,
            0.1602,
            0.0165,
            -0.0955,
            -0.0254,
            -0.0489,
            -0.183
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0582,
            0.1785,
            -0.2327,
            0.0162,
            0.0344,
            -0.2386,
            0.3649,
            0.0404,
            -0.3109,
            -0.0043,
            -0.0514,
            -0.0884,
            0.2382,
            -0.1055,
            -0.2639,
            0.1996
          ],
          "after": [
            0.0183,
            0.1138,
            -0.0456,
            -0.0715,
            0.0361,
            -0.0211,
            0.0972,
            0.1005,
            -0.0812,
            -0.0733,
            0.0026,
            -0.0113,
            0.1085,
            0.0288,
            -0.0591,
            0.23
          ]
        },
        "position_0": {
          "grad": [
            -0.0367,
            0.0518,
            -0.0149,
            0.1248,
            -0.0372,
            -0.0122,
            0.1646,
            0.0534,
            -0.1057,
            0.0081,
            0.0404,
            0.0485,
            0.0805,
            -0.0134,
            -0.0399,
            0.0997
          ],
          "after": [
            0.0464,
            -0.1388,
            -0.2932,
            0.0649,
            0.0319,
            -0.0602,
            -0.1433,
            0.0719,
            0.1,
            0.0112,
            0.02,
            0.0934,
            0.0513,
            0.0106,
            -0.1372,
            0.0536
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            -0.0021,
            -0.0001,
            -0.0008,
            0.0004,
            0.0009,
            -0.0007,
            0.0004,
            -0.0017,
            -0.0003,
            0.0011,
            -0.0006,
            -0.0007,
            0.0004,
            0.0,
            0.0005
          ],
          "after": [
            -0.0874,
            0.0081,
            -0.1052,
            -0.0777,
            0.0791,
            0.1334,
            0.0768,
            -0.1612,
            0.0181,
            -0.0217,
            -0.1265,
            -0.0694,
            0.0927,
            -0.0859,
            0.0359,
            0.08
          ]
        }
      }
    },
    {
      "step": 668,
      "word": "mher",
      "loss": 2.7589,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0024,
            0.0006,
            0.1606,
            -0.0291,
            0.0335,
            -0.0895,
            -0.1056,
            -0.1508,
            -0.0167,
            0.0784,
            0.1597,
            0.0161,
            -0.0949,
            -0.0251,
            -0.0487,
            -0.183
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.13,
            0.0752,
            0.1383,
            0.222,
            -0.0347,
            0.2437,
            -0.1042,
            -0.0157,
            -0.0144,
            0.1325,
            0.1863,
            0.277,
            -0.1943,
            -0.2021,
            0.2037,
            -0.1088
          ],
          "after": [
            0.0187,
            0.1135,
            -0.0458,
            -0.072,
            0.0358,
            -0.0211,
            0.0971,
            0.1004,
            -0.0809,
            -0.0734,
            0.0023,
            -0.0116,
            0.1086,
            0.0292,
            -0.0589,
            0.2299
          ]
        },
        "position_0": {
          "grad": [
            -0.0025,
            0.0067,
            -0.0435,
            0.0577,
            0.0718,
            -0.0514,
            0.2404,
            0.1318,
            0.0853,
            -0.0297,
            -0.0376,
            -0.019,
            -0.102,
            -0.0003,
            0.0762,
            0.1275
          ],
          "after": [
            0.0462,
            -0.1388,
            -0.2933,
            0.0649,
            0.0317,
            -0.0601,
            -0.1434,
            0.0719,
            0.1,
            0.011,
            0.0198,
            0.0934,
            0.0515,
            0.0106,
            -0.1371,
            0.0536
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.002,
            0.0001,
            0.0003,
            0.0025,
            -0.0006,
            0.0031,
            -0.002,
            0.0004,
            -0.0028,
            0.0,
            0.0007,
            0.001,
            -0.0005,
            -0.0004,
            0.0019,
            -0.0003
          ],
          "after": [
            -0.0876,
            0.0082,
            -0.1049,
            -0.0781,
            0.0794,
            0.1328,
            0.077,
            -0.1612,
            0.0187,
            -0.0219,
            -0.1268,
            -0.0693,
            0.0929,
            -0.0859,
            0.0357,
            0.0802
          ]
        }
      }
    },
    {
      "step": 669,
      "word": "azylah",
      "loss": 2.1495,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1802,
            0.0221,
            0.2393,
            0.3345,
            0.3151,
            -0.0541,
            0.3445,
            0.145,
            0.1924,
            0.2124,
            0.0477,
            0.1362,
            0.2303,
            -0.1247,
            -0.2079,
            0.1973
          ],
          "after": [
            0.0026,
            0.0003,
            0.1601,
            -0.0293,
            0.0331,
            -0.0897,
            -0.1057,
            -0.1509,
            -0.0173,
            0.0783,
            0.1593,
            0.0156,
            -0.0947,
            -0.0247,
            -0.0482,
            -0.1832
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0808,
            0.1395,
            -0.141,
            0.0219,
            0.0345,
            -0.2257,
            0.2814,
            0.0071,
            -0.2164,
            -0.0061,
            -0.0267,
            -0.0041,
            0.1338,
            -0.0975,
            -0.2136,
            0.1817
          ],
          "after": [
            0.019,
            0.1131,
            -0.0458,
            -0.0725,
            0.0356,
            -0.0208,
            0.0968,
            0.1003,
            -0.0806,
            -0.0735,
            0.0021,
            -0.0119,
            0.1086,
            0.0296,
            -0.0585,
            0.2297
          ]
        },
        "position_0": {
          "grad": [
            0.0801,
            0.0039,
            0.0571,
            -0.1218,
            0.0585,
            -0.005,
            -0.2098,
            -0.1071,
            0.0837,
            0.0568,
            0.2144,
            0.0613,
            -0.1015,
            0.0207,
            -0.1032,
            -0.154
          ],
          "after": [
            0.0458,
            -0.1388,
            -0.2934,
            0.0651,
            0.0315,
            -0.06,
            -0.1433,
            0.0722,
            0.0999,
            0.0107,
            0.0193,
            0.0933,
            0.0518,
            0.0105,
            -0.1369,
            0.0537
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0005,
            0.0002,
            0.0002,
            0.0001,
            0.0007,
            -0.0003,
            0.0001,
            0.0002,
            0.0009,
            0.0002,
            0.0003,
            -0.0004,
            -0.0,
            -0.0001,
            0.0002
          ],
          "after": [
            -0.0878,
            0.0082,
            -0.1047,
            -0.0786,
            0.0796,
            0.1323,
            0.0773,
            -0.1612,
            0.0193,
            -0.0221,
            -0.127,
            -0.0692,
            0.0931,
            -0.086,
            0.0354,
            0.0803
          ]
        }
      }
    },
    {
      "step": 670,
      "word": "kaislynn",
      "loss": 2.3533,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0502,
            -0.1731,
            0.0693,
            -0.0541,
            0.1127,
            0.1267,
            -0.1022,
            0.0653,
            -0.0298,
            0.0086,
            0.0877,
            -0.0981,
            -0.11,
            -0.0047,
            -0.0245,
            -0.075
          ],
          "after": [
            0.0028,
            0.0002,
            0.1597,
            -0.0295,
            0.0326,
            -0.09,
            -0.1056,
            -0.1511,
            -0.0178,
            0.0783,
            0.1589,
            0.0153,
            -0.0944,
            -0.0243,
            -0.0477,
            -0.1833
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1506,
            0.1739,
            -0.1158,
            0.0328,
            0.0877,
            -0.2409,
            0.4007,
            0.0537,
            -0.2659,
            -0.0364,
            -0.0394,
            0.0525,
            0.0704,
            -0.1547,
            -0.1651,
            0.2662
          ],
          "after": [
            0.0195,
            0.1127,
            -0.0458,
            -0.073,
            0.0353,
            -0.0205,
            0.0965,
            0.1002,
            -0.0802,
            -0.0736,
            0.002,
            -0.0122,
            0.1085,
            0.0301,
            -0.0581,
            0.2294
          ]
        },
        "position_0": {
          "grad": [
            -0.0581,
            0.0541,
            -0.085,
            0.0176,
            0.0691,
            -0.067,
            0.1591,
            -0.0106,
            -0.0633,
            0.028,
            -0.0052,
            0.0292,
            0.0542,
            -0.0525,
            -0.1032,
            -0.0039
          ],
          "after": [
            0.0456,
            -0.1389,
            -0.2934,
            0.0652,
            0.0313,
            -0.0597,
            -0.1434,
            0.0724,
            0.0999,
            0.0104,
            0.0189,
            0.0932,
            0.0521,
            0.0106,
            -0.1365,
            0.0538
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0008,
            -0.0004,
            0.0016,
            0.0001,
            0.0023,
            0.0018,
            0.0009,
            0.0003,
            -0.0003,
            -0.001,
            0.0005,
            0.0003,
            -0.0005,
            -0.0004,
            0.0026
          ],
          "after": [
            -0.0879,
            0.0082,
            -0.1045,
            -0.079,
            0.0797,
            0.1316,
            0.0774,
            -0.1614,
            0.0197,
            -0.0222,
            -0.1271,
            -0.0692,
            0.0932,
            -0.086,
            0.0353,
            0.0801
          ]
        }
      }
    },
    {
      "step": 671,
      "word": "kirtan",
      "loss": 2.0927,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.088,
            0.1529,
            -0.0283,
            0.0586,
            -0.0205,
            -0.0925,
            0.0195,
            -0.0716,
            0.0286,
            -0.2057,
            -0.0742,
            0.1407,
            -0.1005,
            -0.0461,
            -0.0008,
            0.0928
          ],
          "after": [
            0.003,
            -0.0,
            0.1593,
            -0.0296,
            0.0322,
            -0.0901,
            -0.1055,
            -0.1511,
            -0.0182,
            0.0786,
            0.1586,
            0.0149,
            -0.094,
            -0.0239,
            -0.0473,
            -0.1834
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1099,
            0.1844,
            -0.1549,
            0.0253,
            0.0094,
            -0.211,
            0.382,
            0.0517,
            -0.2576,
            0.0119,
            -0.0526,
            -0.0618,
            0.1583,
            -0.0863,
            -0.1686,
            0.1929
          ],
          "after": [
            0.02,
            0.1122,
            -0.0456,
            -0.0734,
            0.035,
            -0.02,
            0.096,
            0.0999,
            -0.0797,
            -0.0736,
            0.002,
            -0.0124,
            0.1083,
            0.0306,
            -0.0577,
            0.229
          ]
        },
        "position_0": {
          "grad": [
            -0.0711,
            0.0489,
            -0.1062,
            0.0241,
            0.0334,
            -0.0805,
            0.1894,
            0.0033,
            -0.0577,
            -0.0046,
            -0.02,
            0.0018,
            0.0617,
            -0.0363,
            -0.0888,
            -0.0106
          ],
          "after": [
            0.0456,
            -0.1391,
            -0.2931,
            0.0653,
            0.031,
            -0.0593,
            -0.1436,
            0.0726,
            0.0999,
            0.0102,
            0.0186,
            0.0931,
            0.0522,
            0.0107,
            -0.136,
            0.054
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0026,
            0.0003,
            0.0004,
            0.0008,
            -0.0008,
            0.001,
            0.0015,
            -0.001,
            0.0009,
            0.0033,
            0.0005,
            0.002,
            0.0009,
            0.0002,
            -0.0045,
            0.0005
          ],
          "after": [
            -0.0882,
            0.0082,
            -0.1043,
            -0.0795,
            0.0799,
            0.1309,
            0.0774,
            -0.1614,
            0.02,
            -0.0226,
            -0.1272,
            -0.0693,
            0.0933,
            -0.086,
            0.0354,
            0.0799
          ]
        }
      }
    },
    {
      "step": 672,
      "word": "mariel",
      "loss": 1.9128,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0699,
            -0.0751,
            -0.0429,
            -0.1234,
            0.0683,
            0.0223,
            0.0388,
            -0.1496,
            0.1712,
            -0.055,
            0.0132,
            0.0321,
            -0.1455,
            -0.1027,
            0.0237,
            0.0569
          ],
          "after": [
            0.0032,
            -0.0001,
            0.159,
            -0.0296,
            0.0318,
            -0.0903,
            -0.1055,
            -0.1509,
            -0.0188,
            0.0789,
            0.1583,
            0.0145,
            -0.0936,
            -0.0235,
            -0.047,
            -0.1836
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.003,
            0.1454,
            -0.0948,
            0.2243,
            -0.416,
            -0.0956,
            -0.0193,
            0.0251,
            -0.1153,
            -0.2204,
            -0.0916,
            0.0861,
            0.1214,
            0.024,
            0.2053,
            0.1521
          ],
          "after": [
            0.0204,
            0.1117,
            -0.0455,
            -0.0741,
            0.0351,
            -0.0196,
            0.0956,
            0.0996,
            -0.0792,
            -0.0734,
            0.0021,
            -0.0126,
            0.1081,
            0.031,
            -0.0575,
            0.2285
          ]
        },
        "position_0": {
          "grad": [
            -0.0023,
            0.0338,
            -0.0393,
            0.0113,
            0.1166,
            -0.0416,
            0.1888,
            0.0733,
            0.0265,
            0.0439,
            -0.0058,
            0.0227,
            -0.0458,
            -0.0572,
            -0.0372,
            0.0859
          ],
          "after": [
            0.0456,
            -0.1393,
            -0.2928,
            0.0653,
            0.0306,
            -0.0589,
            -0.144,
            0.0727,
            0.0999,
            0.0099,
            0.0183,
            0.0929,
            0.0523,
            0.011,
            -0.1356,
            0.054
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0005,
            0.0003,
            0.0008,
            -0.0024,
            -0.0002,
            -0.0028,
            0.0007,
            0.0001,
            -0.0028,
            -0.0008,
            0.0015,
            -0.0013,
            0.0003,
            0.0037,
            0.001
          ],
          "after": [
            -0.0883,
            0.0082,
            -0.1042,
            -0.08,
            0.0803,
            0.1304,
            0.0775,
            -0.1615,
            0.0202,
            -0.0228,
            -0.1273,
            -0.0696,
            0.0934,
            -0.086,
            0.0353,
            0.0797
          ]
        }
      }
    },
    {
      "step": 673,
      "word": "hanvika",
      "loss": 2.4747,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0925,
            0.1323,
            -0.1892,
            -0.097,
            -0.2069,
            -0.1134,
            -0.006,
            -0.096,
            -0.0425,
            -0.0663,
            0.0002,
            -0.1322,
            0.1096,
            0.1818,
            0.035,
            0.0033
          ],
          "after": [
            0.0032,
            -0.0004,
            0.159,
            -0.0295,
            0.0316,
            -0.0902,
            -0.1055,
            -0.1505,
            -0.0193,
            0.0792,
            0.1581,
            0.0144,
            -0.0933,
            -0.0233,
            -0.0468,
            -0.1838
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.13,
            0.1305,
            -0.1274,
            0.0374,
            0.0735,
            -0.2085,
            0.3764,
            0.0661,
            -0.2661,
            0.0134,
            -0.0793,
            0.0311,
            0.1058,
            -0.1092,
            -0.2022,
            0.2135
          ],
          "after": [
            0.0208,
            0.1111,
            -0.0452,
            -0.0748,
            0.0351,
            -0.019,
            0.0951,
            0.0992,
            -0.0786,
            -0.0733,
            0.0024,
            -0.0128,
            0.1078,
            0.0315,
            -0.0571,
            0.228
          ]
        },
        "position_0": {
          "grad": [
            0.0046,
            -0.0608,
            0.074,
            0.033,
            -0.0445,
            0.0979,
            -0.0492,
            0.0857,
            0.1167,
            0.0837,
            -0.0973,
            -0.1128,
            0.1453,
            0.1054,
            0.0323,
            0.0518
          ],
          "after": [
            0.0456,
            -0.1393,
            -0.2927,
            0.0653,
            0.0303,
            -0.0588,
            -0.1442,
            0.0725,
            0.0998,
            0.0095,
            0.0182,
            0.093,
            0.0523,
            0.011,
            -0.1353,
            0.0539
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            -0.0009,
            -0.0002,
            -0.0004,
            0.0011,
            0.0006,
            -0.0005,
            -0.0011,
            -0.0003,
            0.0007,
            0.001,
            -0.0011,
            0.0001,
            0.0005,
            -0.0015,
            -0.0018
          ],
          "after": [
            -0.0886,
            0.0083,
            -0.1041,
            -0.0804,
            0.0805,
            0.1299,
            0.0777,
            -0.1614,
            0.0204,
            -0.0229,
            -0.1274,
            -0.0697,
            0.0936,
            -0.086,
            0.0353,
            0.0796
          ]
        }
      }
    },
    {
      "step": 674,
      "word": "parys",
      "loss": 2.458,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0983,
            -0.1112,
            -0.0759,
            -0.1211,
            0.0453,
            0.0045,
            0.0624,
            -0.1821,
            0.1915,
            -0.0904,
            0.0362,
            0.01,
            -0.1487,
            -0.0831,
            0.0828,
            0.0804
          ],
          "after": [
            0.0032,
            -0.0005,
            0.159,
            -0.0292,
            0.0314,
            -0.0902,
            -0.1056,
            -0.1499,
            -0.02,
            0.0796,
            0.1579,
            0.0142,
            -0.0929,
            -0.0231,
            -0.0468,
            -0.184
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0169,
            0.0408,
            -0.1628,
            0.0009,
            -0.0261,
            -0.181,
            0.2277,
            0.0261,
            -0.2087,
            0.0377,
            0.0004,
            0.0086,
            0.0687,
            -0.105,
            -0.1321,
            0.079
          ],
          "after": [
            0.0212,
            0.1106,
            -0.0449,
            -0.0753,
            0.0352,
            -0.0185,
            0.0945,
            0.0988,
            -0.0779,
            -0.0732,
            0.0026,
            -0.013,
            0.1075,
            0.0319,
            -0.0568,
            0.2275
          ]
        },
        "position_0": {
          "grad": [
            0.1032,
            0.1582,
            -0.0529,
            0.0774,
            -0.0354,
            -0.1794,
            0.3532,
            0.014,
            -0.3425,
            -0.029,
            0.0135,
            -0.0659,
            0.0717,
            0.112,
            0.0056,
            0.2443
          ],
          "after": [
            0.0453,
            -0.1396,
            -0.2924,
            0.0652,
            0.0301,
            -0.0583,
            -0.1447,
            0.0723,
            0.1001,
            0.0092,
            0.0181,
            0.0931,
            0.0522,
            0.0107,
            -0.135,
            0.0537
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0003,
            0.0005,
            -0.0018,
            0.0017,
            -0.0024,
            -0.0004,
            -0.0008,
            -0.0003,
            -0.0014,
            0.0006,
            0.0003,
            -0.0007,
            -0.0001,
            -0.0006,
            -0.0014
          ],
          "after": [
            -0.0888,
            0.0084,
            -0.104,
            -0.0805,
            0.0806,
            0.1297,
            0.0778,
            -0.1613,
            0.0206,
            -0.0229,
            -0.1276,
            -0.0698,
            0.0937,
            -0.0861,
            0.0354,
            0.0797
          ]
        }
      }
    },
    {
      "step": 675,
      "word": "tor",
      "loss": 2.7818,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0031,
            -0.0005,
            0.159,
            -0.029,
            0.0313,
            -0.0901,
            -0.1056,
            -0.1493,
            -0.0205,
            0.08,
            0.1577,
            0.0141,
            -0.0926,
            -0.0229,
            -0.0467,
            -0.1842
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0045,
            0.1011,
            -0.1897,
            0.0681,
            -0.0979,
            -0.1274,
            0.2228,
            0.0018,
            -0.2044,
            0.0381,
            -0.0142,
            -0.1088,
            0.1984,
            -0.0268,
            -0.1748,
            0.0664
          ],
          "after": [
            0.0216,
            0.1101,
            -0.0445,
            -0.0759,
            0.0353,
            -0.0179,
            0.094,
            0.0984,
            -0.0773,
            -0.0732,
            0.0027,
            -0.0131,
            0.1071,
            0.0324,
            -0.0564,
            0.227
          ]
        },
        "position_0": {
          "grad": [
            -0.0305,
            -0.3301,
            0.1269,
            0.0927,
            -0.2593,
            0.0683,
            -0.0077,
            0.0371,
            -0.0874,
            -0.2638,
            -0.1444,
            -0.484,
            0.1508,
            0.1256,
            0.2586,
            0.1567
          ],
          "after": [
            0.0452,
            -0.1394,
            -0.2925,
            0.065,
            0.0303,
            -0.0581,
            -0.1451,
            0.0721,
            0.1005,
            0.0095,
            0.0182,
            0.0939,
            0.052,
            0.0102,
            -0.1352,
            0.0533
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0029,
            0.0012,
            -0.0032,
            0.0019,
            -0.0011,
            -0.0034,
            0.0011,
            0.0002,
            -0.0013,
            -0.0001,
            0.0005,
            -0.002,
            -0.0,
            0.0017,
            -0.0006
          ],
          "after": [
            -0.0889,
            0.0088,
            -0.1041,
            -0.0804,
            0.0805,
            0.1296,
            0.0782,
            -0.1613,
            0.0207,
            -0.0228,
            -0.1277,
            -0.07,
            0.094,
            -0.0861,
            0.0353,
            0.0799
          ]
        }
      }
    },
    {
      "step": 676,
      "word": "nilan",
      "loss": 2.0252,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0358,
            0.1746,
            -0.2039,
            0.0209,
            -0.2059,
            -0.1989,
            0.0811,
            -0.073,
            0.0538,
            -0.0617,
            -0.0366,
            0.0359,
            -0.0708,
            -0.0228,
            0.0897,
            0.0384
          ],
          "after": [
            0.0031,
            -0.0008,
            0.1593,
            -0.0288,
            0.0313,
            -0.0898,
            -0.1058,
            -0.1487,
            -0.0211,
            0.0803,
            0.1575,
            0.0139,
            -0.0922,
            -0.0227,
            -0.0468,
            -0.1844
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0455,
            0.0777,
            -0.1352,
            0.0292,
            0.0268,
            -0.1495,
            0.2983,
            0.0341,
            -0.1975,
            0.0775,
            -0.0113,
            -0.0086,
            0.1202,
            -0.0946,
            -0.2004,
            0.1443
          ],
          "after": [
            0.0219,
            0.1096,
            -0.0441,
            -0.0764,
            0.0353,
            -0.0173,
            0.0934,
            0.0981,
            -0.0766,
            -0.0732,
            0.0029,
            -0.0132,
            0.1067,
            0.0328,
            -0.0559,
            0.2265
          ]
        },
        "position_0": {
          "grad": [
            -0.0491,
            0.1203,
            -0.1068,
            0.0301,
            -0.3624,
            0.0178,
            0.0379,
            0.0054,
            0.0676,
            -0.2235,
            -0.2677,
            -0.0391,
            -0.0196,
            0.1089,
            0.215,
            0.1349
          ],
          "after": [
            0.0452,
            -0.1394,
            -0.2924,
            0.0648,
            0.031,
            -0.0579,
            -0.1455,
            0.0719,
            0.1007,
            0.0101,
            0.0186,
            0.0946,
            0.0518,
            0.0096,
            -0.1356,
            0.0529
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.001,
            0.0005,
            -0.0012,
            0.0013,
            0.0001,
            -0.0011,
            0.0006,
            0.0005,
            -0.0006,
            -0.0008,
            0.0002,
            -0.0004,
            0.0002,
            0.0001,
            -0.0008
          ],
          "after": [
            -0.089,
            0.0092,
            -0.1042,
            -0.0802,
            0.0803,
            0.1295,
            0.0785,
            -0.1613,
            0.0208,
            -0.0227,
            -0.1277,
            -0.0701,
            0.0943,
            -0.0862,
            0.0352,
            0.08
          ]
        }
      }
    },
    {
      "step": 677,
      "word": "marija",
      "loss": 2.0296,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1469,
            -0.0373,
            -0.0581,
            -0.205,
            0.0794,
            -0.0333,
            0.0811,
            -0.1762,
            0.0597,
            0.073,
            0.1406,
            -0.0422,
            -0.0413,
            -0.0666,
            -0.0209,
            -0.051
          ],
          "after": [
            0.0029,
            -0.001,
            0.1595,
            -0.0284,
            0.0313,
            -0.0895,
            -0.106,
            -0.148,
            -0.0216,
            0.0806,
            0.1573,
            0.0138,
            -0.0918,
            -0.0225,
            -0.0469,
            -0.1845
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1243,
            0.1388,
            -0.1473,
            0.0311,
            0.0878,
            -0.1912,
            0.3081,
            0.0613,
            -0.265,
            0.0087,
            -0.0847,
            0.0185,
            0.1248,
            -0.0714,
            -0.2212,
            0.1678
          ],
          "after": [
            0.0223,
            0.1091,
            -0.0436,
            -0.077,
            0.0353,
            -0.0166,
            0.0928,
            0.0976,
            -0.0759,
            -0.0733,
            0.0032,
            -0.0132,
            0.1062,
            0.0333,
            -0.0553,
            0.2259
          ]
        },
        "position_0": {
          "grad": [
            -0.0045,
            0.0569,
            -0.0395,
            0.0188,
            0.1433,
            -0.0376,
            0.2255,
            0.0626,
            -0.0103,
            0.048,
            0.0126,
            0.0588,
            -0.0221,
            -0.0604,
            -0.0534,
            0.1235
          ],
          "after": [
            0.0452,
            -0.1394,
            -0.2922,
            0.0646,
            0.0314,
            -0.0576,
            -0.146,
            0.0715,
            0.1009,
            0.0105,
            0.0189,
            0.0951,
            0.0516,
            0.0092,
            -0.136,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0023,
            -0.0003,
            -0.0001,
            -0.0041,
            0.0051,
            0.0002,
            0.0026,
            -0.0014,
            0.0004,
            0.0018,
            0.0017,
            -0.0022,
            -0.0,
            -0.0012,
            -0.0019,
            -0.0014
          ],
          "after": [
            -0.0888,
            0.0095,
            -0.1043,
            -0.0796,
            0.0797,
            0.1295,
            0.0787,
            -0.1612,
            0.0208,
            -0.0228,
            -0.1279,
            -0.0701,
            0.0945,
            -0.0861,
            0.0353,
            0.0804
          ]
        }
      }
    },
    {
      "step": 678,
      "word": "maleeha",
      "loss": 2.0967,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0243,
            0.1023,
            -0.0973,
            -0.2038,
            0.0622,
            -0.0572,
            -0.0976,
            0.0431,
            -0.1082,
            0.0689,
            -0.0483,
            0.1348,
            -0.0598,
            0.06,
            -0.0224,
            -0.2826
          ],
          "after": [
            0.0028,
            -0.0013,
            0.1598,
            -0.0278,
            0.0312,
            -0.0891,
            -0.1061,
            -0.1474,
            -0.022,
            0.0807,
            0.1571,
            0.0136,
            -0.0915,
            -0.0223,
            -0.0469,
            -0.1844
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0337,
            -0.2151,
            -0.0858,
            -0.1369,
            -0.0899,
            0.367,
            -0.3651,
            0.0476,
            0.2472,
            -0.3024,
            -0.2779,
            0.0967,
            -0.1616,
            0.2246,
            0.6378,
            -0.218
          ],
          "after": [
            0.0226,
            0.1088,
            -0.0432,
            -0.0772,
            0.0354,
            -0.0163,
            0.0924,
            0.0971,
            -0.0754,
            -0.073,
            0.0038,
            -0.0134,
            0.1059,
            0.0335,
            -0.0553,
            0.2256
          ]
        },
        "position_0": {
          "grad": [
            -0.0033,
            0.0428,
            -0.037,
            -0.0018,
            0.1309,
            -0.0433,
            0.166,
            0.0484,
            0.0157,
            0.0522,
            0.0053,
            0.0405,
            -0.0406,
            -0.055,
            -0.0525,
            0.0826
          ],
          "after": [
            0.0452,
            -0.1395,
            -0.2919,
            0.0644,
            0.0315,
            -0.0573,
            -0.1465,
            0.0712,
            0.1011,
            0.0108,
            0.0192,
            0.0955,
            0.0516,
            0.0089,
            -0.1361,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0006,
            -0.0002,
            -0.0012,
            -0.0001,
            -0.0002,
            -0.0012,
            -0.0002,
            0.0003,
            -0.0009,
            -0.0012,
            -0.0002,
            -0.0005,
            0.0012,
            0.0014,
            -0.0013
          ],
          "after": [
            -0.0887,
            0.0099,
            -0.1044,
            -0.079,
            0.0793,
            0.1294,
            0.0789,
            -0.1611,
            0.0208,
            -0.0227,
            -0.128,
            -0.07,
            0.0948,
            -0.0862,
            0.0352,
            0.0807
          ]
        }
      }
    },
    {
      "step": 679,
      "word": "charley",
      "loss": 2.383,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.036,
            -0.0441,
            -0.0211,
            -0.0237,
            0.0955,
            0.1308,
            0.084,
            -0.0927,
            0.0997,
            0.0087,
            0.047,
            0.1153,
            -0.1331,
            -0.1466,
            -0.0341,
            0.1214
          ],
          "after": [
            0.0027,
            -0.0014,
            0.16,
            -0.0273,
            0.0311,
            -0.089,
            -0.1062,
            -0.1467,
            -0.0224,
            0.0807,
            0.1569,
            0.0133,
            -0.091,
            -0.022,
            -0.0468,
            -0.1843
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.136,
            -0.2867,
            -0.1036,
            -0.1498,
            -0.1398,
            0.0953,
            -0.2521,
            0.0186,
            0.0766,
            0.1365,
            -0.0362,
            -0.167,
            0.0954,
            0.1354,
            0.1076,
            -0.1542
          ],
          "after": [
            0.0228,
            0.1088,
            -0.0427,
            -0.0771,
            0.0355,
            -0.0162,
            0.0922,
            0.0966,
            -0.0751,
            -0.0729,
            0.0044,
            -0.0134,
            0.1056,
            0.0335,
            -0.0553,
            0.2255
          ]
        },
        "position_0": {
          "grad": [
            -0.0305,
            0.0368,
            -0.0219,
            0.1049,
            -0.0571,
            -0.0342,
            0.1193,
            0.0529,
            -0.0651,
            0.0218,
            0.0242,
            0.0208,
            0.046,
            -0.0131,
            -0.0328,
            0.0542
          ],
          "after": [
            0.0453,
            -0.1397,
            -0.2917,
            0.0641,
            0.0317,
            -0.057,
            -0.1471,
            0.0707,
            0.1013,
            0.011,
            0.0194,
            0.0958,
            0.0515,
            0.0087,
            -0.1362,
            0.0515
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            0.0015,
            -0.0008,
            0.0007,
            -0.0007,
            -0.0003,
            0.0017,
            -0.0001,
            -0.0005,
            0.0001,
            0.0005,
            -0.0008,
            0.0011,
            -0.0007,
            -0.0005,
            0.0004
          ],
          "after": [
            -0.0886,
            0.0101,
            -0.1043,
            -0.0786,
            0.079,
            0.1294,
            0.0789,
            -0.161,
            0.0209,
            -0.0227,
            -0.1281,
            -0.0699,
            0.0949,
            -0.0862,
            0.0352,
            0.081
          ]
        }
      }
    },
    {
      "step": 680,
      "word": "dalonte",
      "loss": 2.1736,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1226,
            0.1379,
            -0.0617,
            -0.0942,
            0.0639,
            -0.0309,
            -0.0534,
            0.0568,
            -0.0591,
            -0.0127,
            -0.1056,
            0.2031,
            -0.0616,
            0.0147,
            -0.0015,
            -0.1872
          ],
          "after": [
            0.0027,
            -0.0017,
            0.1603,
            -0.0267,
            0.0309,
            -0.0888,
            -0.1063,
            -0.1463,
            -0.0227,
            0.0808,
            0.1568,
            0.0129,
            -0.0905,
            -0.0217,
            -0.0468,
            -0.1841
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3588,
            -0.2351,
            -0.1294,
            -0.0995,
            -0.3018,
            0.0296,
            -0.1881,
            0.0126,
            -0.1022,
            0.142,
            0.0204,
            -0.1957,
            0.0695,
            0.1074,
            0.0966,
            -0.0603
          ],
          "after": [
            0.0227,
            0.1089,
            -0.0422,
            -0.0769,
            0.0358,
            -0.016,
            0.0921,
            0.0962,
            -0.0747,
            -0.073,
            0.0049,
            -0.0132,
            0.1053,
            0.0334,
            -0.0553,
            0.2254
          ]
        },
        "position_0": {
          "grad": [
            -0.0595,
            0.0471,
            -0.0284,
            -0.0567,
            0.0994,
            -0.0658,
            0.0692,
            -0.0582,
            -0.0324,
            -0.0251,
            -0.0484,
            0.04,
            0.0516,
            0.012,
            0.0272,
            0.1268
          ],
          "after": [
            0.0455,
            -0.1399,
            -0.2914,
            0.0639,
            0.0317,
            -0.0567,
            -0.1476,
            0.0705,
            0.1015,
            0.0112,
            0.0196,
            0.096,
            0.0513,
            0.0086,
            -0.1364,
            0.051
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            -0.0027,
            0.0003,
            -0.0014,
            0.0013,
            -0.0021,
            -0.0001,
            0.0008,
            -0.0016,
            0.0015,
            0.0002,
            0.0002,
            -0.0004,
            -0.0006,
            -0.001,
            -0.0003
          ],
          "after": [
            -0.0888,
            0.0105,
            -0.1043,
            -0.0781,
            0.0786,
            0.1296,
            0.079,
            -0.161,
            0.0211,
            -0.0228,
            -0.1282,
            -0.0698,
            0.0951,
            -0.0861,
            0.0353,
            0.0813
          ]
        }
      }
    },
    {
      "step": 681,
      "word": "blaykleigh",
      "loss": 3.0867,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0188,
            -0.0803,
            0.0732,
            -0.0793,
            0.1174,
            0.0939,
            -0.0022,
            0.0512,
            -0.0564,
            0.0405,
            0.0234,
            0.1092,
            0.0139,
            -0.044,
            -0.0332,
            0.0359
          ],
          "after": [
            0.0027,
            -0.0019,
            0.1605,
            -0.0261,
            0.0306,
            -0.0888,
            -0.1063,
            -0.146,
            -0.0228,
            0.0808,
            0.1568,
            0.0124,
            -0.0902,
            -0.0215,
            -0.0467,
            -0.184
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0741,
            -0.1221,
            -0.0738,
            -0.0518,
            -0.0064,
            -0.0158,
            -0.0821,
            0.046,
            -0.0679,
            0.0849,
            -0.0361,
            -0.1072,
            0.0817,
            0.0771,
            -0.0148,
            -0.0623
          ],
          "after": [
            0.0224,
            0.1091,
            -0.0418,
            -0.0767,
            0.0361,
            -0.0159,
            0.092,
            0.0957,
            -0.0744,
            -0.0731,
            0.0053,
            -0.0129,
            0.105,
            0.0333,
            -0.0554,
            0.2253
          ]
        },
        "position_0": {
          "grad": [
            0.0162,
            0.031,
            0.0517,
            0.0337,
            0.06,
            0.0166,
            0.1126,
            -0.0022,
            -0.1095,
            0.0016,
            0.0223,
            0.064,
            0.1131,
            -0.1219,
            -0.058,
            0.1901
          ],
          "after": [
            0.0456,
            -0.1401,
            -0.2913,
            0.0637,
            0.0317,
            -0.0564,
            -0.1482,
            0.0703,
            0.1018,
            0.0114,
            0.0198,
            0.0961,
            0.0511,
            0.0087,
            -0.1364,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0033,
            0.0001,
            0.0004,
            -0.0001,
            0.0013,
            0.0012,
            0.0034,
            0.001,
            -0.0014,
            0.0015,
            -0.0018,
            0.0,
            0.002,
            -0.0006,
            0.0006,
            0.0022
          ],
          "after": [
            -0.0886,
            0.0108,
            -0.1044,
            -0.0777,
            0.0782,
            0.1296,
            0.0788,
            -0.1612,
            0.0214,
            -0.023,
            -0.128,
            -0.0697,
            0.095,
            -0.086,
            0.0353,
            0.0813
          ]
        }
      }
    },
    {
      "step": 682,
      "word": "shyan",
      "loss": 2.1706,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0104,
            0.1657,
            -0.2293,
            0.0166,
            -0.2276,
            -0.2113,
            0.0787,
            -0.0712,
            0.0459,
            -0.0361,
            -0.0282,
            0.0059,
            -0.06,
            -0.0104,
            0.0966,
            0.0262
          ],
          "after": [
            0.0028,
            -0.0023,
            0.1609,
            -0.0256,
            0.0306,
            -0.0885,
            -0.1065,
            -0.1456,
            -0.023,
            0.0809,
            0.1567,
            0.012,
            -0.0898,
            -0.0212,
            -0.0468,
            -0.1839
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.008,
            0.0012,
            -0.1969,
            -0.0341,
            0.009,
            -0.1807,
            0.2381,
            0.0401,
            -0.2105,
            0.0136,
            -0.0585,
            -0.0296,
            0.1148,
            -0.0735,
            -0.1414,
            0.1366
          ],
          "after": [
            0.0223,
            0.1093,
            -0.0413,
            -0.0764,
            0.0364,
            -0.0157,
            0.0919,
            0.0952,
            -0.074,
            -0.0733,
            0.0058,
            -0.0127,
            0.1046,
            0.0332,
            -0.0553,
            0.2252
          ]
        },
        "position_0": {
          "grad": [
            0.0138,
            0.0845,
            -0.0135,
            0.1658,
            -0.2217,
            0.0524,
            -0.0091,
            0.0758,
            -0.0585,
            -0.0305,
            -0.1487,
            0.0254,
            0.1024,
            0.0689,
            0.1915,
            0.0183
          ],
          "after": [
            0.0457,
            -0.1404,
            -0.2911,
            0.0633,
            0.0319,
            -0.0562,
            -0.1486,
            0.07,
            0.1021,
            0.0116,
            0.0201,
            0.0961,
            0.0507,
            0.0086,
            -0.1367,
            0.0499
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0003,
            0.0005,
            0.0,
            -0.0004,
            0.0004,
            -0.001,
            -0.0006,
            -0.0003,
            0.0006,
            0.0013,
            0.0004,
            -0.001,
            -0.0005,
            0.0007,
            -0.0009
          ],
          "after": [
            -0.0884,
            0.0111,
            -0.1044,
            -0.0774,
            0.0778,
            0.1296,
            0.0788,
            -0.1612,
            0.0218,
            -0.0232,
            -0.1281,
            -0.0697,
            0.095,
            -0.0858,
            0.0353,
            0.0814
          ]
        }
      }
    },
    {
      "step": 683,
      "word": "karlina",
      "loss": 1.648,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1376,
            -0.0862,
            -0.0586,
            -0.1581,
            0.0628,
            -0.0379,
            0.075,
            -0.1462,
            0.0461,
            0.0363,
            0.0978,
            -0.0368,
            -0.0568,
            -0.0518,
            0.0106,
            -0.0407
          ],
          "after": [
            0.0027,
            -0.0024,
            0.1612,
            -0.025,
            0.0305,
            -0.0882,
            -0.1067,
            -0.145,
            -0.0232,
            0.0809,
            0.1566,
            0.0117,
            -0.0894,
            -0.0209,
            -0.0468,
            -0.1838
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1031,
            0.1776,
            -0.1247,
            0.0308,
            0.0291,
            -0.2117,
            0.3582,
            0.0286,
            -0.2464,
            0.0074,
            -0.0278,
            -0.0138,
            0.1099,
            -0.1119,
            -0.1682,
            0.2079
          ],
          "after": [
            0.0222,
            0.1094,
            -0.0407,
            -0.0763,
            0.0365,
            -0.0153,
            0.0916,
            0.0948,
            -0.0735,
            -0.0734,
            0.0062,
            -0.0125,
            0.1042,
            0.0333,
            -0.0552,
            0.225
          ]
        },
        "position_0": {
          "grad": [
            -0.0604,
            0.0506,
            -0.0992,
            0.0074,
            0.0513,
            -0.0763,
            0.1607,
            -0.0073,
            -0.0521,
            0.0237,
            -0.0092,
            0.0251,
            0.0533,
            -0.0548,
            -0.105,
            -0.0237
          ],
          "after": [
            0.0459,
            -0.1407,
            -0.2908,
            0.0629,
            0.032,
            -0.0559,
            -0.1491,
            0.0697,
            0.1025,
            0.0117,
            0.0204,
            0.0961,
            0.0504,
            0.0087,
            -0.1368,
            0.0495
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0032,
            0.0008,
            0.0003,
            0.0003,
            -0.0003,
            -0.0011,
            -0.0015,
            -0.003,
            -0.0017,
            0.0001,
            0.0012,
            -0.0029,
            0.0,
            0.0023,
            -0.0015,
            -0.002
          ],
          "after": [
            -0.0885,
            0.0112,
            -0.1045,
            -0.0771,
            0.0776,
            0.1297,
            0.0788,
            -0.1609,
            0.0222,
            -0.0234,
            -0.1282,
            -0.0694,
            0.0951,
            -0.0859,
            0.0353,
            0.0817
          ]
        }
      }
    },
    {
      "step": 684,
      "word": "mirajane",
      "loss": 2.3742,
      "learning_rate": 0.001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3391,
            0.2252,
            0.3022,
            0.1759,
            0.0209,
            -0.0237,
            0.1373,
            0.0221,
            -0.1062,
            -0.4575,
            -0.1976,
            0.1692,
            0.1551,
            -0.1732,
            -0.1821,
            0.394
          ],
          "after": [
            0.0029,
            -0.0029,
            0.1613,
            -0.0247,
            0.0305,
            -0.0879,
            -0.107,
            -0.1446,
            -0.0233,
            0.0815,
            0.1567,
            0.0113,
            -0.0892,
            -0.0205,
            -0.0467,
            -0.1841
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2189,
            0.1481,
            -0.1705,
            -0.0138,
            -0.366,
            0.0565,
            -0.1549,
            -0.0437,
            -0.0705,
            -0.027,
            -0.0739,
            -0.4019,
            0.2321,
            0.1942,
            0.05,
            -0.1249
          ],
          "after": [
            0.022,
            0.1093,
            -0.0402,
            -0.0761,
            0.0369,
            -0.0151,
            0.0914,
            0.0944,
            -0.073,
            -0.0735,
            0.0067,
            -0.012,
            0.1037,
            0.0331,
            -0.0551,
            0.2248
          ]
        },
        "position_0": {
          "grad": [
            -0.0066,
            0.043,
            -0.0369,
            0.0222,
            0.1142,
            -0.0289,
            0.1764,
            0.0431,
            -0.0054,
            0.0551,
            0.0152,
            0.0577,
            -0.0346,
            -0.0653,
            -0.0507,
            0.0829
          ],
          "after": [
            0.0461,
            -0.141,
            -0.2905,
            0.0626,
            0.032,
            -0.0556,
            -0.1497,
            0.0694,
            0.1028,
            0.0117,
            0.0206,
            0.096,
            0.0502,
            0.0089,
            -0.1368,
            0.049
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            0.0004,
            0.0022,
            -0.0019,
            0.001,
            -0.0032,
            -0.0025,
            -0.0018,
            -0.0001,
            0.0027,
            0.0043,
            -0.0002,
            -0.0024,
            -0.0027,
            0.0004,
            -0.0044
          ],
          "after": [
            -0.0886,
            0.0113,
            -0.1048,
            -0.0767,
            0.0773,
            0.13,
            0.079,
            -0.1604,
            0.0227,
            -0.0238,
            -0.1288,
            -0.0692,
            0.0953,
            -0.0858,
            0.0354,
            0.0823
          ]
        }
      }
    },
    {
      "step": 685,
      "word": "raelene",
      "loss": 1.9429,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0377,
            -0.1288,
            0.0614,
            0.0683,
            -0.0367,
            0.1475,
            -0.173,
            -0.0164,
            0.0683,
            0.1279,
            0.0642,
            -0.0113,
            -0.1382,
            -0.0307,
            -0.0002,
            -0.238
          ],
          "after": [
            0.0031,
            -0.0031,
            0.1612,
            -0.0245,
            0.0304,
            -0.0879,
            -0.1071,
            -0.1442,
            -0.0234,
            0.0818,
            0.1567,
            0.0109,
            -0.0889,
            -0.02,
            -0.0465,
            -0.1841
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3489,
            -0.2868,
            -0.0627,
            0.0368,
            -0.466,
            0.8239,
            -0.7331,
            0.0105,
            0.3815,
            -0.2006,
            -0.147,
            -0.238,
            -0.0643,
            0.4293,
            0.6575,
            -0.2861
          ],
          "after": [
            0.0215,
            0.1094,
            -0.0397,
            -0.076,
            0.0376,
            -0.0154,
            0.0916,
            0.0941,
            -0.0728,
            -0.0734,
            0.0073,
            -0.0113,
            0.1033,
            0.0326,
            -0.0554,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            0.0004,
            -0.0231,
            -0.0134,
            -0.0599,
            0.0091,
            0.1078,
            0.0513,
            -0.0468,
            0.1093,
            -0.0286,
            -0.0964,
            0.0548,
            -0.1479,
            -0.1093,
            0.0629,
            0.0755
          ],
          "after": [
            0.0463,
            -0.1413,
            -0.2901,
            0.0624,
            0.032,
            -0.0556,
            -0.1502,
            0.0692,
            0.1029,
            0.0118,
            0.0209,
            0.0959,
            0.0501,
            0.0093,
            -0.1369,
            0.0486
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            0.0,
            0.001,
            0.0005,
            0.0011,
            -0.0004,
            0.0006,
            0.0004,
            -0.0001,
            0.0007,
            0.0008,
            0.0002,
            0.0007,
            -0.0009,
            -0.0021,
            0.0004
          ],
          "after": [
            -0.0887,
            0.0114,
            -0.1052,
            -0.0764,
            0.0769,
            0.1304,
            0.0791,
            -0.16,
            0.023,
            -0.0241,
            -0.1294,
            -0.069,
            0.0954,
            -0.0855,
            0.0355,
            0.0828
          ]
        }
      }
    },
    {
      "step": 686,
      "word": "abijah",
      "loss": 2.3537,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1073,
            -0.0026,
            0.5476,
            0.1146,
            0.2094,
            0.065,
            0.1767,
            0.1116,
            -0.0649,
            0.1183,
            -0.0007,
            -0.0197,
            0.4832,
            -0.1685,
            -0.2384,
            0.5346
          ],
          "after": [
            0.0034,
            -0.0032,
            0.1607,
            -0.0245,
            0.0302,
            -0.0879,
            -0.1073,
            -0.144,
            -0.0234,
            0.0819,
            0.1568,
            0.0106,
            -0.0892,
            -0.0194,
            -0.046,
            -0.1845
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0806,
            0.0889,
            -0.063,
            -0.0022,
            0.0791,
            -0.1793,
            0.1966,
            0.048,
            -0.1731,
            0.0349,
            -0.0395,
            0.026,
            0.0874,
            -0.0384,
            -0.1746,
            0.1194
          ],
          "after": [
            0.0211,
            0.1095,
            -0.0392,
            -0.0759,
            0.038,
            -0.0156,
            0.0917,
            0.0938,
            -0.0726,
            -0.0733,
            0.0079,
            -0.0108,
            0.1029,
            0.0322,
            -0.0556,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            0.0854,
            0.0107,
            0.0793,
            -0.126,
            0.0591,
            0.0012,
            -0.2335,
            -0.1282,
            0.077,
            0.0484,
            0.2184,
            0.0471,
            -0.105,
            0.0469,
            -0.0804,
            -0.14
          ],
          "after": [
            0.0463,
            -0.1415,
            -0.29,
            0.0624,
            0.0319,
            -0.0556,
            -0.1505,
            0.0694,
            0.1029,
            0.0118,
            0.0209,
            0.0957,
            0.0502,
            0.0095,
            -0.1369,
            0.0484
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0008,
            -0.0008,
            -0.0009,
            0.0043,
            0.0045,
            0.0015,
            -0.001,
            0.0022,
            0.0023,
            0.0002,
            -0.0003,
            -0.0014,
            0.0006,
            -0.0053,
            0.0011
          ],
          "after": [
            -0.0888,
            0.0115,
            -0.1054,
            -0.0761,
            0.0763,
            0.1303,
            0.0791,
            -0.1596,
            0.0231,
            -0.0246,
            -0.1298,
            -0.0688,
            0.0956,
            -0.0854,
            0.036,
            0.0832
          ]
        }
      }
    },
    {
      "step": 687,
      "word": "dylann",
      "loss": 1.9111,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0273,
            0.1573,
            -0.1688,
            0.0245,
            -0.1753,
            -0.1626,
            0.0882,
            -0.0543,
            0.0327,
            -0.0386,
            -0.035,
            0.0231,
            -0.0444,
            -0.025,
            0.0726,
            0.0343
          ],
          "after": [
            0.0037,
            -0.0036,
            0.1604,
            -0.0246,
            0.0302,
            -0.0877,
            -0.1076,
            -0.1438,
            -0.0235,
            0.0821,
            0.1568,
            0.0104,
            -0.0894,
            -0.0189,
            -0.0457,
            -0.185
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.071,
            0.13,
            -0.1324,
            0.0762,
            0.0353,
            -0.1947,
            0.3269,
            0.039,
            -0.2598,
            0.0399,
            -0.0324,
            0.0076,
            0.1342,
            -0.1138,
            -0.2157,
            0.1753
          ],
          "after": [
            0.0209,
            0.1094,
            -0.0387,
            -0.076,
            0.0384,
            -0.0156,
            0.0916,
            0.0934,
            -0.0722,
            -0.0733,
            0.0084,
            -0.0104,
            0.1025,
            0.032,
            -0.0556,
            0.2248
          ]
        },
        "position_0": {
          "grad": [
            -0.0713,
            0.0253,
            -0.028,
            -0.0485,
            0.085,
            -0.059,
            0.0704,
            -0.0519,
            -0.0163,
            -0.0487,
            -0.0522,
            0.034,
            0.0462,
            0.0215,
            0.0628,
            0.129
          ],
          "after": [
            0.0464,
            -0.1417,
            -0.2899,
            0.0625,
            0.0317,
            -0.0554,
            -0.1508,
            0.0696,
            0.103,
            0.0118,
            0.021,
            0.0955,
            0.0502,
            0.0097,
            -0.137,
            0.048
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0023,
            -0.0019,
            0.0012,
            -0.0011,
            0.0024,
            -0.0008,
            0.0026,
            0.0016,
            0.0004,
            0.0014,
            -0.0004,
            0.0016,
            0.0012,
            -0.0023,
            -0.0009,
            0.0002
          ],
          "after": [
            -0.0887,
            0.0118,
            -0.1057,
            -0.0757,
            0.0757,
            0.1302,
            0.0789,
            -0.1594,
            0.0231,
            -0.0251,
            -0.1302,
            -0.0688,
            0.0957,
            -0.0851,
            0.0364,
            0.0834
          ]
        }
      }
    },
    {
      "step": 688,
      "word": "charbel",
      "loss": 2.5977,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0358,
            -0.038,
            -0.0231,
            0.0049,
            0.0873,
            0.1393,
            0.1028,
            -0.0902,
            0.0836,
            0.0136,
            0.0543,
            0.1298,
            -0.1257,
            -0.1439,
            -0.0307,
            0.1341
          ],
          "after": [
            0.004,
            -0.0038,
            0.1602,
            -0.0246,
            0.0301,
            -0.0878,
            -0.108,
            -0.1435,
            -0.0236,
            0.0822,
            0.1568,
            0.01,
            -0.0894,
            -0.0183,
            -0.0454,
            -0.1854
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.095,
            -0.1111,
            -0.2345,
            -0.0925,
            -0.131,
            0.1223,
            0.0747,
            -0.0992,
            0.136,
            -0.0228,
            0.0196,
            -0.1796,
            -0.0209,
            -0.1393,
            0.1318,
            0.048
          ],
          "after": [
            0.0206,
            0.1095,
            -0.0381,
            -0.0759,
            0.0388,
            -0.0157,
            0.0915,
            0.0933,
            -0.072,
            -0.0732,
            0.0088,
            -0.0099,
            0.1021,
            0.0319,
            -0.0557,
            0.2247
          ]
        },
        "position_0": {
          "grad": [
            -0.0308,
            0.0338,
            -0.0207,
            0.108,
            -0.049,
            -0.0318,
            0.1147,
            0.053,
            -0.0617,
            0.0246,
            0.038,
            0.0271,
            0.0493,
            -0.013,
            -0.0419,
            0.0518
          ],
          "after": [
            0.0465,
            -0.142,
            -0.2897,
            0.0624,
            0.0316,
            -0.0552,
            -0.1511,
            0.0696,
            0.1031,
            0.0118,
            0.021,
            0.0953,
            0.0501,
            0.0098,
            -0.137,
            0.0477
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0005,
            0.0013,
            0.0,
            0.0002,
            -0.0002,
            0.0004,
            0.0006,
            0.0002,
            0.0016,
            0.0012,
            -0.001,
            0.0015,
            -0.0018,
            -0.0025,
            -0.0006
          ],
          "after": [
            -0.0887,
            0.012,
            -0.1061,
            -0.0754,
            0.0751,
            0.1302,
            0.0788,
            -0.1593,
            0.023,
            -0.0256,
            -0.1307,
            -0.0687,
            0.0957,
            -0.0847,
            0.037,
            0.0837
          ]
        }
      }
    },
    {
      "step": 689,
      "word": "shamshon",
      "loss": 2.5057,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0756,
            0.0479,
            0.1206,
            0.0151,
            0.1569,
            -0.0565,
            0.2092,
            0.0729,
            0.0275,
            -0.0056,
            0.0644,
            0.036,
            -0.0595,
            -0.0746,
            -0.0372,
            0.0313
          ],
          "after": [
            0.0042,
            -0.004,
            0.1599,
            -0.0247,
            0.0299,
            -0.0878,
            -0.1086,
            -0.1433,
            -0.0238,
            0.0823,
            0.1567,
            0.0097,
            -0.0894,
            -0.0176,
            -0.0451,
            -0.1859
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.109,
            0.069,
            -0.1543,
            -0.002,
            0.0904,
            -0.2067,
            0.2864,
            0.0477,
            -0.1933,
            -0.0237,
            -0.0691,
            0.0746,
            0.0609,
            -0.1213,
            -0.1648,
            0.19
          ],
          "after": [
            0.0204,
            0.1095,
            -0.0375,
            -0.0758,
            0.0391,
            -0.0156,
            0.0913,
            0.0931,
            -0.0717,
            -0.0731,
            0.0093,
            -0.0095,
            0.1018,
            0.032,
            -0.0557,
            0.2244
          ]
        },
        "position_0": {
          "grad": [
            0.0054,
            0.0823,
            -0.0122,
            0.114,
            -0.093,
            0.0379,
            0.0202,
            0.036,
            -0.0688,
            0.0194,
            -0.071,
            0.0613,
            0.0858,
            0.0166,
            0.0728,
            0.0282
          ],
          "after": [
            0.0467,
            -0.1423,
            -0.2895,
            0.0622,
            0.0316,
            -0.0552,
            -0.1514,
            0.0696,
            0.1033,
            0.0118,
            0.0211,
            0.095,
            0.05,
            0.0099,
            -0.1371,
            0.0474
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0005,
            -0.0003,
            0.0003,
            0.0011,
            -0.0008,
            0.0015,
            0.0002,
            -0.0006,
            0.0007,
            0.0003,
            0.0013,
            0.0005,
            -0.0011,
            -0.0014,
            0.0005
          ],
          "after": [
            -0.0885,
            0.0122,
            -0.1065,
            -0.0751,
            0.0745,
            0.1303,
            0.0785,
            -0.1593,
            0.0231,
            -0.0262,
            -0.1311,
            -0.0687,
            0.0956,
            -0.0842,
            0.0375,
            0.0839
          ]
        }
      }
    },
    {
      "step": 690,
      "word": "charlette",
      "loss": 2.6188,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0209,
            -0.0239,
            -0.0086,
            -0.0123,
            0.0853,
            0.1125,
            0.0793,
            -0.0732,
            0.0694,
            0.0139,
            0.0348,
            0.1031,
            -0.0999,
            -0.1282,
            -0.0375,
            0.1047
          ],
          "after": [
            0.0044,
            -0.0042,
            0.1597,
            -0.0247,
            0.0296,
            -0.0879,
            -0.1091,
            -0.1431,
            -0.0241,
            0.0824,
            0.1566,
            0.0093,
            -0.0892,
            -0.0169,
            -0.0448,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.4039,
            -0.4614,
            -0.0547,
            -0.2497,
            -0.2751,
            0.1902,
            -0.5378,
            0.0083,
            0.1388,
            0.227,
            0.0271,
            -0.243,
            0.04,
            0.2583,
            0.2392,
            -0.378
          ],
          "after": [
            0.02,
            0.1098,
            -0.0369,
            -0.0753,
            0.0396,
            -0.0157,
            0.0913,
            0.0929,
            -0.0715,
            -0.0733,
            0.0097,
            -0.009,
            0.1014,
            0.0318,
            -0.0558,
            0.2245
          ]
        },
        "position_0": {
          "grad": [
            -0.0255,
            0.0285,
            -0.0201,
            0.0813,
            -0.0374,
            -0.0244,
            0.0949,
            0.0389,
            -0.0534,
            0.0263,
            0.0315,
            0.0247,
            0.0351,
            -0.0124,
            -0.0385,
            0.0387
          ],
          "after": [
            0.0469,
            -0.1426,
            -0.2893,
            0.0619,
            0.0317,
            -0.055,
            -0.1517,
            0.0695,
            0.1035,
            0.0117,
            0.0211,
            0.0948,
            0.0499,
            0.0101,
            -0.1371,
            0.0471
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0014,
            -0.0001,
            0.0008,
            0.0,
            0.0,
            0.0019,
            -0.0004,
            -0.0005,
            -0.0001,
            0.0008,
            0.0001,
            0.001,
            -0.0008,
            -0.0006,
            0.0004
          ],
          "after": [
            -0.0884,
            0.0122,
            -0.1067,
            -0.075,
            0.074,
            0.1303,
            0.0783,
            -0.1592,
            0.0232,
            -0.0266,
            -0.1315,
            -0.0687,
            0.0955,
            -0.0837,
            0.038,
            0.084
          ]
        }
      }
    },
    {
      "step": 691,
      "word": "jermane",
      "loss": 1.9598,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0517,
            0.1355,
            -0.0322,
            0.0312,
            -0.0119,
            -0.0758,
            0.0154,
            -0.0606,
            0.0091,
            -0.1487,
            -0.0633,
            0.0883,
            -0.0553,
            -0.0262,
            -0.0237,
            0.0618
          ],
          "after": [
            0.0046,
            -0.0045,
            0.1595,
            -0.0248,
            0.0294,
            -0.0879,
            -0.1096,
            -0.1427,
            -0.0243,
            0.0826,
            0.1566,
            0.0089,
            -0.0891,
            -0.0162,
            -0.0445,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.5717,
            -0.2504,
            0.2323,
            0.0303,
            -0.2105,
            0.2853,
            -0.8848,
            -0.2895,
            0.501,
            0.1337,
            0.2713,
            -0.2776,
            -0.1577,
            0.3055,
            0.1158,
            -0.4981
          ],
          "after": [
            0.0191,
            0.1102,
            -0.0366,
            -0.075,
            0.0401,
            -0.016,
            0.0917,
            0.0934,
            -0.0717,
            -0.0737,
            0.0096,
            -0.0084,
            0.1013,
            0.0314,
            -0.056,
            0.2248
          ]
        },
        "position_0": {
          "grad": [
            0.0264,
            0.024,
            0.0214,
            0.126,
            -0.0096,
            -0.1116,
            0.1182,
            0.0673,
            -0.1738,
            -0.0476,
            0.032,
            -0.0239,
            0.2639,
            -0.096,
            -0.0966,
            0.0765
          ],
          "after": [
            0.0469,
            -0.1429,
            -0.2892,
            0.0615,
            0.0317,
            -0.0547,
            -0.1521,
            0.0693,
            0.1039,
            0.0118,
            0.0211,
            0.0946,
            0.0495,
            0.0103,
            -0.137,
            0.0468
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0001,
            0.0011,
            -0.0001,
            0.0016,
            -0.0013,
            -0.0002,
            -0.0004,
            0.0001,
            0.0008,
            0.001,
            0.0005,
            0.0001,
            -0.001,
            -0.0017,
            -0.0008
          ],
          "after": [
            -0.0883,
            0.0121,
            -0.107,
            -0.0749,
            0.0734,
            0.1305,
            0.078,
            -0.1591,
            0.0233,
            -0.027,
            -0.132,
            -0.0688,
            0.0954,
            -0.0832,
            0.0385,
            0.0842
          ]
        }
      }
    },
    {
      "step": 692,
      "word": "karson",
      "loss": 1.9964,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0596,
            -0.0881,
            -0.0569,
            -0.0865,
            0.0706,
            -0.0238,
            0.0803,
            -0.1325,
            0.1379,
            -0.0763,
            0.059,
            0.0115,
            -0.0849,
            -0.0885,
            0.0379,
            0.078
          ],
          "after": [
            0.0047,
            -0.0047,
            0.1594,
            -0.0247,
            0.0292,
            -0.0878,
            -0.1101,
            -0.1423,
            -0.0247,
            0.0829,
            0.1565,
            0.0086,
            -0.0888,
            -0.0156,
            -0.0443,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0702,
            0.0916,
            -0.1257,
            0.0052,
            0.0052,
            -0.1857,
            0.282,
            0.049,
            -0.2076,
            0.0208,
            -0.0322,
            0.0084,
            0.0808,
            -0.0974,
            -0.1147,
            0.1463
          ],
          "after": [
            0.0185,
            0.1105,
            -0.0363,
            -0.0747,
            0.0405,
            -0.0161,
            0.0919,
            0.0937,
            -0.0717,
            -0.0739,
            0.0095,
            -0.0078,
            0.1011,
            0.0311,
            -0.056,
            0.225
          ]
        },
        "position_0": {
          "grad": [
            -0.0696,
            0.0448,
            -0.1071,
            0.0113,
            0.0547,
            -0.0782,
            0.1649,
            -0.0113,
            -0.0534,
            0.0104,
            0.0031,
            0.0159,
            0.0699,
            -0.0305,
            -0.1179,
            -0.0297
          ],
          "after": [
            0.0472,
            -0.1432,
            -0.2889,
            0.0611,
            0.0317,
            -0.0543,
            -0.1525,
            0.0692,
            0.1042,
            0.0118,
            0.0211,
            0.0944,
            0.049,
            0.0107,
            -0.1367,
            0.0466
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0011,
            0.0013,
            0.001,
            -0.0003,
            0.0001,
            0.0024,
            -0.0001,
            0.0006,
            0.0018,
            -0.0,
            0.0008,
            0.0018,
            -0.0003,
            -0.0025,
            0.0004
          ],
          "after": [
            -0.0882,
            0.012,
            -0.1074,
            -0.0748,
            0.073,
            0.1306,
            0.0777,
            -0.159,
            0.0233,
            -0.0275,
            -0.1324,
            -0.0689,
            0.0951,
            -0.0828,
            0.0391,
            0.0843
          ]
        }
      }
    },
    {
      "step": 693,
      "word": "sabirin",
      "loss": 2.1313,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0041,
            0.0363,
            0.2622,
            0.0299,
            0.0705,
            -0.0243,
            0.1117,
            0.0246,
            -0.1484,
            0.0071,
            0.0172,
            0.0334,
            0.1477,
            -0.1649,
            -0.0954,
            0.1971
          ],
          "after": [
            0.0049,
            -0.0049,
            0.1591,
            -0.0247,
            0.0289,
            -0.0878,
            -0.1106,
            -0.1419,
            -0.0249,
            0.0832,
            0.1564,
            0.0082,
            -0.0888,
            -0.0148,
            -0.044,
            -0.1877
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0562,
            0.0474,
            -0.0741,
            -0.0014,
            0.1014,
            -0.1503,
            0.2068,
            0.0163,
            -0.1827,
            0.0314,
            -0.0116,
            0.0494,
            0.0405,
            -0.078,
            -0.1868,
            0.1074
          ],
          "after": [
            0.018,
            0.1108,
            -0.0359,
            -0.0745,
            0.0408,
            -0.016,
            0.092,
            0.0939,
            -0.0715,
            -0.0742,
            0.0095,
            -0.0074,
            0.1009,
            0.0309,
            -0.056,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            0.0085,
            0.0815,
            -0.0146,
            0.0829,
            -0.108,
            0.0432,
            -0.0049,
            0.0351,
            -0.0673,
            0.0359,
            -0.0712,
            0.0519,
            0.0865,
            0.0053,
            0.0568,
            0.0017
          ],
          "after": [
            0.0474,
            -0.1436,
            -0.2886,
            0.0606,
            0.0319,
            -0.0541,
            -0.1529,
            0.0689,
            0.1047,
            0.0117,
            0.0212,
            0.0942,
            0.0486,
            0.0109,
            -0.1366,
            0.0464
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0014,
            -0.0007,
            0.0021,
            -0.0011,
            0.0025,
            -0.0004,
            0.0022,
            0.0006,
            -0.001,
            -0.0008,
            0.0035,
            -0.0018,
            -0.0014,
            0.0021,
            0.0028
          ],
          "after": [
            -0.088,
            0.012,
            -0.1077,
            -0.075,
            0.0727,
            0.1305,
            0.0774,
            -0.1591,
            0.0232,
            -0.0279,
            -0.1326,
            -0.0693,
            0.095,
            -0.0823,
            0.0394,
            0.0841
          ]
        }
      }
    },
    {
      "step": 694,
      "word": "abdulwahab",
      "loss": 3.3986,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2564,
            0.0824,
            0.5245,
            0.1854,
            0.2243,
            0.1041,
            0.3463,
            0.1614,
            -0.1249,
            -0.0254,
            -0.1859,
            0.2287,
            0.4197,
            -0.3005,
            -0.2227,
            0.6099
          ],
          "after": [
            0.0052,
            -0.0051,
            0.1584,
            -0.0249,
            0.0285,
            -0.0879,
            -0.1114,
            -0.1418,
            -0.0248,
            0.0834,
            0.1565,
            0.0077,
            -0.0892,
            -0.0138,
            -0.0435,
            -0.1886
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1289,
            0.184,
            -0.0333,
            0.0953,
            0.11,
            -0.2715,
            0.2802,
            0.04,
            -0.2487,
            -0.0304,
            -0.0152,
            0.0904,
            0.0637,
            -0.085,
            -0.2306,
            0.2212
          ],
          "after": [
            0.0176,
            0.1109,
            -0.0356,
            -0.0744,
            0.041,
            -0.0159,
            0.092,
            0.094,
            -0.0713,
            -0.0744,
            0.0096,
            -0.0071,
            0.1006,
            0.0308,
            -0.0558,
            0.2251
          ]
        },
        "position_0": {
          "grad": [
            0.0417,
            0.0137,
            0.0526,
            -0.0464,
            0.059,
            0.0183,
            -0.11,
            -0.0703,
            0.0244,
            0.0326,
            0.1564,
            0.0763,
            -0.0622,
            -0.0044,
            -0.0477,
            -0.0724
          ],
          "after": [
            0.0474,
            -0.144,
            -0.2885,
            0.0603,
            0.0319,
            -0.0539,
            -0.1531,
            0.0689,
            0.105,
            0.0116,
            0.0211,
            0.0939,
            0.0483,
            0.0111,
            -0.1364,
            0.0463
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            0.0035,
            0.0022,
            0.002,
            0.0021,
            -0.0037,
            0.0023,
            -0.0013,
            -0.0019,
            -0.0013,
            0.0019,
            0.0009,
            0.0022,
            -0.0008,
            -0.0028,
            0.0016
          ],
          "after": [
            -0.0879,
            0.0117,
            -0.1082,
            -0.0753,
            0.0723,
            0.1307,
            0.077,
            -0.1591,
            0.0234,
            -0.0281,
            -0.133,
            -0.0697,
            0.0948,
            -0.0818,
            0.0399,
            0.0838
          ]
        }
      }
    },
    {
      "step": 695,
      "word": "salsabil",
      "loss": 2.7442,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.274,
            0.11,
            0.1843,
            -0.0304,
            0.0726,
            0.052,
            0.0912,
            0.0851,
            -0.1101,
            -0.0767,
            -0.1414,
            0.2803,
            0.0785,
            -0.1485,
            -0.0396,
            0.1603
          ],
          "after": [
            0.0057,
            -0.0055,
            0.1577,
            -0.0251,
            0.028,
            -0.088,
            -0.1122,
            -0.1419,
            -0.0246,
            0.0837,
            0.1568,
            0.007,
            -0.0896,
            -0.0127,
            -0.0429,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.12,
            0.1447,
            -0.0952,
            0.0348,
            0.0602,
            -0.2332,
            0.2898,
            -0.0056,
            -0.2357,
            -0.0385,
            0.0048,
            0.1023,
            0.0399,
            -0.1161,
            -0.1757,
            0.2377
          ],
          "after": [
            0.0175,
            0.1108,
            -0.0353,
            -0.0744,
            0.0411,
            -0.0155,
            0.0919,
            0.0941,
            -0.071,
            -0.0746,
            0.0096,
            -0.007,
            0.1004,
            0.0309,
            -0.0556,
            0.2249
          ]
        },
        "position_0": {
          "grad": [
            0.0017,
            0.0745,
            -0.0127,
            0.0923,
            -0.0717,
            0.0444,
            0.0113,
            0.0328,
            -0.0736,
            0.0387,
            -0.0588,
            0.0727,
            0.0866,
            -0.008,
            0.0396,
            0.0088
          ],
          "after": [
            0.0475,
            -0.1444,
            -0.2883,
            0.06,
            0.032,
            -0.0538,
            -0.1533,
            0.0688,
            0.1053,
            0.0114,
            0.021,
            0.0935,
            0.0479,
            0.0113,
            -0.1363,
            0.0462
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            0.001,
            0.0015,
            0.0002,
            0.0005,
            -0.001,
            0.0003,
            -0.0009,
            -0.0004,
            0.0001,
            0.0005,
            -0.0001,
            0.0009,
            0.0003,
            -0.0012,
            -0.0001
          ],
          "after": [
            -0.0878,
            0.0114,
            -0.1087,
            -0.0756,
            0.0719,
            0.131,
            0.0767,
            -0.159,
            0.0235,
            -0.0282,
            -0.1334,
            -0.07,
            0.0945,
            -0.0814,
            0.0404,
            0.0836
          ]
        }
      }
    },
    {
      "step": 696,
      "word": "dakston",
      "loss": 2.5279,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1944,
            0.1225,
            0.0699,
            0.0853,
            0.0464,
            -0.0274,
            0.2516,
            0.0332,
            -0.0955,
            -0.0077,
            -0.0375,
            0.0443,
            0.0143,
            -0.0429,
            -0.112,
            -0.0817
          ],
          "after": [
            0.0064,
            -0.0059,
            0.157,
            -0.0253,
            0.0276,
            -0.0881,
            -0.1131,
            -0.1421,
            -0.0242,
            0.084,
            0.1571,
            0.0064,
            -0.0899,
            -0.0118,
            -0.0423,
            -0.1903
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0735,
            0.1497,
            -0.1257,
            0.0663,
            0.0756,
            -0.2064,
            0.3227,
            0.0545,
            -0.2382,
            0.01,
            -0.0331,
            0.0163,
            0.0859,
            -0.0893,
            -0.2025,
            0.1495
          ],
          "after": [
            0.0174,
            0.1107,
            -0.0349,
            -0.0745,
            0.0411,
            -0.0152,
            0.0916,
            0.094,
            -0.0706,
            -0.0747,
            0.0096,
            -0.0068,
            0.1002,
            0.031,
            -0.0553,
            0.2246
          ]
        },
        "position_0": {
          "grad": [
            -0.0597,
            0.0456,
            -0.029,
            -0.0322,
            0.1001,
            -0.0604,
            0.0788,
            -0.0522,
            -0.0417,
            -0.0306,
            -0.0393,
            0.0534,
            0.0628,
            0.0197,
            0.033,
            0.1355
          ],
          "after": [
            0.0476,
            -0.1448,
            -0.2881,
            0.0597,
            0.032,
            -0.0536,
            -0.1536,
            0.0688,
            0.1057,
            0.0113,
            0.0211,
            0.0932,
            0.0475,
            0.0115,
            -0.1362,
            0.0459
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            0.0015,
            -0.0006,
            -0.0002,
            0.0015,
            -0.0037,
            0.0055,
            0.0006,
            -0.0008,
            0.0023,
            0.0006,
            0.0009,
            0.0005,
            -0.0004,
            -0.0024,
            0.0025
          ],
          "after": [
            -0.0878,
            0.011,
            -0.1091,
            -0.0758,
            0.0715,
            0.1315,
            0.0761,
            -0.159,
            0.0238,
            -0.0286,
            -0.1338,
            -0.0703,
            0.0942,
            -0.0811,
            0.0409,
            0.0832
          ]
        }
      }
    },
    {
      "step": 697,
      "word": "rook",
      "loss": 3.2001,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0069,
            -0.0063,
            0.1564,
            -0.0255,
            0.0273,
            -0.0882,
            -0.1139,
            -0.1422,
            -0.024,
            0.0842,
            0.1573,
            0.0059,
            -0.0902,
            -0.011,
            -0.0418,
            -0.1909
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0521,
            0.1261,
            -0.1492,
            0.0054,
            -0.0582,
            -0.1935,
            0.3047,
            0.0435,
            -0.2126,
            0.0283,
            -0.0307,
            -0.0523,
            0.1301,
            -0.0995,
            -0.1067,
            0.1377
          ],
          "after": [
            0.0173,
            0.1104,
            -0.0345,
            -0.0746,
            0.0412,
            -0.0147,
            0.0913,
            0.0939,
            -0.0701,
            -0.0748,
            0.0097,
            -0.0067,
            0.0999,
            0.0312,
            -0.0549,
            0.2243
          ]
        },
        "position_0": {
          "grad": [
            -0.0289,
            -0.046,
            -0.0041,
            -0.0326,
            -0.0201,
            0.2164,
            0.0983,
            -0.0717,
            0.1559,
            -0.1268,
            -0.1139,
            0.0735,
            -0.2169,
            -0.1204,
            0.1832,
            0.1838
          ],
          "after": [
            0.0479,
            -0.1451,
            -0.288,
            0.0595,
            0.032,
            -0.0538,
            -0.1538,
            0.069,
            0.1058,
            0.0115,
            0.0212,
            0.0927,
            0.0475,
            0.0118,
            -0.1365,
            0.0456
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.001,
            -0.0009,
            0.0013,
            0.0003,
            0.0013,
            -0.0003,
            -0.0004,
            0.0001,
            0.0003,
            -0.0006,
            0.0014,
            -0.0017,
            -0.0006,
            0.0014,
            0.0016
          ],
          "after": [
            -0.0878,
            0.0105,
            -0.1093,
            -0.0761,
            0.0711,
            0.1319,
            0.0757,
            -0.1589,
            0.024,
            -0.0288,
            -0.1341,
            -0.0708,
            0.0941,
            -0.0807,
            0.0413,
            0.0827
          ]
        }
      }
    },
    {
      "step": 698,
      "word": "rita",
      "loss": 2.6223,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.49,
            -0.0915,
            -0.3906,
            -0.14,
            -0.2674,
            -0.0399,
            -0.0727,
            0.059,
            -0.1962,
            0.2634,
            0.1193,
            -0.5234,
            0.2949,
            0.2437,
            0.1196,
            -0.2192
          ],
          "after": [
            0.0069,
            -0.0065,
            0.1563,
            -0.0255,
            0.0273,
            -0.0882,
            -0.1145,
            -0.1423,
            -0.0235,
            0.0841,
            0.1574,
            0.0059,
            -0.0907,
            -0.0106,
            -0.0416,
            -0.1912
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0084,
            0.0745,
            -0.1553,
            0.0278,
            -0.059,
            -0.1187,
            0.1982,
            -0.0156,
            -0.1804,
            0.0411,
            -0.0133,
            -0.099,
            0.1473,
            -0.0322,
            -0.1455,
            0.0545
          ],
          "after": [
            0.0173,
            0.1102,
            -0.0341,
            -0.0748,
            0.0413,
            -0.0142,
            0.091,
            0.0938,
            -0.0696,
            -0.075,
            0.0098,
            -0.0065,
            0.0995,
            0.0314,
            -0.0546,
            0.224
          ]
        },
        "position_0": {
          "grad": [
            0.0061,
            -0.0597,
            -0.0106,
            -0.0729,
            -0.0793,
            0.184,
            0.055,
            -0.0579,
            0.2068,
            -0.1192,
            -0.1782,
            0.0382,
            -0.2613,
            -0.1115,
            0.1969,
            0.1302
          ],
          "after": [
            0.048,
            -0.1452,
            -0.2878,
            0.0594,
            0.0321,
            -0.0544,
            -0.1541,
            0.0693,
            0.1057,
            0.0118,
            0.0215,
            0.0923,
            0.0477,
            0.0124,
            -0.1369,
            0.0452
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0027,
            0.0009,
            -0.0044,
            0.0025,
            -0.0014,
            -0.0021,
            0.0003,
            0.0002,
            -0.0004,
            -0.0006,
            -0.0005,
            -0.0013,
            0.0006,
            0.0003,
            -0.0002
          ],
          "after": [
            -0.0878,
            0.0104,
            -0.1096,
            -0.076,
            0.0706,
            0.1323,
            0.0754,
            -0.1589,
            0.0241,
            -0.0291,
            -0.1342,
            -0.0711,
            0.0941,
            -0.0804,
            0.0416,
            0.0823
          ]
        }
      }
    },
    {
      "step": 699,
      "word": "isaura",
      "loss": 2.5942,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1676,
            -0.0758,
            -0.0638,
            -0.1534,
            0.1628,
            -0.2201,
            -0.2377,
            0.004,
            -0.0798,
            0.1258,
            0.1637,
            -0.2491,
            0.0851,
            -0.075,
            -0.0945,
            -0.1283
          ],
          "after": [
            0.0068,
            -0.0065,
            0.1562,
            -0.0253,
            0.0271,
            -0.0879,
            -0.1147,
            -0.1425,
            -0.0229,
            0.0838,
            0.1573,
            0.0062,
            -0.0913,
            -0.0102,
            -0.0412,
            -0.1914
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0111,
            0.0076,
            -0.0638,
            -0.0219,
            0.0691,
            -0.1393,
            0.1123,
            -0.0001,
            -0.1157,
            0.0318,
            0.0085,
            -0.0014,
            0.0381,
            -0.0605,
            -0.1525,
            0.0401
          ],
          "after": [
            0.0173,
            0.11,
            -0.0336,
            -0.0748,
            0.0413,
            -0.0138,
            0.0906,
            0.0938,
            -0.0691,
            -0.0752,
            0.0099,
            -0.0063,
            0.0991,
            0.0316,
            -0.0541,
            0.2237
          ]
        },
        "position_0": {
          "grad": [
            -0.0383,
            -0.1973,
            0.0744,
            -0.1361,
            0.0659,
            0.1052,
            -0.2036,
            0.0234,
            0.0581,
            -0.0018,
            0.0215,
            -0.1495,
            -0.1609,
            0.0472,
            0.0145,
            -0.1199
          ],
          "after": [
            0.0483,
            -0.1451,
            -0.2878,
            0.0595,
            0.0321,
            -0.055,
            -0.1542,
            0.0695,
            0.1055,
            0.0121,
            0.0218,
            0.0922,
            0.048,
            0.0127,
            -0.1373,
            0.045
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            0.0001,
            -0.0017,
            0.0002,
            -0.0001,
            -0.0002,
            0.0013,
            0.0011,
            0.0,
            -0.0008,
            -0.0011,
            -0.0004,
            0.0009,
            -0.0003,
            0.0005,
            0.0011
          ],
          "after": [
            -0.0878,
            0.0103,
            -0.1097,
            -0.0759,
            0.0701,
            0.1326,
            0.0751,
            -0.159,
            0.0243,
            -0.0292,
            -0.1343,
            -0.0713,
            0.0941,
            -0.0802,
            0.0418,
            0.0819
          ]
        }
      }
    },
    {
      "step": 700,
      "word": "neely",
      "loss": 2.2983,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0066,
            -0.0066,
            0.1561,
            -0.0251,
            0.0269,
            -0.0877,
            -0.1149,
            -0.1427,
            -0.0225,
            0.0836,
            0.1572,
            0.0064,
            -0.0917,
            -0.0099,
            -0.0409,
            -0.1915
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1098,
            0.1534,
            0.1757,
            0.0609,
            0.142,
            0.5157,
            -0.5364,
            -0.2021,
            0.5372,
            -0.1883,
            0.0353,
            0.4272,
            -0.5658,
            0.1231,
            0.5081,
            -0.1403
          ],
          "after": [
            0.0171,
            0.1097,
            -0.0334,
            -0.075,
            0.0413,
            -0.0137,
            0.0905,
            0.0941,
            -0.069,
            -0.0751,
            0.0099,
            -0.0065,
            0.0993,
            0.0317,
            -0.0541,
            0.2236
          ]
        },
        "position_0": {
          "grad": [
            -0.0403,
            0.0931,
            -0.0989,
            -0.0191,
            -0.3629,
            0.0206,
            -0.06,
            -0.0203,
            0.123,
            -0.2092,
            -0.2382,
            -0.0881,
            -0.0507,
            0.1477,
            0.1825,
            0.0826
          ],
          "after": [
            0.0486,
            -0.1451,
            -0.2876,
            0.0597,
            0.0326,
            -0.0556,
            -0.1542,
            0.0697,
            0.1052,
            0.0127,
            0.0223,
            0.0922,
            0.0483,
            0.0127,
            -0.1379,
            0.0447
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0028,
            0.001,
            0.0009,
            -0.0005,
            -0.0016,
            -0.0023,
            -0.0026,
            -0.0008,
            0.0013,
            -0.0001,
            0.0012,
            -0.0037,
            0.002,
            0.0028,
            -0.0028,
            -0.0009
          ],
          "after": [
            -0.0881,
            0.0101,
            -0.1098,
            -0.0758,
            0.0699,
            0.1331,
            0.075,
            -0.159,
            0.0242,
            -0.0293,
            -0.1344,
            -0.0712,
            0.0939,
            -0.0802,
            0.0422,
            0.0816
          ]
        }
      }
    },
    {
      "step": 701,
      "word": "dheeran",
      "loss": 2.1677,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1735,
            0.1558,
            -0.0412,
            0.0784,
            -0.0942,
            -0.0403,
            0.0053,
            -0.081,
            0.1092,
            -0.2597,
            -0.1349,
            0.1601,
            -0.1239,
            -0.0251,
            0.0762,
            0.1885
          ],
          "after": [
            0.0067,
            -0.0069,
            0.1561,
            -0.0251,
            0.0269,
            -0.0874,
            -0.1151,
            -0.1426,
            -0.0223,
            0.0837,
            0.1572,
            0.0064,
            -0.092,
            -0.0096,
            -0.0408,
            -0.1918
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1844,
            0.0467,
            0.1253,
            0.0162,
            0.0829,
            0.3202,
            -0.1168,
            -0.0244,
            0.2126,
            -0.2009,
            0.0231,
            0.3977,
            -0.3622,
            -0.1583,
            0.5902,
            -0.1599
          ],
          "after": [
            0.0172,
            0.1094,
            -0.0333,
            -0.0751,
            0.0412,
            -0.0138,
            0.0905,
            0.0945,
            -0.069,
            -0.0748,
            0.0098,
            -0.0069,
            0.0996,
            0.0319,
            -0.0544,
            0.2236
          ]
        },
        "position_0": {
          "grad": [
            -0.0479,
            0.0136,
            -0.0279,
            -0.0665,
            0.0432,
            -0.075,
            0.0115,
            -0.0528,
            0.0274,
            -0.0406,
            -0.0645,
            -0.0168,
            0.0017,
            0.0487,
            0.0653,
            0.0811
          ],
          "after": [
            0.0489,
            -0.1451,
            -0.2874,
            0.0599,
            0.0329,
            -0.0559,
            -0.1542,
            0.0699,
            0.1049,
            0.0133,
            0.0228,
            0.0922,
            0.0486,
            0.0126,
            -0.1385,
            0.0444
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0024,
            0.0004,
            0.0011,
            -0.0002,
            0.0006,
            0.0008,
            -0.0003,
            -0.0002,
            -0.0008,
            -0.0007,
            0.0012,
            0.0023,
            -0.0019,
            -0.0019,
            0.0025,
            -0.0013
          ],
          "after": [
            -0.0881,
            0.0099,
            -0.1101,
            -0.0757,
            0.0696,
            0.1335,
            0.0749,
            -0.159,
            0.0243,
            -0.0293,
            -0.1346,
            -0.0713,
            0.0939,
            -0.0801,
            0.0424,
            0.0814
          ]
        }
      }
    },
    {
      "step": 702,
      "word": "ajna",
      "loss": 3.1355,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.4188,
            0.1195,
            0.0755,
            0.1268,
            -0.1319,
            -0.5183,
            0.2554,
            0.3548,
            -0.5317,
            0.1993,
            -0.1308,
            -0.45,
            1.0685,
            0.1067,
            -0.1917,
            0.0149
          ],
          "after": [
            0.0064,
            -0.0072,
            0.156,
            -0.0252,
            0.0269,
            -0.0865,
            -0.1155,
            -0.1432,
            -0.0214,
            0.0836,
            0.1574,
            0.0069,
            -0.093,
            -0.0094,
            -0.0404,
            -0.192
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0174,
            0.0238,
            -0.133,
            -0.0416,
            0.0036,
            -0.1542,
            0.1649,
            0.0471,
            -0.1679,
            0.0334,
            -0.0537,
            -0.0522,
            0.1325,
            -0.0001,
            -0.1208,
            0.0801
          ],
          "after": [
            0.0172,
            0.1092,
            -0.0331,
            -0.0752,
            0.0411,
            -0.0138,
            0.0904,
            0.0947,
            -0.069,
            -0.0746,
            0.0099,
            -0.0073,
            0.0999,
            0.0321,
            -0.0546,
            0.2235
          ]
        },
        "position_0": {
          "grad": [
            0.1257,
            -0.0293,
            0.1353,
            -0.1476,
            0.0166,
            0.0105,
            -0.356,
            -0.1235,
            0.1585,
            -0.0036,
            0.2667,
            0.0538,
            -0.1765,
            0.0872,
            -0.0206,
            -0.2385
          ],
          "after": [
            0.0489,
            -0.1451,
            -0.2875,
            0.0602,
            0.0332,
            -0.0563,
            -0.154,
            0.0704,
            0.1044,
            0.0138,
            0.0229,
            0.0922,
            0.049,
            0.0124,
            -0.139,
            0.0443
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            0.0008,
            -0.0019,
            -0.0033,
            0.0024,
            -0.0012,
            0.0013,
            0.0041,
            0.0026,
            -0.0042,
            -0.0066,
            0.0003,
            0.0029,
            0.0028,
            0.0002,
            0.0052
          ],
          "after": [
            -0.088,
            0.0096,
            -0.1101,
            -0.0753,
            0.0693,
            0.1339,
            0.0747,
            -0.1594,
            0.024,
            -0.029,
            -0.1342,
            -0.0714,
            0.0936,
            -0.0802,
            0.0425,
            0.0809
          ]
        }
      }
    },
    {
      "step": 703,
      "word": "annuel",
      "loss": 2.5642,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0243,
            0.1938,
            -0.2937,
            0.154,
            -0.5882,
            0.0039,
            -0.0124,
            -0.0367,
            -0.0164,
            -0.3205,
            -0.309,
            -0.1548,
            0.0815,
            0.2484,
            0.3582,
            0.152
          ],
          "after": [
            0.0061,
            -0.0077,
            0.1562,
            -0.0255,
            0.0275,
            -0.0858,
            -0.1159,
            -0.1435,
            -0.0207,
            0.0838,
            0.1579,
            0.0073,
            -0.0939,
            -0.0096,
            -0.0405,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2225,
            0.0276,
            0.0022,
            0.152,
            -0.1167,
            0.2802,
            0.2276,
            -0.0359,
            0.0052,
            -0.2462,
            -0.2318,
            0.1135,
            -0.0758,
            -0.0471,
            0.1672,
            0.1411
          ],
          "after": [
            0.0175,
            0.1089,
            -0.0329,
            -0.0755,
            0.0411,
            -0.014,
            0.0903,
            0.0949,
            -0.0689,
            -0.0742,
            0.0103,
            -0.0077,
            0.1001,
            0.0322,
            -0.0548,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            0.0851,
            -0.0491,
            0.0945,
            -0.1123,
            -0.0094,
            0.0086,
            -0.2807,
            -0.0797,
            0.1448,
            0.0001,
            0.1691,
            0.0232,
            -0.157,
            0.0454,
            -0.0095,
            -0.2106
          ],
          "after": [
            0.0488,
            -0.145,
            -0.2877,
            0.0607,
            0.0334,
            -0.0566,
            -0.1536,
            0.071,
            0.1039,
            0.0142,
            0.0228,
            0.0921,
            0.0495,
            0.0121,
            -0.1394,
            0.0445
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0017,
            0.0012,
            -0.0009,
            0.0011,
            -0.0019,
            -0.0014,
            -0.0002,
            -0.0015,
            0.0002,
            0.001,
            -0.0025,
            0.0007,
            -0.0001,
            -0.0005,
            -0.0035
          ],
          "after": [
            -0.0881,
            0.0096,
            -0.1102,
            -0.0749,
            0.0689,
            0.1343,
            0.0747,
            -0.1598,
            0.024,
            -0.0288,
            -0.134,
            -0.0713,
            0.0934,
            -0.0804,
            0.0426,
            0.0807
          ]
        }
      }
    },
    {
      "step": 704,
      "word": "haila",
      "loss": 2.0142,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3306,
            -0.4073,
            -0.1347,
            -0.1928,
            0.0801,
            0.1228,
            -0.2695,
            0.2023,
            -0.1421,
            0.2268,
            0.2542,
            -0.5708,
            0.0278,
            0.1958,
            -0.0348,
            -0.3652
          ],
          "after": [
            0.0055,
            -0.0077,
            0.1564,
            -0.0255,
            0.0278,
            -0.0853,
            -0.1159,
            -0.1441,
            -0.0199,
            0.0838,
            0.1581,
            0.0082,
            -0.0947,
            -0.01,
            -0.0406,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0425,
            0.1035,
            -0.1588,
            0.0455,
            -0.0274,
            -0.1488,
            0.2461,
            -0.0032,
            -0.21,
            0.0178,
            -0.0112,
            -0.0517,
            0.1025,
            -0.0837,
            -0.1351,
            0.146
          ],
          "after": [
            0.0177,
            0.1087,
            -0.0327,
            -0.0758,
            0.0411,
            -0.0141,
            0.09,
            0.0951,
            -0.0687,
            -0.0739,
            0.0106,
            -0.008,
            0.1002,
            0.0325,
            -0.0549,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            0.0245,
            -0.1117,
            0.1236,
            -0.0293,
            -0.0615,
            0.1134,
            -0.1725,
            0.0761,
            0.2149,
            0.138,
            -0.1182,
            -0.2121,
            0.1576,
            0.1713,
            0.0323,
            -0.005
          ],
          "after": [
            0.0485,
            -0.1448,
            -0.2882,
            0.0611,
            0.0337,
            -0.057,
            -0.1531,
            0.0713,
            0.1032,
            0.0143,
            0.0228,
            0.0923,
            0.0498,
            0.0115,
            -0.1397,
            0.0446
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0006,
            -0.0001,
            0.0003,
            -0.0004,
            0.0012,
            -0.0011,
            0.001,
            0.0004,
            -0.0006,
            -0.0007,
            0.001,
            -0.0008,
            -0.0001,
            0.0011,
            0.0005
          ],
          "after": [
            -0.0881,
            0.0096,
            -0.1103,
            -0.0746,
            0.0685,
            0.1347,
            0.0747,
            -0.1602,
            0.0239,
            -0.0286,
            -0.1337,
            -0.0713,
            0.0932,
            -0.0804,
            0.0427,
            0.0805
          ]
        }
      }
    },
    {
      "step": 705,
      "word": "tevion",
      "loss": 2.2778,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0051,
            -0.0076,
            0.1566,
            -0.0255,
            0.0282,
            -0.0849,
            -0.1159,
            -0.1447,
            -0.0193,
            0.0837,
            0.1582,
            0.0089,
            -0.0954,
            -0.0103,
            -0.0407,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1716,
            -0.3193,
            0.3489,
            -0.1478,
            0.2421,
            0.1039,
            -0.3692,
            0.0446,
            0.3298,
            0.1975,
            0.0609,
            0.2619,
            -0.2884,
            0.04,
            -0.0436,
            -0.0627
          ],
          "after": [
            0.0177,
            0.1087,
            -0.0327,
            -0.0758,
            0.041,
            -0.0142,
            0.09,
            0.0952,
            -0.0688,
            -0.0738,
            0.0108,
            -0.0084,
            0.1005,
            0.0326,
            -0.055,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.0085,
            -0.1716,
            0.0726,
            0.042,
            -0.1053,
            0.0369,
            -0.0157,
            -0.0117,
            -0.0662,
            -0.1179,
            -0.0546,
            -0.2662,
            0.1068,
            0.0812,
            0.0988,
            0.088
          ],
          "after": [
            0.0484,
            -0.1443,
            -0.2887,
            0.0614,
            0.034,
            -0.0574,
            -0.1526,
            0.0716,
            0.1027,
            0.0146,
            0.0229,
            0.0928,
            0.0499,
            0.0109,
            -0.1402,
            0.0446
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0024,
            -0.0007,
            0.0,
            -0.0013,
            0.0005,
            0.0012,
            -0.0004,
            0.0009,
            0.0014,
            0.0001,
            -0.0005,
            0.0039,
            -0.0028,
            -0.0014,
            0.002,
            0.001
          ],
          "after": [
            -0.0879,
            0.0097,
            -0.1104,
            -0.0743,
            0.0682,
            0.1348,
            0.0747,
            -0.1606,
            0.0237,
            -0.0284,
            -0.1334,
            -0.0715,
            0.0933,
            -0.0804,
            0.0426,
            0.0803
          ]
        }
      }
    },
    {
      "step": 706,
      "word": "nollie",
      "loss": 2.2172,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0047,
            -0.0076,
            0.1568,
            -0.0255,
            0.0284,
            -0.0846,
            -0.1159,
            -0.1451,
            -0.0187,
            0.0837,
            0.1583,
            0.0095,
            -0.096,
            -0.0106,
            -0.0407,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0545,
            0.1604,
            -0.1676,
            0.1327,
            -0.62,
            -0.06,
            0.0488,
            -0.0303,
            0.0349,
            -0.0536,
            0.0524,
            -0.0287,
            0.16,
            -0.0435,
            0.3674,
            0.0836
          ],
          "after": [
            0.0177,
            0.1085,
            -0.0326,
            -0.0761,
            0.0412,
            -0.0143,
            0.0899,
            0.0954,
            -0.0688,
            -0.0736,
            0.0109,
            -0.0088,
            0.1007,
            0.0328,
            -0.0553,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0474,
            0.0945,
            -0.0887,
            0.0238,
            -0.2643,
            0.0382,
            0.0016,
            -0.0214,
            0.0618,
            -0.1685,
            -0.1704,
            -0.0205,
            -0.0064,
            0.0953,
            0.1299,
            0.1107
          ],
          "after": [
            0.0484,
            -0.1441,
            -0.289,
            0.0617,
            0.0346,
            -0.0579,
            -0.1523,
            0.0719,
            0.1022,
            0.0152,
            0.0232,
            0.0933,
            0.0501,
            0.0101,
            -0.1408,
            0.0446
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0001,
            0.0007,
            0.0005,
            -0.0018,
            -0.0009,
            -0.001,
            0.0001,
            0.0003,
            0.0001,
            0.0007,
            0.0001,
            0.0004,
            0.0,
            -0.0,
            0.0003
          ],
          "after": [
            -0.0877,
            0.0097,
            -0.1105,
            -0.074,
            0.0681,
            0.1351,
            0.0748,
            -0.161,
            0.0235,
            -0.0283,
            -0.1332,
            -0.0718,
            0.0934,
            -0.0803,
            0.0426,
            0.0801
          ]
        }
      }
    },
    {
      "step": 707,
      "word": "gian",
      "loss": 2.6738,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1767,
            0.1362,
            -0.2992,
            0.1021,
            -0.4402,
            -0.0374,
            -0.1509,
            -0.1187,
            0.0828,
            -0.0453,
            -0.1446,
            -0.1255,
            0.0296,
            0.29,
            0.062,
            0.0463
          ],
          "after": [
            0.0042,
            -0.0077,
            0.1572,
            -0.0257,
            0.029,
            -0.0842,
            -0.1158,
            -0.1453,
            -0.0183,
            0.0837,
            0.1586,
            0.0101,
            -0.0965,
            -0.0112,
            -0.0408,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0091,
            0.0279,
            -0.1828,
            -0.046,
            -0.0138,
            -0.0912,
            0.127,
            0.0136,
            -0.1786,
            0.0891,
            0.0035,
            -0.0455,
            0.1081,
            -0.0706,
            -0.0948,
            0.0838
          ],
          "after": [
            0.0178,
            0.1084,
            -0.0324,
            -0.0762,
            0.0415,
            -0.0143,
            0.0898,
            0.0955,
            -0.0688,
            -0.0736,
            0.0109,
            -0.009,
            0.1007,
            0.033,
            -0.0555,
            0.2226
          ]
        },
        "position_0": {
          "grad": [
            0.0406,
            0.0079,
            0.029,
            0.1386,
            -0.1792,
            -0.2022,
            -0.0511,
            0.023,
            -0.0891,
            0.022,
            0.0894,
            -0.2555,
            0.0539,
            0.12,
            -0.0319,
            0.0139
          ],
          "after": [
            0.0482,
            -0.1439,
            -0.2893,
            0.0617,
            0.0354,
            -0.0579,
            -0.1519,
            0.0721,
            0.1019,
            0.0156,
            0.0234,
            0.094,
            0.0501,
            0.0093,
            -0.1412,
            0.0445
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            0.0005,
            0.0003,
            -0.0007,
            0.0,
            0.0001,
            -0.0014,
            -0.0018,
            -0.0006,
            -0.0006,
            0.0001,
            -0.0023,
            0.0004,
            0.0023,
            -0.001,
            -0.0009
          ],
          "after": [
            -0.0877,
            0.0097,
            -0.1107,
            -0.0737,
            0.068,
            0.1352,
            0.075,
            -0.1612,
            0.0234,
            -0.0281,
            -0.1331,
            -0.0718,
            0.0934,
            -0.0805,
            0.0426,
            0.08
          ]
        }
      }
    },
    {
      "step": 708,
      "word": "brookelynn",
      "loss": 2.7947,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0038,
            -0.0078,
            0.1575,
            -0.0258,
            0.0295,
            -0.084,
            -0.1157,
            -0.1455,
            -0.018,
            0.0838,
            0.1588,
            0.0106,
            -0.0969,
            -0.0116,
            -0.0409,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0457,
            -0.1042,
            -0.0385,
            -0.0024,
            -0.191,
            0.0628,
            -0.1483,
            -0.0912,
            0.037,
            0.0438,
            0.0878,
            -0.0675,
            0.0865,
            0.0623,
            0.0163,
            -0.0899
          ],
          "after": [
            0.0177,
            0.1084,
            -0.0322,
            -0.0763,
            0.0418,
            -0.0143,
            0.0898,
            0.0957,
            -0.0688,
            -0.0737,
            0.0109,
            -0.0092,
            0.1007,
            0.0332,
            -0.0556,
            0.2225
          ]
        },
        "position_0": {
          "grad": [
            0.0156,
            0.0124,
            0.0503,
            0.0225,
            0.037,
            0.0273,
            0.0942,
            0.0098,
            -0.0864,
            -0.0048,
            0.0229,
            0.0452,
            0.0971,
            -0.1143,
            -0.0444,
            0.1745
          ],
          "after": [
            0.0481,
            -0.1438,
            -0.2896,
            0.0617,
            0.0359,
            -0.058,
            -0.1517,
            0.0723,
            0.1017,
            0.016,
            0.0234,
            0.0946,
            0.05,
            0.0088,
            -0.1415,
            0.0443
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0002,
            -0.0008,
            0.0001,
            -0.0026,
            0.0,
            -0.0005,
            0.0,
            -0.0015,
            0.0006,
            0.0005,
            -0.0005,
            0.0017,
            0.0015,
            0.0016,
            -0.0019
          ],
          "after": [
            -0.0876,
            0.0097,
            -0.1107,
            -0.0735,
            0.0681,
            0.1354,
            0.0751,
            -0.1613,
            0.0234,
            -0.028,
            -0.133,
            -0.0718,
            0.0932,
            -0.0808,
            0.0425,
            0.08
          ]
        }
      }
    },
    {
      "step": 709,
      "word": "kaylor",
      "loss": 1.9784,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0747,
            -0.2002,
            0.077,
            -0.0999,
            0.0781,
            0.1131,
            -0.0884,
            0.0777,
            -0.083,
            0.1336,
            0.0896,
            0.0985,
            0.0049,
            0.0303,
            -0.0496,
            -0.0228
          ],
          "after": [
            0.0034,
            -0.0077,
            0.1577,
            -0.0257,
            0.0298,
            -0.0839,
            -0.1155,
            -0.1457,
            -0.0176,
            0.0836,
            0.1589,
            0.011,
            -0.0973,
            -0.0121,
            -0.041,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1111,
            0.1654,
            -0.1161,
            0.0403,
            0.0028,
            -0.2153,
            0.3425,
            0.0228,
            -0.2345,
            -0.001,
            -0.0225,
            -0.0013,
            0.0993,
            -0.13,
            -0.1412,
            0.213
          ],
          "after": [
            0.0178,
            0.1083,
            -0.032,
            -0.0764,
            0.0421,
            -0.0142,
            0.0896,
            0.0959,
            -0.0686,
            -0.0737,
            0.0109,
            -0.0094,
            0.1006,
            0.0334,
            -0.0557,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            -0.0784,
            0.0526,
            -0.1025,
            -0.0008,
            0.0864,
            -0.0683,
            0.159,
            -0.0214,
            -0.0536,
            0.0341,
            0.0167,
            0.0186,
            0.0725,
            -0.0515,
            -0.1514,
            -0.0313
          ],
          "after": [
            0.0482,
            -0.1437,
            -0.2897,
            0.0616,
            0.0363,
            -0.0579,
            -0.1516,
            0.0725,
            0.1016,
            0.0162,
            0.0235,
            0.095,
            0.0499,
            0.0085,
            -0.1415,
            0.0441
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            0.0006,
            0.0006,
            0.0008,
            -0.0003,
            0.0017,
            0.0011,
            0.001,
            0.0006,
            -0.0007,
            -0.0008,
            0.0021,
            -0.0002,
            -0.0012,
            -0.0005,
            0.0016
          ],
          "after": [
            -0.0875,
            0.0096,
            -0.1108,
            -0.0734,
            0.0682,
            0.1354,
            0.0752,
            -0.1615,
            0.0234,
            -0.0279,
            -0.1328,
            -0.072,
            0.0931,
            -0.0809,
            0.0424,
            0.0799
          ]
        }
      }
    },
    {
      "step": 710,
      "word": "jirah",
      "loss": 2.2733,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0316,
            -0.0688,
            0.2169,
            0.0436,
            0.0879,
            0.1189,
            -0.0187,
            0.147,
            0.1291,
            0.1841,
            -0.0141,
            -0.1414,
            0.2483,
            0.083,
            -0.1347,
            -0.0008
          ],
          "after": [
            0.0031,
            -0.0076,
            0.1577,
            -0.0258,
            0.0301,
            -0.0839,
            -0.1153,
            -0.1462,
            -0.0175,
            0.0833,
            0.1589,
            0.0114,
            -0.0978,
            -0.0125,
            -0.0408,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0743,
            0.0878,
            -0.1588,
            0.0224,
            0.0531,
            -0.1533,
            0.2892,
            0.0559,
            -0.2427,
            0.0098,
            -0.0824,
            -0.0095,
            0.1138,
            -0.0631,
            -0.1891,
            0.1575
          ],
          "after": [
            0.0179,
            0.1081,
            -0.0316,
            -0.0766,
            0.0423,
            -0.014,
            0.0894,
            0.0959,
            -0.0684,
            -0.0737,
            0.0109,
            -0.0095,
            0.1005,
            0.0336,
            -0.0556,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            0.0399,
            0.0236,
            0.0352,
            0.1326,
            -0.0192,
            -0.1527,
            0.1364,
            0.0966,
            -0.2061,
            -0.0741,
            0.0319,
            -0.0515,
            0.3411,
            -0.1142,
            -0.1214,
            0.0973
          ],
          "after": [
            0.0481,
            -0.1437,
            -0.2899,
            0.0615,
            0.0367,
            -0.0576,
            -0.1517,
            0.0724,
            0.1018,
            0.0165,
            0.0235,
            0.0954,
            0.0495,
            0.0085,
            -0.1414,
            0.0439
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.001,
            0.0005,
            -0.002,
            0.0,
            -0.0006,
            -0.0025,
            -0.0007,
            0.0009,
            0.0002,
            0.0011,
            -0.001,
            -0.0008,
            0.0006,
            0.0017,
            -0.0017
          ],
          "after": [
            -0.0874,
            0.0097,
            -0.111,
            -0.0731,
            0.0683,
            0.1354,
            0.0754,
            -0.1616,
            0.0233,
            -0.0278,
            -0.1328,
            -0.072,
            0.0931,
            -0.0811,
            0.0423,
            0.08
          ]
        }
      }
    },
    {
      "step": 711,
      "word": "temiloluwa",
      "loss": 2.838,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0795,
            -0.0083,
            -0.0759,
            -0.0567,
            -0.0196,
            -0.0392,
            -0.053,
            -0.0197,
            -0.0269,
            0.0471,
            0.0599,
            -0.0715,
            0.0058,
            0.0537,
            0.0175,
            -0.1057
          ],
          "after": [
            0.0028,
            -0.0074,
            0.1578,
            -0.0257,
            0.0303,
            -0.0839,
            -0.1151,
            -0.1465,
            -0.0173,
            0.083,
            0.159,
            0.0118,
            -0.0982,
            -0.013,
            -0.0407,
            -0.1924
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0293,
            -0.0978,
            0.2191,
            -0.0348,
            0.2479,
            -0.0609,
            -0.1049,
            0.0177,
            0.1254,
            0.0721,
            0.0597,
            0.2258,
            -0.2012,
            -0.0531,
            -0.1412,
            0.1127
          ],
          "after": [
            0.018,
            0.108,
            -0.0315,
            -0.0767,
            0.0423,
            -0.0138,
            0.0892,
            0.0959,
            -0.0682,
            -0.0739,
            0.0109,
            -0.0097,
            0.1005,
            0.0339,
            -0.0555,
            0.2216
          ]
        },
        "position_0": {
          "grad": [
            -0.0145,
            -0.0828,
            0.0351,
            0.0379,
            -0.0212,
            0.0312,
            0.0313,
            -0.0138,
            -0.0725,
            -0.0335,
            -0.0139,
            -0.1214,
            0.0862,
            0.0089,
            0.0149,
            0.07
          ],
          "after": [
            0.0481,
            -0.1436,
            -0.2901,
            0.0612,
            0.037,
            -0.0574,
            -0.1517,
            0.0724,
            0.102,
            0.0169,
            0.0235,
            0.096,
            0.049,
            0.0084,
            -0.1414,
            0.0437
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            0.0027,
            -0.0001,
            -0.0023,
            0.0013,
            -0.0029,
            -0.0024,
            -0.0018,
            0.0021,
            -0.0001,
            0.0007,
            0.001,
            0.0014,
            0.0007,
            0.0008,
            0.0027
          ],
          "after": [
            -0.0873,
            0.0095,
            -0.1111,
            -0.0727,
            0.0682,
            0.1357,
            0.0757,
            -0.1615,
            0.023,
            -0.0277,
            -0.1329,
            -0.0722,
            0.093,
            -0.0812,
            0.0421,
            0.0798
          ]
        }
      }
    },
    {
      "step": 712,
      "word": "alekhya",
      "loss": 2.4181,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0425,
            0.1402,
            -0.1531,
            -0.1615,
            0.0094,
            -0.0442,
            -0.0948,
            0.036,
            -0.1777,
            0.0458,
            -0.0914,
            0.1929,
            -0.0938,
            0.0789,
            0.0669,
            -0.3899
          ],
          "after": [
            0.0025,
            -0.0074,
            0.158,
            -0.0255,
            0.0305,
            -0.0839,
            -0.1148,
            -0.1468,
            -0.017,
            0.0827,
            0.1591,
            0.012,
            -0.0985,
            -0.0135,
            -0.0407,
            -0.192
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0337,
            -0.2038,
            0.1987,
            0.0295,
            0.2572,
            0.1876,
            -0.2129,
            0.0924,
            0.1854,
            0.1614,
            0.011,
            0.2421,
            -0.2578,
            -0.0462,
            0.01,
            -0.145
          ],
          "after": [
            0.0181,
            0.1081,
            -0.0316,
            -0.0768,
            0.0421,
            -0.0138,
            0.0891,
            0.0958,
            -0.0682,
            -0.0741,
            0.0109,
            -0.0102,
            0.1007,
            0.0342,
            -0.0554,
            0.2214
          ]
        },
        "position_0": {
          "grad": [
            0.0621,
            0.0006,
            0.0741,
            -0.1144,
            0.0786,
            0.0087,
            -0.2117,
            -0.1039,
            0.0808,
            0.0722,
            0.2007,
            0.0667,
            -0.1072,
            -0.0107,
            -0.1003,
            -0.1676
          ],
          "after": [
            0.048,
            -0.1435,
            -0.2904,
            0.0612,
            0.0372,
            -0.0572,
            -0.1516,
            0.0726,
            0.1021,
            0.017,
            0.0233,
            0.0963,
            0.0487,
            0.0084,
            -0.1412,
            0.0436
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            0.0002,
            -0.0001,
            0.0006,
            0.0003,
            -0.0001,
            -0.0005,
            0.0001,
            -0.0004,
            0.0006,
            0.0005,
            -0.0005,
            -0.0003,
            0.0004,
            -0.0013,
            -0.0009
          ],
          "after": [
            -0.0875,
            0.0093,
            -0.1111,
            -0.0724,
            0.0682,
            0.1359,
            0.076,
            -0.1614,
            0.0228,
            -0.0276,
            -0.133,
            -0.0722,
            0.0929,
            -0.0814,
            0.042,
            0.0797
          ]
        }
      }
    },
    {
      "step": 713,
      "word": "edil",
      "loss": 2.5843,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0023,
            -0.0074,
            0.1581,
            -0.0253,
            0.0306,
            -0.0839,
            -0.1146,
            -0.1471,
            -0.0167,
            0.0824,
            0.1592,
            0.0122,
            -0.0988,
            -0.0139,
            -0.0407,
            -0.1917
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3615,
            0.6514,
            0.6137,
            0.022,
            0.0639,
            -0.0568,
            0.5734,
            -0.0792,
            0.0097,
            -0.39,
            -0.0047,
            -0.1006,
            -0.0919,
            0.0371,
            -0.0025,
            -0.0054
          ],
          "after": [
            0.0184,
            0.1077,
            -0.032,
            -0.0769,
            0.042,
            -0.0137,
            0.0889,
            0.0958,
            -0.0682,
            -0.0739,
            0.0109,
            -0.0104,
            0.1009,
            0.0343,
            -0.0553,
            0.2213
          ]
        },
        "position_0": {
          "grad": [
            -0.0027,
            -0.221,
            0.0779,
            0.0312,
            -0.0619,
            0.0946,
            -0.2821,
            -0.088,
            0.15,
            0.1314,
            0.0072,
            -0.0625,
            -0.168,
            0.0227,
            0.0707,
            -0.3574
          ],
          "after": [
            0.0479,
            -0.1431,
            -0.2908,
            0.0611,
            0.0374,
            -0.0572,
            -0.1513,
            0.073,
            0.102,
            0.017,
            0.0232,
            0.0967,
            0.0487,
            0.0083,
            -0.1411,
            0.0438
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            -0.0008,
            0.0005,
            -0.0,
            -0.0012,
            0.0008,
            -0.0004,
            -0.0004,
            -0.0,
            0.001,
            0.0019,
            0.0003,
            -0.0009,
            -0.0012,
            0.0004,
            0.0006
          ],
          "after": [
            -0.0874,
            0.0092,
            -0.1113,
            -0.0721,
            0.0683,
            0.1361,
            0.0763,
            -0.1613,
            0.0226,
            -0.0277,
            -0.1332,
            -0.0723,
            0.0929,
            -0.0815,
            0.042,
            0.0796
          ]
        }
      }
    },
    {
      "step": 714,
      "word": "elliot",
      "loss": 2.5504,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0021,
            -0.0074,
            0.1582,
            -0.0251,
            0.0307,
            -0.0838,
            -0.1144,
            -0.1474,
            -0.0164,
            0.0822,
            0.1592,
            0.0123,
            -0.099,
            -0.0142,
            -0.0407,
            -0.1914
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3149,
            0.5171,
            0.4267,
            0.0238,
            0.0649,
            -0.0817,
            0.5036,
            -0.0806,
            -0.0374,
            -0.2845,
            0.0143,
            -0.0665,
            -0.0358,
            0.0032,
            -0.078,
            0.0686
          ],
          "after": [
            0.019,
            0.107,
            -0.0326,
            -0.0771,
            0.0418,
            -0.0136,
            0.0885,
            0.0959,
            -0.0681,
            -0.0735,
            0.0109,
            -0.0106,
            0.1012,
            0.0345,
            -0.0552,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            -0.0092,
            -0.1457,
            0.058,
            0.0145,
            0.007,
            0.0823,
            -0.1888,
            -0.0764,
            0.0848,
            0.126,
            0.0233,
            -0.0123,
            -0.1016,
            -0.0199,
            0.0029,
            -0.25
          ],
          "after": [
            0.0478,
            -0.1426,
            -0.2912,
            0.0611,
            0.0376,
            -0.0574,
            -0.1509,
            0.0734,
            0.1018,
            0.0167,
            0.023,
            0.0971,
            0.0487,
            0.0083,
            -0.141,
            0.0443
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0038,
            0.005,
            0.0033,
            0.0023,
            0.0031,
            -0.0005,
            0.0063,
            -0.0001,
            0.0005,
            -0.0038,
            0.0018,
            -0.001,
            0.0055,
            -0.0024,
            -0.0048,
            -0.0009
          ],
          "after": [
            -0.0871,
            0.0087,
            -0.1117,
            -0.0721,
            0.0681,
            0.1362,
            0.0762,
            -0.1612,
            0.0224,
            -0.0274,
            -0.1336,
            -0.0723,
            0.0925,
            -0.0813,
            0.0422,
            0.0796
          ]
        }
      }
    },
    {
      "step": 715,
      "word": "miliah",
      "loss": 2.0428,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1252,
            -0.0783,
            0.2357,
            0.0504,
            0.0859,
            0.1375,
            -0.0167,
            0.1226,
            0.1473,
            0.147,
            -0.0581,
            -0.0477,
            0.1721,
            0.0294,
            -0.0968,
            0.0731
          ],
          "after": [
            0.0021,
            -0.0074,
            0.1582,
            -0.0251,
            0.0308,
            -0.084,
            -0.1142,
            -0.1477,
            -0.0164,
            0.0818,
            0.1594,
            0.0125,
            -0.0993,
            -0.0146,
            -0.0406,
            -0.1913
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0852,
            0.1045,
            -0.143,
            0.0337,
            0.0528,
            -0.1595,
            0.2549,
            0.0079,
            -0.2132,
            0.0305,
            -0.0389,
            -0.0228,
            0.1103,
            -0.0598,
            -0.2147,
            0.1725
          ],
          "after": [
            0.0195,
            0.1064,
            -0.033,
            -0.0772,
            0.0416,
            -0.0134,
            0.088,
            0.096,
            -0.068,
            -0.0732,
            0.0109,
            -0.0108,
            0.1013,
            0.0347,
            -0.055,
            0.2208
          ]
        },
        "position_0": {
          "grad": [
            -0.0063,
            0.0259,
            -0.0332,
            0.0014,
            0.1375,
            -0.0349,
            0.1763,
            0.059,
            0.0287,
            0.0427,
            0.0208,
            0.0304,
            -0.0538,
            -0.0467,
            -0.0471,
            0.083
          ],
          "after": [
            0.0478,
            -0.1422,
            -0.2916,
            0.061,
            0.0376,
            -0.0575,
            -0.1507,
            0.0737,
            0.1016,
            0.0164,
            0.0228,
            0.0973,
            0.0488,
            0.0083,
            -0.1409,
            0.0446
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0012,
            0.0033,
            0.0019,
            -0.0027,
            -0.001,
            -0.0054,
            0.0015,
            0.0008,
            -0.0031,
            -0.0011,
            0.0044,
            -0.0028,
            -0.0003,
            0.0024,
            0.0
          ],
          "after": [
            -0.0868,
            0.0083,
            -0.1123,
            -0.0722,
            0.0681,
            0.1364,
            0.0764,
            -0.1613,
            0.0222,
            -0.027,
            -0.1338,
            -0.0727,
            0.0924,
            -0.0812,
            0.0422,
            0.0796
          ]
        }
      }
    },
    {
      "step": 716,
      "word": "deylan",
      "loss": 1.8362,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.056,
            0.143,
            -0.0385,
            0.0583,
            -0.0276,
            -0.1023,
            0.0055,
            -0.079,
            0.0223,
            -0.1843,
            -0.0524,
            0.1092,
            -0.0997,
            -0.0332,
            -0.0019,
            0.0658
          ],
          "after": [
            0.0021,
            -0.0074,
            0.1581,
            -0.0251,
            0.0308,
            -0.084,
            -0.1141,
            -0.1479,
            -0.0164,
            0.0817,
            0.1595,
            0.0125,
            -0.0995,
            -0.0148,
            -0.0405,
            -0.1912
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0573,
            -0.2972,
            0.2946,
            -0.1561,
            0.1679,
            0.1657,
            -0.4143,
            -0.0642,
            0.4785,
            0.2066,
            0.1651,
            0.1669,
            -0.2414,
            0.0248,
            0.0495,
            -0.1271
          ],
          "after": [
            0.0199,
            0.1061,
            -0.0335,
            -0.0772,
            0.0414,
            -0.0134,
            0.0878,
            0.0963,
            -0.0681,
            -0.0731,
            0.0107,
            -0.011,
            0.1015,
            0.0348,
            -0.0549,
            0.2207
          ]
        },
        "position_0": {
          "grad": [
            -0.0624,
            0.0247,
            -0.0305,
            -0.0786,
            0.092,
            -0.076,
            0.0308,
            -0.0672,
            0.0065,
            -0.0364,
            -0.0509,
            0.0062,
            0.0284,
            0.0369,
            0.0387,
            0.1047
          ],
          "after": [
            0.0479,
            -0.1419,
            -0.2918,
            0.0611,
            0.0375,
            -0.0574,
            -0.1506,
            0.074,
            0.1015,
            0.0162,
            0.0227,
            0.0975,
            0.0488,
            0.0083,
            -0.1409,
            0.0447
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0011,
            0.0005,
            -0.0009,
            0.0011,
            0.0013,
            -0.0008,
            -0.0003,
            0.0008,
            0.0019,
            0.0006,
            0.0033,
            -0.0026,
            -0.0008,
            0.0003,
            -0.0004
          ],
          "after": [
            -0.0864,
            0.0082,
            -0.1129,
            -0.0723,
            0.0681,
            0.1365,
            0.0766,
            -0.1613,
            0.0218,
            -0.0268,
            -0.134,
            -0.0732,
            0.0924,
            -0.081,
            0.0422,
            0.0796
          ]
        }
      }
    },
    {
      "step": 717,
      "word": "kinston",
      "loss": 2.2236,
      "learning_rate": 0.0009,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0021,
            -0.0075,
            0.1581,
            -0.0251,
            0.0309,
            -0.0839,
            -0.114,
            -0.1481,
            -0.0163,
            0.0817,
            0.1597,
            0.0126,
            -0.0997,
            -0.015,
            -0.0404,
            -0.1911
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0983,
            0.0969,
            -0.1198,
            0.0184,
            0.0829,
            -0.1686,
            0.2966,
            0.0492,
            -0.1937,
            0.0047,
            -0.0305,
            0.0097,
            0.0694,
            -0.0955,
            -0.1467,
            0.146
          ],
          "after": [
            0.0203,
            0.1058,
            -0.0339,
            -0.0771,
            0.0411,
            -0.0132,
            0.0875,
            0.0963,
            -0.0681,
            -0.073,
            0.0106,
            -0.0112,
            0.1017,
            0.035,
            -0.0546,
            0.2205
          ]
        },
        "position_0": {
          "grad": [
            -0.0727,
            0.0391,
            -0.095,
            0.0296,
            0.0382,
            -0.0563,
            0.1592,
            0.002,
            -0.0375,
            -0.0079,
            -0.0125,
            0.0252,
            0.0497,
            -0.0279,
            -0.0795,
            -0.011
          ],
          "after": [
            0.0481,
            -0.1417,
            -0.2918,
            0.0611,
            0.0374,
            -0.0573,
            -0.1506,
            0.0743,
            0.1014,
            0.0161,
            0.0227,
            0.0977,
            0.0488,
            0.0083,
            -0.1407,
            0.0449
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0015,
            -0.0019,
            -0.0016,
            0.0028,
            -0.0003,
            0.0039,
            0.0002,
            0.0011,
            0.003,
            0.0015,
            0.0024,
            0.0001,
            -0.0014,
            -0.0032,
            0.0002
          ],
          "after": [
            -0.0861,
            0.0081,
            -0.1132,
            -0.0722,
            0.0679,
            0.1366,
            0.0765,
            -0.1613,
            0.0215,
            -0.0268,
            -0.1343,
            -0.0738,
            0.0925,
            -0.0807,
            0.0424,
            0.0796
          ]
        }
      }
    },
    {
      "step": 718,
      "word": "shiya",
      "loss": 2.2829,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3693,
            -0.0797,
            -0.1707,
            -0.1035,
            -0.0396,
            -0.0608,
            -0.0201,
            0.0627,
            -0.1558,
            0.2215,
            0.1064,
            -0.3385,
            0.1926,
            0.1683,
            -0.0221,
            -0.2557
          ],
          "after": [
            0.0018,
            -0.0075,
            0.1582,
            -0.0249,
            0.0309,
            -0.0839,
            -0.1138,
            -0.1484,
            -0.0162,
            0.0813,
            0.1597,
            0.0129,
            -0.1,
            -0.0154,
            -0.0403,
            -0.1908
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0035,
            -0.005,
            -0.1898,
            -0.035,
            0.0052,
            -0.1589,
            0.2,
            0.0152,
            -0.1961,
            0.0234,
            -0.0443,
            -0.061,
            0.1053,
            -0.0524,
            -0.138,
            0.1181
          ],
          "after": [
            0.0207,
            0.1055,
            -0.0341,
            -0.077,
            0.0408,
            -0.013,
            0.0871,
            0.0964,
            -0.068,
            -0.073,
            0.0106,
            -0.0113,
            0.1018,
            0.0353,
            -0.0544,
            0.2202
          ]
        },
        "position_0": {
          "grad": [
            0.0162,
            0.078,
            -0.0087,
            0.1115,
            -0.1717,
            0.0393,
            -0.0644,
            0.048,
            -0.0363,
            0.001,
            -0.1016,
            0.0254,
            0.0841,
            0.0607,
            0.1298,
            -0.0137
          ],
          "after": [
            0.0483,
            -0.1416,
            -0.2918,
            0.0609,
            0.0374,
            -0.0572,
            -0.1505,
            0.0745,
            0.1013,
            0.0159,
            0.0227,
            0.0978,
            0.0487,
            0.0083,
            -0.1408,
            0.045
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0008,
            0.0002,
            0.0004,
            -0.002,
            0.0009,
            -0.0029,
            0.0002,
            0.0004,
            -0.0023,
            -0.0001,
            0.0011,
            -0.0016,
            0.0008,
            0.0025,
            -0.0006
          ],
          "after": [
            -0.0859,
            0.0081,
            -0.1135,
            -0.0721,
            0.0678,
            0.1366,
            0.0766,
            -0.1614,
            0.0211,
            -0.0267,
            -0.1346,
            -0.0744,
            0.0927,
            -0.0805,
            0.0424,
            0.0797
          ]
        }
      }
    },
    {
      "step": 719,
      "word": "munasar",
      "loss": 2.6393,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2317,
            0.0249,
            0.1505,
            0.2945,
            0.1499,
            0.2602,
            0.1352,
            -0.0127,
            0.0992,
            -0.2256,
            -0.2356,
            0.4188,
            -0.137,
            -0.2467,
            0.0977,
            0.3786
          ],
          "after": [
            0.0017,
            -0.0075,
            0.1582,
            -0.0252,
            0.0309,
            -0.0841,
            -0.1139,
            -0.1485,
            -0.0161,
            0.0813,
            0.1599,
            0.0128,
            -0.1001,
            -0.0154,
            -0.0403,
            -0.1909
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0547,
            0.0453,
            -0.1007,
            0.026,
            0.0537,
            -0.1535,
            0.1656,
            0.021,
            -0.1566,
            0.0242,
            -0.0158,
            0.0443,
            0.0522,
            -0.0699,
            -0.1322,
            0.1013
          ],
          "after": [
            0.021,
            0.1052,
            -0.0342,
            -0.077,
            0.0406,
            -0.0127,
            0.0868,
            0.0964,
            -0.0679,
            -0.073,
            0.0105,
            -0.0115,
            0.1018,
            0.0355,
            -0.0541,
            0.2199
          ]
        },
        "position_0": {
          "grad": [
            -0.0121,
            0.0243,
            -0.035,
            0.0243,
            0.1151,
            -0.0278,
            0.1846,
            0.0689,
            0.0105,
            0.0292,
            0.0104,
            0.047,
            -0.0509,
            -0.0555,
            -0.0206,
            0.0838
          ],
          "after": [
            0.0485,
            -0.1416,
            -0.2917,
            0.0608,
            0.0374,
            -0.0571,
            -0.1506,
            0.0744,
            0.1013,
            0.0158,
            0.0228,
            0.0978,
            0.0487,
            0.0083,
            -0.1408,
            0.045
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            0.0023,
            -0.0018,
            0.0021,
            -0.0024,
            -0.0004,
            0.0003,
            -0.0008,
            -0.0024,
            -0.002,
            -0.0022,
            -0.0036,
            0.0038,
            0.0016,
            0.0022,
            0.0011
          ],
          "after": [
            -0.0856,
            0.0078,
            -0.1136,
            -0.0722,
            0.0679,
            0.1366,
            0.0767,
            -0.1613,
            0.0211,
            -0.0264,
            -0.1346,
            -0.0746,
            0.0925,
            -0.0805,
            0.0423,
            0.0796
          ]
        }
      }
    },
    {
      "step": 720,
      "word": "hayzel",
      "loss": 2.2582,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0901,
            -0.1921,
            0.0708,
            -0.0765,
            0.0473,
            0.0962,
            -0.0879,
            0.0702,
            -0.0883,
            0.1244,
            0.0642,
            0.0837,
            0.023,
            0.0636,
            -0.0281,
            -0.0138
          ],
          "after": [
            0.0016,
            -0.0073,
            0.1581,
            -0.0253,
            0.0308,
            -0.0844,
            -0.1138,
            -0.1488,
            -0.016,
            0.0812,
            0.1601,
            0.0127,
            -0.1002,
            -0.0155,
            -0.0403,
            -0.1909
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1193,
            -0.2119,
            0.1814,
            0.0237,
            -0.0358,
            0.1176,
            -0.289,
            -0.0465,
            0.0498,
            0.0018,
            0.0555,
            -0.0341,
            -0.1403,
            0.1017,
            0.1124,
            -0.0303
          ],
          "after": [
            0.0212,
            0.1051,
            -0.0345,
            -0.077,
            0.0404,
            -0.0125,
            0.0866,
            0.0965,
            -0.0678,
            -0.073,
            0.0105,
            -0.0116,
            0.1019,
            0.0356,
            -0.0539,
            0.2197
          ]
        },
        "position_0": {
          "grad": [
            0.0143,
            -0.0887,
            0.0952,
            0.0161,
            -0.0463,
            0.1065,
            -0.1127,
            0.0731,
            0.1646,
            0.1223,
            -0.094,
            -0.1603,
            0.1558,
            0.1422,
            0.0234,
            0.0204
          ],
          "after": [
            0.0486,
            -0.1414,
            -0.2918,
            0.0606,
            0.0374,
            -0.0572,
            -0.1506,
            0.0743,
            0.1011,
            0.0155,
            0.0229,
            0.098,
            0.0485,
            0.0081,
            -0.1408,
            0.0451
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0018,
            0.0007,
            0.0004,
            0.0014,
            0.0009,
            -0.0016,
            0.0013,
            -0.0003,
            0.0009,
            0.0,
            -0.0004,
            -0.0003,
            -0.0002,
            -0.0014,
            -0.0008
          ],
          "after": [
            -0.0854,
            0.0077,
            -0.1137,
            -0.0724,
            0.0679,
            0.1366,
            0.0769,
            -0.1614,
            0.021,
            -0.0262,
            -0.1346,
            -0.0748,
            0.0924,
            -0.0805,
            0.0423,
            0.0796
          ]
        }
      }
    },
    {
      "step": 721,
      "word": "kache",
      "loss": 2.3455,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0894,
            0.0312,
            0.0872,
            0.1894,
            -0.0678,
            -0.0968,
            0.1369,
            0.1372,
            -0.1151,
            0.0413,
            0.0902,
            0.0082,
            0.0243,
            0.0094,
            -0.0916,
            0.0111
          ],
          "after": [
            0.0016,
            -0.0072,
            0.158,
            -0.0256,
            0.0308,
            -0.0846,
            -0.1139,
            -0.1492,
            -0.0157,
            0.081,
            0.1601,
            0.0126,
            -0.1004,
            -0.0156,
            -0.0402,
            -0.191
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.199,
            -0.2017,
            0.0844,
            -0.0773,
            -0.1908,
            0.082,
            -0.4479,
            -0.047,
            -0.0718,
            -0.0995,
            0.0336,
            -0.21,
            0.0825,
            0.2937,
            0.1705,
            -0.1741
          ],
          "after": [
            0.0212,
            0.1051,
            -0.0347,
            -0.0769,
            0.0404,
            -0.0124,
            0.0866,
            0.0967,
            -0.0676,
            -0.0729,
            0.0103,
            -0.0115,
            0.102,
            0.0355,
            -0.0539,
            0.2196
          ]
        },
        "position_0": {
          "grad": [
            -0.0871,
            0.0602,
            -0.1199,
            0.0013,
            0.063,
            -0.1024,
            0.1668,
            -0.0219,
            -0.0384,
            -0.0165,
            -0.0081,
            0.0078,
            0.0643,
            -0.0035,
            -0.1091,
            -0.0133
          ],
          "after": [
            0.0489,
            -0.1414,
            -0.2917,
            0.0605,
            0.0373,
            -0.0571,
            -0.1507,
            0.0742,
            0.1009,
            0.0152,
            0.023,
            0.0982,
            0.0483,
            0.0079,
            -0.1407,
            0.0451
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0003,
            -0.0009,
            -0.0015,
            -0.0025,
            0.0005,
            -0.0009,
            -0.0007,
            0.0008,
            -0.0008,
            -0.0015,
            0.0003,
            -0.0013,
            0.0018,
            0.0044,
            -0.0013
          ],
          "after": [
            -0.0852,
            0.0077,
            -0.1137,
            -0.0724,
            0.0681,
            0.1365,
            0.0771,
            -0.1615,
            0.0209,
            -0.0261,
            -0.1345,
            -0.075,
            0.0924,
            -0.0806,
            0.0421,
            0.0797
          ]
        }
      }
    },
    {
      "step": 722,
      "word": "jin",
      "loss": 2.6729,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0015,
            -0.0071,
            0.1579,
            -0.0259,
            0.0308,
            -0.0847,
            -0.1139,
            -0.1496,
            -0.0155,
            0.0809,
            0.1601,
            0.0125,
            -0.1005,
            -0.0157,
            -0.0401,
            -0.191
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0623,
            0.0691,
            -0.2274,
            -0.0246,
            -0.0219,
            -0.1403,
            0.307,
            0.0716,
            -0.2736,
            0.0225,
            -0.0889,
            -0.0761,
            0.1874,
            -0.0285,
            -0.1408,
            0.1649
          ],
          "after": [
            0.0213,
            0.1051,
            -0.0347,
            -0.0768,
            0.0404,
            -0.0123,
            0.0865,
            0.0967,
            -0.0674,
            -0.0728,
            0.0103,
            -0.0114,
            0.1019,
            0.0354,
            -0.0537,
            0.2195
          ]
        },
        "position_0": {
          "grad": [
            0.0488,
            -0.0112,
            0.0496,
            0.2453,
            -0.1305,
            -0.2244,
            0.1807,
            0.1799,
            -0.2562,
            -0.2168,
            0.002,
            -0.0989,
            0.454,
            -0.0929,
            -0.0487,
            0.1536
          ],
          "after": [
            0.049,
            -0.1413,
            -0.2917,
            0.0601,
            0.0374,
            -0.0567,
            -0.1509,
            0.0738,
            0.1011,
            0.0153,
            0.0231,
            0.0984,
            0.0477,
            0.0079,
            -0.1406,
            0.045
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0003,
            0.0006,
            -0.0016,
            0.0016,
            -0.0007,
            -0.0012,
            -0.0014,
            0.0011,
            0.0003,
            0.0006,
            -0.0014,
            -0.0001,
            0.0006,
            -0.0012,
            -0.0012
          ],
          "after": [
            -0.085,
            0.0077,
            -0.1138,
            -0.0722,
            0.0681,
            0.1365,
            0.0773,
            -0.1613,
            0.0207,
            -0.0259,
            -0.1345,
            -0.075,
            0.0925,
            -0.0808,
            0.042,
            0.0799
          ]
        }
      }
    },
    {
      "step": 723,
      "word": "anne",
      "loss": 2.3252,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0642,
            0.179,
            -0.4094,
            0.1405,
            -0.8721,
            0.0092,
            -0.1494,
            -0.075,
            0.0919,
            -0.4609,
            -0.3427,
            -0.2598,
            0.0091,
            0.3675,
            0.4826,
            0.096
          ],
          "after": [
            0.0015,
            -0.0072,
            0.1581,
            -0.0262,
            0.0313,
            -0.0849,
            -0.1139,
            -0.1497,
            -0.0154,
            0.0812,
            0.1605,
            0.0126,
            -0.1006,
            -0.0161,
            -0.0406,
            -0.1911
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0619,
            0.013,
            0.0692,
            -0.1395,
            -0.089,
            -0.0444,
            -0.4319,
            -0.0964,
            0.0279,
            -0.4181,
            -0.0162,
            -0.1909,
            -0.0285,
            0.0838,
            0.3885,
            -0.1486
          ],
          "after": [
            0.0214,
            0.1051,
            -0.0348,
            -0.0765,
            0.0404,
            -0.0121,
            0.0866,
            0.0969,
            -0.0672,
            -0.0724,
            0.0104,
            -0.0112,
            0.1018,
            0.0352,
            -0.0538,
            0.2194
          ]
        },
        "position_0": {
          "grad": [
            0.1218,
            -0.0772,
            0.1059,
            -0.1517,
            -0.0723,
            -0.022,
            -0.3977,
            -0.0893,
            0.213,
            -0.0228,
            0.204,
            0.0126,
            -0.2485,
            0.074,
            0.0345,
            -0.3138
          ],
          "after": [
            0.0489,
            -0.1412,
            -0.2918,
            0.0599,
            0.0375,
            -0.0563,
            -0.1508,
            0.0736,
            0.101,
            0.0155,
            0.023,
            0.0987,
            0.0475,
            0.0078,
            -0.1405,
            0.0452
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0004,
            -0.0004,
            -0.0001,
            -0.0009,
            -0.0006,
            0.0017,
            -0.0009,
            -0.0011,
            0.0003,
            0.0003,
            -0.0005,
            0.0006,
            0.0003,
            0.0009,
            -0.0017
          ],
          "after": [
            -0.0848,
            0.0077,
            -0.1138,
            -0.0721,
            0.0682,
            0.1365,
            0.0774,
            -0.1611,
            0.0207,
            -0.0258,
            -0.1345,
            -0.0751,
            0.0924,
            -0.0809,
            0.0418,
            0.0802
          ]
        }
      }
    },
    {
      "step": 724,
      "word": "suhas",
      "loss": 2.6548,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0888,
            0.1333,
            0.1451,
            0.1824,
            -0.0485,
            0.004,
            -0.0206,
            0.064,
            -0.1015,
            0.0189,
            -0.0609,
            0.1675,
            0.1003,
            -0.0239,
            0.0111,
            -0.0177
          ],
          "after": [
            0.0015,
            -0.0074,
            0.1582,
            -0.0268,
            0.0319,
            -0.085,
            -0.1138,
            -0.15,
            -0.0152,
            0.0815,
            0.1609,
            0.0126,
            -0.1007,
            -0.0165,
            -0.041,
            -0.1912
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0401,
            -0.0039,
            -0.1578,
            -0.0059,
            0.0164,
            -0.1385,
            0.1418,
            0.016,
            -0.1599,
            0.0293,
            0.0015,
            0.0241,
            0.0255,
            -0.0699,
            -0.1142,
            0.0874
          ],
          "after": [
            0.0215,
            0.1051,
            -0.0348,
            -0.0762,
            0.0405,
            -0.0119,
            0.0866,
            0.097,
            -0.0669,
            -0.072,
            0.0104,
            -0.011,
            0.1017,
            0.0351,
            -0.0538,
            0.2193
          ]
        },
        "position_0": {
          "grad": [
            0.0104,
            0.0938,
            -0.0095,
            0.1415,
            -0.1567,
            0.0445,
            -0.0334,
            0.0492,
            -0.063,
            -0.0171,
            -0.0939,
            0.0498,
            0.1108,
            0.0775,
            0.1291,
            0.0234
          ],
          "after": [
            0.0487,
            -0.1412,
            -0.292,
            0.0596,
            0.0379,
            -0.056,
            -0.1507,
            0.0734,
            0.101,
            0.0156,
            0.023,
            0.0988,
            0.0471,
            0.0075,
            -0.1406,
            0.0453
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0005,
            0.0005,
            0.0012,
            -0.0008,
            0.0011,
            -0.0002,
            -0.0005,
            0.0004,
            0.0018,
            0.0,
            -0.0007,
            0.0005,
            0.0006,
            -0.0007,
            0.0
          ],
          "after": [
            -0.0847,
            0.0077,
            -0.1139,
            -0.0721,
            0.0684,
            0.1365,
            0.0775,
            -0.1609,
            0.0206,
            -0.0258,
            -0.1345,
            -0.075,
            0.0924,
            -0.0811,
            0.0417,
            0.0805
          ]
        }
      }
    },
    {
      "step": 725,
      "word": "scarlett",
      "loss": 2.8483,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0338,
            -0.0363,
            -0.0173,
            0.012,
            0.0665,
            0.1252,
            0.1025,
            -0.0831,
            0.0789,
            0.0098,
            0.0399,
            0.1079,
            -0.1184,
            -0.1298,
            -0.0234,
            0.12
          ],
          "after": [
            0.0015,
            -0.0075,
            0.1582,
            -0.0272,
            0.0323,
            -0.0852,
            -0.1138,
            -0.1501,
            -0.0152,
            0.0817,
            0.1612,
            0.0125,
            -0.1008,
            -0.0167,
            -0.0413,
            -0.1914
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0714,
            -0.2261,
            -0.0632,
            -0.119,
            -0.0582,
            0.0838,
            -0.2188,
            0.0141,
            0.0846,
            0.0955,
            0.006,
            -0.0725,
            0.0237,
            0.0836,
            0.0924,
            -0.1507
          ],
          "after": [
            0.0214,
            0.1053,
            -0.0348,
            -0.0758,
            0.0405,
            -0.0117,
            0.0867,
            0.0971,
            -0.0667,
            -0.0718,
            0.0104,
            -0.0108,
            0.1017,
            0.035,
            -0.0539,
            0.2193
          ]
        },
        "position_0": {
          "grad": [
            0.002,
            0.0857,
            -0.0153,
            0.1115,
            -0.0571,
            0.0458,
            0.0455,
            0.0381,
            -0.0905,
            0.0363,
            -0.0629,
            0.0999,
            0.0949,
            -0.0157,
            0.0434,
            0.0296
          ],
          "after": [
            0.0486,
            -0.1413,
            -0.2921,
            0.0592,
            0.0382,
            -0.0559,
            -0.1507,
            0.0731,
            0.1011,
            0.0157,
            0.023,
            0.0988,
            0.0468,
            0.0074,
            -0.1407,
            0.0454
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0002,
            0.0009,
            0.0004,
            0.0007,
            -0.0005,
            0.0009,
            -0.0003,
            -0.0005,
            -0.0002,
            0.0011,
            0.001,
            -0.0002,
            -0.0009,
            -0.0011,
            -0.0001
          ],
          "after": [
            -0.0845,
            0.0076,
            -0.114,
            -0.0721,
            0.0684,
            0.1365,
            0.0775,
            -0.1607,
            0.0206,
            -0.0259,
            -0.1346,
            -0.075,
            0.0923,
            -0.0812,
            0.0416,
            0.0807
          ]
        }
      }
    },
    {
      "step": 726,
      "word": "krisleigh",
      "loss": 2.8518,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0014,
            -0.0076,
            0.1583,
            -0.0276,
            0.0326,
            -0.0854,
            -0.1138,
            -0.1502,
            -0.0151,
            0.0819,
            0.1614,
            0.0124,
            -0.1008,
            -0.0168,
            -0.0416,
            -0.1915
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0107,
            -0.1765,
            -0.0562,
            -0.1394,
            -0.0441,
            0.0428,
            -0.1298,
            0.0291,
            0.0501,
            0.0938,
            -0.0287,
            -0.0629,
            0.0229,
            0.0693,
            0.0706,
            -0.0732
          ],
          "after": [
            0.0214,
            0.1055,
            -0.0347,
            -0.0753,
            0.0406,
            -0.0116,
            0.0868,
            0.0971,
            -0.0666,
            -0.0717,
            0.0105,
            -0.0105,
            0.1016,
            0.0348,
            -0.054,
            0.2194
          ]
        },
        "position_0": {
          "grad": [
            -0.0544,
            0.051,
            -0.0792,
            -0.0091,
            0.0808,
            -0.0378,
            0.1503,
            -0.0193,
            -0.053,
            0.0426,
            0.0096,
            0.0465,
            0.0438,
            -0.0677,
            -0.1187,
            -0.0086
          ],
          "after": [
            0.0486,
            -0.1415,
            -0.292,
            0.0588,
            0.0384,
            -0.0557,
            -0.1507,
            0.0729,
            0.1012,
            0.0157,
            0.0231,
            0.0987,
            0.0464,
            0.0073,
            -0.1407,
            0.0454
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0006,
            -0.0021,
            -0.0006,
            0.0007,
            0.0021,
            0.0031,
            0.0004,
            -0.0022,
            -0.0004,
            -0.0022,
            -0.0021,
            -0.0013,
            0.0025,
            0.0033,
            0.0018
          ],
          "after": [
            -0.0843,
            0.0077,
            -0.114,
            -0.0721,
            0.0684,
            0.1363,
            0.0774,
            -0.1605,
            0.0208,
            -0.0259,
            -0.1345,
            -0.0749,
            0.0924,
            -0.0815,
            0.0414,
            0.0808
          ]
        }
      }
    },
    {
      "step": 727,
      "word": "kaylena",
      "loss": 1.729,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.155,
            -0.1909,
            0.0465,
            -0.155,
            0.0588,
            0.0692,
            -0.0901,
            0.033,
            -0.1284,
            0.1954,
            0.119,
            0.0224,
            0.0249,
            0.0719,
            -0.0561,
            -0.1156
          ],
          "after": [
            0.0013,
            -0.0076,
            0.1583,
            -0.0278,
            0.0329,
            -0.0857,
            -0.1137,
            -0.1503,
            -0.0149,
            0.0818,
            0.1615,
            0.0123,
            -0.1009,
            -0.017,
            -0.0417,
            -0.1915
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0705,
            -0.2989,
            0.0786,
            -0.0758,
            0.0321,
            0.1215,
            -0.3145,
            0.0851,
            0.0468,
            0.0533,
            -0.0811,
            -0.0324,
            -0.0361,
            0.1632,
            0.0686,
            -0.1769
          ],
          "after": [
            0.0214,
            0.1059,
            -0.0347,
            -0.0747,
            0.0406,
            -0.0116,
            0.087,
            0.0969,
            -0.0665,
            -0.0717,
            0.0106,
            -0.0103,
            0.1015,
            0.0345,
            -0.0541,
            0.2195
          ]
        },
        "position_0": {
          "grad": [
            -0.0682,
            0.0486,
            -0.0944,
            -0.0143,
            0.075,
            -0.0666,
            0.1374,
            -0.0206,
            -0.0346,
            0.0289,
            0.0033,
            0.0307,
            0.0398,
            -0.0542,
            -0.1231,
            -0.0319
          ],
          "after": [
            0.0487,
            -0.1417,
            -0.2918,
            0.0586,
            0.0384,
            -0.0554,
            -0.1509,
            0.0728,
            0.1013,
            0.0156,
            0.0231,
            0.0986,
            0.0461,
            0.0074,
            -0.1405,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0004,
            0.0005,
            0.001,
            0.0004,
            0.0011,
            -0.0009,
            0.0008,
            -0.0005,
            -0.0006,
            -0.0003,
            -0.0001,
            -0.0003,
            -0.0001,
            -0.0005,
            0.0007
          ],
          "after": [
            -0.0842,
            0.0077,
            -0.114,
            -0.0722,
            0.0684,
            0.1361,
            0.0773,
            -0.1604,
            0.021,
            -0.0258,
            -0.1343,
            -0.0748,
            0.0925,
            -0.0817,
            0.0413,
            0.0808
          ]
        }
      }
    },
    {
      "step": 728,
      "word": "monae",
      "loss": 2.2691,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0003,
            -0.2276,
            0.1372,
            0.1206,
            -0.0429,
            0.0509,
            -0.2175,
            -0.0242,
            0.0948,
            0.0085,
            -0.0051,
            -0.0083,
            -0.1854,
            0.0583,
            0.0158,
            -0.2893
          ],
          "after": [
            0.0012,
            -0.0073,
            0.1582,
            -0.028,
            0.0331,
            -0.086,
            -0.1135,
            -0.1504,
            -0.0149,
            0.0818,
            0.1616,
            0.0122,
            -0.1008,
            -0.0172,
            -0.0419,
            -0.1913
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.06,
            0.0232,
            -0.2424,
            0.0564,
            -0.3664,
            0.3587,
            0.0238,
            0.1508,
            0.0728,
            -0.3271,
            -0.3491,
            -0.1739,
            0.3025,
            0.2127,
            0.3015,
            0.232
          ],
          "after": [
            0.0213,
            0.1062,
            -0.0345,
            -0.0743,
            0.0409,
            -0.0118,
            0.0872,
            0.0965,
            -0.0665,
            -0.0714,
            0.0112,
            -0.01,
            0.1013,
            0.0341,
            -0.0544,
            0.2195
          ]
        },
        "position_0": {
          "grad": [
            -0.0077,
            -0.0006,
            -0.0329,
            -0.0122,
            0.1203,
            -0.0514,
            0.1672,
            0.0799,
            0.0617,
            -0.0046,
            0.0095,
            0.0032,
            -0.0945,
            -0.02,
            0.0102,
            0.0863
          ],
          "after": [
            0.0489,
            -0.1418,
            -0.2915,
            0.0584,
            0.0384,
            -0.0551,
            -0.1511,
            0.0725,
            0.1014,
            0.0156,
            0.0231,
            0.0985,
            0.0459,
            0.0075,
            -0.1403,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0003,
            -0.0019,
            -0.0009,
            -0.0032,
            0.001,
            -0.0026,
            -0.0003,
            -0.0004,
            -0.005,
            -0.0033,
            -0.0017,
            -0.0002,
            0.003,
            0.0053,
            0.0005
          ],
          "after": [
            -0.0839,
            0.0077,
            -0.1138,
            -0.0721,
            0.0686,
            0.1358,
            0.0774,
            -0.1604,
            0.0213,
            -0.0254,
            -0.1339,
            -0.0746,
            0.0926,
            -0.0821,
            0.0409,
            0.0807
          ]
        }
      }
    },
    {
      "step": 729,
      "word": "keyaira",
      "loss": 2.2929,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.008,
            -0.261,
            0.14,
            -0.1413,
            0.1957,
            0.1326,
            -0.1823,
            0.0338,
            -0.0529,
            0.0139,
            0.1243,
            -0.1189,
            -0.1217,
            0.0412,
            -0.0812,
            -0.1371
          ],
          "after": [
            0.0011,
            -0.0068,
            0.1581,
            -0.0281,
            0.0332,
            -0.0864,
            -0.113,
            -0.1505,
            -0.0148,
            0.0818,
            0.1615,
            0.0122,
            -0.1006,
            -0.0175,
            -0.0419,
            -0.1911
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1561,
            -0.3127,
            0.2597,
            -0.0096,
            0.2448,
            0.2279,
            -0.5058,
            -0.0916,
            0.3887,
            0.1709,
            0.1615,
            0.1675,
            -0.1992,
            0.048,
            -0.1575,
            -0.3007
          ],
          "after": [
            0.021,
            0.1066,
            -0.0345,
            -0.074,
            0.041,
            -0.0121,
            0.0875,
            0.0964,
            -0.0667,
            -0.0713,
            0.0115,
            -0.0098,
            0.1012,
            0.0336,
            -0.0545,
            0.2197
          ]
        },
        "position_0": {
          "grad": [
            -0.0663,
            0.0434,
            -0.0962,
            -0.0493,
            0.0694,
            -0.066,
            0.1134,
            -0.0349,
            -0.0258,
            0.0451,
            0.0087,
            0.0057,
            0.0242,
            -0.0515,
            -0.1413,
            -0.0507
          ],
          "after": [
            0.0492,
            -0.142,
            -0.2911,
            0.0583,
            0.0382,
            -0.0547,
            -0.1514,
            0.0724,
            0.1015,
            0.0155,
            0.0231,
            0.0984,
            0.0457,
            0.0077,
            -0.14,
            0.0456
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0024,
            -0.0023,
            -0.0005,
            0.0001,
            0.0021,
            -0.005,
            0.0002,
            -0.0001,
            -0.001,
            -0.0002,
            0.0003,
            -0.003,
            0.0026,
            0.0034,
            -0.0018
          ],
          "after": [
            -0.0839,
            0.008,
            -0.1134,
            -0.0721,
            0.0688,
            0.1354,
            0.0777,
            -0.1603,
            0.0215,
            -0.0251,
            -0.1336,
            -0.0744,
            0.0928,
            -0.0827,
            0.0403,
            0.0808
          ]
        }
      }
    },
    {
      "step": 730,
      "word": "enosh",
      "loss": 2.4418,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.001,
            -0.0063,
            0.1579,
            -0.0282,
            0.0333,
            -0.0867,
            -0.1127,
            -0.1505,
            -0.0147,
            0.0817,
            0.1615,
            0.0123,
            -0.1005,
            -0.0177,
            -0.042,
            -0.1908
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3403,
            0.398,
            0.5773,
            -0.0139,
            0.1375,
            -0.0033,
            0.4105,
            -0.0168,
            0.1137,
            -0.2886,
            -0.0248,
            0.0359,
            -0.1715,
            0.0002,
            0.0327,
            -0.069
          ],
          "after": [
            0.0211,
            0.1068,
            -0.0349,
            -0.0737,
            0.041,
            -0.0124,
            0.0877,
            0.0963,
            -0.0669,
            -0.0709,
            0.0117,
            -0.0098,
            0.1013,
            0.0333,
            -0.0547,
            0.2199
          ]
        },
        "position_0": {
          "grad": [
            0.0005,
            -0.205,
            0.0635,
            0.0154,
            -0.0924,
            0.059,
            -0.2694,
            -0.0665,
            0.148,
            0.0689,
            -0.023,
            -0.0726,
            -0.1751,
            0.0524,
            0.1104,
            -0.3017
          ],
          "after": [
            0.0494,
            -0.1419,
            -0.2909,
            0.0581,
            0.0382,
            -0.0545,
            -0.1515,
            0.0724,
            0.1014,
            0.0153,
            0.0231,
            0.0984,
            0.0457,
            0.0077,
            -0.1399,
            0.0459
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0004,
            0.0004,
            -0.0003,
            -0.0017,
            -0.0001,
            -0.0008,
            -0.0006,
            -0.0,
            0.0006,
            0.0012,
            -0.0018,
            0.0008,
            0.0006,
            0.0006,
            -0.0008
          ],
          "after": [
            -0.0838,
            0.0082,
            -0.1131,
            -0.072,
            0.069,
            0.1351,
            0.078,
            -0.1603,
            0.0217,
            -0.0248,
            -0.1334,
            -0.0741,
            0.093,
            -0.0832,
            0.0398,
            0.0809
          ]
        }
      }
    },
    {
      "step": 731,
      "word": "zyree",
      "loss": 2.4877,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0009,
            -0.006,
            0.1578,
            -0.0282,
            0.0333,
            -0.087,
            -0.1124,
            -0.1506,
            -0.0146,
            0.0817,
            0.1614,
            0.0123,
            -0.1003,
            -0.0179,
            -0.042,
            -0.1906
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0521,
            -0.222,
            -0.1452,
            -0.1931,
            -0.1366,
            0.459,
            -0.5246,
            0.0658,
            0.2185,
            -0.5283,
            -0.2909,
            0.0214,
            -0.1786,
            0.2937,
            0.9612,
            -0.2027
          ],
          "after": [
            0.0212,
            0.107,
            -0.0351,
            -0.0731,
            0.041,
            -0.0129,
            0.088,
            0.096,
            -0.0672,
            -0.0702,
            0.0123,
            -0.0097,
            0.1015,
            0.0327,
            -0.0552,
            0.2202
          ]
        },
        "position_0": {
          "grad": [
            0.0452,
            0.0187,
            -0.0225,
            0.1184,
            0.0518,
            -0.0818,
            0.1726,
            -0.002,
            0.0474,
            0.0235,
            0.0438,
            -0.0035,
            -0.0036,
            -0.0431,
            0.034,
            0.0172
          ],
          "after": [
            0.0495,
            -0.1419,
            -0.2907,
            0.0579,
            0.0381,
            -0.0542,
            -0.1516,
            0.0724,
            0.1012,
            0.0151,
            0.0231,
            0.0984,
            0.0457,
            0.0078,
            -0.1399,
            0.0461
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            -0.0008,
            -0.0034,
            -0.0016,
            -0.0028,
            0.0014,
            -0.0004,
            -0.0003,
            0.0002,
            -0.0015,
            -0.0026,
            0.0001,
            -0.001,
            0.0025,
            0.0059,
            -0.0002
          ],
          "after": [
            -0.0837,
            0.0084,
            -0.1126,
            -0.0718,
            0.0694,
            0.1347,
            0.0783,
            -0.1601,
            0.0218,
            -0.0244,
            -0.133,
            -0.0739,
            0.0933,
            -0.0839,
            0.0391,
            0.0811
          ]
        }
      }
    },
    {
      "step": 732,
      "word": "skylor",
      "loss": 2.4563,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0008,
            -0.0057,
            0.1577,
            -0.0283,
            0.0334,
            -0.0872,
            -0.1122,
            -0.1507,
            -0.0145,
            0.0817,
            0.1614,
            0.0123,
            -0.1002,
            -0.018,
            -0.042,
            -0.1905
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0728,
            0.1505,
            -0.1519,
            -0.0049,
            0.0055,
            -0.2557,
            0.3536,
            0.0145,
            -0.2384,
            -0.0134,
            -0.0193,
            0.0002,
            0.1022,
            -0.1334,
            -0.1536,
            0.2231
          ],
          "after": [
            0.0214,
            0.1071,
            -0.0352,
            -0.0727,
            0.0411,
            -0.0131,
            0.0881,
            0.0958,
            -0.0673,
            -0.0696,
            0.0128,
            -0.0096,
            0.1015,
            0.0324,
            -0.0557,
            0.2203
          ]
        },
        "position_0": {
          "grad": [
            -0.0083,
            0.1023,
            -0.0131,
            0.1549,
            -0.0631,
            0.0777,
            0.0632,
            0.0483,
            -0.1149,
            0.0289,
            -0.0634,
            0.1339,
            0.1338,
            -0.0166,
            0.0649,
            0.0553
          ],
          "after": [
            0.0496,
            -0.142,
            -0.2905,
            0.0575,
            0.0382,
            -0.0541,
            -0.1518,
            0.0723,
            0.1013,
            0.0149,
            0.0232,
            0.0983,
            0.0456,
            0.008,
            -0.1399,
            0.0463
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0009,
            0.0003,
            0.0004,
            0.0001,
            0.0009,
            0.0005,
            0.0,
            -0.0005,
            0.0007,
            0.001,
            0.0007,
            -0.0,
            -0.0007,
            -0.0012,
            -0.0004
          ],
          "after": [
            -0.0836,
            0.0087,
            -0.1122,
            -0.0717,
            0.0698,
            0.1343,
            0.0785,
            -0.1601,
            0.022,
            -0.0242,
            -0.1328,
            -0.0738,
            0.0935,
            -0.0844,
            0.0386,
            0.0812
          ]
        }
      }
    },
    {
      "step": 733,
      "word": "clarabella",
      "loss": 2.5871,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1163,
            -0.0022,
            0.1426,
            -0.071,
            0.1325,
            0.1596,
            0.1815,
            -0.0736,
            0.0057,
            -0.0035,
            0.0301,
            0.1678,
            0.02,
            -0.2877,
            -0.1063,
            0.3614
          ],
          "after": [
            0.0009,
            -0.0054,
            0.1575,
            -0.0282,
            0.0333,
            -0.0876,
            -0.1121,
            -0.1506,
            -0.0145,
            0.0816,
            0.1613,
            0.0122,
            -0.1002,
            -0.0178,
            -0.042,
            -0.1906
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1559,
            -0.0462,
            -0.2227,
            -0.0102,
            -0.0984,
            0.0693,
            0.1025,
            -0.0802,
            0.0062,
            -0.0164,
            0.0408,
            -0.2072,
            0.0061,
            -0.1315,
            0.0436,
            0.0175
          ],
          "after": [
            0.0214,
            0.1073,
            -0.0351,
            -0.0723,
            0.0412,
            -0.0134,
            0.0882,
            0.0958,
            -0.0674,
            -0.069,
            0.0132,
            -0.0094,
            0.1016,
            0.0322,
            -0.056,
            0.2204
          ]
        },
        "position_0": {
          "grad": [
            -0.0222,
            0.0416,
            -0.021,
            0.0748,
            0.023,
            -0.016,
            0.127,
            0.0367,
            -0.0807,
            0.0694,
            0.0417,
            0.0674,
            0.0471,
            -0.062,
            -0.0864,
            0.0385
          ],
          "after": [
            0.0497,
            -0.1421,
            -0.2903,
            0.0571,
            0.0381,
            -0.0539,
            -0.1521,
            0.0721,
            0.1014,
            0.0146,
            0.0232,
            0.0981,
            0.0454,
            0.0082,
            -0.1398,
            0.0463
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0002,
            0.0016,
            0.0004,
            0.0012,
            -0.0001,
            -0.0003,
            -0.0004,
            -0.0009,
            -0.001,
            0.001,
            0.0003,
            -0.0002,
            -0.0006,
            -0.001,
            0.0006
          ],
          "after": [
            -0.0835,
            0.009,
            -0.112,
            -0.0716,
            0.07,
            0.134,
            0.0787,
            -0.1599,
            0.0222,
            -0.0239,
            -0.1327,
            -0.0737,
            0.0936,
            -0.0847,
            0.0382,
            0.0813
          ]
        }
      }
    },
    {
      "step": 734,
      "word": "philopateer",
      "loss": 3.0432,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1478,
            -0.0488,
            0.1434,
            0.1003,
            0.0257,
            0.0578,
            0.0766,
            0.0456,
            -0.0245,
            -0.0783,
            -0.0936,
            -0.043,
            0.0581,
            -0.0992,
            -0.0308,
            0.1198
          ],
          "after": [
            0.001,
            -0.0051,
            0.1572,
            -0.0283,
            0.0333,
            -0.088,
            -0.1122,
            -0.1507,
            -0.0144,
            0.0817,
            0.1614,
            0.0121,
            -0.1001,
            -0.0176,
            -0.0418,
            -0.1908
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3802,
            -0.039,
            -0.3335,
            -0.23,
            -0.502,
            0.1146,
            -0.0985,
            0.0512,
            -0.0939,
            0.1393,
            -0.09,
            -0.2268,
            0.1958,
            0.2539,
            0.2915,
            -0.2635
          ],
          "after": [
            0.0211,
            0.1074,
            -0.0349,
            -0.0716,
            0.0416,
            -0.0137,
            0.0883,
            0.0957,
            -0.0675,
            -0.0687,
            0.0136,
            -0.0091,
            0.1015,
            0.0319,
            -0.0565,
            0.2206
          ]
        },
        "position_0": {
          "grad": [
            0.0524,
            0.0859,
            -0.0315,
            0.0363,
            0.0217,
            -0.0847,
            0.1758,
            -0.0049,
            -0.1676,
            0.0067,
            0.0425,
            -0.0174,
            0.0386,
            0.0566,
            -0.037,
            0.128
          ],
          "after": [
            0.0497,
            -0.1423,
            -0.29,
            0.0566,
            0.0381,
            -0.0537,
            -0.1524,
            0.072,
            0.1016,
            0.0144,
            0.0231,
            0.0979,
            0.0453,
            0.0083,
            -0.1397,
            0.0463
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            -0.0016,
            0.0009,
            -0.0006,
            -0.0007,
            0.0009,
            -0.0007,
            0.0012,
            -0.002,
            0.002,
            0.0015,
            0.0008,
            0.0001,
            0.0003,
            -0.0007,
            -0.003
          ],
          "after": [
            -0.0835,
            0.0093,
            -0.1119,
            -0.0715,
            0.0702,
            0.1337,
            0.0789,
            -0.16,
            0.0226,
            -0.0238,
            -0.1327,
            -0.0737,
            0.0938,
            -0.0851,
            0.0379,
            0.0816
          ]
        }
      }
    },
    {
      "step": 735,
      "word": "wisdom",
      "loss": 2.9803,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0011,
            -0.0049,
            0.157,
            -0.0284,
            0.0332,
            -0.0884,
            -0.1122,
            -0.1507,
            -0.0143,
            0.0818,
            0.1614,
            0.0121,
            -0.1001,
            -0.0174,
            -0.0417,
            -0.191
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0899,
            0.1414,
            -0.1437,
            0.0574,
            0.0899,
            -0.2101,
            0.3489,
            0.0487,
            -0.2652,
            -0.0097,
            -0.0336,
            0.0326,
            0.0778,
            -0.115,
            -0.2129,
            0.1821
          ],
          "after": [
            0.021,
            0.1074,
            -0.0346,
            -0.0712,
            0.0418,
            -0.0138,
            0.0882,
            0.0955,
            -0.0674,
            -0.0684,
            0.014,
            -0.0088,
            0.1014,
            0.0316,
            -0.0568,
            0.2207
          ]
        },
        "position_0": {
          "grad": [
            0.1071,
            0.1292,
            -0.0714,
            0.1512,
            -0.0388,
            -0.0766,
            0.2227,
            0.0153,
            -0.0918,
            -0.0445,
            0.0098,
            -0.1346,
            0.1579,
            0.0473,
            -0.0759,
            0.0862
          ],
          "after": [
            0.0495,
            -0.1427,
            -0.2897,
            0.0561,
            0.0381,
            -0.0534,
            -0.1528,
            0.0719,
            0.1019,
            0.0142,
            0.0231,
            0.098,
            0.045,
            0.0083,
            -0.1395,
            0.0462
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0022,
            -0.0014,
            -0.0002,
            -0.0038,
            0.0039,
            -0.0013,
            0.0013,
            -0.0007,
            0.0009,
            0.001,
            0.0016,
            0.0007,
            0.0007,
            0.0007,
            -0.0059,
            -0.0016
          ],
          "after": [
            -0.0837,
            0.0097,
            -0.1118,
            -0.0711,
            0.0701,
            0.1335,
            0.079,
            -0.1599,
            0.0229,
            -0.0238,
            -0.1329,
            -0.0737,
            0.0938,
            -0.0854,
            0.0379,
            0.082
          ]
        }
      }
    },
    {
      "step": 736,
      "word": "jager",
      "loss": 2.4678,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0206,
            -0.0524,
            -0.0499,
            0.2882,
            -0.0568,
            -0.1144,
            -0.0123,
            0.1177,
            -0.2321,
            0.2221,
            0.1585,
            -0.0468,
            0.06,
            -0.0297,
            -0.1125,
            0.118
          ],
          "after": [
            0.0012,
            -0.0046,
            0.1568,
            -0.0287,
            0.0332,
            -0.0885,
            -0.1122,
            -0.1509,
            -0.014,
            0.0816,
            0.1613,
            0.0121,
            -0.1002,
            -0.0172,
            -0.0415,
            -0.1912
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2464,
            -0.123,
            0.232,
            0.0592,
            0.14,
            -0.1465,
            0.1268,
            0.0818,
            0.0081,
            -0.3294,
            -0.1644,
            0.0021,
            -0.0638,
            -0.0648,
            0.122,
            -0.1044
          ],
          "after": [
            0.021,
            0.1075,
            -0.0345,
            -0.0709,
            0.042,
            -0.0138,
            0.0881,
            0.0952,
            -0.0673,
            -0.0678,
            0.0145,
            -0.0086,
            0.1013,
            0.0315,
            -0.0571,
            0.2208
          ]
        },
        "position_0": {
          "grad": [
            0.0338,
            0.0337,
            0.0334,
            0.135,
            0.0211,
            -0.1443,
            0.1308,
            0.0802,
            -0.2143,
            -0.0821,
            0.0436,
            -0.0263,
            0.3556,
            -0.0898,
            -0.1345,
            0.1294
          ],
          "after": [
            0.0492,
            -0.143,
            -0.2895,
            0.0555,
            0.0381,
            -0.0529,
            -0.1533,
            0.0716,
            0.1024,
            0.0142,
            0.023,
            0.098,
            0.0445,
            0.0084,
            -0.1392,
            0.0461
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0018,
            -0.0028,
            -0.0008,
            -0.0017,
            0.0035,
            -0.003,
            0.0018,
            0.0005,
            0.0026,
            0.0016,
            -0.0009,
            -0.0009,
            0.0014,
            0.002,
            0.0028
          ],
          "after": [
            -0.0839,
            0.0102,
            -0.1115,
            -0.0707,
            0.0702,
            0.133,
            0.0793,
            -0.1601,
            0.023,
            -0.024,
            -0.1331,
            -0.0736,
            0.094,
            -0.0858,
            0.0379,
            0.0821
          ]
        }
      }
    },
    {
      "step": 737,
      "word": "alexsandra",
      "loss": 3.1661,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1635,
            0.2745,
            -0.1532,
            -0.054,
            0.0127,
            -0.0371,
            -0.0258,
            -0.0197,
            -0.0571,
            -0.1245,
            -0.1646,
            0.2846,
            -0.1448,
            -0.0259,
            0.0476,
            -0.1491
          ],
          "after": [
            0.0014,
            -0.0047,
            0.1568,
            -0.029,
            0.0332,
            -0.0886,
            -0.1122,
            -0.151,
            -0.0137,
            0.0816,
            0.1613,
            0.0118,
            -0.1001,
            -0.017,
            -0.0414,
            -0.1913
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0603,
            -0.1437,
            0.1337,
            0.0361,
            0.212,
            0.1199,
            -0.1457,
            0.0556,
            0.1272,
            0.0933,
            0.0044,
            0.2161,
            -0.2045,
            -0.0622,
            -0.0018,
            -0.0949
          ],
          "after": [
            0.0211,
            0.1077,
            -0.0345,
            -0.0706,
            0.042,
            -0.0139,
            0.0881,
            0.0948,
            -0.0673,
            -0.0675,
            0.015,
            -0.0086,
            0.1014,
            0.0315,
            -0.0573,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            0.0403,
            0.0102,
            0.0468,
            -0.061,
            0.0489,
            0.0091,
            -0.1268,
            -0.0644,
            0.0391,
            0.0502,
            0.1442,
            0.0686,
            -0.086,
            -0.0236,
            -0.0632,
            -0.1133
          ],
          "after": [
            0.0489,
            -0.1433,
            -0.2894,
            0.0551,
            0.038,
            -0.0525,
            -0.1536,
            0.0715,
            0.1028,
            0.0141,
            0.0227,
            0.098,
            0.0441,
            0.0086,
            -0.1388,
            0.046
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0002,
            -0.0019,
            -0.0014,
            -0.0011,
            -0.0001,
            -0.0021,
            0.0005,
            -0.0012,
            0.0035,
            -0.0002,
            -0.0031,
            0.001,
            0.002,
            0.001,
            -0.0029
          ],
          "after": [
            -0.0841,
            0.0107,
            -0.1111,
            -0.0702,
            0.0703,
            0.1327,
            0.0796,
            -0.1602,
            0.0233,
            -0.0243,
            -0.1333,
            -0.0734,
            0.094,
            -0.0863,
            0.0378,
            0.0824
          ]
        }
      }
    },
    {
      "step": 738,
      "word": "jayden",
      "loss": 1.8886,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0799,
            -0.1886,
            0.0742,
            -0.0788,
            0.0518,
            0.0904,
            -0.074,
            0.0665,
            -0.0825,
            0.1292,
            0.0579,
            0.0912,
            0.0096,
            0.0433,
            -0.0304,
            -0.0289
          ],
          "after": [
            0.0016,
            -0.0046,
            0.1567,
            -0.0291,
            0.0331,
            -0.0888,
            -0.1121,
            -0.1512,
            -0.0133,
            0.0814,
            0.1613,
            0.0116,
            -0.1,
            -0.0168,
            -0.0413,
            -0.1913
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.026,
            -0.3425,
            0.0481,
            -0.1549,
            -0.1167,
            0.1532,
            -0.3443,
            -0.017,
            0.0964,
            0.047,
            -0.0132,
            -0.039,
            -0.0239,
            0.1519,
            0.1639,
            -0.0778
          ],
          "after": [
            0.0211,
            0.1081,
            -0.0345,
            -0.0702,
            0.0421,
            -0.0141,
            0.0882,
            0.0945,
            -0.0673,
            -0.0672,
            0.0154,
            -0.0085,
            0.1015,
            0.0313,
            -0.0576,
            0.2212
          ]
        },
        "position_0": {
          "grad": [
            0.0242,
            0.0276,
            0.0206,
            0.1184,
            0.0239,
            -0.1169,
            0.1341,
            0.0699,
            -0.1916,
            -0.0493,
            0.0402,
            -0.0039,
            0.2967,
            -0.1033,
            -0.1189,
            0.1051
          ],
          "after": [
            0.0486,
            -0.1436,
            -0.2893,
            0.0545,
            0.0379,
            -0.0519,
            -0.1539,
            0.0713,
            0.1033,
            0.0141,
            0.0225,
            0.098,
            0.0436,
            0.0089,
            -0.1384,
            0.0459
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0001,
            0.0008,
            0.001,
            0.0009,
            0.0002,
            -0.0004,
            0.0006,
            -0.0006,
            -0.0006,
            0.0003,
            -0.0005,
            0.0003,
            -0.0006,
            -0.0014,
            0.0005
          ],
          "after": [
            -0.0843,
            0.0111,
            -0.1108,
            -0.07,
            0.0703,
            0.1324,
            0.0799,
            -0.1604,
            0.0236,
            -0.0246,
            -0.1335,
            -0.0731,
            0.094,
            -0.0867,
            0.0378,
            0.0826
          ]
        }
      }
    },
    {
      "step": 739,
      "word": "zavior",
      "loss": 2.243,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0806,
            -0.0065,
            -0.0218,
            0.1058,
            0.2184,
            -0.0122,
            0.0014,
            -0.0394,
            -0.1645,
            0.1111,
            0.0903,
            -0.0072,
            0.0988,
            -0.135,
            -0.1428,
            0.1508
          ],
          "after": [
            0.0017,
            -0.0044,
            0.1567,
            -0.0293,
            0.033,
            -0.089,
            -0.1121,
            -0.1513,
            -0.0128,
            0.0812,
            0.1612,
            0.0114,
            -0.1,
            -0.0166,
            -0.041,
            -0.1915
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0687,
            0.0885,
            -0.124,
            0.0331,
            0.0221,
            -0.1688,
            0.2661,
            0.0123,
            -0.223,
            0.0224,
            -0.0352,
            -0.0032,
            0.1262,
            -0.0687,
            -0.2117,
            0.126
          ],
          "after": [
            0.0212,
            0.1083,
            -0.0344,
            -0.07,
            0.0421,
            -0.0141,
            0.0882,
            0.0943,
            -0.0673,
            -0.067,
            0.0158,
            -0.0085,
            0.1015,
            0.0312,
            -0.0578,
            0.2213
          ]
        },
        "position_0": {
          "grad": [
            0.0319,
            0.0494,
            -0.0227,
            0.131,
            0.0997,
            -0.0624,
            0.1832,
            -0.0138,
            -0.0016,
            0.0488,
            0.0682,
            0.0401,
            0.0371,
            -0.0544,
            -0.0205,
            0.047
          ],
          "after": [
            0.0483,
            -0.1439,
            -0.2892,
            0.054,
            0.0378,
            -0.0514,
            -0.1544,
            0.0712,
            0.1037,
            0.0141,
            0.0222,
            0.0979,
            0.043,
            0.0093,
            -0.1379,
            0.0457
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0025,
            0.0017,
            0.0003,
            0.0002,
            -0.0019,
            0.0003,
            -0.001,
            -0.0006,
            0.0019,
            -0.0019,
            -0.001,
            0.0018,
            -0.0015,
            -0.001,
            0.0026,
            0.0017
          ],
          "after": [
            -0.0843,
            0.0112,
            -0.1105,
            -0.0697,
            0.0705,
            0.1321,
            0.0802,
            -0.1606,
            0.0236,
            -0.0247,
            -0.1336,
            -0.0731,
            0.0941,
            -0.0869,
            0.0376,
            0.0827
          ]
        }
      }
    },
    {
      "step": 740,
      "word": "sofi",
      "loss": 2.9731,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0019,
            -0.0043,
            0.1566,
            -0.0295,
            0.0328,
            -0.0891,
            -0.112,
            -0.1514,
            -0.0124,
            0.081,
            0.1611,
            0.0112,
            -0.1,
            -0.0164,
            -0.0407,
            -0.1916
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0235,
            -0.0138,
            -0.1443,
            -0.0201,
            -0.0131,
            -0.0954,
            0.1437,
            0.0061,
            -0.1536,
            0.0604,
            0.0091,
            -0.0193,
            0.0515,
            -0.0492,
            -0.1252,
            0.0587
          ],
          "after": [
            0.0213,
            0.1085,
            -0.0343,
            -0.0697,
            0.0421,
            -0.0141,
            0.0882,
            0.094,
            -0.0671,
            -0.0669,
            0.0161,
            -0.0084,
            0.1015,
            0.0312,
            -0.0579,
            0.2213
          ]
        },
        "position_0": {
          "grad": [
            -0.0043,
            0.1008,
            -0.011,
            0.1772,
            -0.2196,
            0.0709,
            -0.0684,
            0.0541,
            -0.0662,
            -0.051,
            -0.1001,
            0.0615,
            0.1079,
            0.1038,
            0.1858,
            0.0267
          ],
          "after": [
            0.048,
            -0.1443,
            -0.2891,
            0.0533,
            0.0378,
            -0.0511,
            -0.1547,
            0.071,
            0.1041,
            0.0141,
            0.0221,
            0.0978,
            0.0425,
            0.0094,
            -0.1378,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0019,
            0.0016,
            -0.0012,
            0.0009,
            0.0017,
            0.0022,
            -0.0006,
            -0.0012,
            0.0007,
            -0.0006,
            -0.0015,
            0.0015,
            -0.0028,
            -0.0012,
            0.0033,
            0.0024
          ],
          "after": [
            -0.0841,
            0.0113,
            -0.1102,
            -0.0696,
            0.0705,
            0.1317,
            0.0805,
            -0.1605,
            0.0236,
            -0.0247,
            -0.1335,
            -0.0731,
            0.0944,
            -0.087,
            0.0373,
            0.0826
          ]
        }
      }
    },
    {
      "step": 741,
      "word": "letti",
      "loss": 2.6784,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.002,
            -0.0042,
            0.1566,
            -0.0297,
            0.0327,
            -0.0892,
            -0.112,
            -0.1515,
            -0.012,
            0.0808,
            0.161,
            0.0111,
            -0.1001,
            -0.0162,
            -0.0405,
            -0.1917
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1023,
            -0.3386,
            0.3038,
            -0.07,
            0.3853,
            0.228,
            -0.4844,
            0.0688,
            0.459,
            0.2201,
            0.0313,
            0.1201,
            -0.215,
            0.1383,
            -0.0858,
            -0.3786
          ],
          "after": [
            0.0212,
            0.1089,
            -0.0344,
            -0.0694,
            0.042,
            -0.0142,
            0.0883,
            0.0937,
            -0.0672,
            -0.0669,
            0.0163,
            -0.0085,
            0.1016,
            0.031,
            -0.0579,
            0.2216
          ]
        },
        "position_0": {
          "grad": [
            -0.0988,
            0.0931,
            -0.1178,
            -0.0712,
            -0.0258,
            0.0776,
            -0.1165,
            0.1187,
            0.0033,
            -0.0655,
            -0.1971,
            0.208,
            -0.1518,
            0.0539,
            0.1498,
            -0.1568
          ],
          "after": [
            0.048,
            -0.1447,
            -0.2889,
            0.0527,
            0.038,
            -0.0509,
            -0.1549,
            0.0705,
            0.1045,
            0.0142,
            0.0222,
            0.0974,
            0.0422,
            0.0094,
            -0.1379,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.001,
            0.0005,
            -0.002,
            0.0043,
            0.0016,
            -0.0002,
            0.0002,
            0.0003,
            0.0005,
            -0.0008,
            0.0011,
            -0.0014,
            0.0001,
            -0.0017,
            -0.0012
          ],
          "after": [
            -0.084,
            0.0114,
            -0.11,
            -0.0693,
            0.0702,
            0.1312,
            0.0807,
            -0.1606,
            0.0235,
            -0.0248,
            -0.1334,
            -0.0732,
            0.0947,
            -0.0872,
            0.0372,
            0.0826
          ]
        }
      }
    },
    {
      "step": 742,
      "word": "cydnee",
      "loss": 2.6558,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0021,
            -0.0042,
            0.1565,
            -0.0298,
            0.0325,
            -0.0893,
            -0.1119,
            -0.1515,
            -0.0117,
            0.0807,
            0.161,
            0.0109,
            -0.1001,
            -0.016,
            -0.0404,
            -0.1918
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3273,
            -0.0164,
            -0.2842,
            -0.0745,
            -0.711,
            0.411,
            -0.4707,
            -0.1815,
            0.1547,
            -0.2833,
            -0.1482,
            -0.3389,
            0.0985,
            0.4847,
            0.6953,
            -0.1845
          ],
          "after": [
            0.021,
            0.1092,
            -0.0343,
            -0.069,
            0.0422,
            -0.0145,
            0.0886,
            0.0938,
            -0.0674,
            -0.0668,
            0.0167,
            -0.0083,
            0.1016,
            0.0305,
            -0.0582,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            -0.0288,
            0.0084,
            -0.0176,
            0.0989,
            -0.0508,
            -0.0255,
            0.0993,
            0.0644,
            -0.0523,
            0.0128,
            0.04,
            0.0187,
            0.0325,
            0.0,
            -0.0359,
            0.0456
          ],
          "after": [
            0.048,
            -0.1451,
            -0.2886,
            0.0522,
            0.0381,
            -0.0507,
            -0.1551,
            0.0701,
            0.1049,
            0.0143,
            0.0222,
            0.0971,
            0.0419,
            0.0094,
            -0.1379,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0038,
            0.0018,
            -0.0017,
            0.003,
            -0.0003,
            0.0006,
            0.0019,
            -0.0004,
            -0.0017,
            0.0004,
            -0.0011,
            -0.0014,
            0.0016,
            0.0016,
            -0.0013,
            0.0003
          ],
          "after": [
            -0.0841,
            0.0113,
            -0.1097,
            -0.0693,
            0.07,
            0.1308,
            0.0808,
            -0.1605,
            0.0236,
            -0.0249,
            -0.1333,
            -0.0732,
            0.0949,
            -0.0874,
            0.0371,
            0.0825
          ]
        }
      }
    },
    {
      "step": 743,
      "word": "judia",
      "loss": 2.5076,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3416,
            -0.0738,
            -0.1529,
            -0.1174,
            -0.0417,
            -0.0561,
            -0.0411,
            0.0399,
            -0.1287,
            0.2135,
            0.116,
            -0.3107,
            0.1714,
            0.155,
            -0.015,
            -0.2505
          ],
          "after": [
            0.0019,
            -0.004,
            0.1566,
            -0.0298,
            0.0325,
            -0.0893,
            -0.1119,
            -0.1517,
            -0.0113,
            0.0804,
            0.1608,
            0.011,
            -0.1002,
            -0.016,
            -0.0402,
            -0.1917
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0318,
            0.1216,
            -0.1651,
            0.0093,
            0.0358,
            -0.1806,
            0.2814,
            0.0459,
            -0.267,
            0.0202,
            -0.0425,
            -0.0641,
            0.1537,
            -0.0337,
            -0.1941,
            0.1391
          ],
          "after": [
            0.0208,
            0.1094,
            -0.0341,
            -0.0687,
            0.0424,
            -0.0147,
            0.0887,
            0.0937,
            -0.0674,
            -0.0666,
            0.017,
            -0.008,
            0.1015,
            0.0301,
            -0.0584,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            0.0263,
            0.027,
            0.0276,
            0.1266,
            0.0297,
            -0.1239,
            0.1355,
            0.0689,
            -0.2105,
            -0.0931,
            0.0544,
            -0.0091,
            0.3478,
            -0.0875,
            -0.1246,
            0.1484
          ],
          "after": [
            0.048,
            -0.1455,
            -0.2884,
            0.0516,
            0.0382,
            -0.0504,
            -0.1554,
            0.0696,
            0.1054,
            0.0145,
            0.0222,
            0.0968,
            0.0414,
            0.0096,
            -0.1378,
            0.0454
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0005,
            0.0008,
            0.0001,
            -0.0017,
            -0.0012,
            -0.0012,
            0.0008,
            -0.0013,
            -0.0021,
            -0.0007,
            0.001,
            -0.0001,
            0.0001,
            0.0024,
            -0.0002
          ],
          "after": [
            -0.0842,
            0.0113,
            -0.1095,
            -0.0693,
            0.07,
            0.1305,
            0.081,
            -0.1606,
            0.0239,
            -0.0248,
            -0.133,
            -0.0732,
            0.095,
            -0.0876,
            0.0369,
            0.0825
          ]
        }
      }
    },
    {
      "step": 744,
      "word": "amiko",
      "loss": 2.5379,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1046,
            0.084,
            0.1426,
            0.0047,
            0.2982,
            -0.1424,
            0.3749,
            0.0766,
            0.0716,
            0.0508,
            0.1456,
            0.0179,
            -0.1131,
            -0.1075,
            -0.123,
            0.0453
          ],
          "after": [
            0.0018,
            -0.004,
            0.1566,
            -0.0298,
            0.0322,
            -0.0891,
            -0.1121,
            -0.1519,
            -0.0111,
            0.08,
            0.1605,
            0.0111,
            -0.1003,
            -0.016,
            -0.0399,
            -0.1916
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0639,
            0.0942,
            -0.1014,
            -0.0129,
            0.0718,
            -0.2126,
            0.2179,
            0.0328,
            -0.1609,
            0.0341,
            -0.0396,
            -0.0342,
            0.1031,
            -0.0397,
            -0.1779,
            0.1275
          ],
          "after": [
            0.0207,
            0.1096,
            -0.0339,
            -0.0684,
            0.0425,
            -0.0147,
            0.0887,
            0.0936,
            -0.0673,
            -0.0666,
            0.0174,
            -0.0078,
            0.1014,
            0.0298,
            -0.0585,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0939,
            0.0018,
            0.1116,
            -0.179,
            0.0689,
            0.0144,
            -0.3021,
            -0.1399,
            0.1034,
            0.0522,
            0.263,
            0.0532,
            -0.1457,
            0.0364,
            -0.1005,
            -0.2149
          ],
          "after": [
            0.0478,
            -0.1458,
            -0.2884,
            0.0513,
            0.0382,
            -0.0501,
            -0.1554,
            0.0694,
            0.1057,
            0.0146,
            0.0219,
            0.0965,
            0.0411,
            0.0097,
            -0.1376,
            0.0454
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0013,
            -0.0002,
            -0.0002,
            -0.0006,
            0.0011,
            0.003,
            0.001,
            -0.0014,
            0.0007,
            0.0025,
            0.0013,
            -0.001,
            -0.0017,
            0.0007,
            -0.0014,
            0.0002
          ],
          "after": [
            -0.0843,
            0.0113,
            -0.1093,
            -0.0693,
            0.0699,
            0.13,
            0.0811,
            -0.1605,
            0.024,
            -0.025,
            -0.133,
            -0.0732,
            0.0953,
            -0.0878,
            0.0369,
            0.0825
          ]
        }
      }
    },
    {
      "step": 745,
      "word": "khyren",
      "loss": 2.18,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0018,
            -0.004,
            0.1566,
            -0.0298,
            0.032,
            -0.089,
            -0.1124,
            -0.152,
            -0.0108,
            0.0798,
            0.1603,
            0.0112,
            -0.1003,
            -0.0159,
            -0.0397,
            -0.1916
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0608,
            -0.2575,
            -0.0156,
            -0.0402,
            0.0245,
            0.0323,
            -0.2611,
            0.0768,
            -0.0955,
            -0.0914,
            -0.0995,
            -0.143,
            0.0378,
            0.2146,
            0.0728,
            0.0153
          ],
          "after": [
            0.0205,
            0.1098,
            -0.0337,
            -0.0681,
            0.0426,
            -0.0148,
            0.0889,
            0.0934,
            -0.0672,
            -0.0664,
            0.0178,
            -0.0075,
            0.1012,
            0.0294,
            -0.0586,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0754,
            0.033,
            -0.1086,
            0.0057,
            0.044,
            -0.0556,
            0.1526,
            -0.0169,
            -0.0134,
            0.0019,
            -0.0066,
            0.0154,
            0.0302,
            -0.0239,
            -0.083,
            -0.0279
          ],
          "after": [
            0.0477,
            -0.1461,
            -0.2883,
            0.051,
            0.0381,
            -0.0498,
            -0.1555,
            0.0693,
            0.106,
            0.0147,
            0.0217,
            0.0962,
            0.0408,
            0.0098,
            -0.1373,
            0.0455
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0005,
            0.0017,
            0.0007,
            0.0012,
            -0.0011,
            0.0016,
            0.0016,
            -0.0017,
            -0.001,
            -0.0006,
            -0.0013,
            0.001,
            -0.0003,
            -0.0007,
            0.002
          ],
          "after": [
            -0.0844,
            0.0113,
            -0.1093,
            -0.0693,
            0.0697,
            0.1297,
            0.0811,
            -0.1606,
            0.0243,
            -0.025,
            -0.1329,
            -0.0731,
            0.0954,
            -0.0879,
            0.0368,
            0.0823
          ]
        }
      }
    },
    {
      "step": 746,
      "word": "indigo",
      "loss": 2.9195,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0017,
            -0.004,
            0.1566,
            -0.0298,
            0.0318,
            -0.0889,
            -0.1126,
            -0.1522,
            -0.0107,
            0.0795,
            0.16,
            0.0113,
            -0.1003,
            -0.0158,
            -0.0395,
            -0.1916
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0564,
            0.1026,
            -0.1079,
            -0.0096,
            0.1652,
            -0.1315,
            0.243,
            0.0411,
            -0.1773,
            0.0612,
            -0.0075,
            0.0296,
            0.0709,
            -0.0927,
            -0.2522,
            0.1277
          ],
          "after": [
            0.0205,
            0.11,
            -0.0335,
            -0.0679,
            0.0426,
            -0.0147,
            0.0889,
            0.0932,
            -0.0671,
            -0.0663,
            0.0182,
            -0.0073,
            0.1011,
            0.0291,
            -0.0586,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0541,
            -0.2002,
            0.0917,
            -0.1108,
            0.0975,
            0.1389,
            -0.1779,
            0.0303,
            0.0506,
            -0.0132,
            0.0474,
            -0.0965,
            -0.1686,
            0.0303,
            0.022,
            -0.1019
          ],
          "after": [
            0.0478,
            -0.1461,
            -0.2883,
            0.0509,
            0.038,
            -0.0497,
            -0.1555,
            0.0691,
            0.1062,
            0.0148,
            0.0214,
            0.0961,
            0.0406,
            0.0098,
            -0.1371,
            0.0456
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0021,
            -0.0003,
            -0.0003,
            -0.0043,
            0.005,
            -0.0,
            0.0043,
            -0.0006,
            0.001,
            0.0024,
            0.0012,
            -0.0017,
            0.0009,
            -0.0013,
            -0.0031,
            -0.0007
          ],
          "after": [
            -0.0843,
            0.0114,
            -0.1093,
            -0.069,
            0.0692,
            0.1295,
            0.0809,
            -0.1606,
            0.0244,
            -0.0252,
            -0.1329,
            -0.0729,
            0.0955,
            -0.088,
            0.037,
            0.0823
          ]
        }
      }
    },
    {
      "step": 747,
      "word": "ettalyn",
      "loss": 2.8044,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1731,
            0.1169,
            -0.0313,
            -0.0393,
            0.121,
            0.0366,
            -0.0334,
            0.1344,
            -0.0042,
            -0.0805,
            -0.1471,
            0.3006,
            -0.1061,
            -0.055,
            0.0443,
            -0.153
          ],
          "after": [
            0.0018,
            -0.004,
            0.1566,
            -0.0297,
            0.0315,
            -0.0888,
            -0.1127,
            -0.1525,
            -0.0105,
            0.0794,
            0.16,
            0.0111,
            -0.1002,
            -0.0157,
            -0.0394,
            -0.1914
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3241,
            0.4606,
            0.4153,
            0.0824,
            0.117,
            -0.0902,
            0.4482,
            -0.0464,
            -0.0197,
            -0.2489,
            0.0089,
            0.0069,
            -0.0619,
            -0.0298,
            -0.081,
            0.0192
          ],
          "after": [
            0.0206,
            0.1098,
            -0.0335,
            -0.0678,
            0.0425,
            -0.0147,
            0.0887,
            0.0931,
            -0.0669,
            -0.0661,
            0.0185,
            -0.0071,
            0.101,
            0.0289,
            -0.0585,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0034,
            -0.1396,
            0.0528,
            0.0006,
            -0.057,
            0.0591,
            -0.1906,
            -0.0552,
            0.0979,
            0.0889,
            -0.0132,
            -0.0334,
            -0.141,
            0.0116,
            0.0524,
            -0.2433
          ],
          "after": [
            0.0479,
            -0.146,
            -0.2884,
            0.0508,
            0.0379,
            -0.0498,
            -0.1554,
            0.0691,
            0.1063,
            0.0147,
            0.0212,
            0.0961,
            0.0406,
            0.0098,
            -0.137,
            0.0459
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0005,
            -0.0001,
            -0.0002,
            0.0011,
            -0.0004,
            0.0012,
            -0.0001,
            0.0002,
            -0.0002,
            -0.0,
            0.0017,
            -0.0005,
            -0.0007,
            0.0001,
            0.0006
          ],
          "after": [
            -0.0842,
            0.0114,
            -0.1092,
            -0.0688,
            0.0688,
            0.1293,
            0.0806,
            -0.1606,
            0.0245,
            -0.0253,
            -0.1329,
            -0.0728,
            0.0956,
            -0.0879,
            0.0371,
            0.0821
          ]
        }
      }
    },
    {
      "step": 748,
      "word": "madyx",
      "loss": 2.784,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1495,
            0.0678,
            0.0115,
            -0.0405,
            0.2027,
            -0.1156,
            0.0929,
            -0.0896,
            -0.0638,
            -0.0635,
            -0.0064,
            0.0678,
            0.0767,
            0.0537,
            0.0234,
            0.2035
          ],
          "after": [
            0.002,
            -0.0042,
            0.1566,
            -0.0296,
            0.0312,
            -0.0886,
            -0.1129,
            -0.1526,
            -0.0103,
            0.0794,
            0.16,
            0.0109,
            -0.1003,
            -0.0157,
            -0.0393,
            -0.1914
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0983,
            0.143,
            -0.1462,
            0.0875,
            0.0579,
            -0.2003,
            0.2909,
            0.0257,
            -0.2665,
            0.0264,
            -0.0596,
            0.0042,
            0.1294,
            -0.091,
            -0.1951,
            0.1517
          ],
          "after": [
            0.0208,
            0.1096,
            -0.0335,
            -0.0678,
            0.0424,
            -0.0145,
            0.0885,
            0.0929,
            -0.0666,
            -0.0659,
            0.0188,
            -0.007,
            0.1008,
            0.0288,
            -0.0584,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0126,
            0.0485,
            -0.0381,
            0.0356,
            0.1873,
            -0.0201,
            0.2522,
            0.0652,
            -0.0034,
            0.0394,
            0.0542,
            0.0688,
            -0.0304,
            -0.0534,
            -0.0381,
            0.153
          ],
          "after": [
            0.048,
            -0.1459,
            -0.2885,
            0.0507,
            0.0377,
            -0.0498,
            -0.1554,
            0.0689,
            0.1064,
            0.0146,
            0.021,
            0.096,
            0.0407,
            0.0099,
            -0.1368,
            0.046
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0036,
            -0.0049,
            0.0004,
            -0.0016,
            0.0012,
            -0.0006,
            0.0051,
            0.0002,
            -0.0004,
            0.009,
            0.0043,
            -0.0007,
            0.0015,
            -0.0053,
            -0.0007,
            0.0008
          ],
          "after": [
            -0.0838,
            0.0118,
            -0.1092,
            -0.0684,
            0.0683,
            0.1292,
            0.0802,
            -0.1606,
            0.0246,
            -0.0259,
            -0.1332,
            -0.0727,
            0.0955,
            -0.0875,
            0.0372,
            0.082
          ]
        }
      }
    },
    {
      "step": 749,
      "word": "elizebeth",
      "loss": 2.7508,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0021,
            -0.0043,
            0.1566,
            -0.0296,
            0.0309,
            -0.0885,
            -0.1131,
            -0.1527,
            -0.0101,
            0.0793,
            0.16,
            0.0108,
            -0.1003,
            -0.0156,
            -0.0392,
            -0.1915
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1537,
            -0.1123,
            0.3983,
            -0.065,
            -0.0945,
            0.4133,
            -0.2292,
            -0.14,
            0.3835,
            -0.1502,
            0.0778,
            -0.2954,
            -0.2536,
            0.1226,
            0.3372,
            -0.3379
          ],
          "after": [
            0.0209,
            0.1095,
            -0.0337,
            -0.0677,
            0.0424,
            -0.0146,
            0.0884,
            0.093,
            -0.0666,
            -0.0656,
            0.019,
            -0.0067,
            0.1008,
            0.0286,
            -0.0584,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            -0.0063,
            -0.0913,
            0.0405,
            -0.0121,
            0.0171,
            0.0639,
            -0.1255,
            -0.063,
            0.0501,
            0.1146,
            0.0239,
            -0.0025,
            -0.0797,
            -0.0211,
            -0.0206,
            -0.182
          ],
          "after": [
            0.0481,
            -0.1457,
            -0.2885,
            0.0506,
            0.0374,
            -0.0498,
            -0.1554,
            0.0689,
            0.1064,
            0.0144,
            0.0208,
            0.0958,
            0.0408,
            0.01,
            -0.1366,
            0.0463
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            -0.0015,
            0.0022,
            0.0003,
            0.0024,
            0.0012,
            -0.0007,
            -0.0005,
            0.0001,
            0.0009,
            0.0017,
            -0.0015,
            -0.0002,
            -0.0005,
            -0.0035,
            -0.0006
          ],
          "after": [
            -0.0836,
            0.0122,
            -0.1094,
            -0.0682,
            0.0678,
            0.129,
            0.0798,
            -0.1606,
            0.0247,
            -0.0265,
            -0.1336,
            -0.0726,
            0.0955,
            -0.0871,
            0.0374,
            0.0819
          ]
        }
      }
    },
    {
      "step": 750,
      "word": "sheryl",
      "loss": 2.1725,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0023,
            -0.0044,
            0.1565,
            -0.0295,
            0.0306,
            -0.0883,
            -0.1133,
            -0.1528,
            -0.0099,
            0.0793,
            0.16,
            0.0106,
            -0.1003,
            -0.0156,
            -0.0392,
            -0.1915
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0931,
            -0.005,
            0.2146,
            0.1108,
            0.1083,
            0.1991,
            -0.1023,
            -0.0305,
            0.0934,
            0.0845,
            0.2237,
            0.3108,
            -0.2944,
            -0.1957,
            0.1012,
            -0.0462
          ],
          "after": [
            0.021,
            0.1095,
            -0.0339,
            -0.0678,
            0.0423,
            -0.0147,
            0.0884,
            0.0932,
            -0.0666,
            -0.0654,
            0.0189,
            -0.0066,
            0.101,
            0.0286,
            -0.0585,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            0.0031,
            0.0681,
            -0.0134,
            0.1074,
            -0.1296,
            0.0592,
            -0.0413,
            0.0417,
            -0.0306,
            0.0197,
            -0.0896,
            0.0653,
            0.0615,
            0.0212,
            0.0962,
            -0.0143
          ],
          "after": [
            0.0481,
            -0.1456,
            -0.2886,
            0.0504,
            0.0374,
            -0.05,
            -0.1553,
            0.0688,
            0.1064,
            0.0141,
            0.0207,
            0.0957,
            0.0408,
            0.0101,
            -0.1366,
            0.0465
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0026,
            0.0014,
            0.0003,
            0.0023,
            0.0018,
            -0.0007,
            0.0008,
            -0.0013,
            0.0025,
            0.0009,
            0.0013,
            -0.001,
            -0.0019,
            -0.0004,
            -0.0013
          ],
          "after": [
            -0.0833,
            0.0128,
            -0.1097,
            -0.068,
            0.0672,
            0.1287,
            0.0795,
            -0.1606,
            0.0249,
            -0.0271,
            -0.134,
            -0.0725,
            0.0956,
            -0.0867,
            0.0377,
            0.0819
          ]
        }
      }
    },
    {
      "step": 751,
      "word": "samyra",
      "loss": 2.297,
      "learning_rate": 0.0008,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0335,
            0.0655,
            0.0926,
            -0.1127,
            0.1435,
            -0.1243,
            0.2413,
            0.082,
            -0.0175,
            0.1413,
            0.1381,
            -0.0872,
            -0.0439,
            -0.0253,
            -0.0914,
            -0.1048
          ],
          "after": [
            0.0024,
            -0.0046,
            0.1565,
            -0.0294,
            0.0303,
            -0.0881,
            -0.1136,
            -0.153,
            -0.0098,
            0.0792,
            0.1598,
            0.0106,
            -0.1002,
            -0.0156,
            -0.039,
            -0.1914
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0407,
            0.0617,
            -0.1029,
            0.0367,
            0.0422,
            -0.1333,
            0.2066,
            0.0018,
            -0.162,
            0.0363,
            -0.0135,
            0.052,
            0.0465,
            -0.0894,
            -0.1552,
            0.1112
          ],
          "after": [
            0.0211,
            0.1094,
            -0.0341,
            -0.0679,
            0.0422,
            -0.0148,
            0.0882,
            0.0933,
            -0.0665,
            -0.0653,
            0.0188,
            -0.0066,
            0.1012,
            0.0286,
            -0.0585,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            0.0029,
            0.0836,
            -0.0195,
            0.0812,
            -0.1006,
            0.0529,
            -0.0228,
            0.0365,
            -0.0591,
            0.053,
            -0.0757,
            0.0898,
            0.0748,
            -0.0121,
            0.054,
            -0.0168
          ],
          "after": [
            0.0482,
            -0.1457,
            -0.2886,
            0.0502,
            0.0374,
            -0.0502,
            -0.1553,
            0.0687,
            0.1065,
            0.0138,
            0.0207,
            0.0954,
            0.0408,
            0.0102,
            -0.1367,
            0.0467
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            0.0001,
            -0.0009,
            0.0008,
            -0.0004,
            0.001,
            0.0009,
            0.0011,
            0.0001,
            -0.001,
            -0.0012,
            -0.0002,
            0.0002,
            -0.0004,
            0.0006,
            0.0009
          ],
          "after": [
            -0.0831,
            0.0132,
            -0.1098,
            -0.0679,
            0.0667,
            0.1284,
            0.0793,
            -0.1608,
            0.0251,
            -0.0276,
            -0.1343,
            -0.0724,
            0.0956,
            -0.0863,
            0.0379,
            0.0819
          ]
        }
      }
    },
    {
      "step": 752,
      "word": "kaisen",
      "loss": 1.9378,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0429,
            -0.2824,
            0.0635,
            -0.0762,
            0.1116,
            0.1585,
            -0.2163,
            0.09,
            0.009,
            -0.0055,
            0.1329,
            -0.1656,
            -0.1704,
            0.0153,
            0.0038,
            -0.1
          ],
          "after": [
            0.0025,
            -0.0044,
            0.1564,
            -0.0291,
            0.03,
            -0.088,
            -0.1137,
            -0.1533,
            -0.0097,
            0.079,
            0.1596,
            0.0107,
            -0.1001,
            -0.0155,
            -0.0389,
            -0.1913
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0339,
            -0.0007,
            0.1619,
            0.1171,
            -0.1326,
            0.1286,
            -0.1889,
            -0.0151,
            -0.0418,
            0.0349,
            -0.0964,
            -0.2838,
            0.2413,
            0.2452,
            -0.0549,
            -0.0022
          ],
          "after": [
            0.0212,
            0.1093,
            -0.0343,
            -0.0682,
            0.0422,
            -0.0149,
            0.0882,
            0.0934,
            -0.0665,
            -0.0652,
            0.0189,
            -0.0064,
            0.1011,
            0.0285,
            -0.0584,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            -0.0715,
            0.0394,
            -0.1059,
            -0.0374,
            0.0769,
            -0.0679,
            0.1211,
            -0.0327,
            -0.02,
            0.0282,
            -0.0058,
            0.0218,
            0.0319,
            -0.0382,
            -0.1317,
            -0.0477
          ],
          "after": [
            0.0484,
            -0.1458,
            -0.2885,
            0.05,
            0.0374,
            -0.0503,
            -0.1553,
            0.0686,
            0.1066,
            0.0135,
            0.0207,
            0.0952,
            0.0407,
            0.0103,
            -0.1366,
            0.0469
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            0.0002,
            0.0014,
            0.0027,
            0.0003,
            0.0045,
            -0.0005,
            0.0015,
            0.0001,
            0.0003,
            -0.0019,
            -0.0007,
            0.0012,
            0.0011,
            -0.0012,
            0.0025
          ],
          "after": [
            -0.0829,
            0.0136,
            -0.1101,
            -0.068,
            0.0663,
            0.1278,
            0.0791,
            -0.1611,
            0.0252,
            -0.028,
            -0.1344,
            -0.0723,
            0.0956,
            -0.086,
            0.0381,
            0.0816
          ]
        }
      }
    },
    {
      "step": 753,
      "word": "lys",
      "loss": 2.86,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0026,
            -0.0043,
            0.1563,
            -0.0289,
            0.0297,
            -0.088,
            -0.1138,
            -0.1536,
            -0.0096,
            0.0789,
            0.1593,
            0.0107,
            -0.1,
            -0.0155,
            -0.0388,
            -0.1912
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0052,
            0.0933,
            -0.2244,
            0.0217,
            -0.1124,
            -0.1766,
            0.2816,
            -0.0261,
            -0.254,
            -0.0054,
            0.0119,
            -0.0652,
            0.1334,
            -0.0847,
            -0.1117,
            0.1913
          ],
          "after": [
            0.0213,
            0.1091,
            -0.0344,
            -0.0684,
            0.0422,
            -0.0149,
            0.0881,
            0.0936,
            -0.0663,
            -0.0652,
            0.0189,
            -0.0061,
            0.101,
            0.0284,
            -0.0583,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.1617,
            0.139,
            -0.1821,
            -0.1016,
            -0.0427,
            0.141,
            -0.1215,
            0.1902,
            -0.0102,
            -0.0996,
            -0.2672,
            0.3332,
            -0.2118,
            0.0739,
            0.2391,
            -0.2172
          ],
          "after": [
            0.0488,
            -0.146,
            -0.288,
            0.05,
            0.0374,
            -0.0506,
            -0.1552,
            0.0683,
            0.1067,
            0.0134,
            0.021,
            0.0946,
            0.0408,
            0.0103,
            -0.1367,
            0.0473
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0021,
            -0.0007,
            0.0021,
            0.0004,
            -0.0004,
            -0.0009,
            0.0013,
            -0.0003,
            -0.0008,
            0.0005,
            -0.0011,
            -0.0047,
            0.0046,
            -0.0003,
            0.0011,
            0.0005
          ],
          "after": [
            -0.0826,
            0.014,
            -0.1104,
            -0.0681,
            0.066,
            0.1273,
            0.0789,
            -0.1613,
            0.0254,
            -0.0284,
            -0.1344,
            -0.0719,
            0.0952,
            -0.0857,
            0.0382,
            0.0814
          ]
        }
      }
    },
    {
      "step": 754,
      "word": "alicja",
      "loss": 2.4428,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0495,
            0.197,
            -0.1487,
            -0.1993,
            -0.0124,
            -0.0391,
            -0.0715,
            0.0496,
            -0.1895,
            0.0639,
            -0.056,
            0.224,
            -0.0515,
            0.0783,
            0.069,
            -0.471
          ],
          "after": [
            0.0027,
            -0.0044,
            0.1563,
            -0.0286,
            0.0295,
            -0.0879,
            -0.1138,
            -0.1538,
            -0.0093,
            0.0788,
            0.1592,
            0.0106,
            -0.0998,
            -0.0156,
            -0.0388,
            -0.1908
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0274,
            0.1739,
            -0.1715,
            -0.0019,
            0.0803,
            -0.2116,
            0.3214,
            0.0174,
            -0.2409,
            -0.0346,
            -0.0066,
            -0.0219,
            0.109,
            -0.1095,
            -0.1899,
            0.2222
          ],
          "after": [
            0.0214,
            0.1089,
            -0.0344,
            -0.0686,
            0.0422,
            -0.0148,
            0.0879,
            0.0937,
            -0.066,
            -0.0651,
            0.0189,
            -0.0059,
            0.1009,
            0.0285,
            -0.0582,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            0.0791,
            -0.0011,
            0.096,
            -0.138,
            0.08,
            0.0229,
            -0.2393,
            -0.1101,
            0.0826,
            0.0555,
            0.2218,
            0.0633,
            -0.1116,
            0.0065,
            -0.0944,
            -0.1804
          ],
          "after": [
            0.0491,
            -0.1462,
            -0.2878,
            0.0501,
            0.0373,
            -0.0508,
            -0.155,
            0.0682,
            0.1067,
            0.0132,
            0.021,
            0.0941,
            0.041,
            0.0103,
            -0.1368,
            0.0477
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.003,
            0.0009,
            -0.0035,
            -0.0009,
            0.0019,
            0.0011,
            0.0027,
            -0.0007,
            -0.001,
            0.0025,
            0.0027,
            -0.0017,
            0.0004,
            -0.0012,
            -0.0012,
            0.0011
          ],
          "after": [
            -0.0826,
            0.0143,
            -0.1105,
            -0.0681,
            0.0656,
            0.1269,
            0.0785,
            -0.1614,
            0.0256,
            -0.0288,
            -0.1346,
            -0.0715,
            0.0949,
            -0.0854,
            0.0384,
            0.0811
          ]
        }
      }
    },
    {
      "step": 755,
      "word": "haneef",
      "loss": 2.6984,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0266,
            0.163,
            -0.1653,
            -0.0185,
            -0.256,
            -0.0808,
            -0.0605,
            -0.0667,
            0.0753,
            -0.1487,
            -0.1025,
            -0.0903,
            0.0495,
            0.1504,
            0.0641,
            0.08
          ],
          "after": [
            0.0028,
            -0.0046,
            0.1565,
            -0.0282,
            0.0294,
            -0.0878,
            -0.1137,
            -0.154,
            -0.0091,
            0.0788,
            0.1592,
            0.0106,
            -0.0997,
            -0.0158,
            -0.0388,
            -0.1905
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0316,
            0.0777,
            -0.1564,
            -0.0723,
            -0.3325,
            0.2605,
            -0.3612,
            -0.1044,
            0.0472,
            -0.5931,
            -0.2834,
            -0.0522,
            -0.0956,
            0.2686,
            0.7551,
            -0.1184
          ],
          "after": [
            0.0215,
            0.1087,
            -0.0343,
            -0.0687,
            0.0424,
            -0.0149,
            0.0878,
            0.0939,
            -0.0658,
            -0.0646,
            0.0193,
            -0.0057,
            0.1008,
            0.0283,
            -0.0584,
            0.222
          ]
        },
        "position_0": {
          "grad": [
            0.0145,
            -0.1011,
            0.106,
            0.0098,
            -0.0496,
            0.1108,
            -0.1344,
            0.0718,
            0.1806,
            0.1237,
            -0.0999,
            -0.1483,
            0.1249,
            0.1361,
            0.0297,
            -0.0045
          ],
          "after": [
            0.0493,
            -0.1462,
            -0.2878,
            0.0502,
            0.0373,
            -0.0512,
            -0.1548,
            0.0679,
            0.1065,
            0.0129,
            0.0211,
            0.0938,
            0.0411,
            0.01,
            -0.1369,
            0.0481
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0003,
            0.0009,
            -0.0004,
            0.0009,
            -0.0009,
            -0.0011,
            -0.0006,
            -0.0004,
            -0.0009,
            0.0005,
            -0.0015,
            0.0002,
            0.0003,
            -0.0004,
            -0.001
          ],
          "after": [
            -0.0826,
            0.0145,
            -0.1106,
            -0.0681,
            0.0652,
            0.1266,
            0.0783,
            -0.1614,
            0.0259,
            -0.0292,
            -0.1348,
            -0.071,
            0.0946,
            -0.0852,
            0.0386,
            0.081
          ]
        }
      }
    },
    {
      "step": 756,
      "word": "kendrik",
      "loss": 2.614,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0029,
            -0.0048,
            0.1566,
            -0.028,
            0.0294,
            -0.0877,
            -0.1137,
            -0.1541,
            -0.009,
            0.0788,
            0.1592,
            0.0106,
            -0.0996,
            -0.016,
            -0.0389,
            -0.1903
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0735,
            -0.1686,
            0.2206,
            0.044,
            0.3212,
            0.138,
            -0.2824,
            -0.0449,
            0.3032,
            0.1425,
            0.1149,
            0.2445,
            -0.1783,
            -0.0258,
            -0.2548,
            -0.1811
          ],
          "after": [
            0.0215,
            0.1086,
            -0.0343,
            -0.0688,
            0.0424,
            -0.015,
            0.0879,
            0.0942,
            -0.0657,
            -0.0643,
            0.0194,
            -0.0057,
            0.1008,
            0.0282,
            -0.0585,
            0.222
          ]
        },
        "position_0": {
          "grad": [
            -0.0769,
            0.0429,
            -0.1051,
            0.0117,
            0.0628,
            -0.0435,
            0.164,
            -0.0055,
            -0.0342,
            0.0201,
            -0.0025,
            0.0539,
            0.0341,
            -0.052,
            -0.1007,
            -0.0132
          ],
          "after": [
            0.0495,
            -0.1463,
            -0.2876,
            0.0503,
            0.0373,
            -0.0515,
            -0.1547,
            0.0678,
            0.1064,
            0.0126,
            0.0212,
            0.0935,
            0.0411,
            0.0099,
            -0.1368,
            0.0484
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0001,
            0.0013,
            -0.0003,
            0.0029,
            -0.0009,
            0.0016,
            0.0,
            0.0007,
            0.001,
            0.001,
            0.001,
            0.0009,
            -0.0007,
            -0.0035,
            0.0009
          ],
          "after": [
            -0.0826,
            0.0147,
            -0.1107,
            -0.0681,
            0.0647,
            0.1264,
            0.078,
            -0.1614,
            0.026,
            -0.0295,
            -0.135,
            -0.0707,
            0.0943,
            -0.0849,
            0.0389,
            0.0808
          ]
        }
      }
    },
    {
      "step": 757,
      "word": "jacinto",
      "loss": 2.3706,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0989,
            0.0596,
            0.1044,
            0.1308,
            -0.0042,
            -0.0521,
            0.1463,
            0.0884,
            -0.1101,
            0.0686,
            0.0692,
            0.0394,
            0.0126,
            -0.0501,
            -0.1187,
            -0.0039
          ],
          "after": [
            0.003,
            -0.005,
            0.1566,
            -0.0279,
            0.0294,
            -0.0875,
            -0.1138,
            -0.1543,
            -0.0087,
            0.0787,
            0.1591,
            0.0105,
            -0.0996,
            -0.0161,
            -0.0388,
            -0.1901
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0779,
            0.0917,
            -0.1179,
            0.0149,
            0.0821,
            -0.1222,
            0.2521,
            0.038,
            -0.1912,
            0.0011,
            -0.0264,
            0.0066,
            0.086,
            -0.066,
            -0.1556,
            0.1381
          ],
          "after": [
            0.0216,
            0.1085,
            -0.0342,
            -0.069,
            0.0423,
            -0.0151,
            0.0878,
            0.0944,
            -0.0656,
            -0.064,
            0.0196,
            -0.0057,
            0.1008,
            0.0281,
            -0.0585,
            0.222
          ]
        },
        "position_0": {
          "grad": [
            0.0137,
            0.0291,
            0.0076,
            0.0967,
            0.0222,
            -0.0942,
            0.1165,
            0.0536,
            -0.1621,
            -0.0371,
            0.0299,
            0.027,
            0.2437,
            -0.0927,
            -0.1121,
            0.0998
          ],
          "after": [
            0.0498,
            -0.1464,
            -0.2875,
            0.0502,
            0.0372,
            -0.0516,
            -0.1546,
            0.0675,
            0.1064,
            0.0124,
            0.0212,
            0.0932,
            0.0409,
            0.01,
            -0.1366,
            0.0486
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0025,
            0.001,
            -0.0012,
            -0.0007,
            0.0002,
            -0.0016,
            0.0004,
            -0.0007,
            0.0001,
            0.0011,
            0.0005,
            -0.001,
            0.0007,
            -0.0003,
            -0.0001
          ],
          "after": [
            -0.0826,
            0.015,
            -0.111,
            -0.068,
            0.0643,
            0.1262,
            0.0779,
            -0.1615,
            0.0262,
            -0.0298,
            -0.1353,
            -0.0704,
            0.0942,
            -0.0848,
            0.0392,
            0.0806
          ]
        }
      }
    },
    {
      "step": 758,
      "word": "breyer",
      "loss": 2.331,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0031,
            -0.0052,
            0.1566,
            -0.0278,
            0.0294,
            -0.0873,
            -0.1139,
            -0.1545,
            -0.0085,
            0.0787,
            0.159,
            0.0105,
            -0.0995,
            -0.0161,
            -0.0387,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1262,
            -0.1051,
            0.1774,
            0.1082,
            -0.0386,
            0.3905,
            -0.6947,
            0.0345,
            0.2252,
            -0.2748,
            -0.0201,
            0.1689,
            -0.3187,
            0.3069,
            0.3075,
            -0.1856
          ],
          "after": [
            0.0215,
            0.1084,
            -0.0343,
            -0.0692,
            0.0423,
            -0.0153,
            0.088,
            0.0945,
            -0.0656,
            -0.0636,
            0.0197,
            -0.0058,
            0.101,
            0.0278,
            -0.0586,
            0.222
          ]
        },
        "position_0": {
          "grad": [
            0.0575,
            -0.025,
            0.0909,
            -0.0205,
            -0.0137,
            0.0348,
            0.0644,
            0.019,
            -0.0642,
            -0.0341,
            -0.0092,
            -0.0131,
            0.1068,
            -0.1178,
            -0.0204,
            0.2215
          ],
          "after": [
            0.0499,
            -0.1465,
            -0.2875,
            0.0502,
            0.0371,
            -0.0517,
            -0.1547,
            0.0673,
            0.1065,
            0.0123,
            0.0213,
            0.0929,
            0.0407,
            0.0102,
            -0.1365,
            0.0486
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0022,
            0.0008,
            0.0013,
            -0.0025,
            0.0009,
            -0.0024,
            0.0,
            -0.0024,
            -0.0037,
            -0.0015,
            0.0016,
            -0.0003,
            0.0034,
            0.0036,
            0.0
          ],
          "after": [
            -0.0826,
            0.0152,
            -0.1112,
            -0.068,
            0.0641,
            0.126,
            0.0779,
            -0.1615,
            0.0266,
            -0.0299,
            -0.1355,
            -0.0703,
            0.094,
            -0.0849,
            0.0392,
            0.0805
          ]
        }
      }
    },
    {
      "step": 759,
      "word": "cyniah",
      "loss": 2.2865,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1489,
            -0.0615,
            0.2482,
            0.0612,
            0.0866,
            0.1437,
            -0.0151,
            0.1184,
            0.1586,
            0.131,
            -0.0769,
            -0.0236,
            0.1607,
            0.0242,
            -0.107,
            0.0862
          ],
          "after": [
            0.0033,
            -0.0053,
            0.1565,
            -0.0278,
            0.0293,
            -0.0874,
            -0.1139,
            -0.1548,
            -0.0085,
            0.0785,
            0.1591,
            0.0105,
            -0.0996,
            -0.0162,
            -0.0386,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0085,
            -0.0089,
            -0.1097,
            0.0002,
            0.0489,
            -0.0812,
            0.1424,
            0.0103,
            -0.1436,
            0.0644,
            -0.0047,
            -0.0305,
            0.0725,
            -0.0672,
            -0.1338,
            0.1043
          ],
          "after": [
            0.0215,
            0.1084,
            -0.0343,
            -0.0694,
            0.0423,
            -0.0154,
            0.0881,
            0.0946,
            -0.0656,
            -0.0633,
            0.0199,
            -0.0059,
            0.1011,
            0.0277,
            -0.0586,
            0.222
          ]
        },
        "position_0": {
          "grad": [
            -0.0235,
            0.0016,
            -0.0168,
            0.077,
            -0.0652,
            -0.0396,
            0.0612,
            0.0527,
            -0.0272,
            0.0224,
            0.0353,
            -0.0045,
            0.0071,
            0.0025,
            -0.0394,
            0.0203
          ],
          "after": [
            0.05,
            -0.1466,
            -0.2875,
            0.0501,
            0.0371,
            -0.0517,
            -0.1547,
            0.067,
            0.1067,
            0.0122,
            0.0213,
            0.0927,
            0.0405,
            0.0104,
            -0.1363,
            0.0486
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0006,
            0.0006,
            -0.0001,
            -0.0008,
            -0.0006,
            -0.0011,
            0.0,
            0.0001,
            -0.0008,
            -0.0007,
            -0.0002,
            0.0002,
            0.0002,
            0.0009,
            0.0003
          ],
          "after": [
            -0.0826,
            0.0153,
            -0.1115,
            -0.068,
            0.064,
            0.1258,
            0.0779,
            -0.1616,
            0.0269,
            -0.0299,
            -0.1355,
            -0.0702,
            0.0939,
            -0.085,
            0.0392,
            0.0803
          ]
        }
      }
    },
    {
      "step": 760,
      "word": "athziry",
      "loss": 2.8378,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.048,
            -0.1402,
            0.1817,
            0.0602,
            -0.0076,
            0.0758,
            0.1109,
            0.0552,
            -0.1581,
            -0.069,
            -0.1017,
            -0.2884,
            0.2352,
            0.0818,
            0.0034,
            0.1883
          ],
          "after": [
            0.0036,
            -0.0052,
            0.1562,
            -0.0278,
            0.0293,
            -0.0875,
            -0.1141,
            -0.1552,
            -0.0084,
            0.0784,
            0.1592,
            0.0106,
            -0.0999,
            -0.0164,
            -0.0384,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0841,
            0.1179,
            -0.102,
            0.0415,
            0.0902,
            -0.1406,
            0.2542,
            0.0055,
            -0.1333,
            0.0102,
            -0.023,
            0.0193,
            0.0777,
            -0.0762,
            -0.2171,
            0.1272
          ],
          "after": [
            0.0216,
            0.1083,
            -0.0342,
            -0.0696,
            0.0422,
            -0.0155,
            0.0881,
            0.0946,
            -0.0654,
            -0.063,
            0.02,
            -0.006,
            0.1012,
            0.0276,
            -0.0586,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            0.0812,
            -0.0117,
            0.0816,
            -0.1236,
            0.0111,
            -0.0015,
            -0.229,
            -0.0932,
            0.0954,
            0.0332,
            0.1764,
            0.0051,
            -0.1336,
            0.0366,
            -0.0382,
            -0.1664
          ],
          "after": [
            0.0499,
            -0.1466,
            -0.2877,
            0.0501,
            0.0371,
            -0.0518,
            -0.1546,
            0.0669,
            0.1067,
            0.012,
            0.0211,
            0.0926,
            0.0405,
            0.0105,
            -0.1361,
            0.0487
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            -0.0018,
            -0.0008,
            0.0007,
            0.0001,
            0.0009,
            -0.0011,
            0.0003,
            -0.0006,
            0.0016,
            0.0002,
            -0.0001,
            -0.0009,
            0.0012,
            -0.0005,
            0.0009
          ],
          "after": [
            -0.0827,
            0.0156,
            -0.1117,
            -0.068,
            0.0639,
            0.1257,
            0.078,
            -0.1617,
            0.0272,
            -0.03,
            -0.1356,
            -0.0701,
            0.0938,
            -0.0852,
            0.0393,
            0.0802
          ]
        }
      }
    },
    {
      "step": 761,
      "word": "malaysia",
      "loss": 2.3704,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0919,
            0.0065,
            0.113,
            -0.2177,
            0.1929,
            0.1538,
            -0.1212,
            0.081,
            -0.0839,
            0.1272,
            -0.0756,
            0.3282,
            -0.0758,
            -0.0239,
            -0.1167,
            -0.1996
          ],
          "after": [
            0.0038,
            -0.0052,
            0.1559,
            -0.0276,
            0.0291,
            -0.0878,
            -0.1141,
            -0.1556,
            -0.0081,
            0.0782,
            0.1594,
            0.0106,
            -0.1,
            -0.0165,
            -0.0382,
            -0.1897
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0739,
            0.0825,
            -0.0881,
            0.0394,
            0.0405,
            -0.1368,
            0.1911,
            -0.0146,
            -0.1636,
            0.0049,
            -0.0057,
            0.0209,
            0.0499,
            -0.0629,
            -0.1373,
            0.13
          ],
          "after": [
            0.0216,
            0.1082,
            -0.0341,
            -0.0699,
            0.0421,
            -0.0155,
            0.0881,
            0.0947,
            -0.0653,
            -0.0628,
            0.0202,
            -0.006,
            0.1012,
            0.0275,
            -0.0585,
            0.2218
          ]
        },
        "position_0": {
          "grad": [
            -0.0039,
            0.0334,
            -0.0307,
            -0.0138,
            0.1527,
            -0.0197,
            0.1558,
            0.0379,
            0.006,
            0.0762,
            0.0273,
            0.0604,
            -0.0436,
            -0.0787,
            -0.0746,
            0.067
          ],
          "after": [
            0.0499,
            -0.1467,
            -0.2877,
            0.0502,
            0.037,
            -0.0518,
            -0.1546,
            0.0668,
            0.1067,
            0.0117,
            0.021,
            0.0923,
            0.0404,
            0.0107,
            -0.1358,
            0.0487
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            -0.0002,
            0.0006,
            0.0026,
            -0.0023,
            -0.0002,
            -0.0021,
            0.001,
            -0.0005,
            0.0027,
            -0.0009,
            -0.0025,
            0.002,
            0.0002,
            0.0009,
            -0.0016
          ],
          "after": [
            -0.0828,
            0.0158,
            -0.1119,
            -0.0682,
            0.064,
            0.1255,
            0.0782,
            -0.1618,
            0.0275,
            -0.0302,
            -0.1356,
            -0.0698,
            0.0937,
            -0.0853,
            0.0393,
            0.0801
          ]
        }
      }
    },
    {
      "step": 762,
      "word": "seydina",
      "loss": 2.0884,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0924,
            -0.0328,
            -0.0391,
            -0.0737,
            -0.0063,
            -0.0333,
            -0.0217,
            -0.0366,
            -0.056,
            0.057,
            0.0835,
            -0.0743,
            0.0219,
            0.0614,
            -0.0005,
            -0.0794
          ],
          "after": [
            0.004,
            -0.0051,
            0.1557,
            -0.0274,
            0.029,
            -0.088,
            -0.1141,
            -0.1559,
            -0.0079,
            0.078,
            0.1594,
            0.0106,
            -0.1002,
            -0.0167,
            -0.0379,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0327,
            0.044,
            0.3418,
            0.1173,
            0.2528,
            0.0576,
            -0.191,
            0.0214,
            0.267,
            0.2001,
            -0.0035,
            -0.0094,
            0.0469,
            0.114,
            -0.2538,
            -0.0859
          ],
          "after": [
            0.0217,
            0.108,
            -0.0342,
            -0.0702,
            0.0418,
            -0.0155,
            0.0881,
            0.0947,
            -0.0652,
            -0.0628,
            0.0203,
            -0.0061,
            0.1012,
            0.0274,
            -0.0583,
            0.2217
          ]
        },
        "position_0": {
          "grad": [
            -0.0008,
            0.0622,
            -0.0143,
            0.0627,
            -0.0957,
            0.048,
            -0.0465,
            0.0238,
            -0.0331,
            0.0346,
            -0.062,
            0.0605,
            0.0462,
            -0.0025,
            0.0537,
            -0.0238
          ],
          "after": [
            0.0498,
            -0.1468,
            -0.2878,
            0.0502,
            0.037,
            -0.0519,
            -0.1546,
            0.0666,
            0.1067,
            0.0115,
            0.0209,
            0.0921,
            0.0404,
            0.0109,
            -0.1357,
            0.0488
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            0.0005,
            0.0002,
            -0.0003,
            0.0006,
            -0.0001,
            -0.0006,
            -0.0005,
            -0.0001,
            -0.0008,
            0.0,
            -0.0001,
            -0.0005,
            0.0003,
            0.0002,
            -0.0005
          ],
          "after": [
            -0.083,
            0.016,
            -0.112,
            -0.0684,
            0.064,
            0.1254,
            0.0783,
            -0.1619,
            0.0278,
            -0.0303,
            -0.1356,
            -0.0696,
            0.0935,
            -0.0855,
            0.0392,
            0.0801
          ]
        }
      }
    },
    {
      "step": 763,
      "word": "kendra",
      "loss": 2.1656,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0995,
            0.0025,
            -0.0218,
            -0.1057,
            -0.0246,
            -0.0482,
            -0.0266,
            -0.0192,
            -0.0479,
            0.1025,
            0.0931,
            -0.1046,
            0.0565,
            0.0506,
            -0.0208,
            -0.1323
          ],
          "after": [
            0.004,
            -0.0051,
            0.1555,
            -0.0271,
            0.0288,
            -0.0881,
            -0.114,
            -0.1561,
            -0.0076,
            0.0778,
            0.1594,
            0.0106,
            -0.1003,
            -0.0169,
            -0.0377,
            -0.1894
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1492,
            -0.2139,
            0.2401,
            0.054,
            0.2761,
            0.2033,
            -0.3941,
            -0.0604,
            0.3762,
            0.1767,
            0.1458,
            0.2395,
            -0.1779,
            -0.0031,
            -0.2379,
            -0.2758
          ],
          "after": [
            0.0217,
            0.108,
            -0.0344,
            -0.0706,
            0.0415,
            -0.0156,
            0.0882,
            0.0948,
            -0.0654,
            -0.0628,
            0.0202,
            -0.0063,
            0.1012,
            0.0273,
            -0.058,
            0.2218
          ]
        },
        "position_0": {
          "grad": [
            -0.076,
            0.0344,
            -0.1141,
            0.0081,
            0.0417,
            -0.0633,
            0.157,
            -0.0057,
            -0.0163,
            0.001,
            -0.0179,
            0.0374,
            0.027,
            -0.0384,
            -0.0888,
            -0.0311
          ],
          "after": [
            0.05,
            -0.147,
            -0.2876,
            0.0502,
            0.0369,
            -0.0518,
            -0.1547,
            0.0665,
            0.1068,
            0.0113,
            0.0208,
            0.0918,
            0.0403,
            0.0111,
            -0.1354,
            0.0489
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            -0.0,
            -0.0002,
            0.0007,
            0.0012,
            -0.0003,
            0.0008,
            0.0003,
            0.0008,
            0.002,
            0.001,
            -0.0001,
            0.0018,
            -0.0005,
            -0.0034,
            -0.0004
          ],
          "after": [
            -0.0833,
            0.0161,
            -0.1122,
            -0.0686,
            0.0639,
            0.1253,
            0.0784,
            -0.162,
            0.028,
            -0.0305,
            -0.1357,
            -0.0694,
            0.0933,
            -0.0856,
            0.0394,
            0.0802
          ]
        }
      }
    },
    {
      "step": 764,
      "word": "sarabi",
      "loss": 2.1356,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0104,
            -0.0633,
            0.2993,
            -0.1131,
            0.147,
            0.0935,
            0.1085,
            -0.1849,
            0.088,
            -0.1467,
            0.0569,
            0.1293,
            -0.009,
            -0.308,
            -0.0984,
            0.4278
          ],
          "after": [
            0.0041,
            -0.005,
            0.1551,
            -0.0268,
            0.0287,
            -0.0883,
            -0.1141,
            -0.156,
            -0.0075,
            0.0777,
            0.1593,
            0.0106,
            -0.1005,
            -0.0167,
            -0.0374,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0227,
            0.0598,
            -0.0608,
            0.0134,
            0.0379,
            -0.1501,
            0.1449,
            0.0005,
            -0.1694,
            0.0232,
            0.0043,
            0.0363,
            0.0473,
            -0.0313,
            -0.1506,
            0.0818
          ],
          "after": [
            0.0217,
            0.108,
            -0.0346,
            -0.0709,
            0.0412,
            -0.0156,
            0.0883,
            0.0949,
            -0.0654,
            -0.0629,
            0.0202,
            -0.0065,
            0.1013,
            0.0272,
            -0.0577,
            0.2218
          ]
        },
        "position_0": {
          "grad": [
            0.0091,
            0.0763,
            -0.0199,
            0.0707,
            -0.1123,
            0.0427,
            -0.0396,
            0.0293,
            -0.0501,
            0.0506,
            -0.0766,
            0.0746,
            0.0617,
            -0.0105,
            0.0518,
            -0.0293
          ],
          "after": [
            0.05,
            -0.1472,
            -0.2874,
            0.0501,
            0.0369,
            -0.0519,
            -0.1548,
            0.0663,
            0.1068,
            0.011,
            0.0209,
            0.0915,
            0.0402,
            0.0113,
            -0.1353,
            0.0489
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            0.0007,
            -0.0002,
            0.001,
            -0.0017,
            -0.0017,
            0.0,
            -0.0007,
            -0.0003,
            0.0009,
            0.0004,
            -0.0007,
            0.0009,
            0.0005,
            -0.0003,
            -0.0009
          ],
          "after": [
            -0.0836,
            0.0162,
            -0.1123,
            -0.0688,
            0.064,
            0.1254,
            0.0785,
            -0.1621,
            0.0282,
            -0.0308,
            -0.1357,
            -0.0692,
            0.0931,
            -0.0857,
            0.0395,
            0.0803
          ]
        }
      }
    },
    {
      "step": 765,
      "word": "jerone",
      "loss": 1.943,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0041,
            -0.0049,
            0.1548,
            -0.0265,
            0.0285,
            -0.0885,
            -0.1142,
            -0.156,
            -0.0074,
            0.0776,
            0.1592,
            0.0106,
            -0.1006,
            -0.0166,
            -0.0372,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.5006,
            -0.3254,
            0.3284,
            -0.0526,
            -0.3356,
            0.355,
            -1.0609,
            -0.3202,
            0.6589,
            0.1392,
            0.296,
            -0.2673,
            -0.1957,
            0.3577,
            0.2154,
            -0.4711
          ],
          "after": [
            0.0214,
            0.1082,
            -0.0349,
            -0.0712,
            0.0411,
            -0.0158,
            0.0887,
            0.0955,
            -0.0658,
            -0.0631,
            0.0198,
            -0.0065,
            0.1015,
            0.0269,
            -0.0575,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            0.0229,
            0.0003,
            0.0174,
            0.0917,
            -0.003,
            -0.1089,
            0.0865,
            0.0631,
            -0.1474,
            -0.0626,
            0.021,
            -0.0131,
            0.2513,
            -0.0927,
            -0.1044,
            0.0786
          ],
          "after": [
            0.0501,
            -0.1474,
            -0.2873,
            0.0499,
            0.037,
            -0.0517,
            -0.1549,
            0.0661,
            0.1071,
            0.0109,
            0.0209,
            0.0913,
            0.0399,
            0.0116,
            -0.1351,
            0.049
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0003,
            0.0009,
            -0.0006,
            -0.0,
            -0.0008,
            -0.0023,
            -0.0007,
            0.0007,
            -0.0002,
            0.0008,
            -0.0009,
            -0.0004,
            0.0004,
            -0.0001,
            -0.0005
          ],
          "after": [
            -0.0839,
            0.0163,
            -0.1124,
            -0.0689,
            0.064,
            0.1255,
            0.0787,
            -0.162,
            0.0282,
            -0.031,
            -0.1359,
            -0.069,
            0.0929,
            -0.0859,
            0.0396,
            0.0804
          ]
        }
      }
    },
    {
      "step": 766,
      "word": "dakarai",
      "loss": 2.4071,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3813,
            -0.2096,
            0.1612,
            0.0547,
            0.3558,
            0.3168,
            0.2367,
            0.0435,
            0.1718,
            -0.1793,
            -0.1348,
            0.2558,
            -0.4318,
            -0.2945,
            -0.0993,
            0.1834
          ],
          "after": [
            0.0044,
            -0.0047,
            0.1544,
            -0.0262,
            0.0281,
            -0.0889,
            -0.1144,
            -0.156,
            -0.0074,
            0.0777,
            0.1593,
            0.0104,
            -0.1004,
            -0.0162,
            -0.0369,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0492,
            0.1619,
            -0.0792,
            0.0665,
            0.0616,
            -0.1939,
            0.2831,
            0.0161,
            -0.2063,
            0.0077,
            -0.0271,
            0.0068,
            0.0615,
            -0.0659,
            -0.2038,
            0.1444
          ],
          "after": [
            0.0211,
            0.1082,
            -0.0351,
            -0.0714,
            0.041,
            -0.0159,
            0.0889,
            0.0959,
            -0.066,
            -0.0633,
            0.0195,
            -0.0065,
            0.1016,
            0.0267,
            -0.0573,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0498,
            0.0477,
            -0.0302,
            -0.0807,
            0.1125,
            -0.0617,
            0.0444,
            -0.0673,
            -0.0186,
            0.0087,
            -0.0353,
            0.0504,
            0.0188,
            -0.0056,
            -0.0043,
            0.0925
          ],
          "after": [
            0.0502,
            -0.1476,
            -0.2872,
            0.0498,
            0.0369,
            -0.0516,
            -0.155,
            0.066,
            0.1073,
            0.0107,
            0.0209,
            0.091,
            0.0397,
            0.0119,
            -0.1349,
            0.0489
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0009,
            -0.0006,
            -0.0012,
            0.0002,
            -0.0007,
            -0.0006,
            -0.0008,
            -0.0,
            -0.0005,
            -0.0,
            -0.002,
            0.0007,
            0.0015,
            0.0003,
            -0.0011
          ],
          "after": [
            -0.0842,
            0.0164,
            -0.1125,
            -0.069,
            0.064,
            0.1256,
            0.0789,
            -0.1619,
            0.0283,
            -0.0311,
            -0.136,
            -0.0686,
            0.0927,
            -0.0861,
            0.0397,
            0.0806
          ]
        }
      }
    },
    {
      "step": 767,
      "word": "jazzmin",
      "loss": 2.7267,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0507,
            0.0767,
            0.0512,
            0.2112,
            0.1459,
            -0.1001,
            0.3104,
            0.0119,
            -0.0347,
            0.0863,
            0.094,
            0.1195,
            0.0527,
            -0.1191,
            -0.0435,
            0.0054
          ],
          "after": [
            0.0047,
            -0.0045,
            0.1541,
            -0.0263,
            0.0277,
            -0.0892,
            -0.1149,
            -0.156,
            -0.0075,
            0.0777,
            0.1593,
            0.0101,
            -0.1002,
            -0.0158,
            -0.0366,
            -0.19
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1678,
            0.1955,
            -0.1995,
            0.0413,
            0.0909,
            -0.2129,
            0.3911,
            0.0625,
            -0.2777,
            -0.0333,
            -0.0946,
            0.042,
            0.1489,
            -0.0873,
            -0.23,
            0.2134
          ],
          "after": [
            0.021,
            0.1081,
            -0.0351,
            -0.0717,
            0.0409,
            -0.0158,
            0.089,
            0.0962,
            -0.066,
            -0.0634,
            0.0194,
            -0.0065,
            0.1016,
            0.0266,
            -0.057,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0134,
            0.0437,
            -0.0045,
            0.1185,
            0.0389,
            -0.0772,
            0.1745,
            0.068,
            -0.1948,
            -0.023,
            0.0289,
            0.0662,
            0.2758,
            -0.1138,
            -0.1259,
            0.1279
          ],
          "after": [
            0.0502,
            -0.1478,
            -0.2871,
            0.0496,
            0.0368,
            -0.0513,
            -0.1552,
            0.0658,
            0.1076,
            0.0107,
            0.0209,
            0.0908,
            0.0393,
            0.0123,
            -0.1345,
            0.0487
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0006,
            -0.0005,
            0.0007,
            0.0017,
            0.0001,
            0.0023,
            0.0006,
            -0.0006,
            -0.0002,
            -0.0006,
            0.0015,
            0.0013,
            -0.0007,
            -0.002,
            0.0015
          ],
          "after": [
            -0.0844,
            0.0165,
            -0.1126,
            -0.0691,
            0.0639,
            0.1257,
            0.079,
            -0.1618,
            0.0284,
            -0.0312,
            -0.136,
            -0.0685,
            0.0925,
            -0.0862,
            0.0399,
            0.0806
          ]
        }
      }
    },
    {
      "step": 768,
      "word": "yashua",
      "loss": 2.652,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.074,
            0.1722,
            0.0634,
            0.0108,
            -0.1294,
            -0.0762,
            -0.0406,
            0.0,
            -0.1501,
            0.1595,
            0.0532,
            -0.0448,
            0.198,
            0.0959,
            0.0015,
            -0.2173
          ],
          "after": [
            0.005,
            -0.0045,
            0.1537,
            -0.0263,
            0.0275,
            -0.0894,
            -0.1153,
            -0.1561,
            -0.0073,
            0.0776,
            0.1592,
            0.01,
            -0.1002,
            -0.0155,
            -0.0363,
            -0.19
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0052,
            0.0071,
            -0.0755,
            0.0064,
            0.049,
            -0.1107,
            0.1221,
            0.0124,
            -0.0873,
            0.063,
            0.0171,
            0.0249,
            0.0155,
            -0.0739,
            -0.1195,
            0.0717
          ],
          "after": [
            0.0209,
            0.1081,
            -0.0352,
            -0.0719,
            0.0408,
            -0.0158,
            0.0891,
            0.0964,
            -0.066,
            -0.0635,
            0.0193,
            -0.0066,
            0.1015,
            0.0266,
            -0.0567,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0229,
            -0.1345,
            0.1202,
            -0.1411,
            0.0331,
            0.1423,
            -0.1586,
            0.0333,
            0.0267,
            0.0823,
            -0.0286,
            0.0576,
            -0.1015,
            0.0649,
            0.039,
            -0.0627
          ],
          "after": [
            0.0503,
            -0.1479,
            -0.2871,
            0.0496,
            0.0367,
            -0.0513,
            -0.1552,
            0.0656,
            0.1079,
            0.0105,
            0.0209,
            0.0904,
            0.039,
            0.0125,
            -0.1343,
            0.0487
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0017,
            0.0004,
            0.0004,
            -0.0005,
            0.0004,
            -0.0002,
            -0.0025,
            0.0004,
            0.0004,
            -0.0011,
            -0.0005,
            -0.0007,
            0.0004,
            0.0017,
            0.0005,
            -0.0008
          ],
          "after": [
            -0.0847,
            0.0165,
            -0.1126,
            -0.0691,
            0.0639,
            0.1258,
            0.0791,
            -0.1618,
            0.0285,
            -0.0312,
            -0.136,
            -0.0683,
            0.0922,
            -0.0864,
            0.04,
            0.0807
          ]
        }
      }
    },
    {
      "step": 769,
      "word": "jazelyn",
      "loss": 2.0286,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.032,
            0.0377,
            0.0292,
            0.1818,
            0.1407,
            -0.0919,
            0.226,
            0.005,
            -0.0034,
            0.0809,
            0.0966,
            0.1093,
            0.0061,
            -0.1182,
            -0.0469,
            0.0029
          ],
          "after": [
            0.0052,
            -0.0046,
            0.1534,
            -0.0265,
            0.0272,
            -0.0894,
            -0.1158,
            -0.1561,
            -0.0072,
            0.0774,
            0.159,
            0.0097,
            -0.1002,
            -0.0151,
            -0.036,
            -0.19
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1418,
            -0.0452,
            0.1632,
            -0.0647,
            0.1631,
            -0.0289,
            -0.0229,
            0.0233,
            0.0191,
            -0.0948,
            0.0535,
            0.0932,
            -0.1674,
            -0.1535,
            0.1985,
            0.0276
          ],
          "after": [
            0.0209,
            0.108,
            -0.0353,
            -0.0721,
            0.0406,
            -0.0157,
            0.0891,
            0.0966,
            -0.066,
            -0.0636,
            0.0191,
            -0.0067,
            0.1016,
            0.0266,
            -0.0566,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0141,
            0.0292,
            0.0012,
            0.1081,
            0.0337,
            -0.0921,
            0.1301,
            0.0553,
            -0.1666,
            -0.0229,
            0.0345,
            0.0395,
            0.2501,
            -0.1068,
            -0.1222,
            0.0984
          ],
          "after": [
            0.0503,
            -0.148,
            -0.2872,
            0.0495,
            0.0366,
            -0.0511,
            -0.1554,
            0.0654,
            0.1083,
            0.0103,
            0.0209,
            0.0901,
            0.0386,
            0.0129,
            -0.134,
            0.0485
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0,
            0.0008,
            0.0002,
            0.0009,
            -0.0003,
            0.0012,
            0.0003,
            -0.0001,
            0.0003,
            0.0008,
            0.0005,
            0.0004,
            -0.0014,
            -0.001,
            0.0009
          ],
          "after": [
            -0.0849,
            0.0165,
            -0.1127,
            -0.0691,
            0.0637,
            0.1259,
            0.0792,
            -0.1619,
            0.0286,
            -0.0313,
            -0.1361,
            -0.0681,
            0.092,
            -0.0865,
            0.0402,
            0.0807
          ]
        }
      }
    },
    {
      "step": 770,
      "word": "zakery",
      "loss": 2.2245,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2377,
            0.1296,
            0.0729,
            0.0163,
            0.0736,
            -0.0211,
            0.2488,
            0.0275,
            -0.0537,
            -0.0188,
            -0.0433,
            0.0484,
            -0.0363,
            -0.0743,
            -0.1607,
            -0.0865
          ],
          "after": [
            0.0055,
            -0.0048,
            0.1531,
            -0.0267,
            0.0268,
            -0.0895,
            -0.1164,
            -0.1561,
            -0.0071,
            0.0772,
            0.1589,
            0.0095,
            -0.1002,
            -0.0148,
            -0.0356,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1019,
            -0.1925,
            0.0346,
            -0.0309,
            0.0904,
            0.1139,
            -0.253,
            -0.0741,
            0.0394,
            -0.1314,
            0.0709,
            0.0976,
            -0.0465,
            -0.1376,
            0.097,
            -0.2106
          ],
          "after": [
            0.021,
            0.1081,
            -0.0354,
            -0.0721,
            0.0403,
            -0.0156,
            0.0892,
            0.0968,
            -0.066,
            -0.0635,
            0.0189,
            -0.0068,
            0.1017,
            0.0268,
            -0.0565,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0268,
            0.0439,
            -0.0323,
            0.1216,
            0.0947,
            -0.0604,
            0.185,
            -0.0071,
            0.0087,
            0.0511,
            0.0648,
            0.0641,
            0.0198,
            -0.0701,
            -0.013,
            0.0387
          ],
          "after": [
            0.0502,
            -0.1481,
            -0.2872,
            0.0493,
            0.0363,
            -0.0509,
            -0.1556,
            0.0652,
            0.1086,
            0.0102,
            0.0209,
            0.0898,
            0.0382,
            0.0133,
            -0.1337,
            0.0484
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0004,
            0.0008,
            -0.0008,
            0.0005,
            -0.0007,
            -0.001,
            -0.0008,
            0.0002,
            -0.0003,
            0.0009,
            -0.0002,
            0.0001,
            -0.0007,
            -0.0002,
            -0.0018
          ],
          "after": [
            -0.085,
            0.0166,
            -0.1129,
            -0.0691,
            0.0636,
            0.126,
            0.0793,
            -0.1618,
            0.0286,
            -0.0313,
            -0.1362,
            -0.068,
            0.0918,
            -0.0866,
            0.0403,
            0.0808
          ]
        }
      }
    },
    {
      "step": 771,
      "word": "ossian",
      "loss": 2.4482,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0534,
            0.1519,
            -0.0398,
            0.0713,
            -0.0434,
            -0.1096,
            -0.0132,
            -0.0937,
            0.0375,
            -0.1848,
            -0.0501,
            0.1314,
            -0.1188,
            -0.0328,
            0.0091,
            0.0683
          ],
          "after": [
            0.0058,
            -0.005,
            0.1529,
            -0.027,
            0.0266,
            -0.0894,
            -0.1169,
            -0.1561,
            -0.007,
            0.0773,
            0.1589,
            0.0092,
            -0.1001,
            -0.0144,
            -0.0353,
            -0.1899
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0116,
            -0.0412,
            -0.0675,
            -0.0414,
            0.0715,
            -0.0886,
            0.0842,
            0.0079,
            -0.059,
            0.0309,
            0.0177,
            0.0365,
            -0.0136,
            -0.0528,
            -0.0975,
            0.0346
          ],
          "after": [
            0.0211,
            0.1082,
            -0.0354,
            -0.0721,
            0.0401,
            -0.0156,
            0.0893,
            0.097,
            -0.066,
            -0.0635,
            0.0187,
            -0.007,
            0.1018,
            0.027,
            -0.0564,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            0.1139,
            -0.0872,
            0.1093,
            -0.0826,
            0.0307,
            -0.0285,
            -0.2452,
            -0.0457,
            0.0281,
            0.1212,
            0.0239,
            0.0054,
            -0.1187,
            0.0312,
            -0.0017,
            -0.2135
          ],
          "after": [
            0.0499,
            -0.1481,
            -0.2874,
            0.0491,
            0.0361,
            -0.0507,
            -0.1557,
            0.0651,
            0.1089,
            0.0098,
            0.0208,
            0.0895,
            0.038,
            0.0136,
            -0.1335,
            0.0484
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0002,
            -0.0005,
            -0.0007,
            0.0002,
            0.0001,
            0.001,
            0.0005,
            0.0006,
            -0.0001,
            -0.0003,
            0.0003,
            0.0003,
            -0.0002,
            0.0001,
            0.0007
          ],
          "after": [
            -0.0851,
            0.0167,
            -0.113,
            -0.0691,
            0.0635,
            0.1261,
            0.0794,
            -0.1618,
            0.0286,
            -0.0313,
            -0.1363,
            -0.0679,
            0.0916,
            -0.0866,
            0.0404,
            0.0809
          ]
        }
      }
    },
    {
      "step": 772,
      "word": "jahon",
      "loss": 1.99,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0552,
            -0.1188,
            0.1318,
            0.0646,
            -0.0718,
            0.1573,
            -0.1201,
            0.1578,
            0.1716,
            0.1572,
            -0.0565,
            -0.2923,
            0.2876,
            0.2195,
            -0.0169,
            0.0607
          ],
          "after": [
            0.0061,
            -0.0051,
            0.1526,
            -0.0272,
            0.0264,
            -0.0894,
            -0.1172,
            -0.1562,
            -0.0071,
            0.0772,
            0.1589,
            0.0092,
            -0.1003,
            -0.0144,
            -0.035,
            -0.19
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0546,
            0.0715,
            -0.1498,
            0.022,
            0.0059,
            -0.1284,
            0.2667,
            0.0644,
            -0.2168,
            0.0245,
            -0.0598,
            -0.0021,
            0.1089,
            -0.0503,
            -0.1421,
            0.1355
          ],
          "after": [
            0.0212,
            0.1082,
            -0.0354,
            -0.0722,
            0.0399,
            -0.0154,
            0.0892,
            0.0971,
            -0.0659,
            -0.0635,
            0.0186,
            -0.0071,
            0.1019,
            0.0271,
            -0.0562,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            0.0244,
            0.0062,
            0.0114,
            0.1342,
            -0.0309,
            -0.146,
            0.1039,
            0.0742,
            -0.1694,
            -0.1033,
            0.0132,
            -0.0216,
            0.2935,
            -0.0683,
            -0.075,
            0.1138
          ],
          "after": [
            0.0497,
            -0.1481,
            -0.2876,
            0.0489,
            0.036,
            -0.0503,
            -0.1558,
            0.0649,
            0.1092,
            0.0097,
            0.0207,
            0.0893,
            0.0376,
            0.014,
            -0.1332,
            0.0484
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0006,
            0.0001,
            -0.0001,
            0.001,
            0.0009,
            0.0006,
            0.0015,
            -0.0001,
            -0.0008,
            -0.0005,
            0.0014,
            -0.0004,
            -0.0015,
            0.0011,
            0.0006
          ],
          "after": [
            -0.085,
            0.0168,
            -0.113,
            -0.069,
            0.0633,
            0.1262,
            0.0794,
            -0.162,
            0.0285,
            -0.0313,
            -0.1363,
            -0.0679,
            0.0915,
            -0.0865,
            0.0404,
            0.0809
          ]
        }
      }
    },
    {
      "step": 773,
      "word": "kayan",
      "loss": 1.7999,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1047,
            -0.0894,
            -0.17,
            -0.0966,
            -0.2116,
            -0.0867,
            -0.1146,
            -0.0105,
            0.0569,
            0.0857,
            0.0314,
            0.1063,
            -0.1326,
            0.0899,
            0.1261,
            -0.0354
          ],
          "after": [
            0.0062,
            -0.0052,
            0.1524,
            -0.0273,
            0.0265,
            -0.0894,
            -0.1174,
            -0.1563,
            -0.0072,
            0.077,
            0.1589,
            0.0091,
            -0.1003,
            -0.0144,
            -0.0349,
            -0.19
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.055,
            0.07,
            -0.1075,
            0.0153,
            0.0079,
            -0.161,
            0.2338,
            0.0386,
            -0.1987,
            0.0283,
            -0.0297,
            -0.0325,
            0.0774,
            -0.075,
            -0.0789,
            0.1333
          ],
          "after": [
            0.0213,
            0.1082,
            -0.0353,
            -0.0722,
            0.0398,
            -0.0153,
            0.0891,
            0.0971,
            -0.0658,
            -0.0635,
            0.0185,
            -0.0072,
            0.1018,
            0.0273,
            -0.056,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0833,
            0.0337,
            -0.1239,
            -0.0404,
            0.0606,
            -0.0795,
            0.1306,
            -0.0358,
            -0.0086,
            0.0137,
            -0.0084,
            0.0099,
            0.0256,
            -0.0326,
            -0.1228,
            -0.0581
          ],
          "after": [
            0.0496,
            -0.1481,
            -0.2875,
            0.0488,
            0.0358,
            -0.0499,
            -0.1559,
            0.0647,
            0.1095,
            0.0096,
            0.0206,
            0.0891,
            0.0372,
            0.0144,
            -0.1328,
            0.0484
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0021,
            0.0006,
            -0.0005,
            0.0005,
            0.0016,
            0.001,
            0.0019,
            0.0026,
            0.0001,
            -0.002,
            -0.0013,
            0.0029,
            -0.0009,
            -0.0023,
            0.0009,
            0.0015
          ],
          "after": [
            -0.0849,
            0.0168,
            -0.1131,
            -0.069,
            0.0631,
            0.1261,
            0.0793,
            -0.1624,
            0.0285,
            -0.0312,
            -0.1363,
            -0.0681,
            0.0914,
            -0.0863,
            0.0404,
            0.0808
          ]
        }
      }
    },
    {
      "step": 774,
      "word": "talib",
      "loss": 2.4555,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1162,
            0.143,
            -0.1382,
            -0.1175,
            -0.0099,
            -0.0575,
            -0.1243,
            0.0847,
            -0.0193,
            -0.0776,
            -0.1496,
            0.2121,
            -0.0932,
            0.0909,
            0.0819,
            -0.2658
          ],
          "after": [
            0.0064,
            -0.0053,
            0.1524,
            -0.0273,
            0.0265,
            -0.0893,
            -0.1175,
            -0.1565,
            -0.0073,
            0.0769,
            0.1591,
            0.0089,
            -0.1002,
            -0.0145,
            -0.0348,
            -0.1898
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.046,
            0.1847,
            -0.1275,
            0.0759,
            0.0008,
            -0.2041,
            0.2595,
            -0.0111,
            -0.2497,
            0.0175,
            0.0057,
            -0.0505,
            0.1591,
            -0.0228,
            -0.2238,
            0.1408
          ],
          "after": [
            0.0214,
            0.1081,
            -0.0352,
            -0.0724,
            0.0396,
            -0.015,
            0.089,
            0.0971,
            -0.0655,
            -0.0635,
            0.0184,
            -0.0072,
            0.1017,
            0.0275,
            -0.0558,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0067,
            -0.1936,
            0.0902,
            0.0315,
            -0.1154,
            0.0508,
            -0.0295,
            -0.009,
            -0.0622,
            -0.1158,
            -0.0825,
            -0.2642,
            0.0914,
            0.0597,
            0.1268,
            0.0886
          ],
          "after": [
            0.0496,
            -0.1479,
            -0.2876,
            0.0486,
            0.0357,
            -0.0496,
            -0.1561,
            0.0646,
            0.1099,
            0.0097,
            0.0207,
            0.0892,
            0.0369,
            0.0146,
            -0.1326,
            0.0483
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0024,
            0.003,
            -0.0008,
            0.0034,
            -0.0032,
            0.0014,
            -0.0,
            -0.0026,
            0.0008,
            -0.0001,
            0.0002,
            0.0033,
            0.0015,
            -0.0032,
            -0.0009
          ],
          "after": [
            -0.0847,
            0.0167,
            -0.1133,
            -0.0689,
            0.0627,
            0.1263,
            0.0792,
            -0.1627,
            0.0287,
            -0.0311,
            -0.1362,
            -0.0683,
            0.0912,
            -0.0862,
            0.0406,
            0.0808
          ]
        }
      }
    },
    {
      "step": 775,
      "word": "nyomii",
      "loss": 2.7487,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0066,
            -0.0054,
            0.1524,
            -0.0273,
            0.0265,
            -0.0893,
            -0.1175,
            -0.1567,
            -0.0074,
            0.0769,
            0.1592,
            0.0087,
            -0.1002,
            -0.0145,
            -0.0348,
            -0.1897
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1029,
            0.035,
            -0.0855,
            0.0019,
            0.1137,
            -0.1022,
            0.227,
            0.0406,
            -0.1303,
            0.0587,
            -0.0359,
            0.0178,
            0.0687,
            -0.0808,
            -0.1605,
            0.1051
          ],
          "after": [
            0.0216,
            0.108,
            -0.035,
            -0.0725,
            0.0394,
            -0.0147,
            0.0888,
            0.0971,
            -0.0652,
            -0.0636,
            0.0184,
            -0.0073,
            0.1015,
            0.0278,
            -0.0555,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            -0.0348,
            0.1007,
            -0.0704,
            -0.0054,
            -0.2623,
            0.0321,
            -0.0432,
            -0.0424,
            0.082,
            -0.1346,
            -0.1469,
            -0.0375,
            -0.059,
            0.0828,
            0.117,
            0.0807
          ],
          "after": [
            0.0496,
            -0.1479,
            -0.2876,
            0.0485,
            0.036,
            -0.0494,
            -0.1561,
            0.0646,
            0.1101,
            0.0099,
            0.0208,
            0.0893,
            0.0366,
            0.0146,
            -0.1326,
            0.0482
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0008,
            0.0005,
            -0.0004,
            0.0005,
            0.0007,
            -0.0001,
            -0.0003,
            0.0005,
            -0.0006,
            0.0001,
            0.0008,
            -0.0007,
            0.0001,
            -0.0004,
            0.0002
          ],
          "after": [
            -0.0846,
            0.0165,
            -0.1136,
            -0.0688,
            0.0623,
            0.1264,
            0.0791,
            -0.1629,
            0.0289,
            -0.031,
            -0.1361,
            -0.0685,
            0.091,
            -0.0861,
            0.0407,
            0.0808
          ]
        }
      }
    },
    {
      "step": 776,
      "word": "laren",
      "loss": 1.7834,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1251,
            -0.1209,
            -0.1071,
            -0.1047,
            -0.0102,
            0.0037,
            0.0128,
            -0.1757,
            0.2185,
            -0.1002,
            0.026,
            -0.0149,
            -0.1577,
            -0.063,
            0.111,
            0.0586
          ],
          "after": [
            0.0066,
            -0.0054,
            0.1525,
            -0.0272,
            0.0265,
            -0.0892,
            -0.1176,
            -0.1566,
            -0.0077,
            0.0769,
            0.1593,
            0.0085,
            -0.1,
            -0.0146,
            -0.0349,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1459,
            -0.1913,
            0.0771,
            -0.1349,
            0.2277,
            0.0125,
            -0.2865,
            0.0661,
            0.0223,
            -0.1843,
            0.053,
            0.0296,
            -0.1528,
            -0.0703,
            0.2885,
            -0.0534
          ],
          "after": [
            0.0218,
            0.108,
            -0.0349,
            -0.0724,
            0.0392,
            -0.0145,
            0.0887,
            0.0969,
            -0.065,
            -0.0635,
            0.0184,
            -0.0073,
            0.1015,
            0.028,
            -0.0554,
            0.2218
          ]
        },
        "position_0": {
          "grad": [
            -0.0891,
            0.0954,
            -0.1236,
            -0.104,
            0.0088,
            0.0788,
            -0.0922,
            0.1211,
            -0.006,
            -0.0298,
            -0.1876,
            0.2167,
            -0.1326,
            0.0235,
            0.1108,
            -0.1709
          ],
          "after": [
            0.0498,
            -0.1479,
            -0.2874,
            0.0485,
            0.0361,
            -0.0494,
            -0.1562,
            0.0644,
            0.1103,
            0.0101,
            0.0211,
            0.0892,
            0.0365,
            0.0147,
            -0.1327,
            0.0482
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0011,
            0.0008,
            -0.0009,
            0.0022,
            -0.0005,
            -0.0002,
            0.0012,
            0.0003,
            0.0005,
            0.0002,
            0.0009,
            -0.0003,
            -0.0011,
            -0.0007,
            -0.0006
          ],
          "after": [
            -0.0844,
            0.0164,
            -0.1138,
            -0.0687,
            0.0619,
            0.1265,
            0.079,
            -0.1633,
            0.029,
            -0.0309,
            -0.1361,
            -0.0687,
            0.0908,
            -0.086,
            0.0409,
            0.0808
          ]
        }
      }
    },
    {
      "step": 777,
      "word": "osinachi",
      "loss": 2.8966,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2763,
            0.0399,
            0.149,
            0.1186,
            -0.0461,
            0.02,
            0.1674,
            0.0743,
            -0.016,
            -0.037,
            0.0002,
            0.1419,
            0.01,
            -0.0924,
            -0.0665,
            0.1784
          ],
          "after": [
            0.0069,
            -0.0055,
            0.1524,
            -0.0273,
            0.0265,
            -0.0892,
            -0.1178,
            -0.1566,
            -0.008,
            0.077,
            0.1593,
            0.0083,
            -0.0999,
            -0.0145,
            -0.035,
            -0.1897
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0481,
            0.043,
            -0.0662,
            -0.0135,
            0.1372,
            -0.1036,
            0.1536,
            0.0112,
            -0.09,
            -0.0408,
            -0.0015,
            0.0812,
            -0.0197,
            -0.1009,
            -0.1225,
            0.1641
          ],
          "after": [
            0.022,
            0.108,
            -0.0348,
            -0.0723,
            0.0389,
            -0.0143,
            0.0886,
            0.0968,
            -0.0648,
            -0.0634,
            0.0183,
            -0.0074,
            0.1015,
            0.0283,
            -0.0552,
            0.2216
          ]
        },
        "position_0": {
          "grad": [
            0.0844,
            -0.0621,
            0.0833,
            -0.0721,
            0.0332,
            -0.0203,
            -0.1989,
            -0.0478,
            0.0242,
            0.1113,
            0.0241,
            0.0059,
            -0.105,
            0.013,
            -0.0181,
            -0.1723
          ],
          "after": [
            0.0498,
            -0.1479,
            -0.2873,
            0.0485,
            0.0363,
            -0.0493,
            -0.156,
            0.0643,
            0.1104,
            0.0102,
            0.0214,
            0.0891,
            0.0364,
            0.0146,
            -0.1327,
            0.0483
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0004,
            0.0001,
            0.0004,
            0.0,
            0.0003,
            0.0003,
            0.0,
            0.0002,
            -0.0001,
            -0.0003,
            0.0007,
            -0.0003,
            0.0002,
            -0.0001,
            0.0006
          ],
          "after": [
            -0.0841,
            0.0164,
            -0.1141,
            -0.0686,
            0.0615,
            0.1266,
            0.0789,
            -0.1635,
            0.029,
            -0.0309,
            -0.1361,
            -0.0689,
            0.0908,
            -0.0859,
            0.041,
            0.0808
          ]
        }
      }
    },
    {
      "step": 778,
      "word": "meesha",
      "loss": 2.0851,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1084,
            0.0098,
            -0.0248,
            -0.1105,
            -0.0325,
            -0.0661,
            -0.0281,
            -0.0201,
            -0.0558,
            0.116,
            0.0999,
            -0.1198,
            0.0637,
            0.0523,
            -0.0183,
            -0.1537
          ],
          "after": [
            0.007,
            -0.0055,
            0.1524,
            -0.0272,
            0.0266,
            -0.0891,
            -0.1179,
            -0.1566,
            -0.0081,
            0.0769,
            0.1593,
            0.0082,
            -0.0998,
            -0.0145,
            -0.035,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1293,
            -0.1952,
            0.3125,
            -0.0507,
            0.2504,
            0.5964,
            -0.3834,
            -0.0327,
            0.6629,
            0.0056,
            0.0922,
            0.5149,
            -0.6499,
            -0.0334,
            0.485,
            -0.248
          ],
          "after": [
            0.0222,
            0.1081,
            -0.0349,
            -0.0722,
            0.0385,
            -0.0144,
            0.0886,
            0.0967,
            -0.0649,
            -0.0634,
            0.0182,
            -0.0078,
            0.1019,
            0.0285,
            -0.0553,
            0.2215
          ]
        },
        "position_0": {
          "grad": [
            0.0075,
            0.007,
            -0.0251,
            -0.0368,
            0.1196,
            -0.0367,
            0.1225,
            0.0527,
            0.0722,
            0.0289,
            0.0026,
            -0.0036,
            -0.0954,
            -0.0177,
            -0.0164,
            0.0618
          ],
          "after": [
            0.0497,
            -0.1479,
            -0.2873,
            0.0486,
            0.0363,
            -0.0492,
            -0.156,
            0.0642,
            0.1104,
            0.0102,
            0.0216,
            0.0891,
            0.0365,
            0.0147,
            -0.1328,
            0.0484
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.0005,
            -0.0008,
            0.0001,
            0.0005,
            0.0017,
            0.0001,
            0.0007,
            0.0007,
            0.0006,
            -0.0,
            0.002,
            -0.0023,
            -0.0004,
            0.0004,
            0.0005
          ],
          "after": [
            -0.084,
            0.0164,
            -0.1142,
            -0.0685,
            0.0612,
            0.1266,
            0.0788,
            -0.1638,
            0.029,
            -0.0309,
            -0.136,
            -0.0692,
            0.0908,
            -0.0857,
            0.0411,
            0.0807
          ]
        }
      }
    },
    {
      "step": 779,
      "word": "samwell",
      "loss": 2.8739,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0732,
            0.0565,
            0.0971,
            0.043,
            0.1538,
            -0.0882,
            0.2823,
            0.089,
            0.0168,
            0.0008,
            0.0497,
            0.0181,
            -0.0464,
            -0.0489,
            -0.0376,
            0.0342
          ],
          "after": [
            0.0072,
            -0.0056,
            0.1523,
            -0.0271,
            0.0265,
            -0.089,
            -0.1182,
            -0.1567,
            -0.0082,
            0.0769,
            0.1592,
            0.0081,
            -0.0998,
            -0.0144,
            -0.0349,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0073,
            -0.1059,
            0.0367,
            -0.0077,
            0.0155,
            0.0439,
            -0.2699,
            -0.1132,
            0.0924,
            -0.0602,
            0.0087,
            0.0959,
            -0.0196,
            0.0825,
            -0.043,
            0.0043
          ],
          "after": [
            0.0223,
            0.1083,
            -0.035,
            -0.0721,
            0.0382,
            -0.0145,
            0.0887,
            0.0969,
            -0.065,
            -0.0633,
            0.018,
            -0.0082,
            0.1022,
            0.0287,
            -0.0554,
            0.2215
          ]
        },
        "position_0": {
          "grad": [
            -0.0096,
            0.081,
            -0.0253,
            0.1156,
            -0.077,
            0.0635,
            0.0168,
            0.0409,
            -0.0657,
            0.0303,
            -0.0625,
            0.1149,
            0.0809,
            -0.0109,
            0.0591,
            0.0226
          ],
          "after": [
            0.0497,
            -0.148,
            -0.2872,
            0.0486,
            0.0363,
            -0.0491,
            -0.156,
            0.064,
            0.1105,
            0.0101,
            0.0218,
            0.0889,
            0.0364,
            0.0147,
            -0.1329,
            0.0485
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0006,
            0.0023,
            0.001,
            0.0021,
            -0.0004,
            -0.0002,
            0.0016,
            -0.0002,
            0.0009,
            0.0004,
            -0.0008,
            0.0017,
            -0.0017,
            -0.0041,
            0.0
          ],
          "after": [
            -0.0839,
            0.0164,
            -0.1145,
            -0.0685,
            0.0608,
            0.1266,
            0.0788,
            -0.1643,
            0.029,
            -0.0309,
            -0.136,
            -0.0695,
            0.0908,
            -0.0855,
            0.0414,
            0.0807
          ]
        }
      }
    },
    {
      "step": 780,
      "word": "areen",
      "loss": 1.9537,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.114,
            -0.2241,
            -0.2068,
            -0.0628,
            -0.1456,
            0.2024,
            0.0233,
            -0.2249,
            0.2321,
            -0.1697,
            0.0744,
            -0.0441,
            -0.3423,
            -0.1893,
            0.2347,
            0.0706
          ],
          "after": [
            0.0072,
            -0.0055,
            0.1524,
            -0.0271,
            0.0266,
            -0.089,
            -0.1185,
            -0.1566,
            -0.0086,
            0.077,
            0.1591,
            0.008,
            -0.0995,
            -0.0142,
            -0.0351,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2473,
            -0.2136,
            0.1151,
            -0.0639,
            0.242,
            0.5512,
            -0.4372,
            0.1016,
            0.3656,
            -0.205,
            -0.0674,
            0.4879,
            -0.5667,
            -0.0269,
            0.8249,
            -0.1845
          ],
          "after": [
            0.0225,
            0.1085,
            -0.0351,
            -0.0719,
            0.0377,
            -0.0148,
            0.0889,
            0.0968,
            -0.0652,
            -0.063,
            0.018,
            -0.0089,
            0.1027,
            0.0288,
            -0.0557,
            0.2216
          ]
        },
        "position_0": {
          "grad": [
            0.1185,
            -0.0592,
            0.1323,
            -0.2209,
            -0.0083,
            -0.0089,
            -0.3961,
            -0.1293,
            0.1898,
            0.0401,
            0.2206,
            -0.0461,
            -0.2233,
            0.0584,
            -0.0444,
            -0.3158
          ],
          "after": [
            0.0495,
            -0.148,
            -0.2873,
            0.0488,
            0.0364,
            -0.0491,
            -0.1558,
            0.064,
            0.1104,
            0.0101,
            0.0218,
            0.0888,
            0.0366,
            0.0146,
            -0.1329,
            0.0487
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0004,
            0.0009,
            0.0003,
            0.0,
            -0.0004,
            0.0003,
            -0.0012,
            -0.0019,
            0.0007,
            0.0023,
            -0.0006,
            -0.0006,
            -0.0012,
            0.0003,
            -0.0022
          ],
          "after": [
            -0.0838,
            0.0164,
            -0.1148,
            -0.0686,
            0.0604,
            0.1266,
            0.0787,
            -0.1645,
            0.0292,
            -0.031,
            -0.1362,
            -0.0696,
            0.0908,
            -0.0853,
            0.0416,
            0.0808
          ]
        }
      }
    },
    {
      "step": 781,
      "word": "marika",
      "loss": 1.86,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1657,
            -0.04,
            -0.0769,
            -0.197,
            0.0318,
            -0.0284,
            0.0405,
            -0.18,
            0.0988,
            0.0602,
            0.1274,
            -0.0856,
            -0.0469,
            -0.0547,
            0.0049,
            -0.076
          ],
          "after": [
            0.0071,
            -0.0053,
            0.1525,
            -0.0268,
            0.0266,
            -0.089,
            -0.1188,
            -0.1562,
            -0.009,
            0.077,
            0.1588,
            0.008,
            -0.0992,
            -0.014,
            -0.0353,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0909,
            0.1373,
            -0.1241,
            0.0415,
            0.0586,
            -0.1908,
            0.2792,
            0.0322,
            -0.2115,
            0.0145,
            -0.062,
            -0.0012,
            0.0886,
            -0.0707,
            -0.1744,
            0.1581
          ],
          "after": [
            0.0228,
            0.1086,
            -0.0351,
            -0.0718,
            0.0374,
            -0.015,
            0.089,
            0.0967,
            -0.0654,
            -0.0629,
            0.018,
            -0.0094,
            0.1032,
            0.029,
            -0.056,
            0.2215
          ]
        },
        "position_0": {
          "grad": [
            0.0024,
            0.0237,
            -0.0347,
            -0.0338,
            0.1455,
            -0.0208,
            0.1769,
            0.0578,
            0.0359,
            0.0549,
            0.022,
            0.0399,
            -0.0672,
            -0.0579,
            -0.0511,
            0.0848
          ],
          "after": [
            0.0494,
            -0.1481,
            -0.2873,
            0.049,
            0.0363,
            -0.0491,
            -0.1557,
            0.064,
            0.1103,
            0.0099,
            0.0218,
            0.0886,
            0.0367,
            0.0147,
            -0.1329,
            0.0489
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0012,
            0.0002,
            -0.0018,
            0.0008,
            0.001,
            -0.0016,
            -0.0009,
            0.0007,
            0.0002,
            0.0012,
            -0.0002,
            -0.0008,
            -0.0,
            0.0006,
            -0.0016
          ],
          "after": [
            -0.0836,
            0.0165,
            -0.1151,
            -0.0685,
            0.0601,
            0.1265,
            0.0787,
            -0.1646,
            0.0293,
            -0.0311,
            -0.1364,
            -0.0697,
            0.0908,
            -0.085,
            0.0418,
            0.081
          ]
        }
      }
    },
    {
      "step": 782,
      "word": "ekrem",
      "loss": 2.9366,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0071,
            -0.0052,
            0.1526,
            -0.0266,
            0.0266,
            -0.0891,
            -0.119,
            -0.1559,
            -0.0093,
            0.0771,
            0.1586,
            0.008,
            -0.0989,
            -0.0138,
            -0.0355,
            -0.1896
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.4937,
            0.2382,
            0.7743,
            -0.1455,
            0.3528,
            0.0282,
            -0.0426,
            0.0859,
            0.2138,
            -0.4889,
            -0.0235,
            0.045,
            -0.3106,
            0.0524,
            0.3798,
            -0.2236
          ],
          "after": [
            0.0233,
            0.1086,
            -0.0355,
            -0.0716,
            0.0369,
            -0.0152,
            0.0891,
            0.0965,
            -0.0656,
            -0.0624,
            0.0181,
            -0.0099,
            0.1037,
            0.0291,
            -0.0563,
            0.2216
          ]
        },
        "position_0": {
          "grad": [
            -0.0018,
            -0.159,
            0.0714,
            0.0284,
            -0.004,
            0.0998,
            -0.1888,
            -0.0774,
            0.0877,
            0.1294,
            0.016,
            0.0023,
            -0.1206,
            -0.014,
            0.0237,
            -0.2732
          ],
          "after": [
            0.0492,
            -0.1479,
            -0.2874,
            0.0491,
            0.0362,
            -0.0492,
            -0.1555,
            0.064,
            0.1101,
            0.0096,
            0.0218,
            0.0885,
            0.0369,
            0.0147,
            -0.1329,
            0.0492
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0001,
            0.0021,
            -0.0023,
            0.0006,
            -0.0057,
            0.0021,
            0.0014,
            -0.0004,
            -0.0,
            0.0015,
            -0.0056,
            0.003,
            -0.0007,
            -0.0011,
            0.0011
          ],
          "after": [
            -0.0835,
            0.0166,
            -0.1155,
            -0.0682,
            0.0598,
            0.1269,
            0.0787,
            -0.1648,
            0.0294,
            -0.0311,
            -0.1367,
            -0.0695,
            0.0907,
            -0.0848,
            0.0419,
            0.0811
          ]
        }
      }
    },
    {
      "step": 783,
      "word": "taylen",
      "loss": 1.8193,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0828,
            -0.2142,
            0.0708,
            -0.0992,
            0.0495,
            0.1101,
            -0.1177,
            0.0716,
            -0.0505,
            0.1278,
            0.0612,
            0.0872,
            -0.0133,
            0.0522,
            -0.0187,
            -0.0579
          ],
          "after": [
            0.007,
            -0.005,
            0.1526,
            -0.0263,
            0.0266,
            -0.0892,
            -0.1192,
            -0.1557,
            -0.0095,
            0.077,
            0.1584,
            0.008,
            -0.0987,
            -0.0136,
            -0.0356,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1089,
            -0.3818,
            0.1209,
            -0.0562,
            0.0689,
            0.1991,
            -0.4391,
            0.0607,
            0.1276,
            0.0883,
            -0.0558,
            -0.0155,
            -0.0595,
            0.1838,
            0.0426,
            -0.2804
          ],
          "after": [
            0.0236,
            0.1088,
            -0.0359,
            -0.0713,
            0.0364,
            -0.0154,
            0.0893,
            0.0963,
            -0.0658,
            -0.0621,
            0.0182,
            -0.0103,
            0.1042,
            0.0291,
            -0.0566,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            -0.0052,
            -0.1692,
            0.0727,
            0.0135,
            -0.0966,
            0.045,
            -0.0202,
            -0.0172,
            -0.0595,
            -0.0783,
            -0.0653,
            -0.2379,
            0.0751,
            0.046,
            0.0927,
            0.0606
          ],
          "after": [
            0.0491,
            -0.1476,
            -0.2876,
            0.0492,
            0.0363,
            -0.0493,
            -0.1553,
            0.0641,
            0.11,
            0.0095,
            0.0218,
            0.0886,
            0.0371,
            0.0147,
            -0.133,
            0.0495
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0003,
            0.0004,
            0.0007,
            0.0004,
            0.0003,
            -0.0,
            0.0003,
            -0.0005,
            0.0007,
            0.0005,
            -0.0003,
            0.0003,
            -0.0004,
            -0.0013,
            0.0007
          ],
          "after": [
            -0.0833,
            0.0167,
            -0.1159,
            -0.0681,
            0.0595,
            0.1271,
            0.0786,
            -0.1651,
            0.0295,
            -0.0312,
            -0.137,
            -0.0693,
            0.0906,
            -0.0846,
            0.0422,
            0.0811
          ]
        }
      }
    },
    {
      "step": 784,
      "word": "severide",
      "loss": 2.423,
      "learning_rate": 0.0007,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0069,
            -0.0047,
            0.1526,
            -0.0261,
            0.0266,
            -0.0893,
            -0.1193,
            -0.1556,
            -0.0097,
            0.0769,
            0.1582,
            0.0079,
            -0.0985,
            -0.0135,
            -0.0357,
            -0.1895
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1849,
            -0.385,
            0.48,
            -0.2447,
            -0.0295,
            0.5276,
            -1.0114,
            -0.05,
            0.8218,
            0.2555,
            0.174,
            -0.2816,
            -0.1417,
            0.2556,
            0.3085,
            -0.5475
          ],
          "after": [
            0.0238,
            0.1091,
            -0.0364,
            -0.0707,
            0.0361,
            -0.0158,
            0.0898,
            0.0961,
            -0.0663,
            -0.0619,
            0.0181,
            -0.0104,
            0.1047,
            0.0289,
            -0.057,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0055,
            0.068,
            -0.0193,
            0.062,
            -0.0673,
            0.0547,
            -0.0144,
            0.0174,
            -0.0466,
            0.0388,
            -0.0472,
            0.077,
            0.0471,
            -0.0055,
            0.0334,
            0.0014
          ],
          "after": [
            0.049,
            -0.1474,
            -0.2878,
            0.0493,
            0.0363,
            -0.0495,
            -0.1551,
            0.0642,
            0.11,
            0.0093,
            0.0219,
            0.0887,
            0.0372,
            0.0147,
            -0.1331,
            0.0497
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0,
            -0.0003,
            -0.0009,
            0.0001,
            0.0018,
            0.0001,
            0.0,
            0.0009,
            0.0011,
            -0.0005,
            0.0007,
            -0.001,
            -0.0005,
            0.0005,
            -0.0002
          ],
          "after": [
            -0.0832,
            0.0167,
            -0.1161,
            -0.0679,
            0.0592,
            0.1272,
            0.0785,
            -0.1652,
            0.0296,
            -0.0313,
            -0.1372,
            -0.0691,
            0.0905,
            -0.0843,
            0.0423,
            0.0812
          ]
        }
      }
    },
    {
      "step": 785,
      "word": "arcelia",
      "loss": 2.2447,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1276,
            -0.1138,
            -0.1247,
            -0.1009,
            -0.0131,
            0.1388,
            0.1051,
            -0.2205,
            0.0397,
            -0.0064,
            0.1576,
            -0.0312,
            -0.1851,
            -0.1593,
            0.085,
            0.0065
          ],
          "after": [
            0.0067,
            -0.0044,
            0.1527,
            -0.0258,
            0.0266,
            -0.0895,
            -0.1194,
            -0.1552,
            -0.0099,
            0.0769,
            0.1579,
            0.0079,
            -0.0983,
            -0.0133,
            -0.0358,
            -0.1894
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3318,
            0.0358,
            0.2357,
            -0.0381,
            0.1321,
            -0.117,
            0.0598,
            -0.0019,
            0.1178,
            -0.1232,
            -0.0343,
            0.1198,
            -0.0962,
            -0.1067,
            0.0493,
            -0.0933
          ],
          "after": [
            0.0242,
            0.1094,
            -0.037,
            -0.0703,
            0.0357,
            -0.0162,
            0.0901,
            0.096,
            -0.0667,
            -0.0618,
            0.018,
            -0.0106,
            0.1051,
            0.0288,
            -0.0573,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.0801,
            -0.0135,
            0.0857,
            -0.1447,
            0.0485,
            0.014,
            -0.2306,
            -0.0964,
            0.0929,
            0.0692,
            0.1884,
            0.0372,
            -0.1305,
            -0.0016,
            -0.0868,
            -0.1925
          ],
          "after": [
            0.0488,
            -0.1472,
            -0.288,
            0.0494,
            0.0364,
            -0.0496,
            -0.1549,
            0.0643,
            0.1099,
            0.009,
            0.0217,
            0.0887,
            0.0373,
            0.0147,
            -0.1331,
            0.05
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0007,
            0.001,
            0.0014,
            -0.0031,
            -0.004,
            0.0006,
            -0.0021,
            -0.0017,
            0.0001,
            0.0021,
            -0.0008,
            0.0008,
            -0.001,
            0.0009,
            -0.0026
          ],
          "after": [
            -0.0831,
            0.0168,
            -0.1165,
            -0.0678,
            0.0592,
            0.1275,
            0.0785,
            -0.1652,
            0.0298,
            -0.0315,
            -0.1376,
            -0.069,
            0.0904,
            -0.0841,
            0.0424,
            0.0814
          ]
        }
      }
    },
    {
      "step": 786,
      "word": "makaiah",
      "loss": 2.2,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.4348,
            -0.1883,
            0.4057,
            0.0691,
            0.3613,
            0.2804,
            0.0632,
            0.2509,
            0.1238,
            -0.0207,
            -0.1672,
            0.0209,
            -0.0716,
            -0.0822,
            -0.2993,
            -0.0019
          ],
          "after": [
            0.0068,
            -0.004,
            0.1525,
            -0.0256,
            0.0264,
            -0.09,
            -0.1196,
            -0.1552,
            -0.0102,
            0.0768,
            0.1578,
            0.0079,
            -0.098,
            -0.013,
            -0.0357,
            -0.1894
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0636,
            0.1029,
            -0.1116,
            0.0369,
            0.0386,
            -0.1664,
            0.22,
            0.0262,
            -0.1903,
            0.0303,
            -0.0476,
            -0.0123,
            0.0767,
            -0.0298,
            -0.1581,
            0.149
          ],
          "after": [
            0.0245,
            0.1096,
            -0.0374,
            -0.0699,
            0.0353,
            -0.0164,
            0.0904,
            0.0959,
            -0.0671,
            -0.0616,
            0.0181,
            -0.0108,
            0.1055,
            0.0287,
            -0.0576,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            0.0001,
            0.034,
            -0.0332,
            -0.0271,
            0.1496,
            -0.027,
            0.1578,
            0.0376,
            0.0178,
            0.0649,
            0.0246,
            0.0446,
            -0.0533,
            -0.0536,
            -0.0653,
            0.0817
          ],
          "after": [
            0.0486,
            -0.1471,
            -0.2882,
            0.0496,
            0.0363,
            -0.0497,
            -0.1547,
            0.0644,
            0.1098,
            0.0088,
            0.0216,
            0.0886,
            0.0375,
            0.0148,
            -0.133,
            0.0502
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.001,
            -0.0015,
            0.0015,
            -0.0052,
            -0.0005,
            -0.0026,
            0.0006,
            0.0007,
            -0.0022,
            -0.0015,
            0.0,
            0.0002,
            0.0018,
            0.0039,
            0.0014
          ],
          "after": [
            -0.0831,
            0.0168,
            -0.1166,
            -0.0679,
            0.0595,
            0.1278,
            0.0785,
            -0.1652,
            0.0299,
            -0.0314,
            -0.1377,
            -0.0688,
            0.0903,
            -0.084,
            0.0423,
            0.0815
          ]
        }
      }
    },
    {
      "step": 787,
      "word": "gracielynn",
      "loss": 2.8083,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1146,
            0.045,
            0.1058,
            0.0343,
            0.0367,
            -0.0585,
            0.1325,
            0.0751,
            -0.0677,
            0.0145,
            0.0179,
            0.0303,
            0.0459,
            -0.0779,
            -0.0602,
            -0.0022
          ],
          "after": [
            0.0071,
            -0.0037,
            0.1523,
            -0.0255,
            0.0262,
            -0.0903,
            -0.1199,
            -0.1552,
            -0.0104,
            0.0768,
            0.1576,
            0.0078,
            -0.0978,
            -0.0127,
            -0.0355,
            -0.1894
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0195,
            0.1415,
            -0.1775,
            0.0451,
            -0.3466,
            -0.1139,
            0.0834,
            -0.0273,
            -0.067,
            -0.0555,
            0.0413,
            0.01,
            0.1061,
            -0.0825,
            0.2072,
            0.1321
          ],
          "after": [
            0.0248,
            0.1097,
            -0.0377,
            -0.0696,
            0.0352,
            -0.0165,
            0.0906,
            0.0958,
            -0.0673,
            -0.0615,
            0.018,
            -0.011,
            0.1057,
            0.0287,
            -0.0578,
            0.2233
          ]
        },
        "position_0": {
          "grad": [
            0.011,
            0.0302,
            -0.0077,
            0.0774,
            -0.0319,
            -0.0724,
            0.036,
            0.0089,
            -0.0798,
            0.0674,
            0.0547,
            -0.0455,
            0.0354,
            -0.0099,
            -0.0583,
            0.0258
          ],
          "after": [
            0.0485,
            -0.1471,
            -0.2883,
            0.0497,
            0.0362,
            -0.0497,
            -0.1546,
            0.0645,
            0.1098,
            0.0084,
            0.0215,
            0.0887,
            0.0376,
            0.0148,
            -0.1329,
            0.0503
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0001,
            0.0016,
            -0.0001,
            0.0012,
            -0.0003,
            0.0006,
            0.0001,
            0.0005,
            -0.0008,
            0.0001,
            0.0002,
            0.0005,
            -0.001,
            -0.0013,
            0.0012
          ],
          "after": [
            -0.0829,
            0.0169,
            -0.1169,
            -0.0679,
            0.0596,
            0.1281,
            0.0785,
            -0.1653,
            0.0299,
            -0.0314,
            -0.1379,
            -0.0687,
            0.0902,
            -0.0838,
            0.0423,
            0.0815
          ]
        }
      }
    },
    {
      "step": 788,
      "word": "skaii",
      "loss": 2.6809,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0021,
            -0.3712,
            0.0441,
            -0.1604,
            0.1107,
            0.1124,
            -0.3024,
            0.0737,
            0.0665,
            -0.0394,
            0.1909,
            -0.2588,
            -0.2069,
            0.0334,
            0.0112,
            -0.1425
          ],
          "after": [
            0.0072,
            -0.0032,
            0.1521,
            -0.0252,
            0.0259,
            -0.0907,
            -0.1199,
            -0.1554,
            -0.0106,
            0.0768,
            0.1574,
            0.008,
            -0.0975,
            -0.0125,
            -0.0353,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.007,
            0.0415,
            -0.1743,
            -0.0765,
            0.0562,
            -0.2001,
            0.2273,
            0.0289,
            -0.2186,
            0.0076,
            -0.0401,
            -0.0853,
            0.0901,
            -0.0504,
            -0.132,
            0.1385
          ],
          "after": [
            0.0251,
            0.1097,
            -0.0378,
            -0.0693,
            0.0351,
            -0.0165,
            0.0907,
            0.0957,
            -0.0674,
            -0.0613,
            0.018,
            -0.011,
            0.1059,
            0.0288,
            -0.058,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            0.0021,
            0.1102,
            -0.0243,
            0.1245,
            -0.0862,
            0.0663,
            -0.0085,
            0.0258,
            -0.0768,
            0.0417,
            -0.0996,
            0.1438,
            0.0964,
            -0.0049,
            0.0754,
            0.0285
          ],
          "after": [
            0.0483,
            -0.1471,
            -0.2884,
            0.0496,
            0.0362,
            -0.0498,
            -0.1546,
            0.0645,
            0.1098,
            0.0081,
            0.0214,
            0.0885,
            0.0376,
            0.0149,
            -0.1329,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0013,
            -0.0001,
            -0.0008,
            0.0031,
            0.0016,
            0.0008,
            0.0008,
            -0.001,
            -0.0036,
            -0.0014,
            0.0006,
            -0.0008,
            -0.0009,
            -0.0005,
            0.0004
          ],
          "after": [
            -0.0828,
            0.0171,
            -0.1171,
            -0.0679,
            0.0596,
            0.1283,
            0.0785,
            -0.1654,
            0.03,
            -0.0311,
            -0.1379,
            -0.0687,
            0.0901,
            -0.0836,
            0.0423,
            0.0814
          ]
        }
      }
    },
    {
      "step": 789,
      "word": "adarah",
      "loss": 1.9498,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2197,
            -0.0277,
            0.247,
            -0.126,
            0.3554,
            0.0995,
            0.0957,
            -0.1611,
            0.279,
            0.153,
            -0.0293,
            0.0944,
            0.0361,
            -0.0891,
            -0.1702,
            0.2985
          ],
          "after": [
            0.0075,
            -0.0027,
            0.1518,
            -0.0249,
            0.0255,
            -0.0911,
            -0.1199,
            -0.1553,
            -0.011,
            0.0767,
            0.1572,
            0.008,
            -0.0972,
            -0.0123,
            -0.035,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.023,
            0.1096,
            -0.0796,
            0.04,
            0.0358,
            -0.1721,
            0.1971,
            0.0035,
            -0.1465,
            0.0105,
            -0.0139,
            -0.007,
            0.0791,
            -0.0697,
            -0.1601,
            0.0829
          ],
          "after": [
            0.0253,
            0.1097,
            -0.0379,
            -0.0691,
            0.035,
            -0.0164,
            0.0907,
            0.0956,
            -0.0675,
            -0.0613,
            0.0181,
            -0.0111,
            0.106,
            0.0288,
            -0.0581,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            0.0967,
            -0.0116,
            0.1014,
            -0.1764,
            0.0763,
            -0.0011,
            -0.2797,
            -0.1215,
            0.108,
            0.0825,
            0.2267,
            0.0294,
            -0.1441,
            0.0136,
            -0.1048,
            -0.2183
          ],
          "after": [
            0.048,
            -0.1472,
            -0.2886,
            0.0497,
            0.0362,
            -0.0499,
            -0.1543,
            0.0647,
            0.1097,
            0.0077,
            0.0212,
            0.0884,
            0.0378,
            0.0149,
            -0.1328,
            0.0507
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0003,
            0.0002,
            -0.0001,
            0.0002,
            0.0001,
            -0.0006,
            -0.0002,
            0.0,
            0.0005,
            0.0004,
            0.0001,
            -0.0004,
            0.0001,
            -0.0002,
            -0.0004
          ],
          "after": [
            -0.0827,
            0.0173,
            -0.1173,
            -0.0678,
            0.0595,
            0.1284,
            0.0785,
            -0.1654,
            0.0301,
            -0.031,
            -0.138,
            -0.0686,
            0.0901,
            -0.0835,
            0.0423,
            0.0814
          ]
        }
      }
    },
    {
      "step": 790,
      "word": "tyaira",
      "loss": 2.2066,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0602,
            -0.2773,
            0.0494,
            -0.2406,
            0.1087,
            0.0538,
            -0.2301,
            0.061,
            -0.0152,
            0.0853,
            0.2374,
            -0.2959,
            -0.1044,
            0.0138,
            -0.0362,
            -0.2662
          ],
          "after": [
            0.0077,
            -0.0021,
            0.1514,
            -0.0244,
            0.0251,
            -0.0915,
            -0.1198,
            -0.1553,
            -0.0114,
            0.0765,
            0.1568,
            0.0082,
            -0.0969,
            -0.012,
            -0.0348,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0553,
            0.0753,
            -0.0936,
            0.0396,
            0.058,
            -0.1241,
            0.2009,
            -0.0036,
            -0.1526,
            0.0276,
            -0.0151,
            -0.026,
            0.0869,
            -0.0761,
            -0.1651,
            0.0749
          ],
          "after": [
            0.0255,
            0.1096,
            -0.038,
            -0.069,
            0.0349,
            -0.0163,
            0.0907,
            0.0955,
            -0.0675,
            -0.0612,
            0.0181,
            -0.0111,
            0.106,
            0.029,
            -0.0581,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            -0.0081,
            -0.1726,
            0.0709,
            -0.0339,
            -0.1002,
            0.0517,
            -0.0372,
            -0.0272,
            -0.0541,
            -0.0598,
            -0.0596,
            -0.2632,
            0.0641,
            0.0322,
            0.0703,
            0.044
          ],
          "after": [
            0.0478,
            -0.147,
            -0.2889,
            0.0498,
            0.0362,
            -0.05,
            -0.1541,
            0.0649,
            0.1097,
            0.0074,
            0.021,
            0.0885,
            0.0378,
            0.0149,
            -0.1328,
            0.0509
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0001,
            0.0,
            -0.0004,
            0.0,
            0.0002,
            -0.0007,
            0.0,
            0.0003,
            -0.0003,
            0.0001,
            0.0002,
            -0.0006,
            0.0002,
            0.0005,
            -0.0001
          ],
          "after": [
            -0.0825,
            0.0174,
            -0.1174,
            -0.0678,
            0.0595,
            0.1285,
            0.0786,
            -0.1655,
            0.0302,
            -0.0308,
            -0.138,
            -0.0686,
            0.0901,
            -0.0834,
            0.0423,
            0.0815
          ]
        }
      }
    },
    {
      "step": 791,
      "word": "anas",
      "loss": 2.2322,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0339,
            0.4038,
            -0.2999,
            0.3603,
            -1.0042,
            -0.0555,
            -0.2088,
            -0.0344,
            0.007,
            -0.3693,
            -0.4175,
            -0.0998,
            0.1323,
            0.361,
            0.5362,
            0.0425
          ],
          "after": [
            0.0079,
            -0.0019,
            0.1514,
            -0.0243,
            0.0253,
            -0.0918,
            -0.1195,
            -0.1553,
            -0.0117,
            0.0767,
            0.1569,
            0.0085,
            -0.0968,
            -0.0122,
            -0.035,
            -0.1892
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0096,
            -0.0265,
            -0.0866,
            -0.0348,
            0.0358,
            -0.1161,
            0.1121,
            0.0165,
            -0.0552,
            0.0537,
            0.0145,
            0.0204,
            0.0265,
            -0.0669,
            -0.0726,
            0.0256
          ],
          "after": [
            0.0257,
            0.1096,
            -0.038,
            -0.0688,
            0.0347,
            -0.0162,
            0.0906,
            0.0954,
            -0.0674,
            -0.0612,
            0.0181,
            -0.0112,
            0.106,
            0.0291,
            -0.0581,
            0.2234
          ]
        },
        "position_0": {
          "grad": [
            0.1503,
            -0.0829,
            0.1604,
            -0.2092,
            -0.003,
            -0.0301,
            -0.4513,
            -0.134,
            0.2273,
            0.0215,
            0.2463,
            -0.0026,
            -0.2536,
            0.0835,
            -0.027,
            -0.3453
          ],
          "after": [
            0.0473,
            -0.1468,
            -0.2893,
            0.0501,
            0.0363,
            -0.0501,
            -0.1537,
            0.0653,
            0.1095,
            0.0072,
            0.0207,
            0.0886,
            0.0381,
            0.0148,
            -0.1327,
            0.0512
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0004,
            -0.0007,
            -0.0007,
            -0.0001,
            -0.0017,
            0.0022,
            -0.0003,
            -0.001,
            -0.0,
            0.0011,
            -0.0002,
            0.0002,
            -0.0014,
            0.0006,
            -0.0011
          ],
          "after": [
            -0.0824,
            0.0176,
            -0.1175,
            -0.0677,
            0.0595,
            0.1286,
            0.0785,
            -0.1655,
            0.0303,
            -0.0307,
            -0.1382,
            -0.0686,
            0.0901,
            -0.0832,
            0.0422,
            0.0815
          ]
        }
      }
    },
    {
      "step": 792,
      "word": "azariyah",
      "loss": 2.1993,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1553,
            0.0139,
            0.2053,
            0.1812,
            0.3603,
            0.0994,
            0.2993,
            0.0213,
            0.3243,
            0.1777,
            -0.0162,
            0.2556,
            -0.021,
            -0.3136,
            -0.2497,
            0.1997
          ],
          "after": [
            0.0081,
            -0.0018,
            0.1512,
            -0.0244,
            0.0252,
            -0.0922,
            -0.1196,
            -0.1553,
            -0.0122,
            0.0767,
            0.1569,
            0.0085,
            -0.0967,
            -0.012,
            -0.035,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0536,
            0.0511,
            -0.0751,
            0.006,
            0.0621,
            -0.1333,
            0.1603,
            -0.0013,
            -0.1147,
            0.0148,
            -0.0216,
            0.0048,
            0.0655,
            -0.063,
            -0.1515,
            0.0922
          ],
          "after": [
            0.0259,
            0.1096,
            -0.0379,
            -0.0687,
            0.0346,
            -0.016,
            0.0905,
            0.0953,
            -0.0673,
            -0.0612,
            0.0182,
            -0.0112,
            0.106,
            0.0293,
            -0.0581,
            0.2233
          ]
        },
        "position_0": {
          "grad": [
            0.0799,
            -0.0136,
            0.0799,
            -0.1447,
            0.0839,
            -0.0006,
            -0.2231,
            -0.1045,
            0.0832,
            0.0914,
            0.1766,
            0.0334,
            -0.1181,
            -0.0038,
            -0.1082,
            -0.1791
          ],
          "after": [
            0.0468,
            -0.1466,
            -0.2898,
            0.0505,
            0.0362,
            -0.0502,
            -0.1532,
            0.0657,
            0.1093,
            0.0069,
            0.0203,
            0.0887,
            0.0383,
            0.0147,
            -0.1325,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0025,
            0.0017,
            -0.0027,
            0.0009,
            -0.0025,
            0.0002,
            -0.0006,
            0.0006,
            0.0007,
            -0.001,
            -0.0013,
            -0.0015,
            0.0,
            0.0017,
            0.0013,
            -0.0001
          ],
          "after": [
            -0.0824,
            0.0176,
            -0.1174,
            -0.0676,
            0.0596,
            0.1288,
            0.0785,
            -0.1656,
            0.0304,
            -0.0305,
            -0.1382,
            -0.0685,
            0.0901,
            -0.0831,
            0.0422,
            0.0816
          ]
        }
      }
    },
    {
      "step": 793,
      "word": "brydon",
      "loss": 2.1832,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0083,
            -0.0016,
            0.151,
            -0.0245,
            0.0252,
            -0.0924,
            -0.1196,
            -0.1553,
            -0.0127,
            0.0766,
            0.157,
            0.0086,
            -0.0965,
            -0.0119,
            -0.035,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0441,
            0.1617,
            -0.1468,
            0.0537,
            -0.0355,
            -0.2436,
            0.3043,
            0.0425,
            -0.2757,
            0.035,
            -0.0405,
            -0.0509,
            0.2051,
            -0.0604,
            -0.1998,
            0.1397
          ],
          "after": [
            0.0261,
            0.1094,
            -0.0378,
            -0.0686,
            0.0345,
            -0.0157,
            0.0903,
            0.0952,
            -0.0672,
            -0.0612,
            0.0182,
            -0.0112,
            0.1059,
            0.0295,
            -0.0579,
            0.2232
          ]
        },
        "position_0": {
          "grad": [
            0.047,
            -0.0209,
            0.0851,
            0.0331,
            -0.0445,
            0.0441,
            0.1202,
            0.0378,
            -0.0919,
            -0.077,
            -0.0094,
            -0.0022,
            0.129,
            -0.1002,
            0.0361,
            0.2717
          ],
          "after": [
            0.0463,
            -0.1464,
            -0.2903,
            0.0508,
            0.0362,
            -0.0503,
            -0.1529,
            0.066,
            0.1092,
            0.0067,
            0.0199,
            0.0887,
            0.0385,
            0.0147,
            -0.1324,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            0.0012,
            0.002,
            0.003,
            -0.001,
            -0.0005,
            0.0047,
            0.0012,
            -0.003,
            0.0028,
            0.0,
            0.0012,
            0.0049,
            -0.0,
            -0.0022,
            0.0022
          ],
          "after": [
            -0.0824,
            0.0175,
            -0.1174,
            -0.0678,
            0.0597,
            0.1289,
            0.0783,
            -0.1658,
            0.0307,
            -0.0305,
            -0.1382,
            -0.0684,
            0.0899,
            -0.0831,
            0.0422,
            0.0815
          ]
        }
      }
    },
    {
      "step": 794,
      "word": "jakayln",
      "loss": 2.3858,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2175,
            -0.0412,
            0.2152,
            -0.0134,
            0.1802,
            0.1208,
            0.209,
            0.1041,
            -0.1165,
            0.0829,
            -0.0456,
            0.27,
            -0.0527,
            -0.0823,
            -0.1798,
            -0.0925
          ],
          "after": [
            0.0086,
            -0.0015,
            0.1507,
            -0.0245,
            0.0251,
            -0.0928,
            -0.1197,
            -0.1554,
            -0.013,
            0.0766,
            0.157,
            0.0085,
            -0.0964,
            -0.0117,
            -0.0348,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1198,
            0.1639,
            -0.12,
            0.0484,
            0.0424,
            -0.2206,
            0.3473,
            0.0474,
            -0.2505,
            0.0016,
            -0.0544,
            0.0305,
            0.0919,
            -0.0926,
            -0.1652,
            0.2437
          ],
          "after": [
            0.0263,
            0.1092,
            -0.0377,
            -0.0687,
            0.0344,
            -0.0154,
            0.0901,
            0.095,
            -0.0669,
            -0.0612,
            0.0183,
            -0.0112,
            0.1057,
            0.0297,
            -0.0577,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            0.0033,
            0.0407,
            -0.0091,
            0.1051,
            0.0436,
            -0.0873,
            0.1523,
            0.055,
            -0.1845,
            -0.024,
            0.0391,
            0.0443,
            0.266,
            -0.1167,
            -0.1312,
            0.1153
          ],
          "after": [
            0.0459,
            -0.1463,
            -0.2907,
            0.051,
            0.0362,
            -0.0502,
            -0.1526,
            0.0662,
            0.1092,
            0.0066,
            0.0196,
            0.0888,
            0.0384,
            0.015,
            -0.1322,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            -0.0001,
            0.0008,
            0.001,
            0.0008,
            0.0006,
            0.0015,
            0.0004,
            -0.0004,
            -0.0005,
            -0.0002,
            0.0011,
            0.0004,
            -0.0013,
            -0.0007,
            0.0021
          ],
          "after": [
            -0.0824,
            0.0174,
            -0.1175,
            -0.068,
            0.0598,
            0.129,
            0.078,
            -0.1659,
            0.031,
            -0.0305,
            -0.1382,
            -0.0685,
            0.0896,
            -0.083,
            0.0422,
            0.0813
          ]
        }
      }
    },
    {
      "step": 795,
      "word": "lailah",
      "loss": 1.9981,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1781,
            -0.3456,
            0.3071,
            -0.0266,
            0.2125,
            0.299,
            -0.2315,
            0.2323,
            0.1505,
            0.1699,
            0.0646,
            -0.219,
            0.0228,
            0.0386,
            -0.1342,
            -0.0316
          ],
          "after": [
            0.009,
            -0.0011,
            0.1503,
            -0.0245,
            0.025,
            -0.0934,
            -0.1197,
            -0.1557,
            -0.0134,
            0.0764,
            0.157,
            0.0085,
            -0.0963,
            -0.0116,
            -0.0346,
            -0.1893
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0673,
            0.1739,
            -0.155,
            0.0537,
            -0.0138,
            -0.1948,
            0.299,
            -0.0255,
            -0.2256,
            -0.0098,
            0.0068,
            -0.0305,
            0.1062,
            -0.1151,
            -0.1591,
            0.2082
          ],
          "after": [
            0.0265,
            0.109,
            -0.0375,
            -0.0687,
            0.0343,
            -0.015,
            0.0898,
            0.0949,
            -0.0667,
            -0.0612,
            0.0184,
            -0.0112,
            0.1055,
            0.0299,
            -0.0575,
            0.2227
          ]
        },
        "position_0": {
          "grad": [
            -0.076,
            0.0985,
            -0.106,
            -0.1062,
            0.0656,
            0.0662,
            -0.077,
            0.0803,
            -0.0251,
            0.0127,
            -0.1473,
            0.2007,
            -0.0931,
            0.0076,
            0.0334,
            -0.1333
          ],
          "after": [
            0.0457,
            -0.1463,
            -0.2909,
            0.0512,
            0.0361,
            -0.0503,
            -0.1524,
            0.0663,
            0.1093,
            0.0065,
            0.0194,
            0.0886,
            0.0385,
            0.0151,
            -0.1321,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0011,
            0.0002,
            0.0001,
            -0.0001,
            0.0014,
            -0.0016,
            0.0014,
            0.0007,
            -0.0006,
            -0.0012,
            0.0017,
            -0.0012,
            -0.0,
            0.001,
            0.0005
          ],
          "after": [
            -0.0822,
            0.0175,
            -0.1176,
            -0.0682,
            0.0599,
            0.129,
            0.0778,
            -0.1662,
            0.0312,
            -0.0304,
            -0.1381,
            -0.0686,
            0.0894,
            -0.0829,
            0.0422,
            0.0811
          ]
        }
      }
    },
    {
      "step": 796,
      "word": "alter",
      "loss": 2.5256,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1154,
            0.1478,
            -0.2111,
            -0.0945,
            -0.1005,
            -0.0296,
            -0.1479,
            0.1179,
            -0.1037,
            -0.1037,
            -0.2337,
            0.2953,
            -0.172,
            0.0973,
            0.2113,
            -0.4108
          ],
          "after": [
            0.0094,
            -0.0009,
            0.1501,
            -0.0245,
            0.0249,
            -0.0938,
            -0.1195,
            -0.1561,
            -0.0136,
            0.0763,
            0.1572,
            0.0083,
            -0.0961,
            -0.0116,
            -0.0346,
            -0.1891
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0208,
            -0.2228,
            0.1683,
            -0.185,
            0.0651,
            -0.0781,
            -0.2131,
            0.0288,
            0.0332,
            -0.0872,
            0.0043,
            0.1367,
            -0.1858,
            -0.132,
            0.1829,
            0.0645
          ],
          "after": [
            0.0267,
            0.1089,
            -0.0374,
            -0.0686,
            0.0342,
            -0.0147,
            0.0896,
            0.0948,
            -0.0664,
            -0.0611,
            0.0185,
            -0.0113,
            0.1055,
            0.0302,
            -0.0574,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            0.1102,
            -0.0543,
            0.1287,
            -0.1603,
            0.07,
            0.0178,
            -0.3394,
            -0.1244,
            0.1491,
            0.0507,
            0.2482,
            0.043,
            -0.1711,
            0.0235,
            -0.0826,
            -0.2526
          ],
          "after": [
            0.0453,
            -0.1463,
            -0.2913,
            0.0516,
            0.036,
            -0.0504,
            -0.1521,
            0.0665,
            0.1092,
            0.0063,
            0.0191,
            0.0884,
            0.0386,
            0.0152,
            -0.1319,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0002,
            -0.0001,
            -0.0011,
            -0.0012,
            -0.0025,
            0.0007,
            -0.0008,
            -0.0007,
            -0.0014,
            0.0004,
            -0.0015,
            0.0008,
            -0.0001,
            0.0011,
            0.0013
          ],
          "after": [
            -0.0821,
            0.0175,
            -0.1177,
            -0.0683,
            0.06,
            0.1291,
            0.0777,
            -0.1664,
            0.0314,
            -0.0303,
            -0.1381,
            -0.0686,
            0.0893,
            -0.0828,
            0.0422,
            0.0808
          ]
        }
      }
    },
    {
      "step": 797,
      "word": "aceston",
      "loss": 2.2529,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0961,
            0.0971,
            0.1221,
            0.127,
            0.0172,
            -0.1224,
            0.1585,
            0.1306,
            -0.0726,
            0.0873,
            0.0183,
            0.0559,
            0.1062,
            -0.0026,
            -0.1713,
            0.0374
          ],
          "after": [
            0.0098,
            -0.0008,
            0.1498,
            -0.0245,
            0.0248,
            -0.0941,
            -0.1195,
            -0.1565,
            -0.0137,
            0.0761,
            0.1574,
            0.0082,
            -0.096,
            -0.0116,
            -0.0344,
            -0.1889
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2803,
            0.0167,
            0.2934,
            0.1199,
            0.138,
            0.1337,
            0.0178,
            0.0293,
            0.23,
            0.0425,
            0.0394,
            0.3145,
            -0.2759,
            -0.0727,
            0.0202,
            -0.1373
          ],
          "after": [
            0.0271,
            0.1088,
            -0.0375,
            -0.0686,
            0.034,
            -0.0145,
            0.0895,
            0.0946,
            -0.0663,
            -0.0611,
            0.0185,
            -0.0116,
            0.1056,
            0.0306,
            -0.0574,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0806,
            -0.0182,
            0.0901,
            -0.1159,
            0.0717,
            0.004,
            -0.2356,
            -0.1049,
            0.0904,
            0.048,
            0.1923,
            0.05,
            -0.116,
            0.0278,
            -0.0718,
            -0.1616
          ],
          "after": [
            0.0449,
            -0.1462,
            -0.2917,
            0.052,
            0.0358,
            -0.0505,
            -0.1516,
            0.0668,
            0.1091,
            0.0061,
            0.0186,
            0.0882,
            0.0388,
            0.0153,
            -0.1316,
            0.0527
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0001,
            -0.0005,
            0.0003,
            0.0011,
            0.0006,
            0.0004,
            0.0008,
            0.0011,
            -0.0005,
            -0.0013,
            0.0012,
            0.0001,
            0.0001,
            0.0,
            0.0019
          ],
          "after": [
            -0.082,
            0.0175,
            -0.1177,
            -0.0684,
            0.06,
            0.1292,
            0.0775,
            -0.1666,
            0.0315,
            -0.0302,
            -0.1379,
            -0.0687,
            0.0891,
            -0.0827,
            0.0421,
            0.0805
          ]
        }
      }
    },
    {
      "step": 798,
      "word": "viha",
      "loss": 2.6812,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5663,
            -0.0817,
            -0.4725,
            -0.1257,
            -0.3139,
            -0.0964,
            -0.0863,
            0.0244,
            -0.2121,
            0.2632,
            0.1343,
            -0.6112,
            0.3299,
            0.2809,
            0.1583,
            -0.2439
          ],
          "after": [
            0.0098,
            -0.0007,
            0.1499,
            -0.0245,
            0.0248,
            -0.0942,
            -0.1195,
            -0.1569,
            -0.0136,
            0.0758,
            0.1574,
            0.0084,
            -0.0961,
            -0.0118,
            -0.0344,
            -0.1886
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0052,
            0.0684,
            -0.1903,
            0.0414,
            -0.0378,
            -0.1551,
            0.2546,
            0.0216,
            -0.241,
            0.0717,
            -0.0652,
            -0.0604,
            0.1562,
            -0.0216,
            -0.2019,
            0.1398
          ],
          "after": [
            0.0274,
            0.1087,
            -0.0375,
            -0.0687,
            0.0339,
            -0.0142,
            0.0893,
            0.0945,
            -0.0661,
            -0.0612,
            0.0186,
            -0.0117,
            0.1055,
            0.0308,
            -0.0572,
            0.222
          ]
        },
        "position_0": {
          "grad": [
            -0.086,
            0.0478,
            -0.0101,
            0.1085,
            -0.0069,
            0.0797,
            -0.0379,
            0.0163,
            -0.0345,
            -0.0985,
            -0.0465,
            -0.1884,
            0.0023,
            -0.0082,
            0.0662,
            0.1859
          ],
          "after": [
            0.0446,
            -0.1462,
            -0.292,
            0.0522,
            0.0356,
            -0.0506,
            -0.1512,
            0.0671,
            0.109,
            0.006,
            0.0183,
            0.0882,
            0.0389,
            0.0153,
            -0.1315,
            0.0529
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0029,
            -0.0011,
            0.0021,
            -0.0035,
            0.0007,
            -0.0008,
            -0.0058,
            0.0002,
            0.0022,
            -0.0018,
            0.002,
            0.0011,
            -0.0035,
            -0.0014,
            0.0039,
            -0.0029
          ],
          "after": [
            -0.0817,
            0.0176,
            -0.1179,
            -0.0683,
            0.06,
            0.1293,
            0.0776,
            -0.1668,
            0.0314,
            -0.03,
            -0.1379,
            -0.0689,
            0.0892,
            -0.0826,
            0.0419,
            0.0804
          ]
        }
      }
    },
    {
      "step": 799,
      "word": "darianna",
      "loss": 1.8816,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.007,
            0.1318,
            -0.0357,
            -0.0882,
            0.0132,
            -0.048,
            0.0525,
            -0.2226,
            0.1501,
            -0.1316,
            -0.0405,
            0.1289,
            -0.1761,
            -0.1489,
            -0.0114,
            0.0853
          ],
          "after": [
            0.0098,
            -0.0007,
            0.15,
            -0.0243,
            0.0249,
            -0.0943,
            -0.1195,
            -0.1571,
            -0.0137,
            0.0757,
            0.1575,
            0.0085,
            -0.0961,
            -0.0118,
            -0.0344,
            -0.1884
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.033,
            0.073,
            -0.0883,
            0.0413,
            0.0402,
            -0.123,
            0.1861,
            0.0089,
            -0.1546,
            0.0319,
            -0.0175,
            -0.0027,
            0.0677,
            -0.0608,
            -0.1522,
            0.0854
          ],
          "after": [
            0.0276,
            0.1086,
            -0.0374,
            -0.0687,
            0.0338,
            -0.014,
            0.089,
            0.0943,
            -0.0659,
            -0.0612,
            0.0187,
            -0.0119,
            0.1055,
            0.0311,
            -0.057,
            0.2218
          ]
        },
        "position_0": {
          "grad": [
            -0.0447,
            0.0325,
            -0.0328,
            -0.0814,
            0.0772,
            -0.0595,
            0.0263,
            -0.051,
            0.0014,
            0.0022,
            -0.05,
            0.0149,
            -0.0034,
            -0.0082,
            0.0168,
            0.0586
          ],
          "after": [
            0.0445,
            -0.1462,
            -0.2922,
            0.0525,
            0.0354,
            -0.0507,
            -0.1509,
            0.0674,
            0.1089,
            0.006,
            0.0181,
            0.0881,
            0.039,
            0.0154,
            -0.1314,
            0.053
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0039,
            0.0008,
            -0.0018,
            0.0006,
            -0.0054,
            -0.0001,
            -0.0066,
            -0.001,
            -0.0007,
            -0.0018,
            -0.0014,
            -0.0056,
            -0.0002,
            0.0041,
            0.004,
            -0.002
          ],
          "after": [
            -0.0817,
            0.0176,
            -0.1179,
            -0.0682,
            0.0603,
            0.1294,
            0.078,
            -0.1669,
            0.0313,
            -0.0297,
            -0.1379,
            -0.0687,
            0.0893,
            -0.0827,
            0.0416,
            0.0804
          ]
        }
      }
    },
    {
      "step": 800,
      "word": "ayzlin",
      "loss": 2.1622,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1148,
            -0.2921,
            0.0736,
            -0.0825,
            0.0356,
            0.1751,
            -0.0911,
            0.0956,
            -0.103,
            0.1433,
            0.1264,
            0.1501,
            -0.045,
            0.043,
            0.0117,
            -0.0357
          ],
          "after": [
            0.0097,
            -0.0004,
            0.15,
            -0.0242,
            0.0249,
            -0.0945,
            -0.1194,
            -0.1573,
            -0.0137,
            0.0754,
            0.1574,
            0.0085,
            -0.0961,
            -0.0119,
            -0.0344,
            -0.1882
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0434,
            0.088,
            -0.1267,
            0.0342,
            0.0452,
            -0.1481,
            0.24,
            -0.0003,
            -0.1696,
            0.0476,
            -0.0144,
            -0.0157,
            0.1073,
            -0.0804,
            -0.2,
            0.1466
          ],
          "after": [
            0.0279,
            0.1084,
            -0.0373,
            -0.0689,
            0.0337,
            -0.0137,
            0.0888,
            0.0942,
            -0.0657,
            -0.0613,
            0.0188,
            -0.012,
            0.1054,
            0.0314,
            -0.0568,
            0.2215
          ]
        },
        "position_0": {
          "grad": [
            0.0879,
            -0.0322,
            0.0981,
            -0.1413,
            0.0685,
            0.0206,
            -0.2666,
            -0.1156,
            0.1064,
            0.0568,
            0.2213,
            0.0523,
            -0.1378,
            0.019,
            -0.0839,
            -0.1932
          ],
          "after": [
            0.0442,
            -0.1462,
            -0.2926,
            0.0528,
            0.0352,
            -0.0507,
            -0.1505,
            0.0678,
            0.1088,
            0.0059,
            0.0177,
            0.0881,
            0.0392,
            0.0154,
            -0.1312,
            0.0532
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0005,
            -0.0002,
            0.0005,
            0.0011,
            0.0007,
            0.0019,
            0.0011,
            0.0006,
            0.0004,
            -0.0008,
            0.0022,
            0.0009,
            -0.0007,
            -0.0022,
            0.0016
          ],
          "after": [
            -0.0817,
            0.0176,
            -0.1179,
            -0.0681,
            0.0605,
            0.1295,
            0.0782,
            -0.1671,
            0.0312,
            -0.0295,
            -0.1378,
            -0.0686,
            0.0893,
            -0.0828,
            0.0414,
            0.0804
          ]
        }
      }
    },
    {
      "step": 801,
      "word": "madelynne",
      "loss": 2.1959,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1084,
            0.0678,
            0.0427,
            -0.0477,
            0.1468,
            -0.0513,
            0.0687,
            -0.0512,
            -0.0412,
            0.0044,
            -0.0299,
            0.0698,
            0.0244,
            -0.0223,
            -0.031,
            0.086
          ],
          "after": [
            0.0097,
            -0.0003,
            0.15,
            -0.024,
            0.0249,
            -0.0947,
            -0.1194,
            -0.1574,
            -0.0136,
            0.0752,
            0.1574,
            0.0085,
            -0.0961,
            -0.012,
            -0.0343,
            -0.1881
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2402,
            -0.1567,
            -0.0032,
            -0.1786,
            -0.2674,
            0.1766,
            -0.4816,
            -0.1185,
            0.1185,
            -0.0518,
            0.1018,
            -0.2769,
            0.0136,
            0.2002,
            0.3375,
            -0.3184
          ],
          "after": [
            0.028,
            0.1083,
            -0.0372,
            -0.0688,
            0.0337,
            -0.0135,
            0.0887,
            0.0943,
            -0.0655,
            -0.0613,
            0.0188,
            -0.012,
            0.1053,
            0.0315,
            -0.0567,
            0.2215
          ]
        },
        "position_0": {
          "grad": [
            -0.0134,
            0.0282,
            -0.0353,
            0.0145,
            0.1169,
            -0.0107,
            0.1649,
            0.0481,
            0.0071,
            0.0472,
            0.0161,
            0.0533,
            -0.0395,
            -0.0616,
            -0.0341,
            0.0759
          ],
          "after": [
            0.0441,
            -0.1463,
            -0.2928,
            0.0531,
            0.0349,
            -0.0508,
            -0.1503,
            0.068,
            0.1086,
            0.0058,
            0.0173,
            0.088,
            0.0394,
            0.0155,
            -0.131,
            0.0533
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            0.0003,
            0.0006,
            -0.0004,
            -0.0,
            -0.002,
            -0.0019,
            -0.001,
            -0.0012,
            0.0003,
            0.0018,
            -0.0032,
            0.0006,
            0.0004,
            -0.0012,
            -0.0018
          ],
          "after": [
            -0.0817,
            0.0175,
            -0.118,
            -0.0681,
            0.0606,
            0.1296,
            0.0785,
            -0.1671,
            0.0313,
            -0.0294,
            -0.1378,
            -0.0684,
            0.0892,
            -0.0828,
            0.0412,
            0.0804
          ]
        }
      }
    },
    {
      "step": 802,
      "word": "latia",
      "loss": 2.0804,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3845,
            -0.2847,
            -0.0019,
            -0.0011,
            -0.1407,
            -0.0128,
            -0.0388,
            0.0603,
            -0.3377,
            0.134,
            0.0409,
            -0.7462,
            0.3836,
            0.2687,
            0.0429,
            -0.1529
          ],
          "after": [
            0.0095,
            0.0,
            0.15,
            -0.0238,
            0.0249,
            -0.0948,
            -0.1193,
            -0.1575,
            -0.0133,
            0.0749,
            0.1573,
            0.0088,
            -0.0963,
            -0.0122,
            -0.0344,
            -0.1879
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0227,
            0.1261,
            -0.1754,
            0.0429,
            -0.0197,
            -0.1557,
            0.2397,
            -0.0238,
            -0.2167,
            0.0278,
            0.0053,
            -0.0906,
            0.1382,
            -0.06,
            -0.1665,
            0.1228
          ],
          "after": [
            0.028,
            0.1082,
            -0.0371,
            -0.0687,
            0.0337,
            -0.0133,
            0.0886,
            0.0943,
            -0.0653,
            -0.0613,
            0.0187,
            -0.0119,
            0.1052,
            0.0316,
            -0.0566,
            0.2213
          ]
        },
        "position_0": {
          "grad": [
            -0.0799,
            0.1038,
            -0.1202,
            -0.115,
            0.0107,
            0.0522,
            -0.1145,
            0.1086,
            -0.0019,
            -0.0374,
            -0.1995,
            0.1888,
            -0.1213,
            0.0482,
            0.1031,
            -0.1588
          ],
          "after": [
            0.044,
            -0.1464,
            -0.2928,
            0.0535,
            0.0346,
            -0.0509,
            -0.15,
            0.0681,
            0.1085,
            0.0057,
            0.0172,
            0.0877,
            0.0397,
            0.0156,
            -0.131,
            0.0536
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0005,
            0.0033,
            0.0002,
            0.0028,
            -0.002,
            -0.0023,
            -0.0006,
            -0.0014,
            0.0002,
            0.0014,
            0.0026,
            -0.0022,
            -0.0018,
            -0.0018,
            -0.0028
          ],
          "after": [
            -0.0818,
            0.0175,
            -0.1182,
            -0.068,
            0.0606,
            0.1299,
            0.0789,
            -0.1671,
            0.0314,
            -0.0292,
            -0.1379,
            -0.0684,
            0.0893,
            -0.0828,
            0.0412,
            0.0807
          ]
        }
      }
    },
    {
      "step": 803,
      "word": "jazyah",
      "loss": 2.0464,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1445,
            -0.0449,
            0.2539,
            0.2565,
            0.2216,
            0.0166,
            0.2073,
            0.1372,
            0.1562,
            0.2284,
            0.0366,
            0.0597,
            0.1886,
            -0.0703,
            -0.1454,
            0.0785
          ],
          "after": [
            0.0093,
            0.0003,
            0.1498,
            -0.0239,
            0.0248,
            -0.0949,
            -0.1194,
            -0.1578,
            -0.0131,
            0.0745,
            0.1573,
            0.0091,
            -0.0966,
            -0.0124,
            -0.0343,
            -0.1878
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.071,
            0.0713,
            -0.1563,
            0.0269,
            0.0094,
            -0.1462,
            0.235,
            0.041,
            -0.2175,
            0.0325,
            -0.0614,
            -0.0042,
            0.1229,
            -0.0572,
            -0.1489,
            0.1434
          ],
          "after": [
            0.0281,
            0.1081,
            -0.0369,
            -0.0687,
            0.0337,
            -0.013,
            0.0884,
            0.0943,
            -0.065,
            -0.0614,
            0.0188,
            -0.0118,
            0.105,
            0.0318,
            -0.0565,
            0.2212
          ]
        },
        "position_0": {
          "grad": [
            0.0132,
            0.0401,
            -0.0049,
            0.1135,
            0.0047,
            -0.1204,
            0.1535,
            0.0631,
            -0.1934,
            -0.0535,
            0.0254,
            -0.0029,
            0.2899,
            -0.093,
            -0.1139,
            0.1296
          ],
          "after": [
            0.044,
            -0.1465,
            -0.2929,
            0.0537,
            0.0344,
            -0.0509,
            -0.1498,
            0.0681,
            0.1086,
            0.0057,
            0.0171,
            0.0875,
            0.0397,
            0.0157,
            -0.1308,
            0.0537
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0004,
            0.0003,
            -0.0003,
            0.0001,
            0.0002,
            -0.0006,
            0.0001,
            0.0001,
            0.0,
            0.0003,
            -0.0001,
            -0.0006,
            -0.0002,
            0.0008,
            -0.0005
          ],
          "after": [
            -0.0819,
            0.0176,
            -0.1184,
            -0.068,
            0.0606,
            0.1301,
            0.0792,
            -0.1671,
            0.0315,
            -0.0291,
            -0.138,
            -0.0684,
            0.0894,
            -0.0827,
            0.0412,
            0.0809
          ]
        }
      }
    },
    {
      "step": 804,
      "word": "saban",
      "loss": 1.9647,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1038,
            0.1891,
            -0.0166,
            0.0706,
            -0.2552,
            -0.3114,
            0.1428,
            -0.0773,
            -0.09,
            -0.062,
            -0.0227,
            -0.0515,
            0.1533,
            -0.0779,
            0.0725,
            0.3099
          ],
          "after": [
            0.0092,
            0.0004,
            0.1497,
            -0.024,
            0.0248,
            -0.0947,
            -0.1196,
            -0.158,
            -0.0129,
            0.0742,
            0.1572,
            0.0093,
            -0.097,
            -0.0124,
            -0.0342,
            -0.1878
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0114,
            0.0154,
            -0.1212,
            0.0117,
            -0.0062,
            -0.1464,
            0.128,
            0.016,
            -0.1815,
            0.0493,
            -0.0,
            0.0162,
            0.0643,
            -0.0469,
            -0.1018,
            0.0739
          ],
          "after": [
            0.0282,
            0.108,
            -0.0366,
            -0.0687,
            0.0338,
            -0.0128,
            0.0882,
            0.0943,
            -0.0647,
            -0.0615,
            0.0188,
            -0.0117,
            0.1048,
            0.032,
            -0.0563,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            -0.0067,
            0.0987,
            -0.029,
            0.1191,
            -0.125,
            0.0416,
            -0.0086,
            0.0452,
            -0.0719,
            0.0176,
            -0.1095,
            0.0968,
            0.0969,
            0.0099,
            0.0978,
            0.015
          ],
          "after": [
            0.0439,
            -0.1468,
            -0.2929,
            0.0537,
            0.0343,
            -0.0509,
            -0.1497,
            0.068,
            0.1087,
            0.0056,
            0.0171,
            0.0872,
            0.0397,
            0.0158,
            -0.1308,
            0.0537
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            0.0002,
            -0.0006,
            0.0021,
            -0.0007,
            0.0035,
            0.0003,
            0.002,
            0.0,
            0.0002,
            -0.0012,
            0.002,
            -0.0005,
            -0.0001,
            0.0005,
            0.0029
          ],
          "after": [
            -0.0819,
            0.0176,
            -0.1186,
            -0.0681,
            0.0606,
            0.1301,
            0.0794,
            -0.1673,
            0.0316,
            -0.0291,
            -0.1381,
            -0.0684,
            0.0896,
            -0.0827,
            0.0411,
            0.0809
          ]
        }
      }
    },
    {
      "step": 805,
      "word": "kelci",
      "loss": 2.5515,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0091,
            0.0005,
            0.1496,
            -0.0241,
            0.0249,
            -0.0946,
            -0.1198,
            -0.1581,
            -0.0128,
            0.074,
            0.1572,
            0.0095,
            -0.0973,
            -0.0125,
            -0.0342,
            -0.1879
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3037,
            -0.232,
            0.2166,
            -0.0171,
            0.2424,
            0.2664,
            -0.5239,
            -0.1631,
            0.4582,
            0.181,
            0.2688,
            0.1552,
            -0.1979,
            0.0499,
            -0.2378,
            -0.2889
          ],
          "after": [
            0.0281,
            0.108,
            -0.0365,
            -0.0687,
            0.0337,
            -0.0127,
            0.0882,
            0.0945,
            -0.0646,
            -0.0617,
            0.0186,
            -0.0117,
            0.1048,
            0.0321,
            -0.0561,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            -0.1073,
            0.0487,
            -0.1296,
            0.044,
            0.0393,
            -0.0553,
            0.1937,
            -0.0036,
            -0.0243,
            -0.0291,
            -0.0269,
            0.0265,
            0.052,
            -0.0155,
            -0.0771,
            0.0057
          ],
          "after": [
            0.0441,
            -0.147,
            -0.2927,
            0.0537,
            0.0342,
            -0.0508,
            -0.1497,
            0.068,
            0.1088,
            0.0057,
            0.0171,
            0.087,
            0.0396,
            0.0159,
            -0.1307,
            0.0538
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            0.0019,
            0.0013,
            0.0013,
            0.0026,
            0.0003,
            0.0026,
            -0.0024,
            -0.0001,
            0.001,
            0.0016,
            -0.0016,
            0.0024,
            0.0005,
            -0.0067,
            0.0009
          ],
          "after": [
            -0.082,
            0.0175,
            -0.1188,
            -0.0682,
            0.0605,
            0.13,
            0.0795,
            -0.1672,
            0.0317,
            -0.029,
            -0.1382,
            -0.0684,
            0.0895,
            -0.0827,
            0.0413,
            0.0808
          ]
        }
      }
    },
    {
      "step": 806,
      "word": "nera",
      "loss": 2.1475,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5638,
            -0.064,
            -0.4806,
            -0.0894,
            -0.2835,
            -0.1323,
            -0.0523,
            0.0299,
            -0.2334,
            0.2768,
            0.0899,
            -0.6137,
            0.3447,
            0.3216,
            0.1607,
            -0.2906
          ],
          "after": [
            0.0087,
            0.0006,
            0.1498,
            -0.0241,
            0.0251,
            -0.0943,
            -0.1199,
            -0.1582,
            -0.0125,
            0.0736,
            0.1571,
            0.01,
            -0.0977,
            -0.0128,
            -0.0343,
            -0.1878
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3611,
            -0.0936,
            0.3377,
            -0.0708,
            0.1077,
            0.0926,
            -0.8605,
            -0.2399,
            0.4169,
            -0.0234,
            0.1638,
            -0.0472,
            -0.2285,
            0.3453,
            -0.0798,
            -0.2812
          ],
          "after": [
            0.0278,
            0.1081,
            -0.0366,
            -0.0687,
            0.0335,
            -0.0126,
            0.0884,
            0.095,
            -0.0647,
            -0.0618,
            0.0182,
            -0.0117,
            0.1048,
            0.0319,
            -0.0558,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            -0.03,
            0.1315,
            -0.1,
            -0.0004,
            -0.4373,
            0.0109,
            -0.0465,
            -0.0221,
            0.1399,
            -0.2491,
            -0.2697,
            -0.1018,
            -0.0764,
            0.1519,
            0.2337,
            0.1003
          ],
          "after": [
            0.0442,
            -0.1473,
            -0.2924,
            0.0537,
            0.0344,
            -0.0508,
            -0.1497,
            0.068,
            0.1088,
            0.006,
            0.0173,
            0.0869,
            0.0396,
            0.0158,
            -0.1308,
            0.0538
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            0.0003,
            -0.0007,
            0.0018,
            -0.002,
            0.002,
            0.0001,
            0.0007,
            0.0005,
            -0.0013,
            -0.0024,
            -0.0006,
            0.0014,
            0.0019,
            -0.0008,
            0.0025
          ],
          "after": [
            -0.0822,
            0.0173,
            -0.119,
            -0.0685,
            0.0605,
            0.1299,
            0.0796,
            -0.1672,
            0.0318,
            -0.029,
            -0.1381,
            -0.0684,
            0.0894,
            -0.0828,
            0.0415,
            0.0806
          ]
        }
      }
    },
    {
      "step": 807,
      "word": "myalynn",
      "loss": 2.1077,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0495,
            0.1433,
            -0.0858,
            -0.0164,
            0.0147,
            -0.0091,
            -0.0564,
            0.0818,
            -0.0423,
            0.0321,
            -0.0817,
            0.2216,
            -0.0686,
            0.0266,
            0.0084,
            -0.2044
          ],
          "after": [
            0.0083,
            0.0007,
            0.15,
            -0.0241,
            0.0252,
            -0.0941,
            -0.1199,
            -0.1584,
            -0.0122,
            0.0732,
            0.157,
            0.0103,
            -0.0981,
            -0.013,
            -0.0344,
            -0.1876
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0948,
            0.0819,
            -0.119,
            0.05,
            0.036,
            -0.1602,
            0.2459,
            0.0142,
            -0.1891,
            0.038,
            -0.0262,
            0.0143,
            0.0984,
            -0.1115,
            -0.135,
            0.1629
          ],
          "after": [
            0.0276,
            0.1081,
            -0.0366,
            -0.0686,
            0.0334,
            -0.0125,
            0.0885,
            0.0954,
            -0.0647,
            -0.0619,
            0.018,
            -0.0117,
            0.1048,
            0.0319,
            -0.0556,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            -0.0174,
            0.0175,
            -0.0386,
            0.0063,
            0.1247,
            -0.0077,
            0.1918,
            0.0592,
            0.0179,
            0.0438,
            0.0133,
            0.0382,
            -0.0532,
            -0.0638,
            -0.0255,
            0.0875
          ],
          "after": [
            0.0444,
            -0.1477,
            -0.2922,
            0.0538,
            0.0346,
            -0.0507,
            -0.1497,
            0.0679,
            0.1088,
            0.0062,
            0.0175,
            0.0867,
            0.0396,
            0.0158,
            -0.1309,
            0.0537
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0019,
            -0.0001,
            0.0021,
            0.0005,
            0.0012,
            0.0001,
            -0.0003,
            -0.0009,
            -0.0011,
            0.0007,
            0.0028,
            -0.0019,
            -0.0008,
            -0.0005,
            0.0004
          ],
          "after": [
            -0.0823,
            0.0171,
            -0.1191,
            -0.0688,
            0.0605,
            0.1297,
            0.0797,
            -0.1672,
            0.0319,
            -0.0288,
            -0.1381,
            -0.0685,
            0.0895,
            -0.0828,
            0.0417,
            0.0805
          ]
        }
      }
    },
    {
      "step": 808,
      "word": "jaser",
      "loss": 1.9954,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0549,
            0.2075,
            0.105,
            0.1758,
            -0.1756,
            -0.0364,
            -0.0136,
            0.0334,
            -0.1111,
            0.0404,
            -0.1024,
            0.0681,
            0.1465,
            0.0469,
            0.0595,
            -0.0908
          ],
          "after": [
            0.0081,
            0.0005,
            0.1501,
            -0.0243,
            0.0254,
            -0.0939,
            -0.1199,
            -0.1586,
            -0.0118,
            0.0728,
            0.1571,
            0.0105,
            -0.0984,
            -0.0133,
            -0.0345,
            -0.1874
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3049,
            0.1072,
            0.2594,
            0.0404,
            0.0583,
            0.0405,
            -0.0975,
            0.0335,
            -0.017,
            -0.0706,
            -0.1351,
            -0.2368,
            0.2598,
            0.0184,
            0.1068,
            -0.0628
          ],
          "after": [
            0.0276,
            0.108,
            -0.0367,
            -0.0687,
            0.0333,
            -0.0124,
            0.0886,
            0.0957,
            -0.0647,
            -0.062,
            0.0179,
            -0.0116,
            0.1047,
            0.0318,
            -0.0554,
            0.2212
          ]
        },
        "position_0": {
          "grad": [
            0.0164,
            0.0325,
            -0.0023,
            0.1449,
            -0.008,
            -0.1379,
            0.1674,
            0.0884,
            -0.2116,
            -0.082,
            0.0157,
            -0.0149,
            0.3411,
            -0.1089,
            -0.1192,
            0.1386
          ],
          "after": [
            0.0445,
            -0.1479,
            -0.292,
            0.0536,
            0.0347,
            -0.0505,
            -0.1499,
            0.0677,
            0.1089,
            0.0065,
            0.0176,
            0.0866,
            0.0394,
            0.016,
            -0.1309,
            0.0536
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.003,
            -0.0012,
            -0.0026,
            -0.0011,
            0.0001,
            -0.004,
            -0.0001,
            0.0009,
            0.0018,
            0.0022,
            -0.002,
            -0.0015,
            0.0026,
            0.0019,
            -0.0007
          ],
          "after": [
            -0.0824,
            0.0171,
            -0.1191,
            -0.0689,
            0.0605,
            0.1295,
            0.0799,
            -0.1672,
            0.0319,
            -0.0288,
            -0.1383,
            -0.0684,
            0.0896,
            -0.083,
            0.0418,
            0.0803
          ]
        }
      }
    },
    {
      "step": 809,
      "word": "trillion",
      "loss": 2.3338,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0079,
            0.0004,
            0.1502,
            -0.0244,
            0.0256,
            -0.0937,
            -0.12,
            -0.1588,
            -0.0115,
            0.0726,
            0.1572,
            0.0107,
            -0.0987,
            -0.0135,
            -0.0346,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1126,
            0.2297,
            -0.1661,
            0.0627,
            0.0464,
            -0.2282,
            0.3934,
            -0.0119,
            -0.2436,
            -0.0295,
            -0.0249,
            -0.0268,
            0.1417,
            -0.1337,
            -0.2312,
            0.224
          ],
          "after": [
            0.0277,
            0.1079,
            -0.0367,
            -0.0688,
            0.0331,
            -0.0122,
            0.0886,
            0.0959,
            -0.0646,
            -0.0621,
            0.0178,
            -0.0115,
            0.1045,
            0.0319,
            -0.0552,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            -0.0269,
            -0.121,
            0.0417,
            0.0397,
            -0.0624,
            0.0682,
            0.0672,
            0.0048,
            -0.0861,
            -0.0457,
            -0.0461,
            -0.1505,
            0.0819,
            -0.0052,
            0.0638,
            0.0816
          ],
          "after": [
            0.0446,
            -0.1481,
            -0.2918,
            0.0535,
            0.0349,
            -0.0504,
            -0.15,
            0.0675,
            0.1091,
            0.0067,
            0.0177,
            0.0867,
            0.0392,
            0.0161,
            -0.1309,
            0.0534
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            0.0023,
            -0.0031,
            0.0039,
            -0.003,
            0.0009,
            0.0016,
            0.0006,
            0.0004,
            -0.0,
            -0.0013,
            -0.0005,
            0.0007,
            -0.0013,
            0.004,
            0.0014
          ],
          "after": [
            -0.0825,
            0.017,
            -0.1189,
            -0.0692,
            0.0607,
            0.1293,
            0.08,
            -0.1672,
            0.0319,
            -0.0288,
            -0.1383,
            -0.0684,
            0.0896,
            -0.0831,
            0.0418,
            0.0801
          ]
        }
      }
    },
    {
      "step": 810,
      "word": "annorah",
      "loss": 2.2087,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1505,
            0.1214,
            -0.0083,
            0.2292,
            -0.2994,
            0.1748,
            0.0212,
            0.0825,
            0.177,
            -0.1604,
            -0.3606,
            -0.0388,
            0.1361,
            0.1337,
            0.1062,
            0.2724
          ],
          "after": [
            0.0078,
            0.0003,
            0.1503,
            -0.0247,
            0.0258,
            -0.0937,
            -0.12,
            -0.159,
            -0.0114,
            0.0724,
            0.1575,
            0.0108,
            -0.0991,
            -0.0138,
            -0.0348,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0445,
            0.0215,
            -0.0366,
            0.0273,
            0.0805,
            -0.1116,
            0.1719,
            0.0268,
            -0.0807,
            0.05,
            -0.0125,
            0.0619,
            0.0308,
            -0.0834,
            -0.1494,
            0.0701
          ],
          "after": [
            0.0278,
            0.1078,
            -0.0367,
            -0.0689,
            0.033,
            -0.0121,
            0.0886,
            0.0961,
            -0.0645,
            -0.0621,
            0.0178,
            -0.0114,
            0.1044,
            0.032,
            -0.055,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            0.0775,
            -0.0455,
            0.0798,
            -0.1088,
            0.028,
            -0.001,
            -0.2331,
            -0.0821,
            0.113,
            0.0362,
            0.1662,
            0.0477,
            -0.156,
            0.0044,
            -0.033,
            -0.1885
          ],
          "after": [
            0.0446,
            -0.1481,
            -0.2918,
            0.0535,
            0.035,
            -0.0503,
            -0.15,
            0.0675,
            0.1092,
            0.0069,
            0.0177,
            0.0867,
            0.0391,
            0.0162,
            -0.1309,
            0.0534
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0002,
            0.0011,
            -0.0003,
            -0.0001,
            -0.0009,
            -0.0005,
            -0.0013,
            -0.0007,
            -0.0002,
            0.0012,
            -0.0004,
            -0.0003,
            -0.0004,
            0.0002,
            -0.0015
          ],
          "after": [
            -0.0826,
            0.0168,
            -0.1188,
            -0.0695,
            0.0608,
            0.1292,
            0.0801,
            -0.1671,
            0.0319,
            -0.0288,
            -0.1384,
            -0.0684,
            0.0897,
            -0.0832,
            0.0417,
            0.0801
          ]
        }
      }
    },
    {
      "step": 811,
      "word": "adriel",
      "loss": 2.1393,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.143,
            0.0901,
            0.0379,
            -0.1108,
            0.1614,
            -0.1795,
            0.0695,
            -0.1474,
            -0.0467,
            -0.0198,
            -0.0168,
            0.0141,
            0.0628,
            0.0095,
            0.009,
            0.1021
          ],
          "after": [
            0.0078,
            0.0001,
            0.1503,
            -0.0248,
            0.026,
            -0.0936,
            -0.1201,
            -0.159,
            -0.0113,
            0.0723,
            0.1578,
            0.011,
            -0.0994,
            -0.014,
            -0.0349,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0762,
            0.1106,
            -0.0336,
            0.2218,
            -0.3875,
            -0.1062,
            -0.0661,
            -0.001,
            -0.0715,
            -0.2095,
            -0.0614,
            0.079,
            0.085,
            0.0221,
            0.1824,
            0.0724
          ],
          "after": [
            0.0278,
            0.1076,
            -0.0367,
            -0.0692,
            0.033,
            -0.0119,
            0.0886,
            0.0963,
            -0.0644,
            -0.0621,
            0.0178,
            -0.0114,
            0.1042,
            0.032,
            -0.0548,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            0.0875,
            -0.0374,
            0.096,
            -0.1313,
            0.0821,
            0.0224,
            -0.2457,
            -0.1077,
            0.1006,
            0.0679,
            0.2313,
            0.0708,
            -0.1325,
            -0.0148,
            -0.0938,
            -0.1965
          ],
          "after": [
            0.0445,
            -0.1481,
            -0.2919,
            0.0536,
            0.035,
            -0.0503,
            -0.1499,
            0.0676,
            0.1092,
            0.007,
            0.0176,
            0.0866,
            0.0391,
            0.0163,
            -0.1308,
            0.0535
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            0.0021,
            -0.0004,
            0.0023,
            -0.0017,
            -0.0027,
            -0.0002,
            0.001,
            -0.0008,
            -0.0045,
            -0.0024,
            -0.0003,
            0.0017,
            0.0009,
            0.0024,
            0.0009
          ],
          "after": [
            -0.0827,
            0.0166,
            -0.1187,
            -0.0698,
            0.0611,
            0.1293,
            0.0802,
            -0.1671,
            0.032,
            -0.0286,
            -0.1384,
            -0.0683,
            0.0896,
            -0.0833,
            0.0415,
            0.08
          ]
        }
      }
    },
    {
      "step": 812,
      "word": "vaanya",
      "loss": 2.3568,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1951,
            0.2122,
            -0.0163,
            -0.1285,
            -0.3386,
            0.1364,
            -0.4111,
            -0.1067,
            0.1281,
            0.0046,
            0.2674,
            -0.1275,
            -0.1628,
            0.1934,
            0.0029,
            -0.2222
          ],
          "after": [
            0.0077,
            -0.0002,
            0.1504,
            -0.0248,
            0.0263,
            -0.0936,
            -0.1199,
            -0.159,
            -0.0113,
            0.0722,
            0.1578,
            0.0111,
            -0.0996,
            -0.0144,
            -0.0351,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0325,
            0.0186,
            -0.0761,
            0.0309,
            0.0291,
            -0.1224,
            0.1605,
            0.0186,
            -0.1482,
            0.0609,
            -0.017,
            0.0159,
            0.0688,
            -0.0731,
            -0.1062,
            0.0692
          ],
          "after": [
            0.0278,
            0.1074,
            -0.0367,
            -0.0695,
            0.033,
            -0.0116,
            0.0885,
            0.0964,
            -0.0643,
            -0.062,
            0.0178,
            -0.0114,
            0.104,
            0.0321,
            -0.0547,
            0.2208
          ]
        },
        "position_0": {
          "grad": [
            -0.064,
            0.0498,
            -0.0206,
            0.0306,
            0.0757,
            0.0501,
            -0.0003,
            0.0024,
            -0.0642,
            0.0005,
            -0.0243,
            -0.0815,
            0.0148,
            -0.0792,
            -0.0391,
            0.1227
          ],
          "after": [
            0.0445,
            -0.1482,
            -0.2919,
            0.0536,
            0.0349,
            -0.0503,
            -0.1498,
            0.0677,
            0.1092,
            0.0071,
            0.0174,
            0.0866,
            0.0391,
            0.0165,
            -0.1307,
            0.0535
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.001,
            -0.0002,
            0.0003,
            -0.0012,
            0.0007,
            -0.0014,
            -0.0007,
            0.0006,
            -0.0004,
            0.0003,
            0.0003,
            -0.001,
            0.0011,
            0.0002,
            -0.0001
          ],
          "after": [
            -0.0828,
            0.0163,
            -0.1186,
            -0.0701,
            0.0613,
            0.1293,
            0.0804,
            -0.1671,
            0.032,
            -0.0283,
            -0.1383,
            -0.0683,
            0.0896,
            -0.0834,
            0.0414,
            0.0799
          ]
        }
      }
    },
    {
      "step": 813,
      "word": "oziel",
      "loss": 2.6293,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0076,
            -0.0005,
            0.1504,
            -0.0248,
            0.0265,
            -0.0936,
            -0.1197,
            -0.1589,
            -0.0113,
            0.0722,
            0.1578,
            0.0113,
            -0.0998,
            -0.0147,
            -0.0352,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2076,
            0.1614,
            -0.0254,
            0.1103,
            -0.2183,
            -0.2245,
            0.0303,
            0.0647,
            -0.0508,
            -0.339,
            -0.0498,
            0.2081,
            0.0052,
            -0.1921,
            0.4038,
            0.0559
          ],
          "after": [
            0.028,
            0.1072,
            -0.0367,
            -0.0698,
            0.0332,
            -0.0114,
            0.0885,
            0.0964,
            -0.0641,
            -0.0618,
            0.0179,
            -0.0115,
            0.1038,
            0.0323,
            -0.0547,
            0.2207
          ]
        },
        "position_0": {
          "grad": [
            0.1315,
            -0.095,
            0.1172,
            -0.077,
            0.0225,
            -0.0419,
            -0.2839,
            -0.0579,
            0.0209,
            0.1136,
            0.0428,
            -0.0192,
            -0.0992,
            0.0439,
            -0.0058,
            -0.2064
          ],
          "after": [
            0.0443,
            -0.1482,
            -0.2921,
            0.0537,
            0.0349,
            -0.0503,
            -0.1496,
            0.0678,
            0.1092,
            0.007,
            0.0173,
            0.0867,
            0.0392,
            0.0166,
            -0.1306,
            0.0536
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            -0.0002,
            0.0014,
            0.0009,
            -0.0011,
            -0.0004,
            -0.0019,
            0.0006,
            0.0004,
            -0.0006,
            0.0008,
            0.0024,
            -0.0026,
            -0.0021,
            0.0031,
            0.0002
          ],
          "after": [
            -0.0828,
            0.0161,
            -0.1187,
            -0.0704,
            0.0615,
            0.1293,
            0.0805,
            -0.1671,
            0.032,
            -0.0281,
            -0.1384,
            -0.0684,
            0.0898,
            -0.0834,
            0.0411,
            0.0798
          ]
        }
      }
    },
    {
      "step": 814,
      "word": "ziyah",
      "loss": 2.2898,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0344,
            -0.0477,
            0.1969,
            0.0563,
            0.0642,
            0.099,
            0.0088,
            0.1472,
            0.1277,
            0.2097,
            -0.0106,
            -0.1594,
            0.2709,
            0.0786,
            -0.1432,
            0.0012
          ],
          "after": [
            0.0075,
            -0.0006,
            0.1503,
            -0.0249,
            0.0267,
            -0.0936,
            -0.1196,
            -0.159,
            -0.0114,
            0.072,
            0.1578,
            0.0115,
            -0.1,
            -0.015,
            -0.0352,
            -0.187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0387,
            0.0258,
            -0.154,
            0.0308,
            0.0455,
            -0.135,
            0.1857,
            0.0221,
            -0.2036,
            0.0404,
            -0.0555,
            -0.0319,
            0.1096,
            -0.0653,
            -0.1843,
            0.1153
          ],
          "after": [
            0.0281,
            0.107,
            -0.0366,
            -0.0702,
            0.0333,
            -0.0111,
            0.0884,
            0.0964,
            -0.0639,
            -0.0616,
            0.018,
            -0.0116,
            0.1037,
            0.0325,
            -0.0546,
            0.2205
          ]
        },
        "position_0": {
          "grad": [
            0.0311,
            0.0336,
            -0.0312,
            0.1289,
            0.0511,
            -0.0792,
            0.1604,
            -0.0027,
            0.0537,
            0.0195,
            0.0447,
            -0.0128,
            -0.0065,
            -0.0431,
            0.0349,
            0.0307
          ],
          "after": [
            0.0441,
            -0.1482,
            -0.2922,
            0.0537,
            0.0348,
            -0.0502,
            -0.1495,
            0.0679,
            0.1092,
            0.007,
            0.0171,
            0.0867,
            0.0392,
            0.0168,
            -0.1305,
            0.0537
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            0.0015,
            0.0013,
            -0.0007,
            -0.0012,
            -0.0017,
            -0.0034,
            0.0001,
            0.0016,
            -0.0022,
            0.0011,
            0.0029,
            -0.0036,
            -0.0012,
            0.0036,
            -0.0018
          ],
          "after": [
            -0.0828,
            0.0158,
            -0.1187,
            -0.0707,
            0.0618,
            0.1294,
            0.0808,
            -0.1671,
            0.0319,
            -0.0278,
            -0.1384,
            -0.0686,
            0.0901,
            -0.0833,
            0.0408,
            0.0798
          ]
        }
      }
    },
    {
      "step": 815,
      "word": "alizza",
      "loss": 2.3986,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.038,
            0.2004,
            -0.1556,
            -0.1832,
            -0.0511,
            -0.0695,
            -0.0945,
            0.0628,
            -0.1822,
            0.0627,
            -0.0955,
            0.1809,
            -0.0571,
            0.0816,
            0.0796,
            -0.487
          ],
          "after": [
            0.0075,
            -0.0009,
            0.1504,
            -0.0248,
            0.0269,
            -0.0936,
            -0.1194,
            -0.1592,
            -0.0113,
            0.0717,
            0.1579,
            0.0116,
            -0.1003,
            -0.0153,
            -0.0352,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0729,
            0.1751,
            -0.1972,
            0.0382,
            0.0416,
            -0.2362,
            0.3137,
            0.0055,
            -0.2444,
            -0.0217,
            -0.0467,
            -0.0326,
            0.1651,
            -0.0943,
            -0.2481,
            0.1835
          ],
          "after": [
            0.0283,
            0.1068,
            -0.0364,
            -0.0705,
            0.0333,
            -0.0107,
            0.0882,
            0.0963,
            -0.0637,
            -0.0615,
            0.0181,
            -0.0116,
            0.1034,
            0.0328,
            -0.0545,
            0.2203
          ]
        },
        "position_0": {
          "grad": [
            0.0788,
            -0.0348,
            0.0976,
            -0.1608,
            0.1137,
            0.0288,
            -0.2664,
            -0.1151,
            0.1134,
            0.0799,
            0.2516,
            0.0854,
            -0.1265,
            -0.0112,
            -0.133,
            -0.2038
          ],
          "after": [
            0.0438,
            -0.1481,
            -0.2924,
            0.0538,
            0.0346,
            -0.0501,
            -0.1493,
            0.0682,
            0.1091,
            0.0068,
            0.0168,
            0.0867,
            0.0394,
            0.0169,
            -0.1303,
            0.0539
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0007,
            -0.0015,
            -0.0004,
            0.0011,
            0.0004,
            0.0024,
            -0.0003,
            -0.0006,
            0.0022,
            0.0006,
            -0.0005,
            0.0014,
            -0.0003,
            -0.0023,
            -0.0004
          ],
          "after": [
            -0.0827,
            0.0155,
            -0.1187,
            -0.0708,
            0.062,
            0.1295,
            0.081,
            -0.1671,
            0.0319,
            -0.0277,
            -0.1385,
            -0.0687,
            0.0903,
            -0.0833,
            0.0406,
            0.0799
          ]
        }
      }
    },
    {
      "step": 816,
      "word": "emmajane",
      "loss": 2.6764,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3772,
            0.2123,
            0.3538,
            0.1654,
            0.0428,
            0.0047,
            0.0998,
            0.0248,
            -0.0485,
            -0.4152,
            -0.2037,
            0.2556,
            0.1266,
            -0.209,
            -0.1912,
            0.3525
          ],
          "after": [
            0.0077,
            -0.0013,
            0.1502,
            -0.0249,
            0.027,
            -0.0936,
            -0.1193,
            -0.1593,
            -0.0112,
            0.0718,
            0.1582,
            0.0115,
            -0.1005,
            -0.0154,
            -0.0351,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0548,
            0.4137,
            0.2847,
            -0.0426,
            -0.3847,
            0.1332,
            -0.065,
            -0.0677,
            0.0927,
            -0.2219,
            -0.0718,
            -0.4133,
            0.1371,
            0.3013,
            0.1815,
            -0.2927
          ],
          "after": [
            0.0284,
            0.1064,
            -0.0364,
            -0.0707,
            0.0335,
            -0.0105,
            0.0881,
            0.0964,
            -0.0635,
            -0.0612,
            0.0183,
            -0.0115,
            0.1031,
            0.0328,
            -0.0544,
            0.2203
          ]
        },
        "position_0": {
          "grad": [
            -0.0141,
            -0.0908,
            0.032,
            0.0111,
            0.0625,
            0.072,
            -0.0927,
            -0.0591,
            0.0276,
            0.1396,
            0.0392,
            0.056,
            -0.0607,
            -0.0503,
            -0.0611,
            -0.1643
          ],
          "after": [
            0.0435,
            -0.148,
            -0.2927,
            0.0539,
            0.0344,
            -0.0502,
            -0.1491,
            0.0685,
            0.109,
            0.0065,
            0.0165,
            0.0866,
            0.0395,
            0.0171,
            -0.1301,
            0.0542
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0036,
            -0.0006,
            -0.0008,
            0.0006,
            -0.006,
            0.0009,
            -0.0026,
            -0.0011,
            0.0026,
            0.0013,
            -0.0034,
            0.0035,
            -0.0003,
            -0.0024,
            -0.003
          ],
          "after": [
            -0.0827,
            0.015,
            -0.1187,
            -0.0709,
            0.0621,
            0.1298,
            0.0811,
            -0.1669,
            0.0319,
            -0.0277,
            -0.1387,
            -0.0687,
            0.0903,
            -0.0832,
            0.0405,
            0.0801
          ]
        }
      }
    },
    {
      "step": 817,
      "word": "alonah",
      "loss": 1.8854,
      "learning_rate": 0.0006,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.268,
            0.0735,
            0.0726,
            -0.0524,
            0.0607,
            0.1097,
            -0.1548,
            0.2077,
            0.0609,
            0.0653,
            -0.2515,
            0.2418,
            0.0227,
            0.0808,
            0.0114,
            -0.2672
          ],
          "after": [
            0.008,
            -0.0016,
            0.15,
            -0.0249,
            0.0271,
            -0.0937,
            -0.1192,
            -0.1596,
            -0.0112,
            0.0719,
            0.1585,
            0.0114,
            -0.1007,
            -0.0155,
            -0.035,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0245,
            0.0592,
            -0.0911,
            0.0159,
            0.019,
            -0.1566,
            0.1728,
            -0.0046,
            -0.1341,
            0.0245,
            0.0039,
            -0.0125,
            0.0786,
            -0.0774,
            -0.1315,
            0.1082
          ],
          "after": [
            0.0285,
            0.106,
            -0.0363,
            -0.0709,
            0.0337,
            -0.0102,
            0.0879,
            0.0965,
            -0.0633,
            -0.061,
            0.0185,
            -0.0113,
            0.1029,
            0.0329,
            -0.0544,
            0.2202
          ]
        },
        "position_0": {
          "grad": [
            0.0781,
            -0.0564,
            0.1033,
            -0.164,
            0.0876,
            0.0126,
            -0.312,
            -0.1159,
            0.1447,
            0.0571,
            0.2399,
            0.0556,
            -0.1628,
            0.0066,
            -0.1002,
            -0.2258
          ],
          "after": [
            0.0432,
            -0.1478,
            -0.2929,
            0.0541,
            0.0342,
            -0.0502,
            -0.1487,
            0.0688,
            0.1088,
            0.0062,
            0.0161,
            0.0865,
            0.0397,
            0.0172,
            -0.1299,
            0.0545
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0005,
            0.0014,
            0.0005,
            -0.0007,
            -0.0,
            -0.0027,
            -0.0018,
            -0.0008,
            -0.0023,
            -0.0003,
            -0.001,
            -0.0008,
            0.0015,
            0.0002,
            -0.0005
          ],
          "after": [
            -0.0827,
            0.0146,
            -0.1187,
            -0.071,
            0.0622,
            0.1301,
            0.0813,
            -0.1666,
            0.032,
            -0.0276,
            -0.1389,
            -0.0686,
            0.0903,
            -0.0832,
            0.0404,
            0.0803
          ]
        }
      }
    },
    {
      "step": 818,
      "word": "ilai",
      "loss": 2.493,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0107,
            -0.4574,
            0.0544,
            -0.1529,
            0.1063,
            0.1208,
            -0.3512,
            0.0966,
            0.0584,
            -0.0478,
            0.2578,
            -0.3081,
            -0.2405,
            0.0526,
            0.0128,
            -0.1687
          ],
          "after": [
            0.0082,
            -0.0017,
            0.1499,
            -0.0248,
            0.0271,
            -0.0939,
            -0.1188,
            -0.1599,
            -0.0112,
            0.0719,
            0.1587,
            0.0114,
            -0.1008,
            -0.0157,
            -0.035,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0095,
            0.066,
            -0.1655,
            -0.0278,
            0.0184,
            -0.1722,
            0.2065,
            -0.0364,
            -0.1995,
            0.0412,
            -0.0024,
            -0.1383,
            0.1478,
            -0.0465,
            -0.2062,
            0.1305
          ],
          "after": [
            0.0286,
            0.1057,
            -0.0362,
            -0.0711,
            0.0339,
            -0.0099,
            0.0878,
            0.0966,
            -0.0631,
            -0.0609,
            0.0186,
            -0.0111,
            0.1026,
            0.0329,
            -0.0542,
            0.22
          ]
        },
        "position_0": {
          "grad": [
            -0.0783,
            -0.3247,
            0.1331,
            -0.2033,
            0.1224,
            0.1693,
            -0.3451,
            0.0364,
            0.1288,
            0.0159,
            0.0769,
            -0.2046,
            -0.2635,
            0.0254,
            -0.0023,
            -0.2322
          ],
          "after": [
            0.0431,
            -0.1474,
            -0.2933,
            0.0545,
            0.0339,
            -0.0504,
            -0.1483,
            0.0691,
            0.1085,
            0.0059,
            0.0157,
            0.0865,
            0.0401,
            0.0173,
            -0.1297,
            0.0549
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0028,
            -0.0011,
            -0.0008,
            -0.005,
            0.0075,
            0.0027,
            0.0036,
            0.0006,
            0.001,
            0.0035,
            0.0003,
            -0.0008,
            -0.0004,
            -0.0014,
            -0.0032,
            -0.0015
          ],
          "after": [
            -0.0826,
            0.0143,
            -0.1187,
            -0.0708,
            0.062,
            0.1303,
            0.0813,
            -0.1663,
            0.032,
            -0.0277,
            -0.139,
            -0.0685,
            0.0903,
            -0.0831,
            0.0405,
            0.0805
          ]
        }
      }
    },
    {
      "step": 819,
      "word": "fabian",
      "loss": 2.0395,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0447,
            0.1717,
            0.1983,
            0.0764,
            0.0071,
            -0.1522,
            0.0984,
            -0.0707,
            -0.0897,
            -0.2358,
            -0.0569,
            0.1536,
            0.0295,
            -0.2028,
            -0.0901,
            0.3498
          ],
          "after": [
            0.0084,
            -0.0018,
            0.1496,
            -0.0247,
            0.0271,
            -0.0939,
            -0.1186,
            -0.1601,
            -0.0111,
            0.0721,
            0.1588,
            0.0113,
            -0.1008,
            -0.0157,
            -0.0348,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0334,
            0.0535,
            -0.0915,
            0.0299,
            0.003,
            -0.1416,
            0.1431,
            0.0144,
            -0.175,
            0.0715,
            -0.0149,
            -0.0163,
            0.0944,
            -0.0286,
            -0.1467,
            0.0742
          ],
          "after": [
            0.0287,
            0.1054,
            -0.0361,
            -0.0713,
            0.034,
            -0.0096,
            0.0876,
            0.0966,
            -0.0628,
            -0.0608,
            0.0187,
            -0.0109,
            0.1023,
            0.033,
            -0.054,
            0.2199
          ]
        },
        "position_0": {
          "grad": [
            0.0341,
            0.1894,
            -0.0623,
            0.1529,
            0.0032,
            -0.2425,
            0.1695,
            -0.0214,
            -0.2763,
            -0.105,
            0.1098,
            -0.0775,
            0.2354,
            0.0069,
            0.0011,
            0.1806
          ],
          "after": [
            0.0429,
            -0.1472,
            -0.2936,
            0.0546,
            0.0337,
            -0.0503,
            -0.148,
            0.0694,
            0.1085,
            0.0058,
            0.0153,
            0.0867,
            0.0402,
            0.0174,
            -0.1295,
            0.0552
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0028,
            -0.0026,
            -0.0007,
            0.0003,
            -0.003,
            0.0018,
            -0.0014,
            0.0027,
            0.0031,
            -0.0024,
            -0.0018,
            0.0029,
            -0.0018,
            -0.0016,
            0.0042,
            0.0025
          ],
          "after": [
            -0.0823,
            0.0143,
            -0.1186,
            -0.0707,
            0.0619,
            0.1303,
            0.0814,
            -0.1664,
            0.0318,
            -0.0276,
            -0.139,
            -0.0686,
            0.0905,
            -0.083,
            0.0404,
            0.0806
          ]
        }
      }
    },
    {
      "step": 820,
      "word": "jaxson",
      "loss": 2.5102,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0019,
            -0.0649,
            -0.0897,
            0.4407,
            0.0773,
            -0.2172,
            0.1465,
            0.0796,
            -0.4157,
            0.0494,
            0.0654,
            -0.0745,
            0.4117,
            -0.271,
            -0.1156,
            0.1829
          ],
          "after": [
            0.0086,
            -0.0018,
            0.1494,
            -0.025,
            0.0271,
            -0.0938,
            -0.1185,
            -0.1604,
            -0.0108,
            0.0723,
            0.1589,
            0.0113,
            -0.1011,
            -0.0155,
            -0.0347,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0577,
            0.0646,
            -0.1448,
            0.008,
            0.0245,
            -0.1511,
            0.2138,
            0.0378,
            -0.1868,
            0.0392,
            -0.0445,
            0.0195,
            0.0789,
            -0.0816,
            -0.118,
            0.1129
          ],
          "after": [
            0.0288,
            0.1051,
            -0.0359,
            -0.0714,
            0.0341,
            -0.0093,
            0.0874,
            0.0966,
            -0.0625,
            -0.0607,
            0.0189,
            -0.0108,
            0.102,
            0.0331,
            -0.0539,
            0.2197
          ]
        },
        "position_0": {
          "grad": [
            0.0051,
            0.0586,
            -0.0055,
            0.1913,
            -0.0147,
            -0.1047,
            0.2149,
            0.0765,
            -0.2344,
            -0.1041,
            0.0073,
            0.0137,
            0.3288,
            -0.0618,
            -0.0642,
            0.1992
          ],
          "after": [
            0.0428,
            -0.1471,
            -0.2938,
            0.0546,
            0.0335,
            -0.0502,
            -0.1478,
            0.0695,
            0.1086,
            0.0058,
            0.015,
            0.0867,
            0.0401,
            0.0175,
            -0.1293,
            0.0553
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0019,
            -0.0004,
            -0.0013,
            0.0021,
            -0.0002,
            0.0046,
            0.0012,
            0.0029,
            0.0002,
            -0.0001,
            -0.0015,
            0.003,
            -0.0011,
            -0.0015,
            0.0006,
            0.0038
          ],
          "after": [
            -0.082,
            0.0142,
            -0.1185,
            -0.0707,
            0.0619,
            0.1301,
            0.0814,
            -0.1666,
            0.0316,
            -0.0276,
            -0.1389,
            -0.0688,
            0.0906,
            -0.0828,
            0.0403,
            0.0805
          ]
        }
      }
    },
    {
      "step": 821,
      "word": "allizon",
      "loss": 2.2155,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1481,
            0.1866,
            -0.1006,
            -0.0527,
            0.0339,
            0.0191,
            0.0081,
            0.0675,
            -0.1436,
            -0.0181,
            -0.134,
            0.3045,
            -0.0806,
            -0.02,
            0.0667,
            -0.2946
          ],
          "after": [
            0.0088,
            -0.002,
            0.1494,
            -0.0252,
            0.0271,
            -0.0937,
            -0.1184,
            -0.1607,
            -0.0105,
            0.0724,
            0.159,
            0.0112,
            -0.1013,
            -0.0154,
            -0.0346,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0581,
            0.1531,
            -0.1538,
            0.0464,
            0.0438,
            -0.2135,
            0.2979,
            -0.0099,
            -0.2295,
            -0.0028,
            -0.0212,
            -0.0225,
            0.1353,
            -0.0921,
            -0.2419,
            0.1951
          ],
          "after": [
            0.0289,
            0.1048,
            -0.0357,
            -0.0716,
            0.0341,
            -0.0089,
            0.0871,
            0.0967,
            -0.0622,
            -0.0607,
            0.019,
            -0.0106,
            0.1017,
            0.0333,
            -0.0536,
            0.2195
          ]
        },
        "position_0": {
          "grad": [
            0.055,
            -0.0261,
            0.0727,
            -0.1256,
            0.1279,
            0.0381,
            -0.192,
            -0.098,
            0.0731,
            0.082,
            0.2343,
            0.108,
            -0.0949,
            -0.0378,
            -0.1309,
            -0.1559
          ],
          "after": [
            0.0426,
            -0.147,
            -0.2941,
            0.0547,
            0.0332,
            -0.05,
            -0.1476,
            0.0697,
            0.1087,
            0.0057,
            0.0145,
            0.0867,
            0.0401,
            0.0177,
            -0.129,
            0.0555
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            0.0021,
            -0.0025,
            0.0005,
            0.0027,
            -0.002,
            0.0051,
            -0.0012,
            -0.0012,
            -0.0014,
            -0.001,
            -0.0005,
            0.0021,
            -0.0013,
            -0.0007,
            0.0025
          ],
          "after": [
            -0.0816,
            0.0141,
            -0.1183,
            -0.0707,
            0.0618,
            0.13,
            0.0812,
            -0.1667,
            0.0315,
            -0.0275,
            -0.1387,
            -0.0689,
            0.0907,
            -0.0825,
            0.0402,
            0.0803
          ]
        }
      }
    },
    {
      "step": 822,
      "word": "alhassane",
      "loss": 2.4287,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3294,
            0.449,
            0.0669,
            0.061,
            0.0282,
            0.017,
            0.0364,
            0.0555,
            -0.0925,
            -0.1644,
            -0.3536,
            0.4934,
            -0.0724,
            -0.0887,
            0.0074,
            -0.1116
          ],
          "after": [
            0.0092,
            -0.0024,
            0.1492,
            -0.0254,
            0.0271,
            -0.0936,
            -0.1183,
            -0.1609,
            -0.0101,
            0.0726,
            0.1594,
            0.0109,
            -0.1014,
            -0.0152,
            -0.0345,
            -0.1861
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3425,
            0.0387,
            -0.1705,
            -0.0582,
            -0.3234,
            0.021,
            -0.2145,
            -0.0923,
            -0.0941,
            -0.0148,
            0.0487,
            -0.3357,
            0.1455,
            0.2276,
            0.0763,
            -0.2174
          ],
          "after": [
            0.0288,
            0.1045,
            -0.0354,
            -0.0717,
            0.0343,
            -0.0087,
            0.087,
            0.0968,
            -0.0619,
            -0.0607,
            0.0191,
            -0.0103,
            0.1013,
            0.0333,
            -0.0534,
            0.2194
          ]
        },
        "position_0": {
          "grad": [
            0.0506,
            -0.0215,
            0.0537,
            -0.0968,
            0.0878,
            0.0096,
            -0.1828,
            -0.0796,
            0.0857,
            0.0748,
            0.1661,
            0.0724,
            -0.1105,
            -0.0319,
            -0.0913,
            -0.1486
          ],
          "after": [
            0.0423,
            -0.1469,
            -0.2944,
            0.0549,
            0.0329,
            -0.05,
            -0.1473,
            0.07,
            0.1087,
            0.0056,
            0.014,
            0.0867,
            0.0402,
            0.0179,
            -0.1286,
            0.0557
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0036,
            0.0012,
            -0.0006,
            0.0009,
            -0.0006,
            -0.0017,
            -0.0015,
            -0.0011,
            -0.0022,
            0.0016,
            0.0013,
            -0.0054,
            0.0018,
            0.0014,
            -0.0004,
            -0.0035
          ],
          "after": [
            -0.0815,
            0.0138,
            -0.118,
            -0.0708,
            0.0617,
            0.13,
            0.0811,
            -0.1667,
            0.0316,
            -0.0274,
            -0.1387,
            -0.0688,
            0.0906,
            -0.0824,
            0.0402,
            0.0802
          ]
        }
      }
    },
    {
      "step": 823,
      "word": "jamiah",
      "loss": 1.8664,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.213,
            -0.0118,
            0.3579,
            0.0671,
            0.2606,
            0.0537,
            0.2333,
            0.2168,
            0.1964,
            0.1532,
            -0.0493,
            0.0003,
            0.0512,
            -0.062,
            -0.193,
            0.1322
          ],
          "after": [
            0.0096,
            -0.0027,
            0.149,
            -0.0256,
            0.0269,
            -0.0936,
            -0.1184,
            -0.1614,
            -0.0099,
            0.0727,
            0.1597,
            0.0106,
            -0.1015,
            -0.015,
            -0.0343,
            -0.186
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0687,
            0.0753,
            -0.1455,
            0.0222,
            0.0391,
            -0.1354,
            0.2088,
            0.0305,
            -0.1947,
            0.0414,
            -0.0634,
            -0.0134,
            0.109,
            -0.0402,
            -0.159,
            0.1311
          ],
          "after": [
            0.0288,
            0.1042,
            -0.0352,
            -0.0718,
            0.0345,
            -0.0084,
            0.0868,
            0.0969,
            -0.0616,
            -0.0606,
            0.0192,
            -0.0101,
            0.101,
            0.0333,
            -0.0532,
            0.2192
          ]
        },
        "position_0": {
          "grad": [
            0.0113,
            0.0553,
            -0.0162,
            0.1326,
            0.0027,
            -0.1231,
            0.1844,
            0.0638,
            -0.2135,
            -0.0471,
            0.0008,
            0.0011,
            0.3038,
            -0.0979,
            -0.1137,
            0.1489
          ],
          "after": [
            0.0421,
            -0.1469,
            -0.2946,
            0.0549,
            0.0326,
            -0.0498,
            -0.1472,
            0.0702,
            0.1088,
            0.0055,
            0.0136,
            0.0866,
            0.0401,
            0.0182,
            -0.1282,
            0.0558
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0004,
            0.0009,
            0.0006,
            -0.001,
            -0.0003,
            -0.0014,
            0.0002,
            0.0,
            -0.0011,
            -0.0,
            0.001,
            -0.0009,
            -0.0003,
            0.001,
            -0.0001
          ],
          "after": [
            -0.0815,
            0.0137,
            -0.1178,
            -0.0709,
            0.0616,
            0.13,
            0.0811,
            -0.1668,
            0.0317,
            -0.0274,
            -0.1387,
            -0.0687,
            0.0906,
            -0.0822,
            0.0401,
            0.0802
          ]
        }
      }
    },
    {
      "step": 824,
      "word": "yali",
      "loss": 2.2573,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1109,
            0.1498,
            -0.2068,
            -0.1722,
            -0.0554,
            -0.0773,
            -0.2093,
            0.1037,
            0.0057,
            -0.1081,
            -0.2151,
            0.2485,
            -0.1343,
            0.1333,
            0.1189,
            -0.314
          ],
          "after": [
            0.01,
            -0.003,
            0.1488,
            -0.0257,
            0.0268,
            -0.0935,
            -0.1183,
            -0.1618,
            -0.0097,
            0.0729,
            0.1602,
            0.0102,
            -0.1016,
            -0.0149,
            -0.0342,
            -0.1858
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0137,
            0.0336,
            -0.1365,
            0.0264,
            -0.0133,
            -0.1135,
            0.1875,
            -0.0261,
            -0.1709,
            0.095,
            0.0039,
            -0.0799,
            0.1217,
            -0.0564,
            -0.1734,
            0.1291
          ],
          "after": [
            0.0288,
            0.104,
            -0.0349,
            -0.0719,
            0.0346,
            -0.0081,
            0.0866,
            0.0969,
            -0.0612,
            -0.0607,
            0.0193,
            -0.0098,
            0.1006,
            0.0334,
            -0.053,
            0.2191
          ]
        },
        "position_0": {
          "grad": [
            0.0265,
            -0.2263,
            0.1667,
            -0.1679,
            0.0194,
            0.1983,
            -0.2325,
            0.0657,
            0.0595,
            0.1029,
            -0.0243,
            0.071,
            -0.1454,
            0.0704,
            0.0672,
            -0.1086
          ],
          "after": [
            0.0419,
            -0.1466,
            -0.295,
            0.055,
            0.0324,
            -0.0498,
            -0.147,
            0.0702,
            0.1089,
            0.0054,
            0.0132,
            0.0865,
            0.0401,
            0.0183,
            -0.128,
            0.0559
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0014,
            -0.0002,
            -0.0045,
            0.0055,
            -0.0017,
            0.0029,
            -0.0017,
            0.0009,
            0.0017,
            0.0011,
            -0.002,
            0.0008,
            -0.0,
            -0.0036,
            -0.0023
          ],
          "after": [
            -0.0814,
            0.0135,
            -0.1177,
            -0.0707,
            0.0614,
            0.1301,
            0.081,
            -0.1667,
            0.0317,
            -0.0274,
            -0.1387,
            -0.0686,
            0.0905,
            -0.0821,
            0.0402,
            0.0803
          ]
        }
      }
    },
    {
      "step": 825,
      "word": "exander",
      "loss": 2.8772,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0588,
            0.1691,
            -0.1552,
            0.0423,
            -0.2065,
            -0.0208,
            -0.0084,
            -0.0849,
            0.0148,
            -0.0284,
            -0.111,
            -0.0361,
            0.0469,
            0.0994,
            0.0057,
            0.0668
          ],
          "after": [
            0.0103,
            -0.0034,
            0.1488,
            -0.0258,
            0.0268,
            -0.0934,
            -0.1183,
            -0.1621,
            -0.0096,
            0.073,
            0.1606,
            0.01,
            -0.1016,
            -0.0149,
            -0.0341,
            -0.1857
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1383,
            0.0239,
            0.5693,
            -0.2142,
            -0.1796,
            0.2375,
            -0.1928,
            -0.1266,
            0.4232,
            -0.08,
            0.1279,
            -0.0795,
            -0.1759,
            0.1631,
            0.3423,
            -0.2976
          ],
          "after": [
            0.0288,
            0.1037,
            -0.0349,
            -0.0717,
            0.0348,
            -0.0079,
            0.0864,
            0.0972,
            -0.0611,
            -0.0607,
            0.0193,
            -0.0096,
            0.1004,
            0.0333,
            -0.0529,
            0.219
          ]
        },
        "position_0": {
          "grad": [
            -0.0123,
            -0.1466,
            0.0599,
            0.0227,
            -0.008,
            0.0571,
            -0.1793,
            -0.0572,
            0.0858,
            0.0814,
            0.0248,
            -0.0044,
            -0.0902,
            0.0384,
            0.0005,
            -0.1976
          ],
          "after": [
            0.0417,
            -0.1463,
            -0.2953,
            0.0551,
            0.0322,
            -0.0499,
            -0.1467,
            0.0704,
            0.1089,
            0.0051,
            0.0129,
            0.0864,
            0.0401,
            0.0184,
            -0.1278,
            0.0562
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0014,
            -0.0013,
            -0.0013,
            0.0013,
            -0.0005,
            -0.0005,
            -0.0006,
            0.0012,
            -0.0002,
            -0.0023,
            0.0026,
            -0.0017,
            0.0012,
            0.0013,
            -0.0005
          ],
          "after": [
            -0.0814,
            0.0132,
            -0.1175,
            -0.0705,
            0.0611,
            0.1302,
            0.0809,
            -0.1665,
            0.0316,
            -0.0274,
            -0.1386,
            -0.0686,
            0.0906,
            -0.0821,
            0.0402,
            0.0804
          ]
        }
      }
    },
    {
      "step": 826,
      "word": "ronnell",
      "loss": 2.5312,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0106,
            -0.0037,
            0.1488,
            -0.0258,
            0.0268,
            -0.0933,
            -0.1183,
            -0.1623,
            -0.0095,
            0.0731,
            0.1609,
            0.0097,
            -0.1017,
            -0.0149,
            -0.034,
            -0.1856
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0979,
            0.0162,
            0.0088,
            0.0612,
            -0.1893,
            0.0107,
            -0.2009,
            -0.1135,
            -0.0745,
            -0.1885,
            -0.0074,
            -0.1367,
            0.0493,
            0.1623,
            0.0075,
            0.0321
          ],
          "after": [
            0.0288,
            0.1035,
            -0.0349,
            -0.0717,
            0.035,
            -0.0078,
            0.0864,
            0.0975,
            -0.061,
            -0.0606,
            0.0193,
            -0.0093,
            0.1002,
            0.0332,
            -0.0528,
            0.219
          ]
        },
        "position_0": {
          "grad": [
            -0.0031,
            -0.0342,
            0.0033,
            -0.0031,
            -0.0441,
            0.1325,
            0.0964,
            -0.0244,
            0.1078,
            -0.0902,
            -0.0988,
            0.0408,
            -0.1472,
            -0.082,
            0.1418,
            0.0965
          ],
          "after": [
            0.0416,
            -0.146,
            -0.2957,
            0.0552,
            0.0321,
            -0.0501,
            -0.1465,
            0.0705,
            0.1089,
            0.005,
            0.0127,
            0.0863,
            0.0402,
            0.0185,
            -0.1277,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0003,
            0.0009,
            0.0005,
            0.0004,
            -0.0001,
            0.0,
            0.0002,
            -0.0001,
            0.0007,
            0.0005,
            -0.0003,
            0.0008,
            0.0002,
            -0.0021,
            0.0
          ],
          "after": [
            -0.0814,
            0.013,
            -0.1174,
            -0.0704,
            0.0609,
            0.1303,
            0.0808,
            -0.1665,
            0.0315,
            -0.0274,
            -0.1385,
            -0.0685,
            0.0906,
            -0.0821,
            0.0403,
            0.0805
          ]
        }
      }
    },
    {
      "step": 827,
      "word": "yuchen",
      "loss": 2.6251,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0108,
            -0.004,
            0.1488,
            -0.0259,
            0.0268,
            -0.0933,
            -0.1182,
            -0.1626,
            -0.0094,
            0.0732,
            0.1612,
            0.0095,
            -0.1017,
            -0.0148,
            -0.034,
            -0.1855
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2497,
            -0.3172,
            0.1385,
            -0.0627,
            -0.0761,
            0.1446,
            -0.5205,
            -0.0646,
            0.0356,
            -0.0008,
            0.099,
            -0.1369,
            0.0297,
            0.239,
            0.0864,
            -0.2156
          ],
          "after": [
            0.0287,
            0.1035,
            -0.0349,
            -0.0716,
            0.0353,
            -0.0077,
            0.0865,
            0.0978,
            -0.0608,
            -0.0605,
            0.0192,
            -0.009,
            0.1,
            0.0329,
            -0.0528,
            0.2191
          ]
        },
        "position_0": {
          "grad": [
            0.0113,
            -0.1541,
            0.1352,
            -0.0695,
            0.0093,
            0.1517,
            -0.1296,
            0.0432,
            0.0119,
            0.0244,
            -0.0083,
            0.0662,
            -0.0628,
            0.1247,
            0.0759,
            0.0014
          ],
          "after": [
            0.0415,
            -0.1456,
            -0.2961,
            0.0554,
            0.0319,
            -0.0504,
            -0.1463,
            0.0705,
            0.1088,
            0.0049,
            0.0126,
            0.0862,
            0.0403,
            0.0185,
            -0.1277,
            0.0564
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0008,
            0.0003,
            -0.0003,
            0.0002,
            -0.0004,
            -0.001,
            -0.0002,
            0.0007,
            -0.0008,
            -0.0003,
            0.0006,
            -0.001,
            0.0003,
            -0.0001,
            -0.0006
          ],
          "after": [
            -0.0814,
            0.0128,
            -0.1173,
            -0.0702,
            0.0606,
            0.1304,
            0.0808,
            -0.1664,
            0.0314,
            -0.0274,
            -0.1385,
            -0.0686,
            0.0906,
            -0.0821,
            0.0403,
            0.0806
          ]
        }
      }
    },
    {
      "step": 828,
      "word": "nusaibah",
      "loss": 2.7403,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2254,
            -0.2087,
            0.3253,
            0.019,
            0.2892,
            0.2505,
            -0.1166,
            0.173,
            0.124,
            0.0133,
            -0.0528,
            -0.0031,
            -0.0168,
            -0.0655,
            -0.1717,
            0.0425
          ],
          "after": [
            0.0111,
            -0.0041,
            0.1486,
            -0.026,
            0.0267,
            -0.0934,
            -0.1181,
            -0.1629,
            -0.0094,
            0.0733,
            0.1615,
            0.0094,
            -0.1018,
            -0.0148,
            -0.0338,
            -0.1855
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.05,
            0.0247,
            -0.0598,
            0.0154,
            0.0865,
            -0.143,
            0.1456,
            0.0237,
            -0.1284,
            0.0402,
            -0.0042,
            0.0689,
            0.023,
            -0.0641,
            -0.1494,
            0.1085
          ],
          "after": [
            0.0286,
            0.1035,
            -0.0349,
            -0.0715,
            0.0354,
            -0.0076,
            0.0865,
            0.0981,
            -0.0607,
            -0.0604,
            0.0191,
            -0.0088,
            0.0999,
            0.0327,
            -0.0527,
            0.2191
          ]
        },
        "position_0": {
          "grad": [
            -0.0207,
            0.1271,
            -0.0703,
            0.0449,
            -0.2078,
            0.0139,
            0.0761,
            -0.0183,
            -0.0002,
            -0.1362,
            -0.1644,
            -0.0129,
            -0.0042,
            0.0667,
            0.1318,
            0.1406
          ],
          "after": [
            0.0414,
            -0.1454,
            -0.2963,
            0.0554,
            0.032,
            -0.0507,
            -0.1462,
            0.0706,
            0.1088,
            0.005,
            0.0126,
            0.0861,
            0.0404,
            0.0184,
            -0.1278,
            0.0565
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0005,
            -0.0012,
            0.0002,
            -0.0012,
            -0.0004,
            0.0002,
            -0.0004,
            0.0001,
            0.0005,
            0.0011,
            0.0011,
            -0.0015,
            -0.0007,
            0.0005,
            -0.0
          ],
          "after": [
            -0.0814,
            0.0126,
            -0.1171,
            -0.0701,
            0.0605,
            0.1305,
            0.0807,
            -0.1663,
            0.0313,
            -0.0274,
            -0.1385,
            -0.0686,
            0.0907,
            -0.0821,
            0.0404,
            0.0807
          ]
        }
      }
    },
    {
      "step": 829,
      "word": "jaela",
      "loss": 1.7995,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3535,
            -0.2751,
            -0.1104,
            0.0024,
            -0.1682,
            0.084,
            -0.3181,
            0.0065,
            -0.058,
            0.3472,
            0.1965,
            -0.4375,
            0.0232,
            0.1701,
            0.0489,
            -0.5786
          ],
          "after": [
            0.0112,
            -0.004,
            0.1485,
            -0.026,
            0.0267,
            -0.0936,
            -0.1179,
            -0.1632,
            -0.0094,
            0.0731,
            0.1617,
            0.0094,
            -0.1018,
            -0.0149,
            -0.0337,
            -0.1852
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2828,
            0.1953,
            -0.162,
            0.1618,
            -0.0475,
            0.4438,
            0.3627,
            0.1997,
            0.0251,
            -0.2219,
            -0.2127,
            0.2789,
            -0.0362,
            -0.1513,
            0.3084,
            0.3882
          ],
          "after": [
            0.0286,
            0.1034,
            -0.0349,
            -0.0716,
            0.0356,
            -0.0077,
            0.0864,
            0.0981,
            -0.0606,
            -0.0602,
            0.0192,
            -0.0087,
            0.0998,
            0.0327,
            -0.0528,
            0.2189
          ]
        },
        "position_0": {
          "grad": [
            0.0181,
            0.0587,
            -0.0224,
            0.1287,
            -0.0237,
            -0.1467,
            0.1945,
            0.0891,
            -0.2219,
            -0.0954,
            -0.0272,
            -0.0245,
            0.331,
            -0.1214,
            -0.0947,
            0.1548
          ],
          "after": [
            0.0413,
            -0.1453,
            -0.2966,
            0.0554,
            0.0321,
            -0.0508,
            -0.1461,
            0.0706,
            0.1089,
            0.0051,
            0.0126,
            0.086,
            0.0403,
            0.0185,
            -0.1278,
            0.0564
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0006,
            -0.0021,
            0.0003,
            -0.0032,
            0.0002,
            0.0014,
            0.0005,
            0.0011,
            0.0006,
            -0.0003,
            -0.0025,
            0.0025,
            0.0014,
            0.0004,
            0.0027
          ],
          "after": [
            -0.0814,
            0.0125,
            -0.1169,
            -0.07,
            0.0605,
            0.1306,
            0.0806,
            -0.1662,
            0.0311,
            -0.0275,
            -0.1385,
            -0.0685,
            0.0907,
            -0.0821,
            0.0404,
            0.0807
          ]
        }
      }
    },
    {
      "step": 830,
      "word": "ryah",
      "loss": 2.637,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1045,
            -0.1653,
            0.1231,
            0.1069,
            -0.1466,
            0.1344,
            -0.1207,
            0.1989,
            0.2031,
            0.1792,
            -0.0993,
            -0.3949,
            0.375,
            0.2574,
            0.0178,
            0.0815
          ],
          "after": [
            0.0112,
            -0.0039,
            0.1484,
            -0.0261,
            0.0267,
            -0.0938,
            -0.1176,
            -0.1635,
            -0.0095,
            0.0729,
            0.1618,
            0.0096,
            -0.102,
            -0.0151,
            -0.0336,
            -0.185
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0414,
            0.0772,
            -0.1844,
            0.0384,
            -0.0441,
            -0.1487,
            0.2755,
            0.0117,
            -0.1902,
            0.0664,
            -0.0466,
            -0.0448,
            0.1326,
            -0.1086,
            -0.1288,
            0.1445
          ],
          "after": [
            0.0287,
            0.1032,
            -0.0348,
            -0.0718,
            0.0357,
            -0.0077,
            0.0863,
            0.0981,
            -0.0604,
            -0.0601,
            0.0193,
            -0.0086,
            0.0996,
            0.0327,
            -0.0528,
            0.2187
          ]
        },
        "position_0": {
          "grad": [
            0.0179,
            -0.0606,
            0.0107,
            -0.0704,
            -0.0913,
            0.1807,
            0.0838,
            -0.0748,
            0.1911,
            -0.1337,
            -0.1579,
            -0.0174,
            -0.2495,
            -0.1011,
            0.2264,
            0.1189
          ],
          "after": [
            0.0412,
            -0.1451,
            -0.2967,
            0.0554,
            0.0322,
            -0.051,
            -0.1461,
            0.0706,
            0.1088,
            0.0054,
            0.0127,
            0.0859,
            0.0404,
            0.0187,
            -0.128,
            0.0563
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0011,
            -0.0003,
            0.0004,
            -0.0009,
            0.0013,
            -0.001,
            0.0007,
            0.0004,
            -0.0029,
            -0.0008,
            0.0011,
            -0.0013,
            -0.0001,
            0.0019,
            0.0003
          ],
          "after": [
            -0.0814,
            0.0123,
            -0.1167,
            -0.07,
            0.0606,
            0.1306,
            0.0806,
            -0.1662,
            0.031,
            -0.0274,
            -0.1384,
            -0.0685,
            0.0907,
            -0.0822,
            0.0404,
            0.0806
          ]
        }
      }
    },
    {
      "step": 831,
      "word": "zyana",
      "loss": 2.0933,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5314,
            0.0859,
            -0.4401,
            -0.0169,
            -0.4352,
            -0.1554,
            -0.1594,
            -0.1008,
            -0.0836,
            0.1731,
            -0.0048,
            -0.4794,
            0.2261,
            0.3877,
            0.0598,
            -0.1843
          ],
          "after": [
            0.011,
            -0.0039,
            0.1485,
            -0.0262,
            0.0269,
            -0.0939,
            -0.1173,
            -0.1638,
            -0.0096,
            0.0726,
            0.162,
            0.0099,
            -0.1023,
            -0.0155,
            -0.0336,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0234,
            0.0013,
            -0.1377,
            0.0126,
            0.0004,
            -0.1228,
            0.1559,
            0.0188,
            -0.1627,
            0.0721,
            -0.0267,
            -0.0354,
            0.1136,
            -0.0688,
            -0.1211,
            0.0768
          ],
          "after": [
            0.0288,
            0.1031,
            -0.0346,
            -0.0719,
            0.0359,
            -0.0077,
            0.0862,
            0.0981,
            -0.0603,
            -0.06,
            0.0195,
            -0.0086,
            0.0994,
            0.0327,
            -0.0527,
            0.2185
          ]
        },
        "position_0": {
          "grad": [
            0.0288,
            0.0367,
            -0.0391,
            0.119,
            0.0442,
            -0.0775,
            0.1926,
            0.0169,
            0.0492,
            0.0021,
            0.0202,
            0.0117,
            -0.0074,
            -0.068,
            0.0538,
            0.0401
          ],
          "after": [
            0.041,
            -0.145,
            -0.2969,
            0.0554,
            0.0323,
            -0.0512,
            -0.1462,
            0.0706,
            0.1088,
            0.0056,
            0.0127,
            0.0859,
            0.0405,
            0.0189,
            -0.1283,
            0.0562
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0018,
            -0.0004,
            0.0012,
            -0.0012,
            0.001,
            -0.0014,
            -0.0007,
            -0.0004,
            -0.0018,
            0.0,
            0.0005,
            -0.0011,
            0.001,
            0.0002,
            -0.0002
          ],
          "after": [
            -0.0815,
            0.012,
            -0.1165,
            -0.07,
            0.0607,
            0.1306,
            0.0806,
            -0.1662,
            0.0308,
            -0.0272,
            -0.1383,
            -0.0686,
            0.0908,
            -0.0823,
            0.0403,
            0.0806
          ]
        }
      }
    },
    {
      "step": 832,
      "word": "joyanna",
      "loss": 2.1274,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.094,
            0.1348,
            -0.2039,
            -0.05,
            -0.1517,
            -0.1704,
            0.0468,
            -0.1128,
            -0.0186,
            0.0227,
            0.0264,
            -0.0558,
            -0.0114,
            0.0548,
            0.058,
            -0.0347
          ],
          "after": [
            0.0107,
            -0.0039,
            0.1486,
            -0.0263,
            0.0271,
            -0.0939,
            -0.117,
            -0.1639,
            -0.0096,
            0.0724,
            0.1621,
            0.0103,
            -0.1026,
            -0.0159,
            -0.0336,
            -0.1845
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0533,
            0.0032,
            -0.0755,
            0.0133,
            0.0468,
            -0.093,
            0.1499,
            0.0365,
            -0.1365,
            0.0465,
            -0.0279,
            0.0228,
            0.0556,
            -0.0639,
            -0.1005,
            0.0837
          ],
          "after": [
            0.0289,
            0.103,
            -0.0345,
            -0.072,
            0.036,
            -0.0076,
            0.086,
            0.098,
            -0.0601,
            -0.06,
            0.0196,
            -0.0085,
            0.0992,
            0.0328,
            -0.0527,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            0.0003,
            0.0393,
            -0.0162,
            0.1116,
            -0.0135,
            -0.0898,
            0.1549,
            0.0647,
            -0.1705,
            -0.0673,
            -0.0029,
            -0.0094,
            0.2381,
            -0.1012,
            -0.067,
            0.12
          ],
          "after": [
            0.0409,
            -0.145,
            -0.2969,
            0.0552,
            0.0324,
            -0.0512,
            -0.1464,
            0.0706,
            0.1088,
            0.0058,
            0.0128,
            0.0858,
            0.0404,
            0.0192,
            -0.1284,
            0.056
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0008,
            -0.0003,
            0.0005,
            0.0009,
            -0.0003,
            -0.0001,
            -0.0005,
            -0.0001,
            -0.0003,
            0.0001,
            0.0009,
            -0.0012,
            -0.0009,
            0.0006,
            -0.0001
          ],
          "after": [
            -0.0815,
            0.0117,
            -0.1163,
            -0.0701,
            0.0607,
            0.1305,
            0.0807,
            -0.1661,
            0.0308,
            -0.0271,
            -0.1383,
            -0.0686,
            0.091,
            -0.0823,
            0.0402,
            0.0806
          ]
        }
      }
    },
    {
      "step": 833,
      "word": "darie",
      "loss": 1.8382,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1393,
            -0.1277,
            -0.1355,
            -0.121,
            -0.0016,
            0.0119,
            -0.0246,
            -0.1959,
            0.2195,
            -0.0986,
            0.0183,
            -0.0049,
            -0.1705,
            -0.0683,
            0.0962,
            0.0767
          ],
          "after": [
            0.0105,
            -0.0038,
            0.1488,
            -0.0262,
            0.0274,
            -0.0939,
            -0.1168,
            -0.1639,
            -0.0097,
            0.0722,
            0.1621,
            0.0105,
            -0.1027,
            -0.0161,
            -0.0337,
            -0.1843
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1462,
            0.0469,
            -0.0495,
            0.2303,
            -0.5141,
            -0.0233,
            -0.1903,
            0.0089,
            -0.0445,
            -0.2019,
            -0.0726,
            0.0518,
            0.0931,
            0.0778,
            0.3057,
            -0.0137
          ],
          "after": [
            0.0289,
            0.1029,
            -0.0344,
            -0.0723,
            0.0362,
            -0.0075,
            0.0859,
            0.0979,
            -0.0599,
            -0.0599,
            0.0198,
            -0.0085,
            0.099,
            0.0328,
            -0.0527,
            0.2182
          ]
        },
        "position_0": {
          "grad": [
            -0.0627,
            0.0499,
            -0.0411,
            -0.1199,
            0.0603,
            -0.1006,
            0.0355,
            -0.0719,
            0.0071,
            -0.0703,
            -0.1002,
            -0.0399,
            0.0169,
            0.0524,
            0.0836,
            0.1267
          ],
          "after": [
            0.0409,
            -0.1449,
            -0.297,
            0.0552,
            0.0324,
            -0.0512,
            -0.1465,
            0.0706,
            0.1089,
            0.0061,
            0.0129,
            0.0858,
            0.0403,
            0.0194,
            -0.1286,
            0.0558
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0015,
            0.0026,
            0.0016,
            -0.0005,
            -0.0013,
            -0.0042,
            0.0013,
            -0.0007,
            -0.0023,
            0.0005,
            0.0007,
            -0.0007,
            -0.0006,
            0.0005,
            -0.0002
          ],
          "after": [
            -0.0815,
            0.0116,
            -0.1163,
            -0.0702,
            0.0607,
            0.1306,
            0.0808,
            -0.1661,
            0.0307,
            -0.0269,
            -0.1383,
            -0.0687,
            0.0911,
            -0.0823,
            0.0402,
            0.0806
          ]
        }
      }
    },
    {
      "step": 834,
      "word": "mollie",
      "loss": 2.1829,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0102,
            -0.0038,
            0.149,
            -0.0262,
            0.0275,
            -0.0938,
            -0.1166,
            -0.1638,
            -0.0099,
            0.072,
            0.1622,
            0.0107,
            -0.1028,
            -0.0164,
            -0.0338,
            -0.1842
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0813,
            0.1638,
            -0.1904,
            0.1168,
            -0.6479,
            -0.072,
            -0.0034,
            -0.0442,
            0.0142,
            -0.0597,
            0.0429,
            -0.0629,
            0.1922,
            0.0102,
            0.3661,
            0.0718
          ],
          "after": [
            0.0288,
            0.1028,
            -0.0342,
            -0.0726,
            0.0367,
            -0.0075,
            0.0859,
            0.0979,
            -0.0597,
            -0.0598,
            0.0199,
            -0.0084,
            0.0987,
            0.0328,
            -0.0528,
            0.218
          ]
        },
        "position_0": {
          "grad": [
            -0.0207,
            0.0267,
            -0.0357,
            0.0231,
            0.1245,
            -0.008,
            0.2214,
            0.0693,
            0.0172,
            -0.0001,
            0.0083,
            0.0266,
            -0.0433,
            -0.0325,
            0.0022,
            0.1349
          ],
          "after": [
            0.0409,
            -0.145,
            -0.297,
            0.0552,
            0.0323,
            -0.0511,
            -0.1467,
            0.0706,
            0.1089,
            0.0063,
            0.013,
            0.0858,
            0.0403,
            0.0196,
            -0.1287,
            0.0556
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            0.0017,
            0.0003,
            0.0039,
            -0.0064,
            -0.0017,
            -0.0003,
            -0.0002,
            -0.0002,
            0.0006,
            0.0018,
            0.0,
            0.0023,
            0.0001,
            0.0002,
            0.0027
          ],
          "after": [
            -0.0816,
            0.0113,
            -0.1163,
            -0.0704,
            0.061,
            0.1307,
            0.081,
            -0.1661,
            0.0307,
            -0.0267,
            -0.1384,
            -0.0688,
            0.0911,
            -0.0823,
            0.0401,
            0.0804
          ]
        }
      }
    },
    {
      "step": 835,
      "word": "maxi",
      "loss": 2.4993,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.042,
            -0.1458,
            -0.1711,
            0.561,
            0.0293,
            -0.3034,
            0.0755,
            0.0963,
            -0.509,
            0.024,
            0.0656,
            -0.1274,
            0.4936,
            -0.3413,
            -0.1295,
            0.2464
          ],
          "after": [
            0.01,
            -0.0037,
            0.1492,
            -0.0265,
            0.0277,
            -0.0936,
            -0.1165,
            -0.1639,
            -0.0097,
            0.0719,
            0.1622,
            0.011,
            -0.1031,
            -0.0164,
            -0.0338,
            -0.1842
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0373,
            0.0701,
            -0.1781,
            0.0269,
            0.0101,
            -0.1438,
            0.1814,
            -0.0043,
            -0.1937,
            0.091,
            -0.0518,
            -0.0692,
            0.1386,
            -0.039,
            -0.1726,
            0.108
          ],
          "after": [
            0.0288,
            0.1026,
            -0.0339,
            -0.0729,
            0.0371,
            -0.0073,
            0.0858,
            0.0979,
            -0.0595,
            -0.0597,
            0.0201,
            -0.0084,
            0.0984,
            0.0329,
            -0.0529,
            0.2178
          ]
        },
        "position_0": {
          "grad": [
            -0.01,
            0.0457,
            -0.0463,
            0.0806,
            0.0896,
            -0.0496,
            0.2748,
            0.099,
            0.0447,
            -0.0405,
            -0.0201,
            -0.0226,
            -0.0734,
            0.0276,
            0.0692,
            0.1881
          ],
          "after": [
            0.041,
            -0.145,
            -0.2969,
            0.0551,
            0.0322,
            -0.051,
            -0.147,
            0.0704,
            0.1089,
            0.0066,
            0.0131,
            0.0858,
            0.0403,
            0.0198,
            -0.1289,
            0.0553
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.001,
            0.0004,
            -0.0032,
            0.0027,
            0.0006,
            0.0011,
            0.0002,
            0.0007,
            -0.001,
            0.0011,
            -0.0008,
            -0.0008,
            -0.0005,
            -0.0001,
            -0.0009
          ],
          "after": [
            -0.0816,
            0.0112,
            -0.1163,
            -0.0705,
            0.0611,
            0.1307,
            0.0811,
            -0.1662,
            0.0307,
            -0.0265,
            -0.1385,
            -0.0688,
            0.0912,
            -0.0822,
            0.0401,
            0.0803
          ]
        }
      }
    },
    {
      "step": 836,
      "word": "jiovanny",
      "loss": 2.5032,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0842,
            0.1574,
            -0.0298,
            0.0416,
            -0.0151,
            -0.0556,
            0.043,
            -0.0799,
            0.0262,
            -0.1549,
            -0.0721,
            0.1278,
            -0.077,
            -0.0521,
            -0.0071,
            0.0973
          ],
          "after": [
            0.0099,
            -0.0037,
            0.1494,
            -0.0268,
            0.0278,
            -0.0934,
            -0.1164,
            -0.1638,
            -0.0095,
            0.0719,
            0.1623,
            0.0111,
            -0.1033,
            -0.0164,
            -0.0337,
            -0.1842
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0791,
            0.0401,
            -0.0857,
            0.012,
            0.0549,
            -0.123,
            0.208,
            0.0351,
            -0.1592,
            0.027,
            -0.0463,
            0.0198,
            0.0762,
            -0.0696,
            -0.1226,
            0.1148
          ],
          "after": [
            0.0289,
            0.1024,
            -0.0337,
            -0.0732,
            0.0375,
            -0.0072,
            0.0856,
            0.0979,
            -0.0593,
            -0.0596,
            0.0202,
            -0.0083,
            0.0982,
            0.0329,
            -0.0529,
            0.2176
          ]
        },
        "position_0": {
          "grad": [
            -0.0055,
            0.0379,
            -0.0138,
            0.1207,
            0.0088,
            -0.0778,
            0.1445,
            0.0447,
            -0.166,
            -0.0322,
            0.012,
            0.0072,
            0.2204,
            -0.0835,
            -0.0853,
            0.1124
          ],
          "after": [
            0.041,
            -0.1451,
            -0.2968,
            0.0549,
            0.032,
            -0.0508,
            -0.1473,
            0.0703,
            0.109,
            0.0068,
            0.0132,
            0.0858,
            0.0402,
            0.02,
            -0.129,
            0.055
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0002,
            0.0019,
            0.0011,
            -0.0001,
            -0.0001,
            -0.002,
            0.0,
            0.0005,
            0.0009,
            0.0018,
            0.0026,
            -0.0019,
            -0.0017,
            -0.0005,
            -0.001
          ],
          "after": [
            -0.0815,
            0.0111,
            -0.1164,
            -0.0707,
            0.0613,
            0.1308,
            0.0812,
            -0.1662,
            0.0306,
            -0.0264,
            -0.1388,
            -0.0689,
            0.0913,
            -0.0821,
            0.04,
            0.0803
          ]
        }
      }
    },
    {
      "step": 837,
      "word": "gea",
      "loss": 3.3519,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.6413,
            -0.2317,
            -0.6429,
            -0.0523,
            -0.5765,
            0.0564,
            -0.2145,
            0.144,
            -0.1522,
            0.1299,
            -0.1747,
            -0.9198,
            0.538,
            0.6257,
            0.444,
            -0.1916
          ],
          "after": [
            0.0095,
            -0.0035,
            0.1498,
            -0.027,
            0.0281,
            -0.0932,
            -0.1162,
            -0.1639,
            -0.0093,
            0.0718,
            0.1624,
            0.0116,
            -0.1038,
            -0.0167,
            -0.034,
            -0.1841
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1312,
            -0.4984,
            0.8641,
            0.1533,
            0.4607,
            0.1621,
            -0.5496,
            0.019,
            0.8699,
            -0.0202,
            0.049,
            0.2149,
            -0.4792,
            0.283,
            -0.1131,
            -0.4879
          ],
          "after": [
            0.0288,
            0.1025,
            -0.0338,
            -0.0736,
            0.0376,
            -0.0071,
            0.0857,
            0.0978,
            -0.0594,
            -0.0596,
            0.0203,
            -0.0084,
            0.0981,
            0.0328,
            -0.0529,
            0.2176
          ]
        },
        "position_0": {
          "grad": [
            0.0538,
            0.0353,
            0.0161,
            0.1826,
            -0.2958,
            -0.2808,
            -0.0832,
            0.0341,
            -0.1093,
            -0.0021,
            0.0802,
            -0.3451,
            0.0502,
            0.1552,
            -0.0126,
            0.035
          ],
          "after": [
            0.041,
            -0.1452,
            -0.2968,
            0.0546,
            0.0322,
            -0.0505,
            -0.1475,
            0.0701,
            0.1091,
            0.007,
            0.0132,
            0.0861,
            0.04,
            0.02,
            -0.1291,
            0.0547
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.004,
            -0.0034,
            0.0013,
            -0.0052,
            0.0012,
            -0.005,
            -0.0012,
            0.0067,
            0.0028,
            -0.0094,
            -0.0046,
            -0.0084,
            0.0079,
            -0.0007,
            0.0015,
            0.0042
          ],
          "after": [
            -0.0813,
            0.0112,
            -0.1166,
            -0.0705,
            0.0613,
            0.1311,
            0.0814,
            -0.1666,
            0.0303,
            -0.026,
            -0.1387,
            -0.0687,
            0.0911,
            -0.082,
            0.04,
            0.0801
          ]
        }
      }
    },
    {
      "step": 838,
      "word": "ariadne",
      "loss": 2.3197,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1491,
            -0.0916,
            0.0634,
            -0.1195,
            0.1675,
            0.1362,
            0.1085,
            -0.2538,
            0.173,
            -0.1883,
            0.0018,
            0.1547,
            -0.2604,
            -0.253,
            0.0694,
            0.2399
          ],
          "after": [
            0.0093,
            -0.0034,
            0.1502,
            -0.0271,
            0.0283,
            -0.0932,
            -0.1161,
            -0.1638,
            -0.0092,
            0.0718,
            0.1626,
            0.0119,
            -0.104,
            -0.0168,
            -0.0343,
            -0.1842
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.4153,
            0.04,
            -0.1789,
            -0.035,
            -0.448,
            0.098,
            -0.314,
            -0.1488,
            -0.033,
            -0.0664,
            0.0222,
            -0.4813,
            0.1934,
            0.313,
            0.1522,
            -0.2222
          ],
          "after": [
            0.0286,
            0.1026,
            -0.0339,
            -0.0739,
            0.0378,
            -0.0071,
            0.0857,
            0.0979,
            -0.0595,
            -0.0595,
            0.0204,
            -0.0082,
            0.098,
            0.0326,
            -0.053,
            0.2177
          ]
        },
        "position_0": {
          "grad": [
            0.0713,
            -0.0449,
            0.067,
            -0.1653,
            0.0602,
            0.0071,
            -0.2711,
            -0.1007,
            0.1254,
            0.077,
            0.1966,
            0.037,
            -0.1529,
            -0.0206,
            -0.1054,
            -0.2196
          ],
          "after": [
            0.0408,
            -0.1452,
            -0.2968,
            0.0545,
            0.0322,
            -0.0502,
            -0.1476,
            0.07,
            0.1092,
            0.0071,
            0.0131,
            0.0863,
            0.04,
            0.02,
            -0.129,
            0.0546
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0021,
            0.0011,
            0.0006,
            0.0026,
            0.0002,
            -0.0007,
            -0.0006,
            -0.001,
            -0.0033,
            -0.0016,
            0.001,
            -0.0017,
            -0.0001,
            -0.0001,
            -0.0,
            -0.0019
          ],
          "after": [
            -0.0812,
            0.0112,
            -0.1168,
            -0.0705,
            0.0613,
            0.1313,
            0.0815,
            -0.1669,
            0.0303,
            -0.0255,
            -0.1387,
            -0.0684,
            0.0909,
            -0.0819,
            0.0399,
            0.08
          ]
        }
      }
    },
    {
      "step": 839,
      "word": "graylon",
      "loss": 2.4729,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0188,
            -0.1875,
            0.0775,
            -0.1047,
            0.1037,
            0.1408,
            -0.0592,
            0.0806,
            -0.0376,
            0.0785,
            0.0712,
            0.127,
            -0.0242,
            -0.026,
            -0.0196,
            0.0181
          ],
          "after": [
            0.0091,
            -0.0032,
            0.1504,
            -0.0271,
            0.0285,
            -0.0933,
            -0.116,
            -0.1638,
            -0.0091,
            0.0718,
            0.1626,
            0.0121,
            -0.1042,
            -0.0169,
            -0.0345,
            -0.1842
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0574,
            0.1247,
            -0.1503,
            0.0223,
            0.0034,
            -0.1777,
            0.2572,
            -0.0078,
            -0.2206,
            0.0268,
            -0.01,
            -0.0076,
            0.1145,
            -0.1216,
            -0.151,
            0.1696
          ],
          "after": [
            0.0284,
            0.1026,
            -0.0338,
            -0.0741,
            0.038,
            -0.007,
            0.0857,
            0.098,
            -0.0595,
            -0.0595,
            0.0205,
            -0.0081,
            0.0979,
            0.0324,
            -0.0529,
            0.2178
          ]
        },
        "position_0": {
          "grad": [
            0.0111,
            0.0453,
            -0.0116,
            0.1146,
            -0.0742,
            -0.0991,
            0.0719,
            0.0186,
            -0.1226,
            0.0587,
            0.0687,
            -0.0903,
            0.0716,
            0.0091,
            -0.059,
            0.0641
          ],
          "after": [
            0.0407,
            -0.1453,
            -0.2969,
            0.0544,
            0.0323,
            -0.0498,
            -0.1477,
            0.07,
            0.1093,
            0.0071,
            0.0129,
            0.0865,
            0.04,
            0.02,
            -0.129,
            0.0545
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.0008,
            0.0007,
            0.0007,
            0.0006,
            -0.0011,
            0.0032,
            0.0002,
            -0.0004,
            -0.0003,
            0.0003,
            0.0002,
            0.0016,
            -0.0021,
            -0.0014,
            0.0014
          ],
          "after": [
            -0.0811,
            0.0112,
            -0.117,
            -0.0706,
            0.0614,
            0.1316,
            0.0815,
            -0.1672,
            0.0304,
            -0.0252,
            -0.1387,
            -0.0682,
            0.0906,
            -0.0816,
            0.0399,
            0.0799
          ]
        }
      }
    },
    {
      "step": 840,
      "word": "devarsh",
      "loss": 2.5914,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0594,
            -0.0513,
            0.0132,
            0.0275,
            0.1422,
            0.1417,
            0.1517,
            -0.1074,
            0.1241,
            -0.0594,
            -0.0188,
            0.2198,
            -0.1676,
            -0.183,
            0.0358,
            0.1813
          ],
          "after": [
            0.0089,
            -0.003,
            0.1506,
            -0.0272,
            0.0285,
            -0.0934,
            -0.116,
            -0.1637,
            -0.0091,
            0.0718,
            0.1627,
            0.0122,
            -0.1043,
            -0.0169,
            -0.0347,
            -0.1843
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0234,
            -0.2577,
            0.2742,
            -0.1318,
            0.1912,
            0.1393,
            -0.3438,
            -0.0512,
            0.4159,
            0.1314,
            0.1184,
            0.2289,
            -0.2677,
            0.0306,
            0.0277,
            -0.1087
          ],
          "after": [
            0.0283,
            0.1027,
            -0.0339,
            -0.0742,
            0.0381,
            -0.007,
            0.0858,
            0.0982,
            -0.0596,
            -0.0595,
            0.0204,
            -0.008,
            0.0979,
            0.0323,
            -0.0529,
            0.2178
          ]
        },
        "position_0": {
          "grad": [
            -0.0567,
            0.0517,
            -0.0341,
            -0.0439,
            0.0675,
            -0.0565,
            0.074,
            -0.0507,
            -0.0213,
            -0.0417,
            -0.0594,
            0.0072,
            0.0325,
            0.039,
            0.0536,
            0.1295
          ],
          "after": [
            0.0407,
            -0.1454,
            -0.2968,
            0.0542,
            0.0324,
            -0.0495,
            -0.1478,
            0.07,
            0.1094,
            0.0072,
            0.0128,
            0.0867,
            0.0399,
            0.02,
            -0.1289,
            0.0543
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            0.0012,
            0.001,
            -0.0,
            0.0018,
            -0.0009,
            0.003,
            0.0001,
            0.0003,
            -0.0019,
            -0.0005,
            0.0043,
            -0.0011,
            -0.0037,
            -0.0019,
            0.0013
          ],
          "after": [
            -0.0809,
            0.0111,
            -0.1172,
            -0.0706,
            0.0613,
            0.1319,
            0.0815,
            -0.1674,
            0.0304,
            -0.0248,
            -0.1387,
            -0.0682,
            0.0905,
            -0.0813,
            0.04,
            0.0797
          ]
        }
      }
    },
    {
      "step": 841,
      "word": "seynabou",
      "loss": 2.9094,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.209,
            -0.002,
            0.2518,
            0.0261,
            0.049,
            0.099,
            0.1487,
            0.0302,
            -0.0385,
            -0.0857,
            -0.0419,
            0.1644,
            0.1128,
            -0.2051,
            -0.0997,
            0.4312
          ],
          "after": [
            0.0088,
            -0.0028,
            0.1507,
            -0.0272,
            0.0285,
            -0.0936,
            -0.1161,
            -0.1636,
            -0.0091,
            0.0719,
            0.1628,
            0.0123,
            -0.1044,
            -0.0167,
            -0.0348,
            -0.1846
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0231,
            -0.0061,
            0.3744,
            0.0674,
            0.2185,
            0.0253,
            -0.264,
            -0.0012,
            0.28,
            0.1956,
            0.0497,
            0.0453,
            0.0094,
            0.1236,
            -0.2171,
            -0.1014
          ],
          "after": [
            0.0282,
            0.1027,
            -0.0341,
            -0.0744,
            0.0381,
            -0.007,
            0.086,
            0.0983,
            -0.0598,
            -0.0596,
            0.0203,
            -0.008,
            0.0979,
            0.0321,
            -0.0528,
            0.2179
          ]
        },
        "position_0": {
          "grad": [
            -0.0152,
            0.0674,
            -0.0224,
            0.0908,
            -0.0873,
            0.0419,
            0.0148,
            0.0233,
            -0.0619,
            0.0135,
            -0.0779,
            0.0586,
            0.0539,
            0.0092,
            0.073,
            0.028
          ],
          "after": [
            0.0407,
            -0.1455,
            -0.2968,
            0.0541,
            0.0324,
            -0.0493,
            -0.1479,
            0.0699,
            0.1096,
            0.0072,
            0.0128,
            0.0868,
            0.0398,
            0.02,
            -0.129,
            0.0541
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0023,
            -0.0002,
            0.0022,
            0.0004,
            -0.0005,
            0.0002,
            0.0015,
            0.001,
            -0.0013,
            0.0002,
            0.0003,
            0.0023,
            0.0011,
            0.0003,
            -0.0013,
            0.0
          ],
          "after": [
            -0.0807,
            0.011,
            -0.1175,
            -0.0706,
            0.0613,
            0.1321,
            0.0813,
            -0.1676,
            0.0304,
            -0.0245,
            -0.1387,
            -0.0683,
            0.0903,
            -0.081,
            0.0401,
            0.0796
          ]
        }
      }
    },
    {
      "step": 842,
      "word": "ilyes",
      "loss": 2.448,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0088,
            -0.0026,
            0.1508,
            -0.0272,
            0.0286,
            -0.0937,
            -0.1161,
            -0.1635,
            -0.0091,
            0.0719,
            0.1629,
            0.0123,
            -0.1045,
            -0.0166,
            -0.0349,
            -0.1848
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0901,
            0.2329,
            -0.0671,
            -0.1617,
            0.031,
            -0.1432,
            -0.09,
            -0.0237,
            0.0808,
            -0.4943,
            -0.0119,
            0.0127,
            -0.1058,
            0.0066,
            0.2823,
            -0.0929
          ],
          "after": [
            0.0281,
            0.1027,
            -0.0342,
            -0.0744,
            0.0381,
            -0.0069,
            0.0861,
            0.0984,
            -0.06,
            -0.0595,
            0.0203,
            -0.008,
            0.098,
            0.032,
            -0.0529,
            0.218
          ]
        },
        "position_0": {
          "grad": [
            -0.0823,
            -0.2646,
            0.1072,
            -0.1633,
            0.1425,
            0.1698,
            -0.2384,
            0.0383,
            0.0817,
            0.0152,
            0.0927,
            -0.1295,
            -0.1925,
            -0.0001,
            -0.0214,
            -0.1535
          ],
          "after": [
            0.0408,
            -0.1454,
            -0.2969,
            0.0541,
            0.0324,
            -0.0492,
            -0.1479,
            0.0699,
            0.1096,
            0.0072,
            0.0127,
            0.087,
            0.0398,
            0.0199,
            -0.129,
            0.0541
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0026,
            0.0001,
            -0.001,
            0.0002,
            -0.0037,
            0.0006,
            -0.0002,
            0.0006,
            -0.0035,
            0.0008,
            -0.0003,
            0.0004,
            -0.0013,
            0.0003,
            0.0001
          ],
          "after": [
            -0.0804,
            0.0109,
            -0.1178,
            -0.0706,
            0.0612,
            0.1324,
            0.0812,
            -0.1678,
            0.0305,
            -0.0241,
            -0.1387,
            -0.0683,
            0.0902,
            -0.0807,
            0.0402,
            0.0794
          ]
        }
      }
    },
    {
      "step": 843,
      "word": "jemina",
      "loss": 1.9369,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0986,
            -0.0138,
            -0.0232,
            -0.0934,
            -0.0264,
            -0.0497,
            -0.0272,
            -0.0167,
            -0.0554,
            0.1018,
            0.0952,
            -0.1038,
            0.0538,
            0.042,
            -0.0167,
            -0.1253
          ],
          "after": [
            0.0087,
            -0.0025,
            0.1508,
            -0.0272,
            0.0286,
            -0.0938,
            -0.1162,
            -0.1635,
            -0.009,
            0.0719,
            0.1629,
            0.0124,
            -0.1046,
            -0.0166,
            -0.0349,
            -0.185
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2519,
            -0.2674,
            0.3717,
            0.0149,
            0.2548,
            0.1602,
            -0.575,
            -0.1541,
            0.5309,
            0.2281,
            0.2703,
            0.1303,
            -0.3127,
            0.0453,
            -0.152,
            -0.2935
          ],
          "after": [
            0.0279,
            0.1028,
            -0.0345,
            -0.0744,
            0.038,
            -0.0069,
            0.0863,
            0.0987,
            -0.0603,
            -0.0595,
            0.02,
            -0.0081,
            0.0981,
            0.0318,
            -0.0528,
            0.2182
          ]
        },
        "position_0": {
          "grad": [
            0.0029,
            0.0447,
            -0.0181,
            0.1485,
            -0.0169,
            -0.1052,
            0.1871,
            0.0718,
            -0.1933,
            -0.0692,
            -0.0082,
            0.0059,
            0.2922,
            -0.0871,
            -0.0878,
            0.1488
          ],
          "after": [
            0.0408,
            -0.1454,
            -0.2969,
            0.0539,
            0.0324,
            -0.0491,
            -0.1479,
            0.0698,
            0.1098,
            0.0073,
            0.0127,
            0.0871,
            0.0397,
            0.02,
            -0.129,
            0.0539
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0004,
            0.0011,
            -0.0011,
            -0.0017,
            -0.0005,
            -0.0033,
            -0.0023,
            0.0007,
            -0.0004,
            0.0013,
            -0.0009,
            -0.0015,
            0.0012,
            0.0012,
            -0.0014
          ],
          "after": [
            -0.0802,
            0.0107,
            -0.1181,
            -0.0706,
            0.0613,
            0.1327,
            0.0812,
            -0.1679,
            0.0305,
            -0.0237,
            -0.1389,
            -0.0684,
            0.0901,
            -0.0805,
            0.0402,
            0.0794
          ]
        }
      }
    },
    {
      "step": 844,
      "word": "mak",
      "loss": 2.7545,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.331,
            0.1412,
            0.0383,
            0.0963,
            -0.0612,
            -0.1303,
            0.3774,
            0.095,
            -0.0392,
            -0.131,
            -0.1235,
            -0.0103,
            -0.0453,
            0.0113,
            -0.1435,
            -0.1571
          ],
          "after": [
            0.0088,
            -0.0025,
            0.1509,
            -0.0273,
            0.0287,
            -0.0938,
            -0.1164,
            -0.1635,
            -0.009,
            0.072,
            0.1629,
            0.0124,
            -0.1047,
            -0.0165,
            -0.0349,
            -0.185
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0463,
            0.1627,
            -0.2393,
            0.0616,
            -0.0793,
            -0.2341,
            0.317,
            0.0474,
            -0.2882,
            0.0519,
            -0.1063,
            -0.0871,
            0.1919,
            -0.0186,
            -0.1647,
            0.2287
          ],
          "after": [
            0.0278,
            0.1028,
            -0.0346,
            -0.0744,
            0.038,
            -0.0069,
            0.0865,
            0.0989,
            -0.0604,
            -0.0595,
            0.0199,
            -0.0081,
            0.0982,
            0.0317,
            -0.0528,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            -0.0208,
            0.0447,
            -0.0785,
            0.1049,
            0.1005,
            -0.0687,
            0.376,
            0.1542,
            0.0619,
            -0.0307,
            -0.0468,
            0.0074,
            -0.1156,
            -0.0069,
            0.0962,
            0.2044
          ],
          "after": [
            0.0409,
            -0.1454,
            -0.2969,
            0.0538,
            0.0323,
            -0.0489,
            -0.1482,
            0.0695,
            0.1099,
            0.0074,
            0.0127,
            0.0873,
            0.0397,
            0.0201,
            -0.129,
            0.0537
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0028,
            0.0018,
            -0.0042,
            0.0005,
            -0.0017,
            -0.006,
            -0.0043,
            0.0013,
            -0.0023,
            0.0034,
            -0.0012,
            -0.0007,
            -0.0008,
            0.0014,
            -0.0085
          ],
          "after": [
            -0.08,
            0.0107,
            -0.1184,
            -0.0703,
            0.0613,
            0.133,
            0.0814,
            -0.1677,
            0.0304,
            -0.0233,
            -0.1391,
            -0.0683,
            0.0901,
            -0.0802,
            0.0401,
            0.0797
          ]
        }
      }
    },
    {
      "step": 845,
      "word": "jaicob",
      "loss": 2.6099,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0466,
            -0.2827,
            0.0613,
            -0.0699,
            0.1334,
            0.1492,
            -0.2041,
            0.0695,
            -0.0227,
            -0.0246,
            0.1673,
            -0.1684,
            -0.1309,
            0.0262,
            0.0006,
            -0.0653
          ],
          "after": [
            0.0089,
            -0.0023,
            0.1509,
            -0.0273,
            0.0287,
            -0.0939,
            -0.1165,
            -0.1635,
            -0.0089,
            0.0721,
            0.1629,
            0.0125,
            -0.1047,
            -0.0165,
            -0.0349,
            -0.1851
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0408,
            0.1166,
            -0.1591,
            0.0077,
            0.0337,
            -0.1784,
            0.2355,
            0.0372,
            -0.251,
            0.0216,
            -0.0219,
            -0.0142,
            0.1261,
            -0.0674,
            -0.1574,
            0.1783
          ],
          "after": [
            0.0277,
            0.1028,
            -0.0346,
            -0.0745,
            0.0379,
            -0.0067,
            0.0865,
            0.099,
            -0.0605,
            -0.0595,
            0.0198,
            -0.0081,
            0.0982,
            0.0316,
            -0.0527,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            0.0032,
            0.0531,
            -0.0143,
            0.1335,
            0.0122,
            -0.1062,
            0.1751,
            0.0552,
            -0.2115,
            -0.0593,
            0.0067,
            0.0062,
            0.3077,
            -0.0884,
            -0.1082,
            0.1601
          ],
          "after": [
            0.041,
            -0.1454,
            -0.2968,
            0.0535,
            0.0323,
            -0.0486,
            -0.1484,
            0.0692,
            0.1102,
            0.0075,
            0.0126,
            0.0874,
            0.0395,
            0.0202,
            -0.1289,
            0.0535
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0008,
            -0.0004,
            -0.0002,
            0.0004,
            -0.0011,
            0.0,
            -0.0011,
            0.0001,
            0.0005,
            -0.0006,
            -0.0007,
            0.0004,
            -0.0009,
            -0.0001,
            0.0018,
            -0.0001
          ],
          "after": [
            -0.0798,
            0.0107,
            -0.1187,
            -0.0702,
            0.0613,
            0.1333,
            0.0816,
            -0.1675,
            0.0302,
            -0.023,
            -0.1393,
            -0.0683,
            0.0901,
            -0.08,
            0.0401,
            0.08
          ]
        }
      }
    },
    {
      "step": 846,
      "word": "caelin",
      "loss": 1.8098,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0516,
            -0.1822,
            0.071,
            0.0742,
            -0.0573,
            0.1439,
            -0.1972,
            -0.0194,
            0.0958,
            0.0811,
            0.0855,
            -0.0236,
            -0.1656,
            -0.0391,
            0.016,
            -0.2608
          ],
          "after": [
            0.0089,
            -0.0021,
            0.1509,
            -0.0273,
            0.0287,
            -0.0941,
            -0.1164,
            -0.1636,
            -0.0089,
            0.0721,
            0.1628,
            0.0126,
            -0.1047,
            -0.0165,
            -0.0349,
            -0.185
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2237,
            0.154,
            -0.1416,
            0.1474,
            0.0199,
            0.3822,
            0.2991,
            0.1577,
            0.0045,
            -0.181,
            -0.1428,
            0.2425,
            -0.0571,
            -0.1554,
            0.2022,
            0.3545
          ],
          "after": [
            0.0278,
            0.1027,
            -0.0346,
            -0.0746,
            0.0379,
            -0.0068,
            0.0865,
            0.0989,
            -0.0606,
            -0.0594,
            0.0198,
            -0.0082,
            0.0982,
            0.0316,
            -0.0526,
            0.2182
          ]
        },
        "position_0": {
          "grad": [
            -0.0314,
            0.0207,
            -0.0304,
            0.0879,
            -0.0486,
            -0.0397,
            0.1034,
            0.069,
            -0.0577,
            0.0315,
            0.0101,
            0.0061,
            0.0403,
            -0.0162,
            -0.0546,
            0.0481
          ],
          "after": [
            0.0411,
            -0.1455,
            -0.2968,
            0.0533,
            0.0322,
            -0.0484,
            -0.1486,
            0.0689,
            0.1104,
            0.0076,
            0.0126,
            0.0875,
            0.0393,
            0.0204,
            -0.1288,
            0.0532
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            -0.0001,
            0.002,
            -0.0003,
            0.001,
            -0.0015,
            0.0008,
            -0.0,
            0.001,
            0.0017,
            0.001,
            0.0012,
            0.0007,
            -0.001,
            -0.0024,
            -0.0003
          ],
          "after": [
            -0.0796,
            0.0107,
            -0.119,
            -0.07,
            0.0613,
            0.1336,
            0.0818,
            -0.1673,
            0.0301,
            -0.0228,
            -0.1395,
            -0.0683,
            0.0901,
            -0.0798,
            0.0401,
            0.0802
          ]
        }
      }
    },
    {
      "step": 847,
      "word": "husna",
      "loss": 2.739,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3921,
            -0.089,
            -0.1698,
            -0.0954,
            -0.0585,
            -0.0894,
            -0.053,
            0.0308,
            -0.1561,
            0.2327,
            0.1418,
            -0.3638,
            0.1864,
            0.163,
            0.0028,
            -0.2833
          ],
          "after": [
            0.0089,
            -0.0018,
            0.1509,
            -0.0273,
            0.0287,
            -0.0942,
            -0.1164,
            -0.1636,
            -0.0088,
            0.0719,
            0.1627,
            0.0128,
            -0.1047,
            -0.0165,
            -0.0348,
            -0.1848
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0103,
            0.0308,
            -0.1863,
            0.0107,
            0.0016,
            -0.1454,
            0.2079,
            0.0337,
            -0.1744,
            0.0586,
            -0.0184,
            0.0002,
            0.0728,
            -0.0836,
            -0.124,
            0.1231
          ],
          "after": [
            0.0278,
            0.1026,
            -0.0346,
            -0.0748,
            0.0378,
            -0.0067,
            0.0865,
            0.0988,
            -0.0606,
            -0.0594,
            0.0198,
            -0.0083,
            0.0982,
            0.0317,
            -0.0526,
            0.2181
          ]
        },
        "position_0": {
          "grad": [
            0.0171,
            -0.1336,
            0.1481,
            0.0405,
            -0.0932,
            0.1297,
            -0.1256,
            0.1141,
            0.2113,
            0.0695,
            -0.1377,
            -0.1936,
            0.1733,
            0.208,
            0.1093,
            0.0533
          ],
          "after": [
            0.0412,
            -0.1454,
            -0.2968,
            0.0531,
            0.0323,
            -0.0483,
            -0.1488,
            0.0685,
            0.1104,
            0.0076,
            0.0127,
            0.0877,
            0.0391,
            0.0203,
            -0.1288,
            0.053
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0034,
            -0.0001,
            -0.0034,
            0.001,
            -0.002,
            -0.001,
            0.0022,
            -0.0004,
            -0.0022,
            -0.0024,
            -0.0003,
            -0.0011,
            0.0004,
            0.0013,
            -0.0004,
            0.002
          ],
          "after": [
            -0.0796,
            0.0107,
            -0.1191,
            -0.0699,
            0.0614,
            0.1339,
            0.0818,
            -0.1672,
            0.0301,
            -0.0225,
            -0.1397,
            -0.0683,
            0.09,
            -0.0797,
            0.0401,
            0.0803
          ]
        }
      }
    },
    {
      "step": 848,
      "word": "caven",
      "loss": 1.9898,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0687,
            0.0006,
            -0.0893,
            0.1609,
            0.1654,
            -0.0675,
            -0.005,
            -0.0654,
            -0.2049,
            0.0659,
            0.082,
            -0.0664,
            0.1311,
            -0.0818,
            -0.0722,
            0.1786
          ],
          "after": [
            0.0088,
            -0.0016,
            0.151,
            -0.0273,
            0.0287,
            -0.0942,
            -0.1163,
            -0.1636,
            -0.0086,
            0.0718,
            0.1625,
            0.013,
            -0.1048,
            -0.0165,
            -0.0348,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.019,
            -0.244,
            0.1341,
            -0.1832,
            0.1844,
            0.2112,
            -0.5344,
            -0.029,
            0.3063,
            -0.0937,
            0.1973,
            -0.0263,
            -0.2979,
            -0.0486,
            0.2751,
            -0.1992
          ],
          "after": [
            0.0278,
            0.1026,
            -0.0346,
            -0.0748,
            0.0377,
            -0.0068,
            0.0865,
            0.0988,
            -0.0607,
            -0.0593,
            0.0197,
            -0.0084,
            0.0983,
            0.0318,
            -0.0526,
            0.218
          ]
        },
        "position_0": {
          "grad": [
            -0.0363,
            0.0276,
            -0.0304,
            0.143,
            -0.0851,
            -0.0461,
            0.1482,
            0.0928,
            -0.0838,
            -0.0112,
            0.0127,
            0.007,
            0.0628,
            0.0185,
            -0.0114,
            0.0957
          ],
          "after": [
            0.0413,
            -0.1454,
            -0.2969,
            0.0528,
            0.0324,
            -0.0482,
            -0.149,
            0.0681,
            0.1105,
            0.0077,
            0.0127,
            0.0879,
            0.0389,
            0.0202,
            -0.1288,
            0.0528
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            -0.0003,
            0.0006,
            -0.0,
            -0.0001,
            0.0011,
            -0.0014,
            0.0007,
            0.0011,
            0.0004,
            -0.0001,
            0.0007,
            -0.001,
            -0.0002,
            -0.0,
            0.0004
          ],
          "after": [
            -0.0795,
            0.0107,
            -0.1192,
            -0.0698,
            0.0615,
            0.1341,
            0.0819,
            -0.1671,
            0.03,
            -0.0223,
            -0.1398,
            -0.0683,
            0.09,
            -0.0796,
            0.0401,
            0.0804
          ]
        }
      }
    },
    {
      "step": 849,
      "word": "kavian",
      "loss": 1.6734,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1802,
            0.1995,
            -0.0588,
            0.1929,
            0.1391,
            -0.1393,
            0.0328,
            -0.1693,
            -0.1338,
            -0.1146,
            -0.0199,
            0.1398,
            -0.0435,
            -0.1811,
            -0.1033,
            0.2414
          ],
          "after": [
            0.0088,
            -0.0015,
            0.1511,
            -0.0275,
            0.0286,
            -0.0942,
            -0.1163,
            -0.1635,
            -0.0084,
            0.0718,
            0.1624,
            0.0131,
            -0.1048,
            -0.0164,
            -0.0347,
            -0.1848
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.062,
            0.0845,
            -0.1226,
            0.0202,
            0.0293,
            -0.1671,
            0.2648,
            0.0295,
            -0.2122,
            0.0436,
            -0.051,
            -0.0323,
            0.1061,
            -0.0632,
            -0.1406,
            0.1469
          ],
          "after": [
            0.0278,
            0.1026,
            -0.0345,
            -0.0748,
            0.0376,
            -0.0068,
            0.0865,
            0.0987,
            -0.0607,
            -0.0593,
            0.0197,
            -0.0085,
            0.0984,
            0.0319,
            -0.0526,
            0.2179
          ]
        },
        "position_0": {
          "grad": [
            -0.0764,
            0.0636,
            -0.1222,
            0.0185,
            0.0458,
            -0.0597,
            0.1869,
            -0.009,
            -0.0506,
            0.0133,
            -0.0414,
            0.0201,
            0.0609,
            -0.0223,
            -0.0992,
            0.0107
          ],
          "after": [
            0.0415,
            -0.1454,
            -0.2968,
            0.0525,
            0.0324,
            -0.0481,
            -0.1492,
            0.0678,
            0.1107,
            0.0077,
            0.0128,
            0.088,
            0.0387,
            0.0201,
            -0.1288,
            0.0526
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0016,
            -0.0014,
            0.0011,
            0.0008,
            -0.0038,
            -0.0003,
            -0.0039,
            0.0018,
            0.0018,
            -0.0024,
            -0.0006,
            0.0026,
            -0.0016,
            -0.0002,
            0.0039,
            0.0007
          ],
          "after": [
            -0.0793,
            0.0108,
            -0.1194,
            -0.0698,
            0.0616,
            0.1343,
            0.0821,
            -0.1671,
            0.0299,
            -0.022,
            -0.1399,
            -0.0684,
            0.0901,
            -0.0795,
            0.04,
            0.0804
          ]
        }
      }
    },
    {
      "step": 850,
      "word": "elliett",
      "loss": 2.5701,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0089,
            -0.0015,
            0.1512,
            -0.0276,
            0.0285,
            -0.0941,
            -0.1163,
            -0.1634,
            -0.0082,
            0.0717,
            0.1623,
            0.0132,
            -0.1049,
            -0.0163,
            -0.0346,
            -0.1848
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1949,
            0.4262,
            0.5055,
            0.2125,
            -0.2929,
            -0.0185,
            0.1115,
            -0.0658,
            0.1087,
            -0.4337,
            0.0117,
            0.0835,
            -0.0751,
            0.0737,
            0.2728,
            -0.0711
          ],
          "after": [
            0.028,
            0.1024,
            -0.0346,
            -0.0749,
            0.0376,
            -0.0068,
            0.0865,
            0.0987,
            -0.0607,
            -0.0591,
            0.0196,
            -0.0085,
            0.0985,
            0.0319,
            -0.0526,
            0.2179
          ]
        },
        "position_0": {
          "grad": [
            -0.0194,
            -0.143,
            0.051,
            -0.0165,
            0.031,
            0.0802,
            -0.18,
            -0.0683,
            0.088,
            0.1164,
            0.0328,
            0.0022,
            -0.102,
            -0.0314,
            -0.0281,
            -0.2298
          ],
          "after": [
            0.0416,
            -0.1453,
            -0.2968,
            0.0523,
            0.0324,
            -0.048,
            -0.1493,
            0.0676,
            0.1107,
            0.0076,
            0.0128,
            0.0881,
            0.0385,
            0.0201,
            -0.1287,
            0.0525
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.0022,
            0.002,
            0.0036,
            -0.0014,
            -0.002,
            0.0004,
            -0.0008,
            -0.0003,
            -0.004,
            0.001,
            -0.0003,
            0.0023,
            -0.0006,
            -0.0003,
            0.0011
          ],
          "after": [
            -0.0792,
            0.0108,
            -0.1196,
            -0.0699,
            0.0618,
            0.1345,
            0.0823,
            -0.1671,
            0.0297,
            -0.0216,
            -0.14,
            -0.0685,
            0.0901,
            -0.0794,
            0.0399,
            0.0804
          ]
        }
      }
    },
    {
      "step": 851,
      "word": "rakeb",
      "loss": 2.7495,
      "learning_rate": 0.0005,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2331,
            0.1148,
            0.0471,
            0.0744,
            0.0294,
            -0.0759,
            0.2747,
            0.0344,
            -0.0537,
            -0.062,
            -0.0504,
            0.0247,
            -0.0159,
            -0.0251,
            -0.1324,
            -0.0781
          ],
          "after": [
            0.009,
            -0.0015,
            0.1512,
            -0.0278,
            0.0285,
            -0.094,
            -0.1164,
            -0.1633,
            -0.008,
            0.0717,
            0.1622,
            0.0133,
            -0.1049,
            -0.0163,
            -0.0344,
            -0.1848
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0079,
            -0.2861,
            0.124,
            -0.0489,
            0.0475,
            0.1716,
            -0.4693,
            -0.1035,
            0.134,
            -0.0864,
            0.1438,
            0.0696,
            -0.0735,
            -0.0446,
            0.161,
            -0.3476
          ],
          "after": [
            0.0281,
            0.1024,
            -0.0348,
            -0.075,
            0.0376,
            -0.0068,
            0.0866,
            0.0988,
            -0.0608,
            -0.0588,
            0.0194,
            -0.0086,
            0.0985,
            0.032,
            -0.0528,
            0.2179
          ]
        },
        "position_0": {
          "grad": [
            0.0075,
            -0.0202,
            0.0112,
            -0.0291,
            -0.014,
            0.1539,
            0.0961,
            -0.0593,
            0.1352,
            -0.0909,
            -0.1039,
            0.0568,
            -0.1692,
            -0.1014,
            0.1394,
            0.137
          ],
          "after": [
            0.0418,
            -0.1452,
            -0.2968,
            0.0521,
            0.0325,
            -0.0481,
            -0.1495,
            0.0675,
            0.1107,
            0.0076,
            0.0129,
            0.0882,
            0.0385,
            0.0202,
            -0.1287,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            -0.0042,
            -0.0028,
            -0.0034,
            -0.0002,
            0.0014,
            -0.0007,
            -0.0011,
            0.0046,
            0.0024,
            0.004,
            -0.0038,
            -0.003,
            -0.0041,
            -0.0001,
            -0.002
          ],
          "after": [
            -0.079,
            0.011,
            -0.1197,
            -0.0699,
            0.062,
            0.1347,
            0.0824,
            -0.167,
            0.0294,
            -0.0214,
            -0.1403,
            -0.0684,
            0.0902,
            -0.0791,
            0.0398,
            0.0805
          ]
        }
      }
    },
    {
      "step": 852,
      "word": "damyra",
      "loss": 2.1588,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0231,
            0.0611,
            0.1019,
            -0.0959,
            0.1529,
            -0.1261,
            0.2666,
            0.0604,
            -0.0172,
            0.1302,
            0.1407,
            -0.0732,
            -0.0568,
            -0.0528,
            -0.0965,
            -0.0921
          ],
          "after": [
            0.0091,
            -0.0015,
            0.1512,
            -0.0279,
            0.0283,
            -0.0939,
            -0.1166,
            -0.1633,
            -0.0078,
            0.0716,
            0.162,
            0.0134,
            -0.1049,
            -0.0162,
            -0.0343,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0676,
            0.1313,
            -0.1296,
            0.0912,
            0.0353,
            -0.1746,
            0.2694,
            0.0172,
            -0.2263,
            0.0371,
            -0.0514,
            0.0084,
            0.1149,
            -0.0894,
            -0.2015,
            0.1413
          ],
          "after": [
            0.0282,
            0.1023,
            -0.0349,
            -0.0752,
            0.0376,
            -0.0068,
            0.0866,
            0.0989,
            -0.0608,
            -0.0586,
            0.0193,
            -0.0087,
            0.0986,
            0.0321,
            -0.0528,
            0.2179
          ]
        },
        "position_0": {
          "grad": [
            -0.0609,
            0.0672,
            -0.0457,
            -0.081,
            0.0995,
            -0.0656,
            0.0884,
            -0.0667,
            -0.0322,
            -0.0153,
            -0.0605,
            0.0254,
            0.0447,
            0.017,
            0.0292,
            0.1364
          ],
          "after": [
            0.042,
            -0.1452,
            -0.2967,
            0.052,
            0.0324,
            -0.0481,
            -0.1496,
            0.0674,
            0.1107,
            0.0076,
            0.013,
            0.0882,
            0.0385,
            0.0202,
            -0.1287,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0013,
            0.0002,
            -0.0004,
            -0.0005,
            -0.0004,
            -0.0013,
            -0.0005,
            -0.0006,
            -0.0001,
            -0.0004,
            -0.0,
            -0.0016,
            0.0006,
            0.0012,
            0.0002,
            -0.0011
          ],
          "after": [
            -0.0789,
            0.0111,
            -0.1197,
            -0.0698,
            0.0622,
            0.1348,
            0.0825,
            -0.1669,
            0.0291,
            -0.0212,
            -0.1405,
            -0.0683,
            0.0902,
            -0.0789,
            0.0398,
            0.0806
          ]
        }
      }
    },
    {
      "step": 853,
      "word": "artie",
      "loss": 2.3192,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1306,
            -0.226,
            -0.1939,
            -0.0334,
            -0.1381,
            0.2088,
            0.0673,
            -0.2339,
            0.2029,
            -0.1708,
            0.0954,
            -0.0392,
            -0.3169,
            -0.1793,
            0.2399,
            0.1318
          ],
          "after": [
            0.0091,
            -0.0014,
            0.1513,
            -0.0279,
            0.0283,
            -0.0939,
            -0.1168,
            -0.1632,
            -0.0078,
            0.0717,
            0.1619,
            0.0135,
            -0.1048,
            -0.016,
            -0.0342,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1533,
            0.0269,
            -0.0359,
            0.1917,
            -0.5238,
            -0.0157,
            -0.2382,
            -0.0174,
            0.0299,
            -0.2004,
            -0.0518,
            0.0212,
            0.1046,
            0.105,
            0.324,
            -0.0617
          ],
          "after": [
            0.0282,
            0.1022,
            -0.0349,
            -0.0754,
            0.0378,
            -0.0067,
            0.0867,
            0.099,
            -0.0608,
            -0.0584,
            0.0193,
            -0.0088,
            0.0986,
            0.0321,
            -0.0529,
            0.218
          ]
        },
        "position_0": {
          "grad": [
            0.1051,
            -0.0708,
            0.1142,
            -0.2173,
            0.0288,
            0.008,
            -0.3941,
            -0.1318,
            0.1875,
            0.0411,
            0.2436,
            0.016,
            -0.2221,
            0.0314,
            -0.077,
            -0.2886
          ],
          "after": [
            0.042,
            -0.1451,
            -0.2968,
            0.0521,
            0.0324,
            -0.0481,
            -0.1496,
            0.0675,
            0.1105,
            0.0075,
            0.013,
            0.0882,
            0.0385,
            0.0203,
            -0.1287,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0013,
            -0.0005,
            0.0031,
            0.0015,
            -0.0017,
            -0.0031,
            -0.0014,
            -0.001,
            -0.0022,
            -0.001,
            0.0019,
            -0.0013,
            0.0009,
            -0.0014,
            -0.0011,
            -0.0018
          ],
          "after": [
            -0.0789,
            0.0113,
            -0.1199,
            -0.0698,
            0.0624,
            0.1351,
            0.0827,
            -0.1668,
            0.029,
            -0.021,
            -0.1408,
            -0.0682,
            0.0902,
            -0.0787,
            0.0398,
            0.0808
          ]
        }
      }
    },
    {
      "step": 854,
      "word": "eleonora",
      "loss": 2.3802,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0277,
            -0.0057,
            0.0056,
            -0.0355,
            -0.0146,
            -0.0042,
            -0.027,
            -0.032,
            0.0034,
            0.0297,
            0.04,
            -0.0386,
            0.0075,
            0.008,
            0.0021,
            -0.0289
          ],
          "after": [
            0.0091,
            -0.0013,
            0.1513,
            -0.028,
            0.0283,
            -0.0939,
            -0.117,
            -0.163,
            -0.0078,
            0.0717,
            0.1617,
            0.0136,
            -0.1046,
            -0.0159,
            -0.0342,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2106,
            0.0051,
            0.6987,
            0.016,
            0.2367,
            0.3128,
            -0.166,
            0.0525,
            0.3413,
            -0.0669,
            0.071,
            0.2374,
            -0.3824,
            0.0592,
            0.1342,
            -0.3484
          ],
          "after": [
            0.0283,
            0.1022,
            -0.0351,
            -0.0757,
            0.0378,
            -0.0068,
            0.0867,
            0.099,
            -0.0609,
            -0.0581,
            0.0192,
            -0.009,
            0.0987,
            0.0321,
            -0.053,
            0.2181
          ]
        },
        "position_0": {
          "grad": [
            -0.0165,
            -0.1316,
            0.0434,
            -0.0368,
            0.028,
            0.0558,
            -0.1959,
            -0.0725,
            0.0935,
            0.117,
            0.0312,
            -0.0177,
            -0.1217,
            -0.0322,
            -0.0309,
            -0.2357
          ],
          "after": [
            0.0421,
            -0.145,
            -0.2969,
            0.0522,
            0.0323,
            -0.0482,
            -0.1495,
            0.0677,
            0.1104,
            0.0074,
            0.0129,
            0.0882,
            0.0386,
            0.0203,
            -0.1287,
            0.0523
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.004,
            -0.0005,
            -0.0054,
            -0.0003,
            -0.0016,
            0.0009,
            -0.0052,
            0.0009,
            -0.0004,
            0.0023,
            -0.0027,
            -0.0067,
            -0.0007,
            0.0034,
            0.0052,
            -0.0023
          ],
          "after": [
            -0.079,
            0.0114,
            -0.1197,
            -0.0698,
            0.0626,
            0.1353,
            0.083,
            -0.1667,
            0.0289,
            -0.0209,
            -0.141,
            -0.0679,
            0.0903,
            -0.0787,
            0.0396,
            0.081
          ]
        }
      }
    },
    {
      "step": 855,
      "word": "magdalina",
      "loss": 2.3239,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2311,
            0.1524,
            0.0075,
            0.0596,
            0.1251,
            0.0227,
            0.0863,
            0.1471,
            -0.1792,
            0.1538,
            -0.0055,
            0.2588,
            -0.0182,
            -0.1596,
            -0.1232,
            -0.0217
          ],
          "after": [
            0.0092,
            -0.0014,
            0.1514,
            -0.028,
            0.0282,
            -0.0939,
            -0.1172,
            -0.163,
            -0.0076,
            0.0716,
            0.1616,
            0.0136,
            -0.1045,
            -0.0157,
            -0.0341,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0886,
            0.1419,
            -0.109,
            0.0534,
            0.0614,
            -0.1573,
            0.2287,
            0.0043,
            -0.1807,
            0.0135,
            -0.0122,
            0.0362,
            0.0655,
            -0.094,
            -0.1627,
            0.1635
          ],
          "after": [
            0.0285,
            0.102,
            -0.0353,
            -0.0759,
            0.0379,
            -0.0068,
            0.0868,
            0.099,
            -0.0609,
            -0.0579,
            0.0191,
            -0.0091,
            0.0988,
            0.0321,
            -0.0531,
            0.2182
          ]
        },
        "position_0": {
          "grad": [
            -0.0123,
            0.044,
            -0.0448,
            0.021,
            0.1107,
            -0.0138,
            0.1851,
            0.0414,
            -0.0082,
            0.0474,
            0.0042,
            0.0583,
            -0.0341,
            -0.0474,
            -0.0317,
            0.096
          ],
          "after": [
            0.0421,
            -0.1449,
            -0.2969,
            0.0522,
            0.0322,
            -0.0482,
            -0.1495,
            0.0678,
            0.1102,
            0.0073,
            0.0129,
            0.0882,
            0.0387,
            0.0204,
            -0.1286,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0023,
            0.0026,
            0.0005,
            -0.0007,
            0.0026,
            -0.0023,
            0.0002,
            -0.0028,
            -0.0027,
            0.0012,
            0.0031,
            -0.0003,
            0.0002,
            -0.0002,
            -0.0032,
            -0.002
          ],
          "after": [
            -0.0793,
            0.0114,
            -0.1197,
            -0.0698,
            0.0627,
            0.1355,
            0.0832,
            -0.1666,
            0.029,
            -0.0209,
            -0.1412,
            -0.0676,
            0.0903,
            -0.0787,
            0.0395,
            0.0812
          ]
        }
      }
    },
    {
      "step": 856,
      "word": "leeonna",
      "loss": 2.2153,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.084,
            -0.0306,
            -0.0343,
            -0.0617,
            -0.0079,
            -0.0315,
            -0.02,
            -0.0318,
            -0.0531,
            0.0503,
            0.0764,
            -0.0651,
            0.0194,
            0.0468,
            0.0019,
            -0.0735
          ],
          "after": [
            0.0092,
            -0.0013,
            0.1514,
            -0.028,
            0.0281,
            -0.0939,
            -0.1174,
            -0.1629,
            -0.0075,
            0.0715,
            0.1614,
            0.0136,
            -0.1045,
            -0.0156,
            -0.0341,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0375,
            -0.2348,
            0.255,
            -0.0308,
            0.2936,
            0.5418,
            -0.4155,
            0.0793,
            0.5414,
            0.0278,
            -0.0446,
            0.5166,
            -0.5285,
            0.0692,
            0.4359,
            -0.3267
          ],
          "after": [
            0.0286,
            0.102,
            -0.0355,
            -0.0761,
            0.0378,
            -0.007,
            0.0869,
            0.0989,
            -0.0611,
            -0.0578,
            0.0191,
            -0.0095,
            0.0991,
            0.0321,
            -0.0533,
            0.2184
          ]
        },
        "position_0": {
          "grad": [
            -0.0742,
            0.077,
            -0.0907,
            -0.0907,
            0.0336,
            0.054,
            -0.0782,
            0.0845,
            0.0037,
            -0.0129,
            -0.1575,
            0.1647,
            -0.1065,
            0.0154,
            0.0691,
            -0.124
          ],
          "after": [
            0.0423,
            -0.1449,
            -0.2969,
            0.0523,
            0.032,
            -0.0483,
            -0.1495,
            0.0677,
            0.1101,
            0.0071,
            0.0129,
            0.0881,
            0.0389,
            0.0204,
            -0.1286,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0022,
            0.0004,
            -0.0014,
            -0.0003,
            -0.0016,
            0.0002,
            -0.0042,
            -0.0025,
            -0.001,
            -0.0014,
            -0.0004,
            -0.0017,
            -0.0025,
            0.0031,
            0.0031,
            -0.0021
          ],
          "after": [
            -0.0796,
            0.0114,
            -0.1196,
            -0.0698,
            0.0629,
            0.1357,
            0.0836,
            -0.1663,
            0.0291,
            -0.0208,
            -0.1414,
            -0.0673,
            0.0904,
            -0.0788,
            0.0394,
            0.0815
          ]
        }
      }
    },
    {
      "step": 857,
      "word": "remy",
      "loss": 2.743,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0093,
            -0.0013,
            0.1515,
            -0.028,
            0.0281,
            -0.0939,
            -0.1175,
            -0.1629,
            -0.0074,
            0.0714,
            0.1613,
            0.0136,
            -0.1044,
            -0.0154,
            -0.034,
            -0.1846
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.203,
            -0.4639,
            0.4711,
            -0.1233,
            0.5221,
            0.2724,
            -0.8492,
            0.0363,
            0.6202,
            0.233,
            0.1789,
            0.2337,
            -0.4126,
            0.234,
            -0.025,
            -0.2908
          ],
          "after": [
            0.0286,
            0.1022,
            -0.0359,
            -0.0762,
            0.0376,
            -0.0073,
            0.0871,
            0.0988,
            -0.0614,
            -0.0578,
            0.019,
            -0.0098,
            0.0994,
            0.032,
            -0.0534,
            0.2186
          ]
        },
        "position_0": {
          "grad": [
            0.009,
            -0.0477,
            0.0147,
            -0.0337,
            -0.0738,
            0.1923,
            0.1126,
            -0.071,
            0.1748,
            -0.1343,
            -0.1314,
            0.0249,
            -0.2251,
            -0.0788,
            0.2091,
            0.1506
          ],
          "after": [
            0.0424,
            -0.1448,
            -0.2969,
            0.0524,
            0.032,
            -0.0485,
            -0.1495,
            0.0678,
            0.1099,
            0.0072,
            0.013,
            0.0879,
            0.0391,
            0.0206,
            -0.1288,
            0.0525
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0013,
            0.0004,
            0.0009,
            -0.0011,
            0.0002,
            -0.0008,
            -0.0033,
            -0.0004,
            0.0011,
            -0.0026,
            -0.0002,
            -0.0004,
            -0.0011,
            0.002,
            0.0001,
            -0.0011
          ],
          "after": [
            -0.0799,
            0.0114,
            -0.1195,
            -0.0697,
            0.063,
            0.136,
            0.0839,
            -0.166,
            0.0291,
            -0.0206,
            -0.1415,
            -0.067,
            0.0906,
            -0.079,
            0.0393,
            0.0818
          ]
        }
      }
    },
    {
      "step": 858,
      "word": "pranshi",
      "loss": 2.7262,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0747,
            0.1676,
            -0.1706,
            0.0447,
            -0.2145,
            -0.0316,
            -0.0171,
            -0.1045,
            0.0167,
            -0.0334,
            -0.1015,
            -0.0465,
            0.0561,
            0.1232,
            0.0102,
            0.0865
          ],
          "after": [
            0.0093,
            -0.0014,
            0.1516,
            -0.0281,
            0.0281,
            -0.0938,
            -0.1176,
            -0.1628,
            -0.0073,
            0.0714,
            0.1612,
            0.0136,
            -0.1044,
            -0.0154,
            -0.034,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0465,
            0.0654,
            -0.1444,
            -0.0013,
            0.0414,
            -0.1776,
            0.2538,
            0.0181,
            -0.1724,
            0.0061,
            -0.0321,
            0.0266,
            0.0585,
            -0.11,
            -0.1291,
            0.1154
          ],
          "after": [
            0.0287,
            0.1023,
            -0.0361,
            -0.0762,
            0.0374,
            -0.0074,
            0.0873,
            0.0987,
            -0.0616,
            -0.0578,
            0.0189,
            -0.0102,
            0.0997,
            0.032,
            -0.0535,
            0.2188
          ]
        },
        "position_0": {
          "grad": [
            0.0876,
            0.1352,
            -0.0794,
            0.0806,
            -0.06,
            -0.1432,
            0.3146,
            0.0234,
            -0.2656,
            -0.011,
            -0.0125,
            -0.0574,
            0.051,
            0.0815,
            0.0284,
            0.2051
          ],
          "after": [
            0.0423,
            -0.1448,
            -0.2968,
            0.0525,
            0.0319,
            -0.0485,
            -0.1496,
            0.0678,
            0.1099,
            0.0072,
            0.0131,
            0.0879,
            0.0392,
            0.0206,
            -0.1289,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0022,
            -0.0007,
            -0.0024,
            -0.0016,
            -0.0005,
            -0.0016,
            0.0033,
            -0.0011,
            -0.0033,
            0.0002,
            0.0024,
            -0.0029,
            0.0004,
            0.0005,
            -0.0005,
            -0.0023
          ],
          "after": [
            -0.0802,
            0.0114,
            -0.1193,
            -0.0696,
            0.0631,
            0.1362,
            0.0841,
            -0.1657,
            0.0293,
            -0.0205,
            -0.1418,
            -0.0667,
            0.0907,
            -0.0792,
            0.0392,
            0.0822
          ]
        }
      }
    },
    {
      "step": 859,
      "word": "stellah",
      "loss": 2.3259,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1589,
            -0.0589,
            0.2041,
            0.1411,
            0.126,
            0.1687,
            0.0369,
            0.1475,
            0.1492,
            0.0479,
            -0.1659,
            0.042,
            0.1308,
            -0.01,
            -0.111,
            0.1705
          ],
          "after": [
            0.0094,
            -0.0014,
            0.1516,
            -0.0282,
            0.0281,
            -0.0939,
            -0.1177,
            -0.1628,
            -0.0073,
            0.0713,
            0.1613,
            0.0136,
            -0.1044,
            -0.0154,
            -0.0339,
            -0.1847
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0242,
            -0.1139,
            0.1729,
            0.0302,
            0.0935,
            0.0851,
            -0.0562,
            0.026,
            0.0418,
            0.0328,
            0.0854,
            0.3137,
            -0.3155,
            -0.1475,
            0.0549,
            0.149
          ],
          "after": [
            0.0287,
            0.1024,
            -0.0363,
            -0.0763,
            0.0372,
            -0.0076,
            0.0874,
            0.0986,
            -0.0618,
            -0.0577,
            0.0187,
            -0.0106,
            0.1001,
            0.032,
            -0.0536,
            0.2189
          ]
        },
        "position_0": {
          "grad": [
            -0.0073,
            0.0876,
            -0.0261,
            0.1301,
            -0.0935,
            0.0484,
            0.0474,
            0.0404,
            -0.071,
            0.0088,
            -0.1055,
            0.1057,
            0.0764,
            0.0107,
            0.0965,
            0.0515
          ],
          "after": [
            0.0423,
            -0.1449,
            -0.2967,
            0.0524,
            0.032,
            -0.0486,
            -0.1498,
            0.0678,
            0.11,
            0.0072,
            0.0133,
            0.0877,
            0.0393,
            0.0206,
            -0.1291,
            0.0523
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0012,
            0.0014,
            0.0006,
            0.0003,
            -0.0014,
            0.0014,
            -0.0012,
            -0.0008,
            0.0007,
            0.0024,
            0.0004,
            0.0003,
            -0.0019,
            -0.0023,
            0.0005
          ],
          "after": [
            -0.0805,
            0.0113,
            -0.1193,
            -0.0695,
            0.0632,
            0.1364,
            0.0843,
            -0.1654,
            0.0295,
            -0.0204,
            -0.1421,
            -0.0664,
            0.0907,
            -0.0792,
            0.0392,
            0.0824
          ]
        }
      }
    },
    {
      "step": 860,
      "word": "castulo",
      "loss": 2.4972,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0371,
            0.1643,
            0.0835,
            0.1308,
            -0.08,
            -0.0133,
            0.0288,
            -0.0093,
            -0.1213,
            0.047,
            -0.0389,
            0.0835,
            0.1521,
            0.0333,
            0.0173,
            -0.0347
          ],
          "after": [
            0.0094,
            -0.0015,
            0.1516,
            -0.0283,
            0.0281,
            -0.094,
            -0.1178,
            -0.1628,
            -0.0073,
            0.0712,
            0.1613,
            0.0136,
            -0.1045,
            -0.0153,
            -0.0338,
            -0.1848
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.002,
            0.1004,
            -0.159,
            0.0361,
            0.022,
            -0.1715,
            0.2089,
            -0.0066,
            -0.1922,
            0.0042,
            0.0216,
            -0.0123,
            0.0841,
            -0.1,
            -0.1501,
            0.1514
          ],
          "after": [
            0.0287,
            0.1025,
            -0.0365,
            -0.0764,
            0.037,
            -0.0077,
            0.0875,
            0.0986,
            -0.0619,
            -0.0578,
            0.0186,
            -0.0109,
            0.1004,
            0.0321,
            -0.0537,
            0.2189
          ]
        },
        "position_0": {
          "grad": [
            -0.0333,
            0.0296,
            -0.0267,
            0.1059,
            -0.0433,
            -0.0283,
            0.1248,
            0.0651,
            -0.0724,
            0.0086,
            0.0223,
            0.0303,
            0.0493,
            -0.0024,
            -0.0271,
            0.0805
          ],
          "after": [
            0.0424,
            -0.145,
            -0.2965,
            0.0523,
            0.0321,
            -0.0487,
            -0.1499,
            0.0678,
            0.11,
            0.0072,
            0.0134,
            0.0876,
            0.0394,
            0.0206,
            -0.1293,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0002,
            0.0013,
            -0.0005,
            0.0004,
            -0.0006,
            -0.0012,
            0.0,
            0.0004,
            0.0002,
            -0.0003,
            0.0007,
            -0.0004,
            0.0,
            -0.0003,
            0.0003
          ],
          "after": [
            -0.0807,
            0.0113,
            -0.1193,
            -0.0694,
            0.0632,
            0.1367,
            0.0844,
            -0.1652,
            0.0297,
            -0.0203,
            -0.1423,
            -0.0663,
            0.0908,
            -0.0793,
            0.0392,
            0.0826
          ]
        }
      }
    },
    {
      "step": 861,
      "word": "conrad",
      "loss": 2.5513,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3507,
            0.0175,
            0.1256,
            -0.0251,
            0.1524,
            -0.0328,
            0.099,
            -0.0679,
            0.0559,
            -0.1857,
            -0.1043,
            0.2333,
            -0.0583,
            -0.0479,
            0.0407,
            0.3114
          ],
          "after": [
            0.0096,
            -0.0016,
            0.1515,
            -0.0284,
            0.0281,
            -0.094,
            -0.1179,
            -0.1627,
            -0.0072,
            0.0713,
            0.1614,
            0.0135,
            -0.1045,
            -0.0153,
            -0.0337,
            -0.1849
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0205,
            0.1099,
            -0.1524,
            0.0569,
            0.0207,
            -0.1472,
            0.2676,
            0.0242,
            -0.2118,
            0.0366,
            -0.0283,
            -0.003,
            0.1122,
            -0.117,
            -0.1698,
            0.1531
          ],
          "after": [
            0.0288,
            0.1025,
            -0.0366,
            -0.0765,
            0.0368,
            -0.0077,
            0.0875,
            0.0985,
            -0.062,
            -0.0578,
            0.0185,
            -0.0112,
            0.1005,
            0.0322,
            -0.0536,
            0.2189
          ]
        },
        "position_0": {
          "grad": [
            -0.0399,
            0.0137,
            -0.0268,
            0.128,
            -0.0766,
            -0.032,
            0.1267,
            0.0852,
            -0.0613,
            -0.0051,
            0.0134,
            0.0141,
            0.0365,
            -0.0017,
            -0.0039,
            0.0744
          ],
          "after": [
            0.0425,
            -0.1451,
            -0.2964,
            0.0522,
            0.0322,
            -0.0487,
            -0.1501,
            0.0676,
            0.1101,
            0.0072,
            0.0135,
            0.0875,
            0.0394,
            0.0206,
            -0.1294,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            -0.0025,
            -0.0006,
            -0.0016,
            -0.0007,
            0.0001,
            -0.0021,
            -0.0008,
            0.0003,
            0.0018,
            0.0013,
            -0.0009,
            -0.0013,
            0.001,
            0.0008,
            -0.0014
          ],
          "after": [
            -0.081,
            0.0114,
            -0.1192,
            -0.0693,
            0.0633,
            0.1369,
            0.0846,
            -0.1649,
            0.0298,
            -0.0204,
            -0.1426,
            -0.0661,
            0.0909,
            -0.0794,
            0.0392,
            0.0829
          ]
        }
      }
    },
    {
      "step": 862,
      "word": "dionisio",
      "loss": 2.6708,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0098,
            -0.0017,
            0.1514,
            -0.0285,
            0.028,
            -0.094,
            -0.118,
            -0.1627,
            -0.0072,
            0.0713,
            0.1615,
            0.0134,
            -0.1046,
            -0.0153,
            -0.0337,
            -0.185
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0568,
            0.0228,
            -0.07,
            0.0063,
            0.1189,
            -0.1063,
            0.1735,
            0.0259,
            -0.1417,
            0.025,
            -0.022,
            0.0069,
            0.0387,
            -0.0664,
            -0.1695,
            0.0854
          ],
          "after": [
            0.0288,
            0.1026,
            -0.0366,
            -0.0766,
            0.0367,
            -0.0077,
            0.0875,
            0.0984,
            -0.062,
            -0.0578,
            0.0184,
            -0.0114,
            0.1007,
            0.0323,
            -0.0536,
            0.2188
          ]
        },
        "position_0": {
          "grad": [
            -0.052,
            0.0324,
            -0.0229,
            -0.0795,
            0.0763,
            -0.0466,
            0.0115,
            -0.065,
            0.0035,
            -0.0254,
            -0.0424,
            -0.0042,
            -0.0004,
            0.0291,
            0.0252,
            0.0855
          ],
          "after": [
            0.0426,
            -0.1452,
            -0.2963,
            0.0521,
            0.0322,
            -0.0487,
            -0.1502,
            0.0676,
            0.1102,
            0.0072,
            0.0136,
            0.0874,
            0.0394,
            0.0206,
            -0.1295,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            -0.0001,
            0.0014,
            0.0034,
            -0.0052,
            0.0015,
            -0.0066,
            0.0008,
            0.0007,
            0.0024,
            -0.0,
            -0.0006,
            -0.0007,
            0.0012,
            0.0023,
            -0.0016
          ],
          "after": [
            -0.0813,
            0.0114,
            -0.1193,
            -0.0693,
            0.0635,
            0.137,
            0.0849,
            -0.1648,
            0.0298,
            -0.0204,
            -0.1428,
            -0.0659,
            0.0911,
            -0.0795,
            0.0391,
            0.0831
          ]
        }
      }
    },
    {
      "step": 863,
      "word": "radwan",
      "loss": 2.5545,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2036,
            0.2325,
            -0.0471,
            0.0488,
            0.1034,
            -0.2313,
            0.1186,
            -0.1815,
            -0.0081,
            -0.2945,
            -0.1051,
            0.1897,
            -0.0577,
            0.0163,
            0.0648,
            0.2596
          ],
          "after": [
            0.01,
            -0.0019,
            0.1514,
            -0.0287,
            0.028,
            -0.0939,
            -0.1182,
            -0.1626,
            -0.0072,
            0.0715,
            0.1616,
            0.0133,
            -0.1046,
            -0.0153,
            -0.0337,
            -0.1852
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0512,
            0.1495,
            -0.1425,
            0.0783,
            0.0009,
            -0.1828,
            0.3274,
            0.0327,
            -0.2379,
            0.0211,
            -0.031,
            -0.0176,
            0.1059,
            -0.1047,
            -0.1631,
            0.1467
          ],
          "after": [
            0.0289,
            0.1025,
            -0.0366,
            -0.0767,
            0.0365,
            -0.0076,
            0.0875,
            0.0982,
            -0.0619,
            -0.0578,
            0.0184,
            -0.0116,
            0.1008,
            0.0325,
            -0.0535,
            0.2187
          ]
        },
        "position_0": {
          "grad": [
            0.0053,
            -0.0194,
            -0.0013,
            -0.0104,
            -0.0088,
            0.1353,
            0.1265,
            -0.0443,
            0.088,
            -0.0665,
            -0.0916,
            0.0669,
            -0.139,
            -0.0937,
            0.1214,
            0.1231
          ],
          "after": [
            0.0427,
            -0.1453,
            -0.2962,
            0.052,
            0.0323,
            -0.0488,
            -0.1504,
            0.0676,
            0.1102,
            0.0073,
            0.0137,
            0.0872,
            0.0395,
            0.0207,
            -0.1297,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.003,
            0.0026,
            0.0012,
            0.0032,
            0.0028,
            -0.001,
            0.0023,
            -0.0006,
            -0.0019,
            0.001,
            0.0008,
            0.0007,
            0.0013,
            -0.0007,
            -0.0046,
            -0.0008
          ],
          "after": [
            -0.0816,
            0.0114,
            -0.1193,
            -0.0694,
            0.0636,
            0.1371,
            0.0852,
            -0.1646,
            0.03,
            -0.0205,
            -0.143,
            -0.0658,
            0.0911,
            -0.0796,
            0.0392,
            0.0834
          ]
        }
      }
    },
    {
      "step": 864,
      "word": "ford",
      "loss": 3.1759,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0101,
            -0.002,
            0.1514,
            -0.0287,
            0.0279,
            -0.0938,
            -0.1183,
            -0.1624,
            -0.0072,
            0.0716,
            0.1617,
            0.0132,
            -0.1046,
            -0.0152,
            -0.0337,
            -0.1854
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0389,
            0.1907,
            -0.1969,
            0.0989,
            -0.0741,
            -0.1907,
            0.3518,
            0.0319,
            -0.2609,
            0.0397,
            -0.0513,
            -0.0398,
            0.1714,
            -0.0912,
            -0.1954,
            0.1449
          ],
          "after": [
            0.029,
            0.1024,
            -0.0366,
            -0.0769,
            0.0364,
            -0.0075,
            0.0873,
            0.0981,
            -0.0618,
            -0.0579,
            0.0184,
            -0.0118,
            0.1008,
            0.0327,
            -0.0533,
            0.2186
          ]
        },
        "position_0": {
          "grad": [
            0.036,
            0.2136,
            -0.0784,
            0.2514,
            -0.0484,
            -0.3139,
            0.2412,
            0.0025,
            -0.3631,
            -0.2208,
            0.1422,
            -0.1218,
            0.3079,
            0.0322,
            0.0975,
            0.2572
          ],
          "after": [
            0.0427,
            -0.1455,
            -0.296,
            0.0518,
            0.0323,
            -0.0486,
            -0.1506,
            0.0675,
            0.1104,
            0.0076,
            0.0138,
            0.0872,
            0.0395,
            0.0207,
            -0.1299,
            0.0515
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0038,
            0.0042,
            -0.0015,
            0.0048,
            -0.0042,
            0.0018,
            0.0053,
            -0.0024,
            0.0006,
            0.0041,
            -0.0004,
            0.0011,
            0.0024,
            0.0002,
            -0.0047,
            0.0033
          ],
          "after": [
            -0.082,
            0.0112,
            -0.1193,
            -0.0697,
            0.0638,
            0.1372,
            0.0852,
            -0.1643,
            0.0301,
            -0.0207,
            -0.1432,
            -0.0657,
            0.0911,
            -0.0797,
            0.0393,
            0.0834
          ]
        }
      }
    },
    {
      "step": 865,
      "word": "sabiha",
      "loss": 2.0251,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1763,
            0.0041,
            0.1755,
            -0.1144,
            -0.0057,
            -0.0896,
            0.0445,
            -0.0115,
            -0.2093,
            0.1077,
            0.1138,
            -0.153,
            0.2562,
            -0.0666,
            -0.1003,
            0.1226
          ],
          "after": [
            0.0102,
            -0.0021,
            0.1513,
            -0.0288,
            0.0279,
            -0.0937,
            -0.1184,
            -0.1623,
            -0.0071,
            0.0717,
            0.1618,
            0.0132,
            -0.1047,
            -0.0152,
            -0.0337,
            -0.1856
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0192,
            0.0314,
            -0.1118,
            0.0089,
            0.0473,
            -0.1316,
            0.159,
            0.0135,
            -0.1727,
            0.0318,
            -0.0196,
            0.0121,
            0.0514,
            -0.0499,
            -0.135,
            0.1005
          ],
          "after": [
            0.0291,
            0.1023,
            -0.0365,
            -0.0771,
            0.0363,
            -0.0073,
            0.0872,
            0.098,
            -0.0616,
            -0.0579,
            0.0184,
            -0.0119,
            0.1008,
            0.0329,
            -0.0532,
            0.2184
          ]
        },
        "position_0": {
          "grad": [
            -0.0067,
            0.0903,
            -0.0248,
            0.0954,
            -0.1112,
            0.0432,
            -0.0166,
            0.028,
            -0.0531,
            0.022,
            -0.1031,
            0.0757,
            0.0675,
            0.01,
            0.0853,
            0.0162
          ],
          "after": [
            0.0428,
            -0.1458,
            -0.2959,
            0.0516,
            0.0325,
            -0.0485,
            -0.1508,
            0.0675,
            0.1106,
            0.0078,
            0.0138,
            0.0871,
            0.0394,
            0.0207,
            -0.1302,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.002,
            -0.0007,
            -0.0009,
            -0.001,
            0.0005,
            0.0032,
            0.0004,
            0.0008,
            0.0012,
            0.0007,
            0.0002,
            0.0,
            -0.001,
            -0.0009,
            0.001,
            0.0009
          ],
          "after": [
            -0.0823,
            0.011,
            -0.1193,
            -0.0699,
            0.064,
            0.1371,
            0.0852,
            -0.1641,
            0.0301,
            -0.0209,
            -0.1433,
            -0.0656,
            0.0911,
            -0.0797,
            0.0395,
            0.0835
          ]
        }
      }
    },
    {
      "step": 866,
      "word": "andon",
      "loss": 2.1792,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0679,
            0.1832,
            -0.3526,
            0.1494,
            -0.6516,
            0.0115,
            -0.068,
            -0.1179,
            0.0262,
            -0.3538,
            -0.1999,
            -0.1932,
            0.0688,
            0.2935,
            0.3822,
            0.1726
          ],
          "after": [
            0.0103,
            -0.0023,
            0.1514,
            -0.0289,
            0.028,
            -0.0936,
            -0.1185,
            -0.1622,
            -0.007,
            0.0719,
            0.1619,
            0.0132,
            -0.1048,
            -0.0153,
            -0.0338,
            -0.1858
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.006,
            0.0244,
            -0.0779,
            0.0317,
            0.0609,
            -0.1235,
            0.1869,
            0.0454,
            -0.1183,
            0.0668,
            -0.0194,
            0.0056,
            0.0838,
            -0.0566,
            -0.1693,
            0.0604
          ],
          "after": [
            0.0291,
            0.1022,
            -0.0365,
            -0.0772,
            0.0362,
            -0.0072,
            0.087,
            0.0978,
            -0.0615,
            -0.058,
            0.0184,
            -0.012,
            0.1008,
            0.033,
            -0.053,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            0.0958,
            -0.0691,
            0.1231,
            -0.1556,
            0.0249,
            0.0085,
            -0.3507,
            -0.1089,
            0.1808,
            -0.0053,
            0.2495,
            0.0524,
            -0.1917,
            0.0533,
            -0.033,
            -0.2485
          ],
          "after": [
            0.0427,
            -0.1459,
            -0.2959,
            0.0515,
            0.0325,
            -0.0485,
            -0.1508,
            0.0676,
            0.1107,
            0.0079,
            0.0138,
            0.087,
            0.0394,
            0.0207,
            -0.1304,
            0.0512
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0005,
            0.0006,
            0.0011,
            -0.0003,
            0.0002,
            -0.0001,
            -0.0011,
            -0.0014,
            -0.0,
            0.001,
            -0.0003,
            -0.0006,
            -0.0004,
            -0.0,
            -0.001
          ],
          "after": [
            -0.0826,
            0.0109,
            -0.1193,
            -0.0701,
            0.0642,
            0.137,
            0.0852,
            -0.1639,
            0.0302,
            -0.0211,
            -0.1435,
            -0.0655,
            0.0911,
            -0.0797,
            0.0396,
            0.0836
          ]
        }
      }
    },
    {
      "step": 867,
      "word": "kester",
      "loss": 2.1556,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0103,
            -0.0025,
            0.1514,
            -0.0289,
            0.0282,
            -0.0935,
            -0.1186,
            -0.162,
            -0.0069,
            0.0721,
            0.162,
            0.0132,
            -0.1049,
            -0.0154,
            -0.034,
            -0.186
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.434,
            -0.7384,
            0.5067,
            -0.1572,
            0.1556,
            0.4554,
            -1.1163,
            -0.1179,
            0.6479,
            0.1555,
            0.1916,
            0.3256,
            -0.4694,
            0.2306,
            0.0508,
            -0.4706
          ],
          "after": [
            0.029,
            0.1024,
            -0.0365,
            -0.0772,
            0.0361,
            -0.0072,
            0.0871,
            0.0978,
            -0.0615,
            -0.0581,
            0.0183,
            -0.0123,
            0.1009,
            0.0331,
            -0.0529,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            -0.0767,
            0.0483,
            -0.116,
            0.0324,
            0.0193,
            -0.0484,
            0.1762,
            0.005,
            -0.0293,
            -0.0099,
            -0.0532,
            0.0047,
            0.0509,
            -0.0057,
            -0.0663,
            0.0086
          ],
          "after": [
            0.0427,
            -0.1461,
            -0.2958,
            0.0514,
            0.0326,
            -0.0484,
            -0.1509,
            0.0676,
            0.1107,
            0.0081,
            0.0138,
            0.087,
            0.0394,
            0.0207,
            -0.1305,
            0.0511
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0029,
            -0.0021,
            0.0006,
            -0.001,
            0.0013,
            -0.0016,
            -0.002,
            -0.0003,
            -0.0001,
            0.0007,
            0.0021,
            -0.0003,
            -0.0001,
            0.0002,
            -0.0038,
            -0.0006
          ],
          "after": [
            -0.0829,
            0.0108,
            -0.1193,
            -0.0703,
            0.0642,
            0.137,
            0.0853,
            -0.1638,
            0.0303,
            -0.0213,
            -0.1437,
            -0.0655,
            0.0911,
            -0.0797,
            0.0398,
            0.0836
          ]
        }
      }
    },
    {
      "step": 868,
      "word": "samari",
      "loss": 1.913,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1112,
            0.0061,
            0.1123,
            -0.0161,
            0.323,
            0.0767,
            0.3907,
            -0.0586,
            0.2047,
            -0.0346,
            0.0284,
            0.2372,
            -0.3255,
            -0.2862,
            -0.0469,
            0.2183
          ],
          "after": [
            0.0104,
            -0.0026,
            0.1514,
            -0.029,
            0.0282,
            -0.0935,
            -0.1188,
            -0.1619,
            -0.0069,
            0.0723,
            0.1621,
            0.0131,
            -0.1049,
            -0.0153,
            -0.0341,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0489,
            0.0671,
            -0.129,
            0.0304,
            0.0537,
            -0.1411,
            0.2088,
            0.0018,
            -0.1769,
            0.0218,
            -0.0335,
            0.037,
            0.0494,
            -0.082,
            -0.1609,
            0.1198
          ],
          "after": [
            0.0289,
            0.1025,
            -0.0366,
            -0.0773,
            0.0359,
            -0.0072,
            0.0871,
            0.0978,
            -0.0615,
            -0.0582,
            0.0183,
            -0.0125,
            0.101,
            0.0332,
            -0.0527,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            -0.0065,
            0.0956,
            -0.0354,
            0.0947,
            -0.0965,
            0.0444,
            0.0093,
            0.0317,
            -0.0681,
            0.0499,
            -0.1042,
            0.1,
            0.0731,
            -0.0178,
            0.057,
            0.0109
          ],
          "after": [
            0.0427,
            -0.1463,
            -0.2956,
            0.0512,
            0.0327,
            -0.0483,
            -0.151,
            0.0677,
            0.1108,
            0.0082,
            0.0138,
            0.0868,
            0.0394,
            0.0207,
            -0.1306,
            0.051
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0009,
            -0.0,
            0.0017,
            -0.0004,
            0.0001,
            0.001,
            0.0005,
            -0.0003,
            -0.0001,
            -0.0006,
            0.0009,
            0.0004,
            -0.001,
            -0.0004,
            0.0006
          ],
          "after": [
            -0.0832,
            0.0108,
            -0.1193,
            -0.0705,
            0.0643,
            0.137,
            0.0854,
            -0.1636,
            0.0303,
            -0.0214,
            -0.1439,
            -0.0655,
            0.0911,
            -0.0797,
            0.04,
            0.0837
          ]
        }
      }
    },
    {
      "step": 869,
      "word": "fanuel",
      "loss": 2.4749,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0458,
            0.1644,
            -0.2269,
            -0.0192,
            -0.2698,
            -0.0651,
            -0.0822,
            -0.0905,
            0.0961,
            -0.1808,
            -0.1116,
            -0.0999,
            0.0644,
            0.1829,
            0.1014,
            0.116
          ],
          "after": [
            0.0104,
            -0.0028,
            0.1515,
            -0.029,
            0.0283,
            -0.0934,
            -0.1189,
            -0.1617,
            -0.007,
            0.0725,
            0.1622,
            0.0131,
            -0.1049,
            -0.0153,
            -0.0342,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2254,
            0.0175,
            -0.0105,
            0.1874,
            -0.1725,
            0.2556,
            0.2026,
            -0.049,
            -0.0515,
            -0.21,
            -0.2319,
            0.0913,
            -0.0663,
            -0.0402,
            0.1427,
            0.1048
          ],
          "after": [
            0.0289,
            0.1026,
            -0.0366,
            -0.0775,
            0.0359,
            -0.0072,
            0.0871,
            0.0978,
            -0.0615,
            -0.0583,
            0.0184,
            -0.0127,
            0.1011,
            0.0332,
            -0.0526,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            0.0262,
            0.1711,
            -0.0678,
            0.1559,
            0.0157,
            -0.2307,
            0.1602,
            -0.0156,
            -0.2584,
            -0.1162,
            0.1011,
            -0.057,
            0.2225,
            -0.0051,
            0.0057,
            0.1716
          ],
          "after": [
            0.0427,
            -0.1465,
            -0.2955,
            0.051,
            0.0328,
            -0.0481,
            -0.1511,
            0.0677,
            0.111,
            0.0083,
            0.0138,
            0.0867,
            0.0392,
            0.0207,
            -0.1307,
            0.0509
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0004,
            0.002,
            -0.0002,
            0.0006,
            -0.0005,
            -0.0017,
            -0.0011,
            -0.0,
            -0.0017,
            0.0001,
            -0.0008,
            -0.0005,
            0.0001,
            -0.0007,
            -0.0009
          ],
          "after": [
            -0.0834,
            0.0107,
            -0.1194,
            -0.0706,
            0.0644,
            0.137,
            0.0854,
            -0.1635,
            0.0304,
            -0.0215,
            -0.144,
            -0.0654,
            0.0912,
            -0.0797,
            0.0401,
            0.0837
          ]
        }
      }
    },
    {
      "step": 870,
      "word": "dominik",
      "loss": 2.6928,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0105,
            -0.0029,
            0.1516,
            -0.0291,
            0.0284,
            -0.0934,
            -0.1191,
            -0.1615,
            -0.0071,
            0.0727,
            0.1623,
            0.0131,
            -0.1049,
            -0.0154,
            -0.0343,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0923,
            0.0885,
            -0.1052,
            0.0474,
            0.1184,
            -0.1458,
            0.2512,
            0.0373,
            -0.1928,
            0.0318,
            -0.0519,
            0.0172,
            0.0779,
            -0.0687,
            -0.2139,
            0.1437
          ],
          "after": [
            0.029,
            0.1027,
            -0.0366,
            -0.0776,
            0.0358,
            -0.0072,
            0.0871,
            0.0978,
            -0.0614,
            -0.0583,
            0.0185,
            -0.0128,
            0.1011,
            0.0333,
            -0.0525,
            0.2182
          ]
        },
        "position_0": {
          "grad": [
            -0.069,
            0.0515,
            -0.0325,
            -0.0568,
            0.0953,
            -0.0366,
            0.0696,
            -0.0632,
            -0.0253,
            -0.0304,
            -0.0315,
            0.0202,
            0.0407,
            0.0298,
            0.0349,
            0.134
          ],
          "after": [
            0.0428,
            -0.1468,
            -0.2953,
            0.0509,
            0.0328,
            -0.048,
            -0.1512,
            0.0678,
            0.1112,
            0.0085,
            0.0138,
            0.0867,
            0.0391,
            0.0206,
            -0.1309,
            0.0507
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0011,
            0.0022,
            -0.0051,
            -0.0003,
            -0.0023,
            -0.002,
            -0.0016,
            0.0011,
            -0.0015,
            0.0017,
            -0.0024,
            -0.0011,
            0.0024,
            0.0009,
            -0.0007
          ],
          "after": [
            -0.0836,
            0.0107,
            -0.1196,
            -0.0705,
            0.0644,
            0.1371,
            0.0855,
            -0.1632,
            0.0304,
            -0.0215,
            -0.1442,
            -0.0653,
            0.0912,
            -0.0797,
            0.0403,
            0.0838
          ]
        }
      }
    },
    {
      "step": 871,
      "word": "ileen",
      "loss": 2.1025,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0105,
            -0.0031,
            0.1516,
            -0.0291,
            0.0285,
            -0.0933,
            -0.1192,
            -0.1614,
            -0.0072,
            0.0728,
            0.1624,
            0.0131,
            -0.1049,
            -0.0154,
            -0.0344,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1657,
            -0.3353,
            0.1361,
            -0.152,
            0.2048,
            0.615,
            -0.4493,
            0.1201,
            0.4704,
            -0.1317,
            -0.1514,
            0.4022,
            -0.4828,
            0.04,
            0.8146,
            -0.4516
          ],
          "after": [
            0.0291,
            0.1028,
            -0.0366,
            -0.0777,
            0.0357,
            -0.0074,
            0.0871,
            0.0977,
            -0.0615,
            -0.0582,
            0.0187,
            -0.0131,
            0.1013,
            0.0334,
            -0.0526,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            -0.0699,
            -0.278,
            0.1216,
            -0.1914,
            0.1312,
            0.1513,
            -0.2882,
            0.0394,
            0.1092,
            0.0018,
            0.0731,
            -0.1561,
            -0.2194,
            0.0155,
            -0.0064,
            -0.1954
          ],
          "after": [
            0.0429,
            -0.1469,
            -0.2952,
            0.0509,
            0.0328,
            -0.0479,
            -0.1513,
            0.0678,
            0.1113,
            0.0086,
            0.0137,
            0.0867,
            0.0391,
            0.0206,
            -0.131,
            0.0507
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0015,
            -0.0014,
            0.0001,
            -0.0005,
            0.0014,
            0.0011,
            -0.0006,
            0.0015,
            0.0003,
            0.0002,
            -0.0009,
            0.0028,
            -0.0019,
            -0.0011,
            0.0013,
            -0.001
          ],
          "after": [
            -0.0836,
            0.0108,
            -0.1197,
            -0.0705,
            0.0644,
            0.1371,
            0.0856,
            -0.1631,
            0.0304,
            -0.0215,
            -0.1444,
            -0.0653,
            0.0913,
            -0.0798,
            0.0403,
            0.0839
          ]
        }
      }
    },
    {
      "step": 872,
      "word": "joon",
      "loss": 2.5182,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0105,
            -0.0032,
            0.1517,
            -0.0291,
            0.0286,
            -0.0933,
            -0.1193,
            -0.1613,
            -0.0072,
            0.073,
            0.1625,
            0.0131,
            -0.1048,
            -0.0154,
            -0.0345,
            -0.1869
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0591,
            0.0278,
            -0.1407,
            -0.0067,
            -0.0024,
            -0.1051,
            0.2205,
            0.072,
            -0.1821,
            0.0402,
            -0.0481,
            -0.0184,
            0.1153,
            -0.0595,
            -0.0932,
            0.1037
          ],
          "after": [
            0.0292,
            0.1029,
            -0.0366,
            -0.0777,
            0.0356,
            -0.0075,
            0.0871,
            0.0976,
            -0.0615,
            -0.0582,
            0.0189,
            -0.0134,
            0.1015,
            0.0335,
            -0.0526,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            -0.0008,
            0.0115,
            -0.0049,
            0.1823,
            -0.08,
            -0.1472,
            0.1675,
            0.0936,
            -0.2337,
            -0.1768,
            0.0006,
            -0.0458,
            0.3618,
            -0.0564,
            -0.0461,
            0.1774
          ],
          "after": [
            0.0431,
            -0.1469,
            -0.2952,
            0.0508,
            0.0328,
            -0.0478,
            -0.1513,
            0.0677,
            0.1115,
            0.0089,
            0.0137,
            0.0867,
            0.039,
            0.0206,
            -0.131,
            0.0506
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            0.0013,
            -0.0012,
            0.0018,
            0.0006,
            0.0013,
            0.0003,
            -0.0005,
            0.0005,
            0.0009,
            -0.0005,
            0.0024,
            -0.0022,
            -0.0017,
            0.0016,
            0.0021
          ],
          "after": [
            -0.0837,
            0.0108,
            -0.1198,
            -0.0705,
            0.0644,
            0.1371,
            0.0857,
            -0.163,
            0.0304,
            -0.0216,
            -0.1445,
            -0.0654,
            0.0915,
            -0.0797,
            0.0403,
            0.0839
          ]
        }
      }
    },
    {
      "step": 873,
      "word": "courtlyn",
      "loss": 2.8008,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0105,
            -0.0033,
            0.1517,
            -0.0291,
            0.0286,
            -0.0933,
            -0.1193,
            -0.1612,
            -0.0072,
            0.0731,
            0.1626,
            0.0131,
            -0.1048,
            -0.0154,
            -0.0345,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0607,
            0.1217,
            -0.1184,
            0.0622,
            0.0487,
            -0.1556,
            0.256,
            0.006,
            -0.1784,
            0.0034,
            -0.0033,
            0.0062,
            0.0925,
            -0.1233,
            -0.1566,
            0.1726
          ],
          "after": [
            0.0293,
            0.103,
            -0.0365,
            -0.0778,
            0.0354,
            -0.0075,
            0.087,
            0.0974,
            -0.0615,
            -0.0582,
            0.019,
            -0.0136,
            0.1016,
            0.0336,
            -0.0526,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            -0.0372,
            0.0167,
            -0.0224,
            0.0854,
            -0.0296,
            -0.0065,
            0.1043,
            0.0486,
            -0.0629,
            0.0131,
            0.039,
            0.0197,
            0.0352,
            -0.0067,
            -0.0298,
            0.0645
          ],
          "after": [
            0.0432,
            -0.147,
            -0.2952,
            0.0506,
            0.0328,
            -0.0476,
            -0.1514,
            0.0677,
            0.1117,
            0.0091,
            0.0136,
            0.0868,
            0.0388,
            0.0207,
            -0.131,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0017,
            0.0013,
            0.0004,
            0.0015,
            0.0012,
            0.0005,
            0.0022,
            -0.0004,
            -0.0001,
            -0.0007,
            0.0001,
            0.0011,
            0.0004,
            -0.0016,
            -0.0004,
            0.0012
          ],
          "after": [
            -0.0836,
            0.0107,
            -0.1199,
            -0.0705,
            0.0644,
            0.1371,
            0.0858,
            -0.1629,
            0.0304,
            -0.0216,
            -0.1445,
            -0.0654,
            0.0916,
            -0.0796,
            0.0404,
            0.0839
          ]
        }
      }
    },
    {
      "step": 874,
      "word": "oceanna",
      "loss": 2.3643,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0983,
            0.1205,
            -0.2005,
            -0.0593,
            -0.1463,
            -0.1734,
            0.0333,
            -0.113,
            -0.0137,
            0.0268,
            0.0329,
            -0.0586,
            -0.0144,
            0.0544,
            0.0596,
            -0.048
          ],
          "after": [
            0.0105,
            -0.0034,
            0.1518,
            -0.0291,
            0.0287,
            -0.0931,
            -0.1194,
            -0.161,
            -0.0073,
            0.0732,
            0.1626,
            0.0131,
            -0.1048,
            -0.0155,
            -0.0346,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2448,
            -0.0641,
            0.3581,
            0.1052,
            0.1403,
            0.1648,
            -0.0711,
            0.0288,
            0.259,
            0.0915,
            0.0653,
            0.3078,
            -0.2865,
            -0.0497,
            0.0244,
            -0.1645
          ],
          "after": [
            0.0295,
            0.1031,
            -0.0366,
            -0.0779,
            0.0353,
            -0.0076,
            0.087,
            0.0973,
            -0.0615,
            -0.0582,
            0.0191,
            -0.0139,
            0.1017,
            0.0337,
            -0.0527,
            0.2183
          ]
        },
        "position_0": {
          "grad": [
            0.0909,
            -0.0775,
            0.0894,
            -0.1058,
            0.0674,
            -0.0259,
            -0.2362,
            -0.0549,
            0.0317,
            0.1353,
            0.0615,
            0.0251,
            -0.0994,
            -0.0044,
            -0.06,
            -0.202
          ],
          "after": [
            0.0432,
            -0.147,
            -0.2952,
            0.0506,
            0.0327,
            -0.0475,
            -0.1514,
            0.0676,
            0.1119,
            0.0092,
            0.0136,
            0.0868,
            0.0387,
            0.0207,
            -0.131,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            0.0005,
            0.0003,
            0.0004,
            -0.0001,
            -0.0002,
            -0.0018,
            -0.0014,
            -0.0,
            0.0,
            0.0,
            0.0001,
            -0.0014,
            0.0008,
            0.0005,
            -0.0005
          ],
          "after": [
            -0.0836,
            0.0107,
            -0.12,
            -0.0706,
            0.0643,
            0.1371,
            0.0858,
            -0.1627,
            0.0303,
            -0.0216,
            -0.1446,
            -0.0655,
            0.0918,
            -0.0796,
            0.0404,
            0.0839
          ]
        }
      }
    },
    {
      "step": 875,
      "word": "kenzee",
      "loss": 2.3905,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0105,
            -0.0035,
            0.1519,
            -0.0291,
            0.0288,
            -0.093,
            -0.1195,
            -0.1609,
            -0.0073,
            0.0733,
            0.1626,
            0.0131,
            -0.1048,
            -0.0155,
            -0.0347,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.5701,
            -0.6337,
            0.3355,
            -0.2027,
            -0.32,
            0.9047,
            -1.2543,
            -0.247,
            1.0254,
            0.0918,
            0.1501,
            0.0982,
            -0.4462,
            0.4838,
            0.8065,
            -0.7493
          ],
          "after": [
            0.0295,
            0.1033,
            -0.0367,
            -0.0779,
            0.0353,
            -0.0079,
            0.0872,
            0.0974,
            -0.0617,
            -0.0583,
            0.0191,
            -0.0141,
            0.102,
            0.0336,
            -0.0529,
            0.2185
          ]
        },
        "position_0": {
          "grad": [
            -0.0752,
            0.0398,
            -0.1146,
            0.0161,
            0.0205,
            -0.0517,
            0.1522,
            0.0044,
            -0.0127,
            -0.0152,
            -0.0674,
            0.0039,
            0.0323,
            -0.0053,
            -0.062,
            -0.0071
          ],
          "after": [
            0.0433,
            -0.147,
            -0.2951,
            0.0505,
            0.0327,
            -0.0474,
            -0.1514,
            0.0676,
            0.112,
            0.0092,
            0.0135,
            0.0868,
            0.0387,
            0.0207,
            -0.131,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0021,
            -0.001,
            -0.0003,
            0.0003,
            0.0005,
            0.0003,
            0.0004,
            0.0,
            0.001,
            0.0028,
            0.0009,
            0.0002,
            0.001,
            0.0004,
            -0.0025,
            -0.0007
          ],
          "after": [
            -0.0836,
            0.0107,
            -0.12,
            -0.0707,
            0.0643,
            0.1371,
            0.0859,
            -0.1626,
            0.0303,
            -0.0217,
            -0.1447,
            -0.0656,
            0.0919,
            -0.0795,
            0.0404,
            0.0839
          ]
        }
      }
    },
    {
      "step": 876,
      "word": "nahom",
      "loss": 2.5355,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0518,
            -0.0796,
            0.137,
            0.081,
            -0.0646,
            0.157,
            -0.0634,
            0.1682,
            0.1292,
            0.1811,
            -0.0595,
            -0.3058,
            0.3528,
            0.207,
            -0.0388,
            0.0747
          ],
          "after": [
            0.0105,
            -0.0035,
            0.1519,
            -0.0292,
            0.0289,
            -0.093,
            -0.1195,
            -0.1609,
            -0.0074,
            0.0732,
            0.1627,
            0.0132,
            -0.1049,
            -0.0156,
            -0.0348,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0651,
            0.0486,
            -0.1153,
            0.0689,
            0.0537,
            -0.1167,
            0.2342,
            0.0474,
            -0.1565,
            0.0585,
            -0.0514,
            0.039,
            0.0747,
            -0.0705,
            -0.1808,
            0.127
          ],
          "after": [
            0.0295,
            0.1035,
            -0.0368,
            -0.0779,
            0.0353,
            -0.0081,
            0.0873,
            0.0974,
            -0.0619,
            -0.0584,
            0.0191,
            -0.0144,
            0.1022,
            0.0336,
            -0.053,
            0.2187
          ]
        },
        "position_0": {
          "grad": [
            -0.0274,
            0.1487,
            -0.0936,
            0.0581,
            -0.3728,
            -0.0056,
            -0.0036,
            -0.0273,
            0.0908,
            -0.2115,
            -0.2581,
            -0.0815,
            -0.05,
            0.1386,
            0.2398,
            0.1338
          ],
          "after": [
            0.0434,
            -0.1471,
            -0.295,
            0.0504,
            0.0329,
            -0.0473,
            -0.1515,
            0.0676,
            0.1121,
            0.0094,
            0.0136,
            0.0869,
            0.0386,
            0.0206,
            -0.1311,
            0.0503
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            -0.0017,
            0.0003,
            -0.0019,
            -0.003,
            0.0016,
            -0.0013,
            0.0001,
            0.0016,
            0.0012,
            0.0022,
            -0.0013,
            -0.0017,
            0.0006,
            0.002,
            -0.0011
          ],
          "after": [
            -0.0837,
            0.0107,
            -0.1201,
            -0.0707,
            0.0644,
            0.137,
            0.0859,
            -0.1625,
            0.0302,
            -0.0218,
            -0.1449,
            -0.0656,
            0.0921,
            -0.0795,
            0.0404,
            0.084
          ]
        }
      }
    },
    {
      "step": 877,
      "word": "kiaya",
      "loss": 2.1249,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3901,
            -0.3686,
            -0.0548,
            -0.2512,
            0.0376,
            0.1079,
            -0.2024,
            0.139,
            -0.1397,
            0.29,
            0.2265,
            -0.1863,
            0.0932,
            0.154,
            0.0033,
            -0.2787
          ],
          "after": [
            0.0103,
            -0.0034,
            0.152,
            -0.0291,
            0.0289,
            -0.0931,
            -0.1195,
            -0.161,
            -0.0074,
            0.0731,
            0.1626,
            0.0133,
            -0.1051,
            -0.0158,
            -0.0348,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0442,
            0.0383,
            -0.1111,
            -0.0099,
            0.0216,
            -0.1364,
            0.1974,
            0.0273,
            -0.1854,
            0.0492,
            -0.0322,
            -0.0747,
            0.094,
            -0.0524,
            -0.0994,
            0.1255
          ],
          "after": [
            0.0295,
            0.1036,
            -0.0368,
            -0.0779,
            0.0352,
            -0.0083,
            0.0873,
            0.0974,
            -0.062,
            -0.0585,
            0.0192,
            -0.0146,
            0.1023,
            0.0336,
            -0.0531,
            0.2188
          ]
        },
        "position_0": {
          "grad": [
            -0.0804,
            0.0475,
            -0.1282,
            -0.0524,
            0.048,
            -0.0739,
            0.118,
            -0.0297,
            -0.0026,
            0.0324,
            -0.0447,
            -0.0272,
            0.0307,
            -0.0291,
            -0.1365,
            -0.0529
          ],
          "after": [
            0.0436,
            -0.1472,
            -0.2948,
            0.0504,
            0.033,
            -0.0471,
            -0.1516,
            0.0676,
            0.1122,
            0.0096,
            0.0138,
            0.0869,
            0.0386,
            0.0205,
            -0.1311,
            0.0503
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0007,
            0.0009,
            0.006,
            -0.0072,
            0.0024,
            -0.0094,
            0.0019,
            0.0015,
            -0.0037,
            0.0006,
            0.009,
            -0.0063,
            -0.0015,
            0.0058,
            -0.0001
          ],
          "after": [
            -0.0837,
            0.0107,
            -0.1202,
            -0.0708,
            0.0646,
            0.1368,
            0.0862,
            -0.1625,
            0.03,
            -0.0218,
            -0.145,
            -0.0659,
            0.0924,
            -0.0795,
            0.0403,
            0.084
          ]
        }
      }
    },
    {
      "step": 878,
      "word": "jethro",
      "loss": 2.9344,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0102,
            -0.0034,
            0.152,
            -0.029,
            0.029,
            -0.0931,
            -0.1194,
            -0.161,
            -0.0074,
            0.073,
            0.1626,
            0.0135,
            -0.1052,
            -0.0159,
            -0.0348,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2271,
            -0.169,
            0.3344,
            0.0719,
            0.2214,
            0.1067,
            -0.4536,
            -0.163,
            0.4846,
            0.1918,
            0.254,
            0.1448,
            -0.2936,
            -0.0217,
            -0.1717,
            -0.2616
          ],
          "after": [
            0.0294,
            0.1038,
            -0.0369,
            -0.0779,
            0.0351,
            -0.0084,
            0.0875,
            0.0976,
            -0.0622,
            -0.0586,
            0.0191,
            -0.0148,
            0.1025,
            0.0335,
            -0.0531,
            0.2189
          ]
        },
        "position_0": {
          "grad": [
            0.0069,
            0.0191,
            -0.0133,
            0.1327,
            -0.0536,
            -0.1037,
            0.1417,
            0.0723,
            -0.1642,
            -0.1014,
            -0.0145,
            -0.0145,
            0.2527,
            -0.0543,
            -0.0442,
            0.1307
          ],
          "after": [
            0.0437,
            -0.1473,
            -0.2946,
            0.0503,
            0.0331,
            -0.047,
            -0.1517,
            0.0676,
            0.1123,
            0.0098,
            0.0139,
            0.087,
            0.0384,
            0.0205,
            -0.131,
            0.0502
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0001,
            -0.0012,
            0.0003,
            0.0,
            0.0005,
            -0.0003,
            -0.0003,
            -0.0001,
            -0.0001,
            -0.0012,
            0.0023,
            -0.0026,
            -0.0012,
            0.0022,
            0.0008
          ],
          "after": [
            -0.0838,
            0.0108,
            -0.1202,
            -0.071,
            0.0648,
            0.1367,
            0.0864,
            -0.1624,
            0.0298,
            -0.0218,
            -0.1451,
            -0.0661,
            0.0927,
            -0.0793,
            0.0401,
            0.084
          ]
        }
      }
    },
    {
      "step": 879,
      "word": "angelyne",
      "loss": 2.3685,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0167,
            0.1748,
            -0.1882,
            0.0563,
            -0.3326,
            0.0173,
            -0.0304,
            -0.1164,
            -0.011,
            -0.1692,
            -0.1144,
            -0.0951,
            0.0555,
            0.1434,
            0.1515,
            0.11
          ],
          "after": [
            0.0101,
            -0.0034,
            0.1521,
            -0.0289,
            0.0291,
            -0.0932,
            -0.1194,
            -0.161,
            -0.0074,
            0.073,
            0.1626,
            0.0136,
            -0.1053,
            -0.0161,
            -0.0349,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2368,
            -0.1286,
            0.1391,
            -0.021,
            -0.3468,
            0.1459,
            -0.4084,
            -0.0563,
            0.2381,
            -0.169,
            -0.0561,
            -0.4503,
            0.0893,
            0.2237,
            0.312,
            -0.3856
          ],
          "after": [
            0.0292,
            0.104,
            -0.0371,
            -0.078,
            0.0352,
            -0.0086,
            0.0876,
            0.0977,
            -0.0624,
            -0.0587,
            0.019,
            -0.0148,
            0.1027,
            0.0334,
            -0.0532,
            0.2192
          ]
        },
        "position_0": {
          "grad": [
            0.0612,
            -0.0223,
            0.0727,
            -0.0887,
            0.0473,
            0.0107,
            -0.2119,
            -0.0876,
            0.0951,
            0.0333,
            0.1624,
            0.055,
            -0.1267,
            0.0134,
            -0.0484,
            -0.1534
          ],
          "after": [
            0.0438,
            -0.1474,
            -0.2945,
            0.0502,
            0.0332,
            -0.0468,
            -0.1517,
            0.0676,
            0.1124,
            0.0099,
            0.0139,
            0.087,
            0.0384,
            0.0205,
            -0.131,
            0.0502
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0014,
            0.0012,
            0.0015,
            -0.0003,
            0.0008,
            -0.0005,
            -0.0017,
            -0.0019,
            0.0004,
            0.0022,
            -0.0001,
            -0.0005,
            -0.0012,
            -0.0009,
            0.0006
          ],
          "after": [
            -0.0838,
            0.0107,
            -0.1203,
            -0.0712,
            0.065,
            0.1365,
            0.0866,
            -0.1624,
            0.0298,
            -0.0218,
            -0.1453,
            -0.0664,
            0.093,
            -0.0792,
            0.04,
            0.084
          ]
        }
      }
    },
    {
      "step": 880,
      "word": "addley",
      "loss": 2.5872,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1714,
            0.1027,
            0.0571,
            -0.0579,
            0.2072,
            -0.1401,
            0.1629,
            -0.1458,
            -0.1013,
            -0.023,
            0.0277,
            0.0599,
            0.1044,
            -0.0073,
            0.0102,
            0.1506
          ],
          "after": [
            0.0101,
            -0.0034,
            0.1522,
            -0.0289,
            0.0292,
            -0.0931,
            -0.1194,
            -0.1609,
            -0.0073,
            0.0729,
            0.1626,
            0.0137,
            -0.1054,
            -0.0162,
            -0.035,
            -0.1874
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1737,
            -0.3533,
            0.0936,
            -0.0501,
            0.0636,
            0.1471,
            -0.4006,
            0.0823,
            0.0652,
            0.0607,
            -0.0916,
            -0.046,
            -0.0179,
            0.2174,
            -0.0006,
            -0.2702
          ],
          "after": [
            0.0291,
            0.1042,
            -0.0372,
            -0.078,
            0.0352,
            -0.0088,
            0.0878,
            0.0978,
            -0.0625,
            -0.0587,
            0.019,
            -0.0148,
            0.1028,
            0.0333,
            -0.0533,
            0.2194
          ]
        },
        "position_0": {
          "grad": [
            0.0783,
            -0.0192,
            0.0903,
            -0.1209,
            0.0956,
            0.0353,
            -0.2166,
            -0.1095,
            0.0784,
            0.0653,
            0.2346,
            0.0968,
            -0.1204,
            -0.0095,
            -0.0904,
            -0.1697
          ],
          "after": [
            0.0438,
            -0.1474,
            -0.2945,
            0.0503,
            0.0332,
            -0.0467,
            -0.1516,
            0.0678,
            0.1124,
            0.01,
            0.0138,
            0.087,
            0.0383,
            0.0205,
            -0.1309,
            0.0503
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0088,
            0.0083,
            -0.0028,
            0.007,
            0.0021,
            -0.0037,
            0.0079,
            0.0012,
            -0.0035,
            -0.0024,
            -0.0032,
            -0.0042,
            0.0077,
            0.002,
            -0.0062,
            0.001
          ],
          "after": [
            -0.0841,
            0.0104,
            -0.1202,
            -0.0715,
            0.0651,
            0.1365,
            0.0866,
            -0.1623,
            0.0299,
            -0.0217,
            -0.1453,
            -0.0664,
            0.0931,
            -0.0792,
            0.04,
            0.084
          ]
        }
      }
    },
    {
      "step": 881,
      "word": "emanii",
      "loss": 2.2775,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1144,
            0.1463,
            -0.2148,
            0.0433,
            -0.2812,
            -0.0599,
            -0.0946,
            -0.1205,
            0.055,
            -0.0365,
            -0.1137,
            -0.0816,
            0.0201,
            0.1476,
            0.0218,
            0.0729
          ],
          "after": [
            0.01,
            -0.0035,
            0.1523,
            -0.0288,
            0.0293,
            -0.0931,
            -0.1194,
            -0.1608,
            -0.0073,
            0.0729,
            0.1626,
            0.0138,
            -0.1056,
            -0.0164,
            -0.0351,
            -0.1874
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3346,
            0.3881,
            0.5183,
            0.0055,
            0.1597,
            -0.0681,
            0.3341,
            -0.0214,
            0.0329,
            -0.2183,
            -0.0091,
            0.0307,
            -0.1044,
            0.008,
            -0.0651,
            -0.0835
          ],
          "after": [
            0.029,
            0.1043,
            -0.0374,
            -0.078,
            0.0351,
            -0.0089,
            0.0879,
            0.0979,
            -0.0627,
            -0.0587,
            0.019,
            -0.0148,
            0.103,
            0.0331,
            -0.0533,
            0.2197
          ]
        },
        "position_0": {
          "grad": [
            0.001,
            -0.1451,
            0.0636,
            -0.0329,
            0.0259,
            0.0614,
            -0.2289,
            -0.0956,
            0.0985,
            0.137,
            0.0086,
            -0.0184,
            -0.1235,
            -0.0179,
            -0.0255,
            -0.2659
          ],
          "after": [
            0.0437,
            -0.1474,
            -0.2945,
            0.0503,
            0.0332,
            -0.0467,
            -0.1515,
            0.0679,
            0.1123,
            0.01,
            0.0137,
            0.087,
            0.0384,
            0.0205,
            -0.1308,
            0.0504
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0022,
            0.0003,
            0.0021,
            -0.002,
            0.002,
            0.0011,
            0.0005,
            -0.0014,
            0.0009,
            -0.0004,
            0.0005,
            -0.0002,
            0.0002,
            0.0003,
            -0.0021,
            0.0005
          ],
          "after": [
            -0.0842,
            0.0101,
            -0.1202,
            -0.0717,
            0.0651,
            0.1365,
            0.0866,
            -0.1623,
            0.03,
            -0.0216,
            -0.1453,
            -0.0665,
            0.0931,
            -0.0792,
            0.0401,
            0.0839
          ]
        }
      }
    },
    {
      "step": 882,
      "word": "edelyn",
      "loss": 1.9566,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0099,
            -0.0036,
            0.1524,
            -0.0288,
            0.0294,
            -0.093,
            -0.1193,
            -0.1607,
            -0.0073,
            0.0729,
            0.1627,
            0.0138,
            -0.1057,
            -0.0165,
            -0.0352,
            -0.1875
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3475,
            0.1572,
            0.7929,
            -0.0103,
            0.0873,
            0.3278,
            -0.0453,
            -0.1078,
            0.428,
            -0.1229,
            0.1831,
            0.2533,
            -0.4504,
            -0.0418,
            0.2779,
            -0.1929
          ],
          "after": [
            0.0291,
            0.1044,
            -0.0378,
            -0.078,
            0.0351,
            -0.0091,
            0.0881,
            0.098,
            -0.0629,
            -0.0587,
            0.0189,
            -0.0149,
            0.1032,
            0.033,
            -0.0534,
            0.2199
          ]
        },
        "position_0": {
          "grad": [
            -0.0094,
            -0.1565,
            0.0643,
            0.0026,
            0.0132,
            0.073,
            -0.2028,
            -0.0788,
            0.0924,
            0.1112,
            0.0139,
            -0.0096,
            -0.1248,
            -0.0112,
            0.014,
            -0.2531
          ],
          "after": [
            0.0437,
            -0.1473,
            -0.2946,
            0.0503,
            0.0332,
            -0.0467,
            -0.1514,
            0.0681,
            0.1123,
            0.0099,
            0.0136,
            0.0869,
            0.0384,
            0.0206,
            -0.1307,
            0.0506
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0018,
            0.0018,
            0.0006,
            0.0014,
            0.0009,
            0.0003,
            0.0005,
            -0.0002,
            0.0024,
            0.0013,
            0.0009,
            0.0002,
            -0.0015,
            -0.0026,
            0.0008
          ],
          "after": [
            -0.0843,
            0.01,
            -0.1203,
            -0.0719,
            0.0651,
            0.1364,
            0.0866,
            -0.1622,
            0.0301,
            -0.0216,
            -0.1454,
            -0.0666,
            0.0931,
            -0.0791,
            0.0403,
            0.0839
          ]
        }
      }
    },
    {
      "step": 883,
      "word": "zaia",
      "loss": 2.2774,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            0.507,
            -0.5945,
            -0.4106,
            -0.297,
            -0.1739,
            0.0948,
            -0.4676,
            0.2067,
            -0.1645,
            0.252,
            0.2884,
            -0.8666,
            0.0011,
            0.3084,
            0.1969,
            -0.448
          ],
          "after": [
            0.0097,
            -0.0034,
            0.1526,
            -0.0287,
            0.0296,
            -0.093,
            -0.1192,
            -0.1607,
            -0.0072,
            0.0728,
            0.1626,
            0.0141,
            -0.1058,
            -0.0168,
            -0.0353,
            -0.1874
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0027,
            0.027,
            -0.1606,
            0.0039,
            -0.0194,
            -0.1095,
            0.1335,
            -0.0035,
            -0.1775,
            0.0388,
            -0.0209,
            -0.0813,
            0.1237,
            -0.0207,
            -0.1438,
            0.0717
          ],
          "after": [
            0.0292,
            0.1044,
            -0.0381,
            -0.078,
            0.035,
            -0.0092,
            0.0881,
            0.0981,
            -0.0631,
            -0.0586,
            0.0188,
            -0.0149,
            0.1034,
            0.0329,
            -0.0535,
            0.2201
          ]
        },
        "position_0": {
          "grad": [
            0.0474,
            0.0361,
            -0.0434,
            0.1138,
            0.0588,
            -0.1109,
            0.1549,
            -0.0056,
            0.0798,
            0.028,
            0.0294,
            -0.0049,
            -0.0189,
            -0.0537,
            0.0296,
            -0.0018
          ],
          "after": [
            0.0437,
            -0.1472,
            -0.2946,
            0.0503,
            0.0332,
            -0.0466,
            -0.1513,
            0.0683,
            0.1122,
            0.0098,
            0.0135,
            0.0869,
            0.0385,
            0.0206,
            -0.1307,
            0.0508
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0017,
            0.0003,
            0.0007,
            0.0003,
            0.0001,
            -0.0006,
            -0.0002,
            0.0005,
            0.0002,
            0.0013,
            0.0017,
            0.0025,
            -0.002,
            -0.0027,
            0.0013,
            -0.0004
          ],
          "after": [
            -0.0844,
            0.0098,
            -0.1204,
            -0.0721,
            0.0651,
            0.1364,
            0.0866,
            -0.1622,
            0.0301,
            -0.0217,
            -0.1455,
            -0.0667,
            0.0931,
            -0.0789,
            0.0404,
            0.0838
          ]
        }
      }
    },
    {
      "step": 884,
      "word": "lorcan",
      "loss": 2.1841,
      "learning_rate": 0.0004,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0602,
            0.1696,
            -0.0559,
            0.0682,
            -0.034,
            -0.1121,
            0.0031,
            -0.1139,
            0.0371,
            -0.2099,
            -0.0657,
            0.1331,
            -0.1236,
            -0.0356,
            0.0148,
            0.0865
          ],
          "after": [
            0.0096,
            -0.0034,
            0.1528,
            -0.0286,
            0.0297,
            -0.093,
            -0.119,
            -0.1606,
            -0.0072,
            0.0728,
            0.1625,
            0.0143,
            -0.1058,
            -0.017,
            -0.0355,
            -0.1874
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.006,
            0.175,
            -0.2169,
            0.0403,
            -0.0386,
            -0.174,
            0.3265,
            0.0038,
            -0.2479,
            0.0164,
            0.0085,
            -0.0441,
            0.1412,
            -0.1298,
            -0.1591,
            0.2178
          ],
          "after": [
            0.0292,
            0.1044,
            -0.0382,
            -0.078,
            0.035,
            -0.0093,
            0.0881,
            0.0982,
            -0.0632,
            -0.0586,
            0.0188,
            -0.015,
            0.1035,
            0.0329,
            -0.0535,
            0.2202
          ]
        },
        "position_0": {
          "grad": [
            -0.0902,
            0.0931,
            -0.1048,
            -0.0698,
            0.0436,
            0.0864,
            -0.0215,
            0.1092,
            -0.0378,
            -0.0216,
            -0.1646,
            0.2172,
            -0.0772,
            0.0178,
            0.093,
            -0.0956
          ],
          "after": [
            0.0437,
            -0.1472,
            -0.2946,
            0.0503,
            0.0332,
            -0.0466,
            -0.1512,
            0.0684,
            0.1121,
            0.0097,
            0.0135,
            0.0868,
            0.0386,
            0.0207,
            -0.1307,
            0.051
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            0.0049,
            -0.0004,
            0.0061,
            -0.0031,
            0.0013,
            0.0065,
            -0.0022,
            -0.0004,
            0.0032,
            -0.0003,
            0.0019,
            0.0022,
            -0.0014,
            -0.0045,
            0.0031
          ],
          "after": [
            -0.0845,
            0.0096,
            -0.1205,
            -0.0724,
            0.0651,
            0.1363,
            0.0864,
            -0.1621,
            0.0302,
            -0.0218,
            -0.1456,
            -0.0669,
            0.0931,
            -0.0787,
            0.0405,
            0.0837
          ]
        }
      }
    },
    {
      "step": 885,
      "word": "merissa",
      "loss": 2.2392,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0961,
            -0.0356,
            -0.0402,
            -0.0713,
            -0.0096,
            -0.0382,
            -0.0199,
            -0.0363,
            -0.0606,
            0.0602,
            0.0907,
            -0.0757,
            0.0204,
            0.0508,
            0.0004,
            -0.0843
          ],
          "after": [
            0.0094,
            -0.0033,
            0.153,
            -0.0284,
            0.0298,
            -0.0929,
            -0.1189,
            -0.1606,
            -0.0071,
            0.0728,
            0.1624,
            0.0145,
            -0.1058,
            -0.0171,
            -0.0356,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2611,
            -0.2726,
            0.3538,
            -0.135,
            0.2523,
            0.169,
            -0.3844,
            -0.0595,
            0.4397,
            0.1424,
            0.2187,
            0.1494,
            -0.3439,
            0.0022,
            -0.0458,
            -0.2385
          ],
          "after": [
            0.0292,
            0.1044,
            -0.0385,
            -0.0779,
            0.0349,
            -0.0094,
            0.0882,
            0.0984,
            -0.0633,
            -0.0586,
            0.0186,
            -0.015,
            0.1037,
            0.0329,
            -0.0535,
            0.2204
          ]
        },
        "position_0": {
          "grad": [
            -0.007,
            0.0214,
            -0.0411,
            0.002,
            0.1134,
            -0.0094,
            0.1805,
            0.0572,
            0.0339,
            0.0447,
            -0.0077,
            0.0295,
            -0.0673,
            -0.0512,
            -0.0186,
            0.0849
          ],
          "after": [
            0.0437,
            -0.1472,
            -0.2945,
            0.0503,
            0.033,
            -0.0466,
            -0.1512,
            0.0684,
            0.1121,
            0.0096,
            0.0135,
            0.0867,
            0.0387,
            0.0207,
            -0.1307,
            0.0511
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0013,
            0.0002,
            0.0021,
            0.0003,
            0.0009,
            0.0019,
            -0.0027,
            -0.0015,
            0.0006,
            0.0041,
            0.0008,
            -0.0052,
            0.0016,
            0.0029,
            -0.0002,
            -0.003
          ],
          "after": [
            -0.0846,
            0.0093,
            -0.1207,
            -0.0727,
            0.0652,
            0.1362,
            0.0864,
            -0.1619,
            0.0302,
            -0.022,
            -0.1457,
            -0.0669,
            0.0931,
            -0.0787,
            0.0407,
            0.0837
          ]
        }
      }
    },
    {
      "step": 886,
      "word": "riyah",
      "loss": 2.1758,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0657,
            -0.0563,
            0.2216,
            0.0533,
            0.0829,
            0.1055,
            0.0149,
            0.1573,
            0.1252,
            0.2145,
            -0.0234,
            -0.1352,
            0.262,
            0.0614,
            -0.1508,
            0.0169
          ],
          "after": [
            0.0093,
            -0.0032,
            0.1531,
            -0.0284,
            0.0299,
            -0.0929,
            -0.1187,
            -0.1606,
            -0.0071,
            0.0727,
            0.1624,
            0.0147,
            -0.1059,
            -0.0173,
            -0.0356,
            -0.1873
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0363,
            0.0422,
            -0.1206,
            0.0295,
            0.0269,
            -0.1105,
            0.2138,
            0.0073,
            -0.172,
            0.0441,
            -0.0377,
            -0.0338,
            0.0945,
            -0.0735,
            -0.1613,
            0.1128
          ],
          "after": [
            0.0292,
            0.1045,
            -0.0386,
            -0.0779,
            0.0348,
            -0.0095,
            0.0882,
            0.0985,
            -0.0634,
            -0.0587,
            0.0184,
            -0.0151,
            0.1038,
            0.0329,
            -0.0535,
            0.2205
          ]
        },
        "position_0": {
          "grad": [
            0.0185,
            -0.0505,
            0.0202,
            -0.0861,
            -0.0291,
            0.1504,
            0.026,
            -0.088,
            0.175,
            -0.0681,
            -0.0985,
            -0.0035,
            -0.2134,
            -0.0898,
            0.124,
            0.0727
          ],
          "after": [
            0.0437,
            -0.1472,
            -0.2945,
            0.0504,
            0.033,
            -0.0467,
            -0.1512,
            0.0685,
            0.1119,
            0.0096,
            0.0135,
            0.0866,
            0.0388,
            0.0209,
            -0.1308,
            0.0512
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            0.0009,
            0.0006,
            -0.0007,
            -0.0011,
            -0.0014,
            -0.0042,
            0.0005,
            0.0011,
            -0.0023,
            0.0008,
            0.0026,
            -0.0031,
            -0.0004,
            0.0034,
            -0.0015
          ],
          "after": [
            -0.0847,
            0.0091,
            -0.1208,
            -0.0729,
            0.0652,
            0.1362,
            0.0864,
            -0.1618,
            0.0302,
            -0.0221,
            -0.1458,
            -0.0669,
            0.0931,
            -0.0786,
            0.0407,
            0.0837
          ]
        }
      }
    },
    {
      "step": 887,
      "word": "infiniti",
      "loss": 2.899,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0092,
            -0.0031,
            0.1532,
            -0.0283,
            0.03,
            -0.0929,
            -0.1186,
            -0.1606,
            -0.0071,
            0.0726,
            0.1623,
            0.0149,
            -0.106,
            -0.0175,
            -0.0357,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0935,
            0.0258,
            -0.0535,
            0.0016,
            0.1562,
            -0.0913,
            0.2085,
            0.0217,
            -0.0969,
            0.0553,
            -0.0286,
            0.0263,
            0.0367,
            -0.0708,
            -0.2047,
            0.1001
          ],
          "after": [
            0.0292,
            0.1045,
            -0.0388,
            -0.0779,
            0.0347,
            -0.0095,
            0.0882,
            0.0985,
            -0.0635,
            -0.0587,
            0.0184,
            -0.0151,
            0.1039,
            0.0329,
            -0.0534,
            0.2206
          ]
        },
        "position_0": {
          "grad": [
            -0.0565,
            -0.1625,
            0.069,
            -0.0717,
            0.0698,
            0.1186,
            -0.1134,
            0.0389,
            0.0296,
            -0.0115,
            0.0452,
            -0.0652,
            -0.1323,
            0.0175,
            0.0236,
            -0.0687
          ],
          "after": [
            0.0438,
            -0.147,
            -0.2945,
            0.0505,
            0.0329,
            -0.0468,
            -0.1512,
            0.0685,
            0.1118,
            0.0096,
            0.0136,
            0.0865,
            0.039,
            0.021,
            -0.1309,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0019,
            0.0,
            -0.0026,
            -0.0016,
            0.0005,
            -0.0036,
            -0.0002,
            0.0002,
            0.0015,
            -0.0008,
            -0.0019,
            -0.0016,
            0.0012,
            0.0007,
            0.0002
          ],
          "after": [
            -0.0848,
            0.009,
            -0.1209,
            -0.073,
            0.0653,
            0.1361,
            0.0865,
            -0.1617,
            0.0301,
            -0.0223,
            -0.1459,
            -0.0669,
            0.0932,
            -0.0786,
            0.0408,
            0.0837
          ]
        }
      }
    },
    {
      "step": 888,
      "word": "kinley",
      "loss": 2.1727,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0091,
            -0.0031,
            0.1532,
            -0.0282,
            0.03,
            -0.0929,
            -0.1186,
            -0.1606,
            -0.0071,
            0.0726,
            0.1623,
            0.015,
            -0.1061,
            -0.0176,
            -0.0357,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1162,
            -0.3899,
            0.0742,
            -0.1078,
            0.0538,
            0.173,
            -0.3494,
            0.1037,
            0.0639,
            0.0931,
            -0.0993,
            -0.0631,
            -0.0239,
            0.1901,
            0.0652,
            -0.2104
          ],
          "after": [
            0.0292,
            0.1046,
            -0.0389,
            -0.0778,
            0.0346,
            -0.0096,
            0.0883,
            0.0985,
            -0.0635,
            -0.0588,
            0.0183,
            -0.0151,
            0.104,
            0.0329,
            -0.0534,
            0.2207
          ]
        },
        "position_0": {
          "grad": [
            -0.0834,
            0.0391,
            -0.1179,
            0.0237,
            0.0148,
            -0.036,
            0.1713,
            0.005,
            -0.0175,
            -0.0054,
            -0.0509,
            0.0122,
            0.0353,
            -0.0199,
            -0.0688,
            -0.0043
          ],
          "after": [
            0.0439,
            -0.147,
            -0.2944,
            0.0505,
            0.0328,
            -0.0469,
            -0.1512,
            0.0686,
            0.1117,
            0.0095,
            0.0136,
            0.0864,
            0.0392,
            0.0211,
            -0.1309,
            0.0514
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0024,
            -0.0002,
            -0.0035,
            0.0027,
            0.0017,
            -0.0018,
            -0.0012,
            0.0019,
            0.0012,
            -0.0009,
            -0.0009,
            0.0004,
            0.0027,
            -0.0022,
            -0.0028
          ],
          "after": [
            -0.0849,
            0.0089,
            -0.121,
            -0.073,
            0.0653,
            0.136,
            0.0866,
            -0.1616,
            0.03,
            -0.0224,
            -0.1459,
            -0.0669,
            0.0932,
            -0.0787,
            0.0408,
            0.0838
          ]
        }
      }
    },
    {
      "step": 889,
      "word": "omeir",
      "loss": 2.6216,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.009,
            -0.003,
            0.1533,
            -0.0282,
            0.0301,
            -0.0929,
            -0.1185,
            -0.1607,
            -0.0071,
            0.0725,
            0.1622,
            0.0152,
            -0.1062,
            -0.0177,
            -0.0357,
            -0.1872
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1297,
            -0.2258,
            0.3534,
            0.0516,
            0.1281,
            0.3931,
            -0.2398,
            -0.0283,
            0.2774,
            0.1412,
            0.2657,
            0.2027,
            -0.4121,
            -0.1364,
            0.0904,
            -0.1703
          ],
          "after": [
            0.0291,
            0.1048,
            -0.0391,
            -0.0777,
            0.0344,
            -0.0097,
            0.0883,
            0.0985,
            -0.0636,
            -0.0589,
            0.0182,
            -0.0152,
            0.1041,
            0.0329,
            -0.0534,
            0.2208
          ]
        },
        "position_0": {
          "grad": [
            0.1201,
            -0.1095,
            0.1279,
            -0.1546,
            0.1007,
            -0.0195,
            -0.3151,
            -0.0868,
            0.0313,
            0.1826,
            0.109,
            0.015,
            -0.1121,
            -0.0031,
            -0.0932,
            -0.2598
          ],
          "after": [
            0.044,
            -0.1468,
            -0.2944,
            0.0506,
            0.0327,
            -0.0469,
            -0.1511,
            0.0686,
            0.1116,
            0.0094,
            0.0136,
            0.0864,
            0.0393,
            0.0212,
            -0.1308,
            0.0515
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0002,
            0.0003,
            0.0005,
            -0.0014,
            0.0013,
            -0.0007,
            0.0004,
            0.0002,
            -0.0022,
            -0.0008,
            0.0032,
            -0.0033,
            -0.0016,
            0.0041,
            -0.0004
          ],
          "after": [
            -0.085,
            0.0089,
            -0.1211,
            -0.073,
            0.0653,
            0.1359,
            0.0867,
            -0.1615,
            0.0299,
            -0.0225,
            -0.1459,
            -0.067,
            0.0934,
            -0.0787,
            0.0408,
            0.0839
          ]
        }
      }
    },
    {
      "step": 890,
      "word": "zinnia",
      "loss": 2.167,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0871,
            -0.0136,
            -0.0214,
            -0.0845,
            -0.0221,
            -0.0424,
            -0.0253,
            -0.0135,
            -0.0457,
            0.0881,
            0.0832,
            -0.0893,
            0.0443,
            0.0347,
            -0.0154,
            -0.1151
          ],
          "after": [
            0.009,
            -0.003,
            0.1533,
            -0.0281,
            0.0301,
            -0.0929,
            -0.1184,
            -0.1607,
            -0.007,
            0.0724,
            0.1622,
            0.0153,
            -0.1063,
            -0.0178,
            -0.0357,
            -0.1871
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0552,
            0.0314,
            -0.1408,
            0.0003,
            0.0788,
            -0.1096,
            0.2273,
            0.0382,
            -0.1665,
            0.0645,
            -0.0394,
            -0.0237,
            0.1042,
            -0.0708,
            -0.1953,
            0.1081
          ],
          "after": [
            0.0291,
            0.1049,
            -0.0392,
            -0.0777,
            0.0343,
            -0.0098,
            0.0884,
            0.0985,
            -0.0637,
            -0.059,
            0.0181,
            -0.0153,
            0.1043,
            0.033,
            -0.0533,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            0.0258,
            0.0187,
            -0.0329,
            0.1238,
            0.0272,
            -0.0604,
            0.1597,
            0.0151,
            0.0495,
            0.001,
            0.0169,
            0.0258,
            -0.0162,
            -0.0417,
            0.0487,
            0.0262
          ],
          "after": [
            0.0439,
            -0.1468,
            -0.2944,
            0.0507,
            0.0325,
            -0.047,
            -0.1511,
            0.0687,
            0.1115,
            0.0093,
            0.0136,
            0.0863,
            0.0395,
            0.0213,
            -0.1308,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0012,
            -0.0002,
            0.0011,
            0.0003,
            -0.0033,
            -0.001,
            -0.0033,
            -0.0008,
            0.0014,
            0.0009,
            0.0008,
            0.0007,
            -0.0013,
            0.0014,
            0.0017,
            -0.0004
          ],
          "after": [
            -0.0851,
            0.0089,
            -0.1212,
            -0.073,
            0.0655,
            0.1358,
            0.0869,
            -0.1614,
            0.0298,
            -0.0226,
            -0.146,
            -0.067,
            0.0935,
            -0.0788,
            0.0408,
            0.084
          ]
        }
      }
    },
    {
      "step": 891,
      "word": "kaycion",
      "loss": 2.1076,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.073,
            -0.1702,
            0.0716,
            -0.0975,
            0.0645,
            0.1051,
            -0.0989,
            0.0491,
            -0.0649,
            0.1245,
            0.0672,
            0.0863,
            -0.003,
            0.0379,
            -0.0381,
            -0.0325
          ],
          "after": [
            0.0089,
            -0.0029,
            0.1533,
            -0.028,
            0.0301,
            -0.093,
            -0.1183,
            -0.1607,
            -0.007,
            0.0723,
            0.1621,
            0.0154,
            -0.1063,
            -0.0179,
            -0.0357,
            -0.187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0312,
            0.0954,
            -0.1601,
            0.0044,
            0.0485,
            -0.1506,
            0.2659,
            0.0402,
            -0.2273,
            0.0261,
            -0.0149,
            -0.045,
            0.0945,
            -0.1005,
            -0.1234,
            0.1924
          ],
          "after": [
            0.0291,
            0.105,
            -0.0393,
            -0.0777,
            0.0342,
            -0.0098,
            0.0883,
            0.0985,
            -0.0637,
            -0.0592,
            0.018,
            -0.0153,
            0.1044,
            0.033,
            -0.0532,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            -0.0691,
            0.0511,
            -0.104,
            0.0011,
            0.0524,
            -0.0441,
            0.1474,
            -0.0178,
            -0.0391,
            0.025,
            -0.0269,
            0.0148,
            0.0472,
            -0.0255,
            -0.0967,
            0.0043
          ],
          "after": [
            0.044,
            -0.1467,
            -0.2943,
            0.0507,
            0.0324,
            -0.047,
            -0.1511,
            0.0688,
            0.1114,
            0.0092,
            0.0136,
            0.0863,
            0.0396,
            0.0214,
            -0.1308,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0018,
            -0.0005,
            0.0032,
            -0.0029,
            0.0025,
            0.0002,
            0.0014,
            0.0001,
            0.0023,
            -0.0008,
            0.0025,
            -0.0006,
            -0.0003,
            0.0017,
            0.0031
          ],
          "after": [
            -0.0851,
            0.0089,
            -0.1213,
            -0.0731,
            0.0656,
            0.1357,
            0.087,
            -0.1613,
            0.0297,
            -0.0227,
            -0.146,
            -0.0672,
            0.0937,
            -0.0788,
            0.0407,
            0.084
          ]
        }
      }
    },
    {
      "step": 892,
      "word": "knox",
      "loss": 3.5696,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0088,
            -0.0028,
            0.1534,
            -0.0279,
            0.0301,
            -0.093,
            -0.1182,
            -0.1607,
            -0.0069,
            0.0722,
            0.162,
            0.0154,
            -0.1064,
            -0.018,
            -0.0357,
            -0.187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1058,
            0.1328,
            -0.193,
            -0.0173,
            -0.0064,
            -0.2341,
            0.3814,
            0.0849,
            -0.2376,
            0.0437,
            -0.0789,
            -0.0682,
            0.1769,
            -0.1129,
            -0.0966,
            0.1938
          ],
          "after": [
            0.0291,
            0.105,
            -0.0393,
            -0.0776,
            0.0341,
            -0.0098,
            0.0883,
            0.0984,
            -0.0637,
            -0.0593,
            0.0179,
            -0.0153,
            0.1044,
            0.0331,
            -0.0531,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            -0.1121,
            0.0569,
            -0.1536,
            0.0944,
            -0.0137,
            -0.0529,
            0.2719,
            0.0193,
            -0.053,
            -0.0911,
            -0.0765,
            0.0062,
            0.0878,
            0.0374,
            -0.0074,
            0.0524
          ],
          "after": [
            0.0441,
            -0.1467,
            -0.2942,
            0.0507,
            0.0323,
            -0.0469,
            -0.1512,
            0.0688,
            0.1114,
            0.0092,
            0.0136,
            0.0862,
            0.0396,
            0.0215,
            -0.1307,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0029,
            0.0048,
            0.0006,
            0.0036,
            -0.0034,
            -0.0042,
            0.0135,
            0.005,
            -0.0092,
            0.0005,
            -0.0012,
            -0.0067,
            0.0042,
            -0.0021,
            0.0016,
            0.0068
          ],
          "after": [
            -0.0851,
            0.0088,
            -0.1214,
            -0.0733,
            0.0658,
            0.1357,
            0.0869,
            -0.1615,
            0.0299,
            -0.0228,
            -0.1459,
            -0.0671,
            0.0937,
            -0.0788,
            0.0406,
            0.0838
          ]
        }
      }
    },
    {
      "step": 893,
      "word": "jemar",
      "loss": 2.0387,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0617,
            -0.0931,
            -0.0023,
            0.0397,
            0.1059,
            0.1136,
            0.177,
            -0.1559,
            0.1967,
            -0.0998,
            0.0098,
            0.267,
            -0.288,
            -0.2018,
            0.1061,
            0.2011
          ],
          "after": [
            0.0087,
            -0.0027,
            0.1534,
            -0.0279,
            0.0301,
            -0.0931,
            -0.1182,
            -0.1607,
            -0.007,
            0.0721,
            0.1619,
            0.0154,
            -0.1063,
            -0.018,
            -0.0357,
            -0.187
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2928,
            -0.2517,
            0.3881,
            0.0754,
            0.2195,
            0.1257,
            -0.6218,
            -0.2045,
            0.5515,
            0.2407,
            0.2764,
            0.1575,
            -0.3286,
            0.0274,
            -0.1759,
            -0.3247
          ],
          "after": [
            0.029,
            0.1051,
            -0.0394,
            -0.0777,
            0.0339,
            -0.0099,
            0.0883,
            0.0984,
            -0.0637,
            -0.0595,
            0.0178,
            -0.0154,
            0.1045,
            0.0332,
            -0.053,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            0.0118,
            0.0456,
            -0.0209,
            0.1472,
            0.0021,
            -0.1211,
            0.1974,
            0.0727,
            -0.228,
            -0.0665,
            0.0064,
            0.0077,
            0.345,
            -0.1061,
            -0.1269,
            0.1618
          ],
          "after": [
            0.0442,
            -0.1467,
            -0.294,
            0.0506,
            0.0322,
            -0.0468,
            -0.1513,
            0.0688,
            0.1115,
            0.0092,
            0.0136,
            0.0862,
            0.0395,
            0.0216,
            -0.1306,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            -0.0017,
            -0.0005,
            -0.0014,
            -0.0007,
            0.0001,
            -0.0038,
            -0.0006,
            0.0011,
            0.0014,
            0.0012,
            -0.0012,
            -0.0012,
            0.0023,
            0.0013,
            -0.0008
          ],
          "after": [
            -0.085,
            0.0088,
            -0.1214,
            -0.0734,
            0.066,
            0.1357,
            0.0868,
            -0.1616,
            0.03,
            -0.023,
            -0.1459,
            -0.067,
            0.0937,
            -0.0788,
            0.0404,
            0.0837
          ]
        }
      }
    },
    {
      "step": 894,
      "word": "alila",
      "loss": 1.7391,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2155,
            0.0562,
            -0.3207,
            -0.2478,
            -0.0886,
            -0.0946,
            -0.2367,
            0.1208,
            -0.206,
            0.1634,
            -0.0713,
            -0.0179,
            -0.0249,
            0.1923,
            0.1202,
            -0.7338
          ],
          "after": [
            0.0086,
            -0.0027,
            0.1535,
            -0.0277,
            0.0301,
            -0.0931,
            -0.1181,
            -0.1607,
            -0.0069,
            0.072,
            0.1619,
            0.0154,
            -0.1063,
            -0.0181,
            -0.0358,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0107,
            0.1603,
            -0.1462,
            0.0258,
            -0.0149,
            -0.1981,
            0.2688,
            -0.0502,
            -0.1859,
            0.0038,
            0.0246,
            -0.0809,
            0.1309,
            -0.0841,
            -0.182,
            0.1851
          ],
          "after": [
            0.0289,
            0.1052,
            -0.0394,
            -0.0777,
            0.0338,
            -0.0098,
            0.0883,
            0.0985,
            -0.0637,
            -0.0596,
            0.0176,
            -0.0154,
            0.1045,
            0.0333,
            -0.0529,
            0.2209
          ]
        },
        "position_0": {
          "grad": [
            0.0965,
            -0.0491,
            0.1196,
            -0.2038,
            0.1014,
            0.0192,
            -0.3693,
            -0.1389,
            0.1724,
            0.0782,
            0.2835,
            0.0611,
            -0.1748,
            0.0019,
            -0.1396,
            -0.2738
          ],
          "after": [
            0.0442,
            -0.1467,
            -0.294,
            0.0506,
            0.0321,
            -0.0468,
            -0.1514,
            0.0689,
            0.1114,
            0.0091,
            0.0135,
            0.0861,
            0.0395,
            0.0217,
            -0.1305,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0024,
            0.0022,
            -0.0005,
            0.0007,
            0.0003,
            -0.0041,
            0.0002,
            -0.0006,
            0.0001,
            0.0003,
            0.0015,
            -0.0024,
            0.0,
            0.0008,
            -0.0037
          ],
          "after": [
            -0.085,
            0.0088,
            -0.1215,
            -0.0735,
            0.0662,
            0.1357,
            0.0869,
            -0.1617,
            0.0302,
            -0.0231,
            -0.1459,
            -0.067,
            0.0938,
            -0.0789,
            0.0403,
            0.0837
          ]
        }
      }
    },
    {
      "step": 895,
      "word": "destyn",
      "loss": 2.0735,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0085,
            -0.0026,
            0.1536,
            -0.0276,
            0.0301,
            -0.0931,
            -0.1181,
            -0.1607,
            -0.0069,
            0.0719,
            0.1619,
            0.0154,
            -0.1063,
            -0.0181,
            -0.0359,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0528,
            -0.3199,
            0.2468,
            -0.1688,
            0.1905,
            0.1841,
            -0.4193,
            -0.0616,
            0.489,
            0.1788,
            0.1483,
            0.2112,
            -0.2944,
            0.0278,
            0.0754,
            -0.1624
          ],
          "after": [
            0.0289,
            0.1053,
            -0.0395,
            -0.0776,
            0.0337,
            -0.0098,
            0.0883,
            0.0986,
            -0.0639,
            -0.0598,
            0.0174,
            -0.0155,
            0.1047,
            0.0334,
            -0.0528,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            -0.0623,
            0.0398,
            -0.0301,
            -0.0558,
            0.061,
            -0.0612,
            0.0579,
            -0.0514,
            -0.0009,
            -0.0707,
            -0.0758,
            0.005,
            0.0227,
            0.0646,
            0.093,
            0.1365
          ],
          "after": [
            0.0443,
            -0.1467,
            -0.2939,
            0.0507,
            0.0319,
            -0.0467,
            -0.1514,
            0.069,
            0.1114,
            0.0092,
            0.0135,
            0.086,
            0.0395,
            0.0218,
            -0.1304,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0021,
            0.0014,
            -0.0017,
            0.0015,
            -0.0013,
            -0.0004,
            0.0002,
            0.0008,
            0.0015,
            0.001,
            0.0004,
            0.0003,
            -0.0008,
            -0.0022,
            0.0004
          ],
          "after": [
            -0.085,
            0.0089,
            -0.1217,
            -0.0735,
            0.0663,
            0.1357,
            0.0869,
            -0.1618,
            0.0302,
            -0.0232,
            -0.146,
            -0.067,
            0.0939,
            -0.0789,
            0.0403,
            0.0836
          ]
        }
      }
    },
    {
      "step": 896,
      "word": "sherwin",
      "loss": 2.5244,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0085,
            -0.0026,
            0.1537,
            -0.0275,
            0.0302,
            -0.0931,
            -0.118,
            -0.1607,
            -0.0069,
            0.0719,
            0.1619,
            0.0154,
            -0.1062,
            -0.0182,
            -0.036,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0715,
            -0.0335,
            0.2075,
            0.1062,
            0.1417,
            0.1691,
            -0.0609,
            -0.0038,
            0.0549,
            0.0821,
            0.1981,
            0.2567,
            -0.2787,
            -0.1652,
            0.0547,
            -0.0681
          ],
          "after": [
            0.0288,
            0.1054,
            -0.0396,
            -0.0776,
            0.0335,
            -0.0099,
            0.0884,
            0.0987,
            -0.0639,
            -0.06,
            0.0172,
            -0.0157,
            0.1048,
            0.0335,
            -0.0527,
            0.221
          ]
        },
        "position_0": {
          "grad": [
            -0.0098,
            0.0747,
            -0.0212,
            0.125,
            -0.1177,
            0.0578,
            0.0292,
            0.042,
            -0.058,
            -0.0112,
            -0.1018,
            0.0806,
            0.0699,
            0.0278,
            0.118,
            0.047
          ],
          "after": [
            0.0443,
            -0.1467,
            -0.2939,
            0.0507,
            0.0319,
            -0.0466,
            -0.1514,
            0.069,
            0.1114,
            0.0092,
            0.0135,
            0.0859,
            0.0395,
            0.0218,
            -0.1304,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.002,
            -0.0001,
            0.0011,
            -0.0007,
            0.0023,
            0.0009,
            0.0018,
            -0.0,
            -0.0001,
            -0.0001,
            0.0006,
            0.0017,
            -0.0007,
            -0.0014,
            -0.0008,
            0.0005
          ],
          "after": [
            -0.0849,
            0.009,
            -0.1218,
            -0.0735,
            0.0663,
            0.1357,
            0.0869,
            -0.1618,
            0.0303,
            -0.0234,
            -0.146,
            -0.067,
            0.0939,
            -0.0789,
            0.0402,
            0.0836
          ]
        }
      }
    },
    {
      "step": 897,
      "word": "avery",
      "loss": 2.1578,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.139,
            -0.0121,
            -0.1057,
            0.1957,
            0.2545,
            0.0389,
            0.019,
            -0.0718,
            -0.2503,
            0.1451,
            0.1484,
            -0.0552,
            0.1944,
            -0.1994,
            -0.158,
            0.3326
          ],
          "after": [
            0.0085,
            -0.0025,
            0.1538,
            -0.0275,
            0.0301,
            -0.0931,
            -0.118,
            -0.1607,
            -0.0068,
            0.0717,
            0.1618,
            0.0155,
            -0.1062,
            -0.0182,
            -0.0359,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0362,
            -0.1966,
            0.2483,
            0.0041,
            0.1718,
            0.3831,
            -0.4881,
            -0.0669,
            0.4096,
            0.1272,
            0.2594,
            0.1443,
            -0.4277,
            -0.0731,
            0.1173,
            -0.1805
          ],
          "after": [
            0.0288,
            0.1055,
            -0.0398,
            -0.0776,
            0.0333,
            -0.01,
            0.0885,
            0.0988,
            -0.0641,
            -0.0602,
            0.0169,
            -0.0158,
            0.1051,
            0.0336,
            -0.0527,
            0.2211
          ]
        },
        "position_0": {
          "grad": [
            0.1032,
            -0.0381,
            0.1222,
            -0.1869,
            0.0663,
            0.0208,
            -0.3434,
            -0.139,
            0.1462,
            0.0444,
            0.2865,
            0.0349,
            -0.1614,
            0.0474,
            -0.1041,
            -0.2267
          ],
          "after": [
            0.0443,
            -0.1468,
            -0.2939,
            0.0507,
            0.0318,
            -0.0466,
            -0.1514,
            0.0692,
            0.1114,
            0.0092,
            0.0133,
            0.0858,
            0.0395,
            0.0218,
            -0.1303,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0017,
            -0.0017,
            -0.0013,
            -0.0011,
            0.0012,
            -0.0025,
            0.0009,
            0.0035,
            -0.0012,
            -0.0007,
            0.0011,
            -0.002,
            0.0015,
            0.001,
            0.0012
          ],
          "after": [
            -0.0848,
            0.009,
            -0.1219,
            -0.0735,
            0.0664,
            0.1357,
            0.0869,
            -0.1619,
            0.0303,
            -0.0234,
            -0.1461,
            -0.067,
            0.094,
            -0.0789,
            0.0402,
            0.0835
          ]
        }
      }
    },
    {
      "step": 898,
      "word": "anarely",
      "loss": 2.007,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0251,
            0.1668,
            -0.1828,
            -0.0365,
            -0.2411,
            0.1562,
            0.0417,
            -0.2296,
            0.166,
            -0.1532,
            -0.1173,
            0.0248,
            -0.1433,
            -0.0714,
            0.0757,
            0.1648
          ],
          "after": [
            0.0084,
            -0.0025,
            0.1539,
            -0.0274,
            0.0301,
            -0.0932,
            -0.1179,
            -0.1605,
            -0.0067,
            0.0717,
            0.1618,
            0.0155,
            -0.1062,
            -0.0181,
            -0.036,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1108,
            -0.3108,
            0.0858,
            -0.0448,
            0.0557,
            0.1189,
            -0.3967,
            0.0195,
            0.06,
            -0.0254,
            0.0152,
            -0.0552,
            -0.0535,
            0.1764,
            0.079,
            -0.1043
          ],
          "after": [
            0.0287,
            0.1057,
            -0.0399,
            -0.0776,
            0.0332,
            -0.0101,
            0.0886,
            0.0989,
            -0.0642,
            -0.0604,
            0.0166,
            -0.0159,
            0.1053,
            0.0337,
            -0.0527,
            0.2212
          ]
        },
        "position_0": {
          "grad": [
            0.0764,
            -0.041,
            0.0797,
            -0.1572,
            0.0539,
            0.005,
            -0.2708,
            -0.0934,
            0.1352,
            0.075,
            0.1908,
            0.0428,
            -0.1537,
            -0.011,
            -0.102,
            -0.2232
          ],
          "after": [
            0.0442,
            -0.1468,
            -0.294,
            0.0508,
            0.0317,
            -0.0466,
            -0.1512,
            0.0694,
            0.1113,
            0.0091,
            0.0132,
            0.0857,
            0.0396,
            0.0217,
            -0.1302,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0018,
            0.0023,
            0.0013,
            0.0011,
            -0.0022,
            0.0024,
            -0.001,
            -0.0009,
            0.0003,
            0.0024,
            -0.0007,
            0.0019,
            -0.0023,
            -0.0037,
            0.0001
          ],
          "after": [
            -0.0848,
            0.0089,
            -0.1221,
            -0.0735,
            0.0664,
            0.1357,
            0.0869,
            -0.162,
            0.0303,
            -0.0235,
            -0.1462,
            -0.0671,
            0.0941,
            -0.0788,
            0.0402,
            0.0835
          ]
        }
      }
    },
    {
      "step": 899,
      "word": "jeramy",
      "loss": 2.2635,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2532,
            0.0315,
            0.2803,
            0.0399,
            0.254,
            0.1352,
            0.2014,
            0.0891,
            0.1218,
            -0.1481,
            -0.0208,
            0.1778,
            -0.1438,
            -0.135,
            -0.0835,
            0.2232
          ],
          "after": [
            0.0085,
            -0.0026,
            0.1539,
            -0.0274,
            0.03,
            -0.0934,
            -0.118,
            -0.1605,
            -0.0067,
            0.0717,
            0.1618,
            0.0154,
            -0.1062,
            -0.018,
            -0.0359,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2293,
            -0.2385,
            0.342,
            0.0628,
            0.2101,
            0.1071,
            -0.5269,
            -0.171,
            0.473,
            0.2099,
            0.2341,
            0.1484,
            -0.291,
            0.0095,
            -0.1443,
            -0.2737
          ],
          "after": [
            0.0286,
            0.1059,
            -0.0401,
            -0.0776,
            0.033,
            -0.0103,
            0.0888,
            0.0991,
            -0.0644,
            -0.0606,
            0.0162,
            -0.0161,
            0.1055,
            0.0337,
            -0.0527,
            0.2213
          ]
        },
        "position_0": {
          "grad": [
            0.0061,
            0.0394,
            -0.0266,
            0.1129,
            0.001,
            -0.0982,
            0.179,
            0.0568,
            -0.1971,
            -0.0291,
            0.0104,
            0.0149,
            0.2756,
            -0.1126,
            -0.1265,
            0.1264
          ],
          "after": [
            0.0441,
            -0.1468,
            -0.294,
            0.0509,
            0.0316,
            -0.0465,
            -0.1512,
            0.0695,
            0.1113,
            0.0091,
            0.013,
            0.0856,
            0.0396,
            0.0218,
            -0.13,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            0.0022,
            -0.0002,
            0.0011,
            0.0016,
            -0.002,
            0.0009,
            -0.001,
            -0.0004,
            0.0011,
            0.0008,
            0.0007,
            0.0006,
            -0.001,
            -0.0027,
            -0.0009
          ],
          "after": [
            -0.0847,
            0.0089,
            -0.1222,
            -0.0735,
            0.0664,
            0.1358,
            0.0869,
            -0.162,
            0.0303,
            -0.0236,
            -0.1463,
            -0.0671,
            0.0941,
            -0.0787,
            0.0403,
            0.0835
          ]
        }
      }
    },
    {
      "step": 900,
      "word": "boyce",
      "loss": 2.7409,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0085,
            -0.0026,
            0.154,
            -0.0274,
            0.03,
            -0.0935,
            -0.118,
            -0.1604,
            -0.0067,
            0.0717,
            0.1618,
            0.0154,
            -0.1061,
            -0.0179,
            -0.0359,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1005,
            -0.3155,
            0.3605,
            0.056,
            -0.0642,
            0.1045,
            -0.3738,
            -0.0153,
            0.1758,
            0.0675,
            -0.0809,
            0.03,
            -0.0317,
            0.2679,
            -0.0021,
            -0.2965
          ],
          "after": [
            0.0285,
            0.1061,
            -0.0403,
            -0.0776,
            0.0328,
            -0.0104,
            0.089,
            0.0993,
            -0.0646,
            -0.0608,
            0.016,
            -0.0162,
            0.1057,
            0.0336,
            -0.0526,
            0.2215
          ]
        },
        "position_0": {
          "grad": [
            0.0641,
            -0.0279,
            0.1179,
            -0.0172,
            -0.0751,
            0.0199,
            0.0315,
            0.0081,
            -0.0672,
            -0.1256,
            -0.022,
            -0.0815,
            0.1313,
            -0.0406,
            0.0593,
            0.302
          ],
          "after": [
            0.044,
            -0.1468,
            -0.2941,
            0.051,
            0.0316,
            -0.0465,
            -0.1511,
            0.0696,
            0.1113,
            0.0091,
            0.0129,
            0.0856,
            0.0395,
            0.0219,
            -0.1299,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0013,
            -0.0028,
            -0.0019,
            0.0036,
            -0.0022,
            0.0058,
            0.0008,
            -0.0015,
            0.0004,
            0.0068,
            -0.0022,
            0.0022,
            -0.0023,
            0.0009,
            0.0017,
            0.003
          ],
          "after": [
            -0.0847,
            0.0089,
            -0.1222,
            -0.0736,
            0.0664,
            0.1357,
            0.0869,
            -0.1619,
            0.0303,
            -0.0238,
            -0.1463,
            -0.0672,
            0.0942,
            -0.0787,
            0.0404,
            0.0834
          ]
        }
      }
    },
    {
      "step": 901,
      "word": "dalany",
      "loss": 2.0208,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1392,
            0.3487,
            -0.2729,
            -0.126,
            -0.0959,
            -0.1811,
            -0.0251,
            -0.0208,
            0.0455,
            -0.0673,
            -0.2049,
            0.2734,
            -0.1423,
            0.0145,
            0.0687,
            -0.1769
          ],
          "after": [
            0.0086,
            -0.0027,
            0.1541,
            -0.0274,
            0.03,
            -0.0935,
            -0.1181,
            -0.1604,
            -0.0068,
            0.0718,
            0.1619,
            0.0153,
            -0.106,
            -0.0179,
            -0.0359,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0324,
            0.1272,
            -0.1187,
            0.0824,
            0.0115,
            -0.1628,
            0.2541,
            0.0041,
            -0.2145,
            0.02,
            -0.0075,
            -0.0195,
            0.1076,
            -0.0832,
            -0.1773,
            0.1447
          ],
          "after": [
            0.0285,
            0.1063,
            -0.0405,
            -0.0777,
            0.0327,
            -0.0105,
            0.0892,
            0.0994,
            -0.0647,
            -0.061,
            0.0158,
            -0.0163,
            0.1059,
            0.0336,
            -0.0526,
            0.2216
          ]
        },
        "position_0": {
          "grad": [
            -0.0659,
            0.0556,
            -0.0416,
            -0.0848,
            0.1211,
            -0.0601,
            0.064,
            -0.0656,
            -0.0174,
            -0.0139,
            -0.0591,
            0.0444,
            0.0336,
            0.0078,
            0.0191,
            0.1204
          ],
          "after": [
            0.0439,
            -0.1468,
            -0.2942,
            0.051,
            0.0315,
            -0.0464,
            -0.1511,
            0.0697,
            0.1113,
            0.0092,
            0.0128,
            0.0855,
            0.0394,
            0.022,
            -0.1298,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0004,
            0.0014,
            -0.0011,
            -0.0001,
            -0.0014,
            -0.001,
            -0.0014,
            -0.0005,
            0.0004,
            0.0007,
            -0.0013,
            0.0004,
            0.0002,
            0.0002,
            -0.0015
          ],
          "after": [
            -0.0846,
            0.0089,
            -0.1223,
            -0.0737,
            0.0664,
            0.1357,
            0.0869,
            -0.1618,
            0.0303,
            -0.024,
            -0.1463,
            -0.0672,
            0.0942,
            -0.0787,
            0.0404,
            0.0833
          ]
        }
      }
    },
    {
      "step": 902,
      "word": "kennard",
      "loss": 2.4877,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.189,
            -0.0519,
            0.0623,
            0.0364,
            0.1474,
            0.1885,
            0.1508,
            -0.0965,
            0.1473,
            -0.1386,
            -0.0872,
            0.3113,
            -0.2176,
            -0.212,
            0.0343,
            0.2984
          ],
          "after": [
            0.0087,
            -0.0028,
            0.1541,
            -0.0273,
            0.0299,
            -0.0936,
            -0.1181,
            -0.1603,
            -0.0068,
            0.0719,
            0.162,
            0.0152,
            -0.1059,
            -0.0178,
            -0.036,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1277,
            -0.2462,
            0.2409,
            0.0242,
            0.2497,
            0.188,
            -0.4015,
            -0.0942,
            0.3814,
            0.1531,
            0.154,
            0.2493,
            -0.2176,
            0.0073,
            -0.1655,
            -0.2732
          ],
          "after": [
            0.0284,
            0.1065,
            -0.0407,
            -0.0778,
            0.0325,
            -0.0106,
            0.0893,
            0.0996,
            -0.0649,
            -0.0612,
            0.0156,
            -0.0165,
            0.1061,
            0.0336,
            -0.0525,
            0.2218
          ]
        },
        "position_0": {
          "grad": [
            -0.071,
            0.0311,
            -0.1132,
            0.0398,
            -0.0123,
            -0.0514,
            0.1619,
            0.0199,
            -0.0117,
            -0.0038,
            -0.0695,
            0.013,
            0.0041,
            -0.022,
            -0.0295,
            -0.0154
          ],
          "after": [
            0.0439,
            -0.1468,
            -0.2942,
            0.0511,
            0.0314,
            -0.0463,
            -0.1511,
            0.0698,
            0.1114,
            0.0092,
            0.0128,
            0.0854,
            0.0394,
            0.0221,
            -0.1298,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0025,
            -0.0012,
            -0.0001,
            -0.0,
            -0.0003,
            0.0003,
            -0.0029,
            -0.0006,
            0.0006,
            0.0004,
            0.0005,
            -0.0018,
            0.0008,
            0.0023,
            -0.0016,
            -0.0008
          ],
          "after": [
            -0.0846,
            0.0089,
            -0.1223,
            -0.0737,
            0.0664,
            0.1356,
            0.0869,
            -0.1617,
            0.0303,
            -0.0242,
            -0.1464,
            -0.0672,
            0.0943,
            -0.0787,
            0.0404,
            0.0833
          ]
        }
      }
    },
    {
      "step": 903,
      "word": "jamira",
      "loss": 1.8379,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0083,
            0.0678,
            0.1107,
            -0.0886,
            0.1639,
            -0.1225,
            0.2609,
            0.0628,
            -0.0038,
            0.1283,
            0.1173,
            -0.0491,
            -0.075,
            -0.0678,
            -0.1107,
            -0.0846
          ],
          "after": [
            0.0088,
            -0.0029,
            0.1541,
            -0.0273,
            0.0298,
            -0.0936,
            -0.1183,
            -0.1602,
            -0.0069,
            0.0719,
            0.162,
            0.015,
            -0.1058,
            -0.0176,
            -0.0359,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1025,
            0.1434,
            -0.1585,
            0.0404,
            0.049,
            -0.1633,
            0.31,
            0.0403,
            -0.2378,
            0.0171,
            -0.0782,
            -0.003,
            0.1326,
            -0.065,
            -0.1973,
            0.1731
          ],
          "after": [
            0.0284,
            0.1066,
            -0.0408,
            -0.0779,
            0.0324,
            -0.0106,
            0.0894,
            0.0997,
            -0.065,
            -0.0614,
            0.0154,
            -0.0166,
            0.1062,
            0.0336,
            -0.0524,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            0.0069,
            0.0518,
            -0.0315,
            0.11,
            0.0214,
            -0.1036,
            0.1952,
            0.0559,
            -0.2132,
            -0.0215,
            0.01,
            0.0331,
            0.2927,
            -0.1183,
            -0.1342,
            0.1386
          ],
          "after": [
            0.0439,
            -0.1469,
            -0.2941,
            0.0511,
            0.0314,
            -0.0462,
            -0.1512,
            0.0698,
            0.1115,
            0.0093,
            0.0127,
            0.0854,
            0.0393,
            0.0222,
            -0.1296,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            -0.0014,
            0.0007,
            -0.0013,
            -0.0012,
            -0.0006,
            -0.0026,
            -0.0002,
            0.0004,
            -0.0005,
            0.0008,
            -0.0005,
            -0.0012,
            0.0009,
            0.0021,
            -0.0012
          ],
          "after": [
            -0.0846,
            0.009,
            -0.1224,
            -0.0738,
            0.0665,
            0.1356,
            0.087,
            -0.1617,
            0.0303,
            -0.0243,
            -0.1465,
            -0.0672,
            0.0943,
            -0.0788,
            0.0404,
            0.0833
          ]
        }
      }
    },
    {
      "step": 904,
      "word": "aaliah",
      "loss": 2.0966,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1668,
            0.16,
            0.4367,
            -0.3742,
            0.349,
            0.1198,
            -0.5891,
            0.2498,
            0.5176,
            0.032,
            0.0967,
            0.3114,
            -0.2389,
            0.0848,
            -0.3586,
            -0.366
          ],
          "after": [
            0.0089,
            -0.003,
            0.1541,
            -0.0271,
            0.0297,
            -0.0937,
            -0.1182,
            -0.1603,
            -0.0071,
            0.0719,
            0.162,
            0.0149,
            -0.1056,
            -0.0176,
            -0.0358,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0022,
            0.0423,
            -0.0543,
            0.0258,
            0.0408,
            -0.0927,
            0.1189,
            -0.0283,
            -0.091,
            0.0463,
            0.0204,
            -0.0135,
            0.0395,
            -0.0425,
            -0.1433,
            0.0839
          ],
          "after": [
            0.0283,
            0.1067,
            -0.0409,
            -0.078,
            0.0323,
            -0.0107,
            0.0895,
            0.0998,
            -0.0651,
            -0.0615,
            0.0152,
            -0.0167,
            0.1063,
            0.0336,
            -0.0522,
            0.2219
          ]
        },
        "position_0": {
          "grad": [
            0.09,
            -0.0338,
            0.0925,
            -0.2071,
            0.1125,
            -0.0015,
            -0.3309,
            -0.1366,
            0.1341,
            0.1161,
            0.2399,
            0.0432,
            -0.1549,
            -0.017,
            -0.1652,
            -0.2541
          ],
          "after": [
            0.0439,
            -0.1469,
            -0.2942,
            0.0512,
            0.0313,
            -0.0461,
            -0.1512,
            0.07,
            0.1115,
            0.0092,
            0.0126,
            0.0853,
            0.0392,
            0.0224,
            -0.1294,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0005,
            0.0014,
            0.0001,
            -0.0012,
            -0.0014,
            -0.0016,
            -0.0009,
            -0.0007,
            -0.0007,
            0.0006,
            -0.0,
            -0.0005,
            0.0,
            0.001,
            -0.0019
          ],
          "after": [
            -0.0847,
            0.0091,
            -0.1225,
            -0.0738,
            0.0666,
            0.1356,
            0.0871,
            -0.1616,
            0.0303,
            -0.0244,
            -0.1465,
            -0.0672,
            0.0944,
            -0.0788,
            0.0404,
            0.0834
          ]
        }
      }
    },
    {
      "step": 905,
      "word": "darrielle",
      "loss": 2.2842,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0383,
            -0.0097,
            -0.0125,
            -0.0804,
            0.0815,
            0.0348,
            0.0819,
            -0.12,
            0.095,
            0.0038,
            0.0094,
            0.0493,
            -0.0831,
            -0.1148,
            -0.0221,
            0.0309
          ],
          "after": [
            0.009,
            -0.0031,
            0.154,
            -0.0269,
            0.0295,
            -0.0938,
            -0.1182,
            -0.1603,
            -0.0074,
            0.0719,
            0.162,
            0.0147,
            -0.1054,
            -0.0175,
            -0.0356,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3654,
            -0.141,
            -0.1356,
            -0.0669,
            -0.5804,
            0.1661,
            -0.3713,
            0.0236,
            0.116,
            0.1212,
            0.0147,
            -0.2183,
            0.1759,
            0.227,
            0.4101,
            -0.3796
          ],
          "after": [
            0.0282,
            0.1069,
            -0.0409,
            -0.078,
            0.0323,
            -0.0107,
            0.0896,
            0.0999,
            -0.0652,
            -0.0617,
            0.0151,
            -0.0168,
            0.1063,
            0.0335,
            -0.0522,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0448,
            0.039,
            -0.0385,
            -0.0701,
            0.0773,
            -0.0392,
            0.0592,
            -0.0457,
            -0.0161,
            0.0161,
            -0.0424,
            0.0247,
            0.0112,
            -0.0112,
            0.0062,
            0.0713
          ],
          "after": [
            0.0439,
            -0.147,
            -0.2942,
            0.0513,
            0.0311,
            -0.046,
            -0.1512,
            0.0701,
            0.1116,
            0.0092,
            0.0125,
            0.0852,
            0.0391,
            0.0225,
            -0.1293,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0033,
            -0.0012,
            -0.0029,
            0.001,
            -0.0037,
            0.0007,
            -0.0022,
            0.0017,
            -0.0003,
            0.0026,
            -0.0014,
            -0.0008,
            -0.0002,
            0.0022,
            0.0025,
            -0.0028
          ],
          "after": [
            -0.0848,
            0.0092,
            -0.1225,
            -0.0738,
            0.0667,
            0.1356,
            0.0872,
            -0.1615,
            0.0303,
            -0.0245,
            -0.1466,
            -0.0671,
            0.0944,
            -0.0789,
            0.0404,
            0.0835
          ]
        }
      }
    },
    {
      "step": 906,
      "word": "sameya",
      "loss": 2.0377,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0337,
            0.0362,
            0.083,
            -0.1087,
            0.1576,
            -0.1195,
            0.2021,
            0.0621,
            0.0157,
            0.1276,
            0.1264,
            -0.0685,
            -0.0765,
            -0.0376,
            -0.1057,
            -0.0923
          ],
          "after": [
            0.0091,
            -0.0032,
            0.1539,
            -0.0267,
            0.0294,
            -0.0938,
            -0.1182,
            -0.1603,
            -0.0076,
            0.0719,
            0.1619,
            0.0146,
            -0.1052,
            -0.0174,
            -0.0355,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.064,
            -0.294,
            0.1837,
            -0.1862,
            0.0902,
            0.1035,
            -0.2878,
            -0.007,
            0.1565,
            -0.0903,
            0.1624,
            0.0085,
            -0.2568,
            -0.1185,
            0.2921,
            -0.2051
          ],
          "after": [
            0.0281,
            0.107,
            -0.041,
            -0.078,
            0.0323,
            -0.0108,
            0.0897,
            0.1,
            -0.0653,
            -0.0618,
            0.0149,
            -0.0168,
            0.1064,
            0.0335,
            -0.0522,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            -0.004,
            0.0906,
            -0.0251,
            0.0729,
            -0.0786,
            0.0476,
            -0.0175,
            0.022,
            -0.0618,
            0.0532,
            -0.0943,
            0.0867,
            0.0775,
            -0.0127,
            0.0462,
            0.0045
          ],
          "after": [
            0.0438,
            -0.1471,
            -0.2942,
            0.0513,
            0.0311,
            -0.0459,
            -0.1512,
            0.0702,
            0.1116,
            0.0092,
            0.0125,
            0.0851,
            0.0391,
            0.0226,
            -0.1292,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            -0.0011,
            -0.0006,
            0.0008,
            0.0004,
            0.0014,
            0.0005,
            0.0011,
            -0.0005,
            0.0004,
            -0.0011,
            -0.0001,
            0.0003,
            -0.0002,
            0.0003,
            0.001
          ],
          "after": [
            -0.0848,
            0.0093,
            -0.1225,
            -0.0739,
            0.0668,
            0.1356,
            0.0872,
            -0.1615,
            0.0303,
            -0.0247,
            -0.1466,
            -0.0671,
            0.0945,
            -0.079,
            0.0403,
            0.0836
          ]
        }
      }
    },
    {
      "step": 907,
      "word": "eion",
      "loss": 2.5418,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0091,
            -0.0033,
            0.1538,
            -0.0265,
            0.0292,
            -0.0938,
            -0.1183,
            -0.1603,
            -0.0077,
            0.0718,
            0.1619,
            0.0145,
            -0.1051,
            -0.0173,
            -0.0353,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3798,
            0.4626,
            0.7979,
            -0.0033,
            0.1303,
            0.0225,
            0.329,
            -0.0492,
            0.1697,
            -0.2958,
            0.0553,
            0.0395,
            -0.216,
            0.0274,
            0.0281,
            -0.2169
          ],
          "after": [
            0.0281,
            0.1071,
            -0.0412,
            -0.0779,
            0.0322,
            -0.0109,
            0.0898,
            0.1001,
            -0.0654,
            -0.0618,
            0.0148,
            -0.0168,
            0.1065,
            0.0335,
            -0.0523,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            -0.0029,
            -0.2912,
            0.1269,
            -0.1144,
            -0.0508,
            0.0779,
            -0.4921,
            -0.1324,
            0.2509,
            0.131,
            0.033,
            -0.135,
            -0.2653,
            0.0421,
            0.0128,
            -0.4863
          ],
          "after": [
            0.0438,
            -0.147,
            -0.2942,
            0.0514,
            0.031,
            -0.0459,
            -0.1511,
            0.0703,
            0.1116,
            0.009,
            0.0124,
            0.0851,
            0.0391,
            0.0227,
            -0.1291,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0017,
            0.0001,
            -0.0014,
            -0.0008,
            0.0019,
            -0.0004,
            0.004,
            0.0016,
            0.001,
            0.0024,
            -0.0001,
            0.0025,
            0.0002,
            -0.0024,
            -0.0002,
            0.0006
          ],
          "after": [
            -0.0849,
            0.0093,
            -0.1224,
            -0.0739,
            0.0669,
            0.1356,
            0.0873,
            -0.1616,
            0.0303,
            -0.0248,
            -0.1465,
            -0.0671,
            0.0945,
            -0.079,
            0.0403,
            0.0836
          ]
        }
      }
    },
    {
      "step": 908,
      "word": "denver",
      "loss": 2.4023,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0092,
            -0.0034,
            0.1538,
            -0.0264,
            0.0291,
            -0.0938,
            -0.1183,
            -0.1604,
            -0.0079,
            0.0718,
            0.1618,
            0.0145,
            -0.1049,
            -0.0172,
            -0.0352,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2425,
            -0.6731,
            0.4641,
            -0.2761,
            0.1571,
            0.5335,
            -1.1185,
            -0.1312,
            0.8521,
            0.1689,
            0.2473,
            0.0975,
            -0.4995,
            0.234,
            0.2902,
            -0.4348
          ],
          "after": [
            0.028,
            0.1072,
            -0.0415,
            -0.0777,
            0.0322,
            -0.011,
            0.09,
            0.1003,
            -0.0656,
            -0.0619,
            0.0145,
            -0.0169,
            0.1067,
            0.0334,
            -0.0524,
            0.2226
          ]
        },
        "position_0": {
          "grad": [
            -0.0654,
            0.042,
            -0.033,
            -0.0612,
            0.0855,
            -0.0506,
            0.061,
            -0.0534,
            -0.0046,
            -0.0545,
            -0.0758,
            0.0128,
            0.0289,
            0.044,
            0.0663,
            0.1345
          ],
          "after": [
            0.0439,
            -0.147,
            -0.2942,
            0.0515,
            0.031,
            -0.0458,
            -0.151,
            0.0705,
            0.1115,
            0.009,
            0.0124,
            0.0851,
            0.0391,
            0.0227,
            -0.129,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            -0.0018,
            0.0014,
            -0.0007,
            0.0018,
            -0.0009,
            0.0002,
            0.0008,
            0.0016,
            0.0037,
            0.002,
            0.0001,
            0.0013,
            -0.0005,
            -0.0048,
            0.0004
          ],
          "after": [
            -0.0849,
            0.0095,
            -0.1224,
            -0.0739,
            0.0669,
            0.1356,
            0.0873,
            -0.1617,
            0.0302,
            -0.025,
            -0.1466,
            -0.0672,
            0.0945,
            -0.079,
            0.0403,
            0.0837
          ]
        }
      }
    },
    {
      "step": 909,
      "word": "ardis",
      "loss": 2.3077,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0792,
            -0.1583,
            -0.132,
            0.0349,
            -0.0634,
            0.2438,
            0.2166,
            -0.2236,
            0.1192,
            -0.1447,
            0.1263,
            0.0375,
            -0.251,
            -0.236,
            0.2028,
            0.1681
          ],
          "after": [
            0.0092,
            -0.0034,
            0.1537,
            -0.0263,
            0.029,
            -0.094,
            -0.1184,
            -0.1603,
            -0.008,
            0.0718,
            0.1618,
            0.0144,
            -0.1048,
            -0.0171,
            -0.0352,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0067,
            0.1047,
            -0.1469,
            0.0194,
            0.0436,
            -0.1761,
            0.2566,
            0.0055,
            -0.1813,
            0.0247,
            -0.0345,
            -0.0462,
            0.1191,
            -0.0589,
            -0.2125,
            0.1033
          ],
          "after": [
            0.028,
            0.1074,
            -0.0416,
            -0.0776,
            0.0321,
            -0.0111,
            0.0901,
            0.1004,
            -0.0657,
            -0.0619,
            0.0143,
            -0.0169,
            0.1069,
            0.0334,
            -0.0524,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.0985,
            -0.0453,
            0.1041,
            -0.1801,
            0.0608,
            0.0292,
            -0.3017,
            -0.1129,
            0.1329,
            0.0684,
            0.2612,
            0.0652,
            -0.1633,
            0.0067,
            -0.105,
            -0.2378
          ],
          "after": [
            0.0438,
            -0.1469,
            -0.2943,
            0.0517,
            0.0309,
            -0.0458,
            -0.1508,
            0.0707,
            0.1114,
            0.0089,
            0.0123,
            0.085,
            0.0391,
            0.0227,
            -0.129,
            0.0523
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0003,
            -0.001,
            -0.004,
            0.0025,
            -0.0036,
            0.0053,
            -0.002,
            -0.0014,
            -0.0014,
            0.0017,
            -0.0004,
            -0.0,
            -0.0031,
            -0.0012,
            -0.002
          ],
          "after": [
            -0.0849,
            0.0096,
            -0.1224,
            -0.0738,
            0.0668,
            0.1357,
            0.0872,
            -0.1617,
            0.0302,
            -0.0252,
            -0.1467,
            -0.0672,
            0.0945,
            -0.0789,
            0.0404,
            0.0837
          ]
        }
      }
    },
    {
      "step": 910,
      "word": "riott",
      "loss": 2.869,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0092,
            -0.0034,
            0.1537,
            -0.0262,
            0.0289,
            -0.094,
            -0.1184,
            -0.1602,
            -0.0082,
            0.0718,
            0.1617,
            0.0143,
            -0.1046,
            -0.017,
            -0.0352,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0758,
            0.1681,
            -0.1636,
            0.0628,
            0.0205,
            -0.1357,
            0.3087,
            0.0008,
            -0.1892,
            -0.0083,
            -0.0229,
            -0.0798,
            0.1568,
            -0.0804,
            -0.1828,
            0.1005
          ],
          "after": [
            0.0279,
            0.1074,
            -0.0418,
            -0.0775,
            0.0321,
            -0.0112,
            0.0902,
            0.1005,
            -0.0658,
            -0.062,
            0.0142,
            -0.0169,
            0.107,
            0.0333,
            -0.0524,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            0.015,
            -0.0591,
            0.0284,
            -0.0512,
            -0.0617,
            0.1522,
            0.0165,
            -0.0757,
            0.18,
            -0.1112,
            -0.1134,
            -0.0143,
            -0.2163,
            -0.0674,
            0.1761,
            0.085
          ],
          "after": [
            0.0438,
            -0.1468,
            -0.2944,
            0.0518,
            0.0308,
            -0.0459,
            -0.1507,
            0.0709,
            0.1113,
            0.0088,
            0.0123,
            0.085,
            0.0392,
            0.0228,
            -0.129,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            0.0029,
            0.0,
            0.0039,
            -0.0001,
            0.0005,
            0.001,
            0.001,
            0.0006,
            0.0009,
            0.0022,
            0.0027,
            0.0,
            -0.0022,
            -0.0003,
            -0.001
          ],
          "after": [
            -0.0849,
            0.0096,
            -0.1224,
            -0.0738,
            0.0668,
            0.1357,
            0.0871,
            -0.1617,
            0.0302,
            -0.0253,
            -0.1468,
            -0.0672,
            0.0945,
            -0.0788,
            0.0405,
            0.0838
          ]
        }
      }
    },
    {
      "step": 911,
      "word": "sarkis",
      "loss": 2.2854,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0787,
            -0.0503,
            -0.0834,
            -0.0847,
            0.0746,
            0.0034,
            0.1213,
            -0.1613,
            0.1481,
            -0.0824,
            0.0371,
            0.0254,
            -0.0699,
            -0.0891,
            0.0515,
            0.0835
          ],
          "after": [
            0.0092,
            -0.0034,
            0.1537,
            -0.0261,
            0.0289,
            -0.0941,
            -0.1185,
            -0.1601,
            -0.0083,
            0.0719,
            0.1616,
            0.0143,
            -0.1045,
            -0.0168,
            -0.0352,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0437,
            0.0963,
            -0.1534,
            -0.0081,
            0.0459,
            -0.1753,
            0.2875,
            0.0297,
            -0.1871,
            0.0169,
            -0.0279,
            0.0188,
            0.0428,
            -0.1124,
            -0.1295,
            0.1643
          ],
          "after": [
            0.0279,
            0.1075,
            -0.0418,
            -0.0774,
            0.032,
            -0.0112,
            0.0902,
            0.1006,
            -0.0659,
            -0.062,
            0.0141,
            -0.0169,
            0.1071,
            0.0334,
            -0.0524,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.0096,
            0.0922,
            -0.0323,
            0.111,
            -0.0943,
            0.0605,
            0.0425,
            0.0454,
            -0.0859,
            0.0365,
            -0.1076,
            0.1112,
            0.0932,
            -0.0133,
            0.0746,
            0.0345
          ],
          "after": [
            0.0438,
            -0.1468,
            -0.2945,
            0.0519,
            0.0308,
            -0.046,
            -0.1506,
            0.0711,
            0.1112,
            0.0088,
            0.0123,
            0.0849,
            0.0393,
            0.0228,
            -0.129,
            0.0525
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0007,
            0.0006,
            0.0005,
            -0.0006,
            0.0007,
            -0.0014,
            0.0008,
            -0.001,
            -0.0,
            0.0004,
            0.0005,
            -0.0004,
            0.0006,
            0.0001,
            -0.0014,
            -0.0008
          ],
          "after": [
            -0.0849,
            0.0096,
            -0.1223,
            -0.0738,
            0.0667,
            0.1358,
            0.087,
            -0.1617,
            0.0302,
            -0.0254,
            -0.1469,
            -0.0673,
            0.0944,
            -0.0787,
            0.0405,
            0.0839
          ]
        }
      }
    },
    {
      "step": 912,
      "word": "lynex",
      "loss": 2.9121,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0092,
            -0.0034,
            0.1537,
            -0.026,
            0.0288,
            -0.0942,
            -0.1186,
            -0.16,
            -0.0084,
            0.0719,
            0.1615,
            0.0142,
            -0.1043,
            -0.0167,
            -0.0352,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0665,
            0.0343,
            -0.0052,
            -0.0751,
            -0.0929,
            -0.1013,
            -0.2833,
            -0.1279,
            -0.0958,
            -0.3467,
            -0.0059,
            -0.1858,
            0.0105,
            0.0387,
            0.2811,
            -0.0344
          ],
          "after": [
            0.0279,
            0.1075,
            -0.0419,
            -0.0773,
            0.032,
            -0.0112,
            0.0903,
            0.1007,
            -0.0659,
            -0.062,
            0.014,
            -0.0169,
            0.1071,
            0.0334,
            -0.0524,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            -0.1086,
            0.11,
            -0.1159,
            -0.0436,
            0.0176,
            0.1082,
            -0.0119,
            0.1337,
            -0.0421,
            -0.0604,
            -0.1857,
            0.2436,
            -0.0784,
            0.0432,
            0.141,
            -0.082
          ],
          "after": [
            0.0438,
            -0.1468,
            -0.2945,
            0.052,
            0.0308,
            -0.0461,
            -0.1506,
            0.0711,
            0.1112,
            0.0088,
            0.0124,
            0.0847,
            0.0394,
            0.0229,
            -0.1291,
            0.0525
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0007,
            0.0015,
            -0.0011,
            -0.0011,
            -0.0031,
            0.0001,
            -0.0008,
            -0.0009,
            0.0002,
            0.0029,
            -0.0053,
            0.0031,
            -0.0009,
            -0.0008,
            0.0004
          ],
          "after": [
            -0.0849,
            0.0096,
            -0.1224,
            -0.0738,
            0.0667,
            0.136,
            0.087,
            -0.1617,
            0.0302,
            -0.0255,
            -0.1471,
            -0.0672,
            0.0944,
            -0.0786,
            0.0406,
            0.084
          ]
        }
      }
    },
    {
      "step": 913,
      "word": "rhyan",
      "loss": 2.3507,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0011,
            0.2045,
            -0.2563,
            0.0388,
            -0.2267,
            -0.2274,
            0.0604,
            -0.1168,
            0.0848,
            -0.0584,
            -0.0246,
            0.0275,
            -0.0934,
            0.0102,
            0.1213,
            0.0518
          ],
          "after": [
            0.0092,
            -0.0035,
            0.1538,
            -0.0259,
            0.0288,
            -0.0941,
            -0.1187,
            -0.1599,
            -0.0086,
            0.072,
            0.1615,
            0.0142,
            -0.1042,
            -0.0166,
            -0.0352,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0392,
            0.0974,
            -0.2167,
            0.0189,
            -0.0365,
            -0.1977,
            0.3276,
            0.0303,
            -0.2406,
            0.0226,
            -0.0848,
            -0.0784,
            0.193,
            -0.075,
            -0.1766,
            0.1753
          ],
          "after": [
            0.028,
            0.1075,
            -0.0419,
            -0.0773,
            0.032,
            -0.0111,
            0.0903,
            0.1008,
            -0.0659,
            -0.0619,
            0.0139,
            -0.0168,
            0.1071,
            0.0334,
            -0.0524,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            0.0127,
            -0.056,
            0.0181,
            -0.0384,
            -0.0764,
            0.1599,
            0.0712,
            -0.0579,
            0.1729,
            -0.1219,
            -0.1304,
            0.0122,
            -0.2065,
            -0.0644,
            0.1957,
            0.1064
          ],
          "after": [
            0.0439,
            -0.1469,
            -0.2945,
            0.0521,
            0.0309,
            -0.0462,
            -0.1505,
            0.0712,
            0.1111,
            0.0089,
            0.0125,
            0.0845,
            0.0395,
            0.0229,
            -0.1293,
            0.0526
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            0.0018,
            0.0001,
            0.0002,
            0.0001,
            -0.0009,
            0.0023,
            0.0003,
            -0.0003,
            -0.0021,
            0.0001,
            0.0011,
            -0.0001,
            -0.0023,
            0.0009,
            -0.001
          ],
          "after": [
            -0.0848,
            0.0096,
            -0.1224,
            -0.0738,
            0.0667,
            0.1361,
            0.0869,
            -0.1616,
            0.0302,
            -0.0256,
            -0.1473,
            -0.0672,
            0.0943,
            -0.0784,
            0.0407,
            0.0841
          ]
        }
      }
    },
    {
      "step": 914,
      "word": "tedrick",
      "loss": 2.9333,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0092,
            -0.0035,
            0.1539,
            -0.0259,
            0.0288,
            -0.0941,
            -0.1187,
            -0.1598,
            -0.0087,
            0.072,
            0.1615,
            0.0141,
            -0.1041,
            -0.0166,
            -0.0353,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0599,
            -0.1092,
            0.2367,
            -0.0719,
            0.2802,
            -0.0232,
            -0.1436,
            0.0387,
            0.176,
            0.0571,
            0.0294,
            0.2943,
            -0.2731,
            -0.0605,
            -0.1084,
            0.1229
          ],
          "after": [
            0.0279,
            0.1075,
            -0.042,
            -0.0772,
            0.0319,
            -0.0111,
            0.0903,
            0.1008,
            -0.0659,
            -0.0619,
            0.0139,
            -0.0169,
            0.1072,
            0.0334,
            -0.0523,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            -0.0218,
            -0.1287,
            0.0562,
            0.0447,
            -0.0726,
            0.065,
            0.0453,
            0.0037,
            -0.0845,
            -0.0634,
            -0.0565,
            -0.1888,
            0.095,
            0.0359,
            0.0833,
            0.0908
          ],
          "after": [
            0.0439,
            -0.1468,
            -0.2945,
            0.0521,
            0.0309,
            -0.0464,
            -0.1505,
            0.0713,
            0.111,
            0.009,
            0.0126,
            0.0845,
            0.0395,
            0.023,
            -0.1294,
            0.0526
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0002,
            0.0004,
            -0.0001,
            0.0014,
            -0.0,
            0.0005,
            0.0001,
            -0.0003,
            0.0008,
            -0.0004,
            0.0005,
            -0.0003,
            0.0,
            -0.0014,
            -0.0
          ],
          "after": [
            -0.0848,
            0.0096,
            -0.1224,
            -0.0738,
            0.0667,
            0.1362,
            0.0868,
            -0.1616,
            0.0302,
            -0.0257,
            -0.1474,
            -0.0672,
            0.0942,
            -0.0783,
            0.0407,
            0.0841
          ]
        }
      }
    },
    {
      "step": 915,
      "word": "ronza",
      "loss": 2.4342,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3451,
            -0.0774,
            -0.1508,
            -0.0828,
            -0.0518,
            -0.071,
            -0.0467,
            0.0319,
            -0.1327,
            0.1996,
            0.1294,
            -0.3228,
            0.1681,
            0.1455,
            0.0019,
            -0.2575
          ],
          "after": [
            0.0091,
            -0.0035,
            0.1539,
            -0.0258,
            0.0288,
            -0.0941,
            -0.1188,
            -0.1597,
            -0.0088,
            0.072,
            0.1614,
            0.0141,
            -0.1041,
            -0.0166,
            -0.0353,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0603,
            0.1321,
            -0.1885,
            0.0401,
            -0.0416,
            -0.1508,
            0.3155,
            0.029,
            -0.1995,
            0.0352,
            -0.0335,
            -0.022,
            0.1582,
            -0.1002,
            -0.1697,
            0.1316
          ],
          "after": [
            0.028,
            0.1075,
            -0.042,
            -0.0771,
            0.0318,
            -0.011,
            0.0903,
            0.1009,
            -0.0659,
            -0.0619,
            0.0139,
            -0.0169,
            0.1072,
            0.0335,
            -0.0523,
            0.2231
          ]
        },
        "position_0": {
          "grad": [
            0.0103,
            -0.0429,
            0.0144,
            -0.0497,
            -0.0363,
            0.1593,
            0.0839,
            -0.0548,
            0.1543,
            -0.1137,
            -0.1283,
            0.044,
            -0.1852,
            -0.0807,
            0.1644,
            0.1251
          ],
          "after": [
            0.0439,
            -0.1467,
            -0.2945,
            0.0522,
            0.031,
            -0.0466,
            -0.1505,
            0.0714,
            0.1109,
            0.0091,
            0.0127,
            0.0844,
            0.0397,
            0.023,
            -0.1296,
            0.0525
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            -0.0013,
            -0.0007,
            0.0001,
            -0.0009,
            0.0015,
            0.001,
            0.0014,
            0.0012,
            0.002,
            -0.0002,
            0.0002,
            0.0006,
            0.0009,
            -0.0005,
            0.0011
          ],
          "after": [
            -0.0848,
            0.0096,
            -0.1225,
            -0.0738,
            0.0666,
            0.1363,
            0.0867,
            -0.1617,
            0.0302,
            -0.0258,
            -0.1475,
            -0.0672,
            0.0942,
            -0.0782,
            0.0408,
            0.0842
          ]
        }
      }
    },
    {
      "step": 916,
      "word": "mica",
      "loss": 2.3139,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5644,
            -0.1094,
            -0.4969,
            -0.1569,
            -0.2807,
            -0.1299,
            -0.0351,
            0.0597,
            -0.2274,
            0.2918,
            0.1437,
            -0.6154,
            0.3146,
            0.2692,
            0.14,
            -0.293
          ],
          "after": [
            0.0089,
            -0.0035,
            0.1541,
            -0.0257,
            0.0289,
            -0.094,
            -0.1188,
            -0.1597,
            -0.0087,
            0.0719,
            0.1613,
            0.0143,
            -0.1041,
            -0.0166,
            -0.0354,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.022,
            0.107,
            -0.276,
            0.0248,
            -0.0408,
            -0.1597,
            0.2323,
            0.0171,
            -0.2927,
            0.0494,
            -0.0516,
            -0.1186,
            0.2027,
            -0.0196,
            -0.1962,
            0.1904
          ],
          "after": [
            0.028,
            0.1075,
            -0.042,
            -0.0771,
            0.0318,
            -0.0109,
            0.0902,
            0.1009,
            -0.0659,
            -0.0619,
            0.0139,
            -0.0169,
            0.1072,
            0.0335,
            -0.0522,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.0125,
            0.0372,
            -0.0538,
            0.0206,
            0.139,
            -0.0174,
            0.2684,
            0.0967,
            0.0552,
            -0.0037,
            -0.0274,
            0.023,
            -0.0788,
            -0.017,
            0.0196,
            0.17
          ],
          "after": [
            0.044,
            -0.1467,
            -0.2945,
            0.0522,
            0.031,
            -0.0468,
            -0.1505,
            0.0714,
            0.1108,
            0.0092,
            0.0128,
            0.0844,
            0.0398,
            0.0231,
            -0.1298,
            0.0525
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0024,
            -0.002,
            0.004,
            -0.0113,
            0.0094,
            -0.0043,
            0.0022,
            -0.0034,
            0.0037,
            0.0014,
            0.0008,
            -0.0057,
            0.0011,
            0.0014,
            -0.0052,
            -0.0072
          ],
          "after": [
            -0.0847,
            0.0096,
            -0.1226,
            -0.0736,
            0.0665,
            0.1364,
            0.0866,
            -0.1616,
            0.0301,
            -0.0259,
            -0.1477,
            -0.067,
            0.0941,
            -0.0781,
            0.0409,
            0.0843
          ]
        }
      }
    },
    {
      "step": 917,
      "word": "jubran",
      "loss": 2.2342,
      "learning_rate": 0.0003,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0816,
            0.177,
            -0.0476,
            0.0771,
            -0.0356,
            -0.11,
            0.0061,
            -0.1132,
            0.046,
            -0.222,
            -0.0711,
            0.1503,
            -0.1326,
            -0.0439,
            0.0157,
            0.1029
          ],
          "after": [
            0.0088,
            -0.0035,
            0.1543,
            -0.0256,
            0.0289,
            -0.0939,
            -0.1189,
            -0.1596,
            -0.0087,
            0.0719,
            0.1612,
            0.0144,
            -0.1041,
            -0.0167,
            -0.0355,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0645,
            0.1542,
            -0.1533,
            0.022,
            -0.0269,
            -0.2011,
            0.2854,
            0.0565,
            -0.2448,
            0.0371,
            -0.0507,
            -0.0234,
            0.1764,
            -0.049,
            -0.1499,
            0.1641
          ],
          "after": [
            0.028,
            0.1074,
            -0.0419,
            -0.077,
            0.0318,
            -0.0108,
            0.0902,
            0.1009,
            -0.0658,
            -0.0619,
            0.0139,
            -0.0169,
            0.1071,
            0.0336,
            -0.0521,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0015,
            0.0408,
            -0.0186,
            0.1615,
            -0.0105,
            -0.0872,
            0.2175,
            0.0686,
            -0.2255,
            -0.088,
            0.0101,
            0.0231,
            0.3007,
            -0.0791,
            -0.0642,
            0.1801
          ],
          "after": [
            0.044,
            -0.1467,
            -0.2945,
            0.0522,
            0.031,
            -0.0469,
            -0.1506,
            0.0714,
            0.1108,
            0.0093,
            0.0129,
            0.0843,
            0.0398,
            0.0232,
            -0.13,
            0.0524
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0004,
            0.0009,
            0.0002,
            0.0009,
            -0.0004,
            0.0015,
            0.0006,
            -0.0015,
            -0.0004,
            -0.0004,
            0.0005,
            0.0011,
            0.0001,
            -0.0007,
            0.0003
          ],
          "after": [
            -0.0847,
            0.0097,
            -0.1227,
            -0.0734,
            0.0663,
            0.1366,
            0.0865,
            -0.1616,
            0.0301,
            -0.0259,
            -0.1477,
            -0.067,
            0.094,
            -0.0781,
            0.0411,
            0.0845
          ]
        }
      }
    },
    {
      "step": 918,
      "word": "blaine",
      "loss": 2.3384,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0351,
            -0.2961,
            0.0906,
            -0.134,
            0.1444,
            0.1003,
            -0.1993,
            0.0849,
            0.0243,
            -0.0217,
            0.1729,
            -0.1805,
            -0.1582,
            -0.0326,
            -0.0281,
            -0.1317
          ],
          "after": [
            0.0087,
            -0.0035,
            0.1544,
            -0.0255,
            0.029,
            -0.0938,
            -0.1188,
            -0.1595,
            -0.0087,
            0.0718,
            0.1611,
            0.0145,
            -0.1041,
            -0.0167,
            -0.0355,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2648,
            0.1003,
            -0.1753,
            -0.121,
            -0.5964,
            0.0168,
            -0.3127,
            -0.1946,
            -0.0504,
            -0.1024,
            0.0577,
            -0.4853,
            0.2628,
            0.3512,
            0.2269,
            -0.059
          ],
          "after": [
            0.0279,
            0.1074,
            -0.0419,
            -0.077,
            0.0319,
            -0.0108,
            0.0902,
            0.101,
            -0.0657,
            -0.0619,
            0.0139,
            -0.0167,
            0.107,
            0.0335,
            -0.0521,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            0.0518,
            -0.0067,
            0.0763,
            0.0086,
            -0.0156,
            0.034,
            0.111,
            0.0251,
            -0.0938,
            -0.0506,
            -0.0315,
            0.0076,
            0.1366,
            -0.1118,
            -0.0007,
            0.2671
          ],
          "after": [
            0.044,
            -0.1467,
            -0.2945,
            0.0522,
            0.031,
            -0.047,
            -0.1507,
            0.0713,
            0.1108,
            0.0095,
            0.013,
            0.0842,
            0.0398,
            0.0234,
            -0.1301,
            0.0522
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            0.0036,
            0.004,
            -0.0005,
            -0.0089,
            -0.0016,
            -0.0081,
            -0.0026,
            -0.0027,
            -0.0054,
            -0.0003,
            -0.0077,
            0.0048,
            0.0079,
            0.0042,
            0.0018
          ],
          "after": [
            -0.0846,
            0.0096,
            -0.1229,
            -0.0733,
            0.0663,
            0.1367,
            0.0865,
            -0.1615,
            0.0301,
            -0.0259,
            -0.1478,
            -0.0668,
            0.0939,
            -0.0783,
            0.0411,
            0.0846
          ]
        }
      }
    },
    {
      "step": 919,
      "word": "kolette",
      "loss": 2.4211,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0086,
            -0.0034,
            0.1545,
            -0.0254,
            0.029,
            -0.0938,
            -0.1188,
            -0.1595,
            -0.0087,
            0.0718,
            0.161,
            0.0146,
            -0.104,
            -0.0167,
            -0.0355,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3021,
            -0.5923,
            0.0754,
            -0.2995,
            -0.1482,
            0.2352,
            -0.6339,
            0.1207,
            0.1655,
            0.118,
            -0.0372,
            -0.1545,
            -0.1309,
            0.1892,
            0.4291,
            -0.3776
          ],
          "after": [
            0.0278,
            0.1074,
            -0.0418,
            -0.0768,
            0.032,
            -0.0107,
            0.0902,
            0.101,
            -0.0657,
            -0.0619,
            0.0139,
            -0.0166,
            0.107,
            0.0335,
            -0.0521,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0789,
            0.0458,
            -0.1011,
            0.0172,
            0.0385,
            -0.0304,
            0.1566,
            -0.0119,
            -0.0391,
            0.002,
            -0.0133,
            0.0177,
            0.0468,
            -0.0134,
            -0.0786,
            0.0167
          ],
          "after": [
            0.044,
            -0.1467,
            -0.2945,
            0.0522,
            0.031,
            -0.047,
            -0.1508,
            0.0713,
            0.1109,
            0.0096,
            0.0131,
            0.0842,
            0.0397,
            0.0235,
            -0.1301,
            0.0521
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0008,
            0.0004,
            -0.0003,
            0.003,
            -0.0006,
            -0.0008,
            0.0009,
            -0.0002,
            0.0007,
            0.0006,
            -0.0007,
            0.0004,
            -0.0012,
            -0.0018,
            -0.0028
          ],
          "after": [
            -0.0846,
            0.0096,
            -0.1231,
            -0.0731,
            0.0663,
            0.1369,
            0.0866,
            -0.1614,
            0.0301,
            -0.0259,
            -0.1479,
            -0.0666,
            0.0937,
            -0.0784,
            0.0412,
            0.0847
          ]
        }
      }
    },
    {
      "step": 920,
      "word": "swan",
      "loss": 2.6116,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2169,
            0.1938,
            -0.3587,
            0.1458,
            -0.4987,
            -0.0947,
            -0.1429,
            -0.1675,
            0.0946,
            -0.0724,
            -0.1384,
            -0.1198,
            0.0149,
            0.3087,
            0.0829,
            0.114
          ],
          "after": [
            0.0084,
            -0.0035,
            0.1547,
            -0.0254,
            0.0291,
            -0.0937,
            -0.1187,
            -0.1594,
            -0.0088,
            0.0718,
            0.1609,
            0.0147,
            -0.104,
            -0.0168,
            -0.0356,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0055,
            0.072,
            -0.2626,
            -0.0154,
            -0.0361,
            -0.2267,
            0.3704,
            0.0696,
            -0.3168,
            0.0039,
            -0.04,
            -0.0648,
            0.1308,
            -0.116,
            -0.0995,
            0.1798
          ],
          "after": [
            0.0277,
            0.1075,
            -0.0418,
            -0.0766,
            0.0321,
            -0.0107,
            0.0902,
            0.101,
            -0.0656,
            -0.0619,
            0.0139,
            -0.0165,
            0.1069,
            0.0334,
            -0.0521,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0042,
            0.1237,
            -0.0237,
            0.2555,
            -0.2188,
            0.0735,
            0.0829,
            0.0848,
            -0.1198,
            -0.091,
            -0.1882,
            0.1517,
            0.149,
            0.0965,
            0.2555,
            0.1235
          ],
          "after": [
            0.0441,
            -0.1467,
            -0.2945,
            0.0521,
            0.031,
            -0.0471,
            -0.1509,
            0.0712,
            0.1109,
            0.0097,
            0.0132,
            0.0841,
            0.0397,
            0.0236,
            -0.1303,
            0.052
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            -0.0001,
            -0.0006,
            0.0002,
            -0.0005,
            -0.0007,
            0.001,
            -0.0008,
            -0.0011,
            0.0001,
            0.0011,
            0.0001,
            -0.0005,
            -0.0002,
            -0.0004,
            -0.0004
          ],
          "after": [
            -0.0845,
            0.0096,
            -0.1232,
            -0.073,
            0.0662,
            0.137,
            0.0866,
            -0.1613,
            0.0302,
            -0.0259,
            -0.148,
            -0.0665,
            0.0936,
            -0.0784,
            0.0412,
            0.0848
          ]
        }
      }
    },
    {
      "step": 921,
      "word": "kane",
      "loss": 2.0032,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.114,
            0.1673,
            -0.363,
            -0.0317,
            -0.5008,
            -0.14,
            -0.2103,
            -0.0999,
            0.1998,
            -0.3181,
            -0.1949,
            -0.1897,
            0.0344,
            0.3201,
            0.228,
            0.137
          ],
          "after": [
            0.0083,
            -0.0035,
            0.1549,
            -0.0254,
            0.0293,
            -0.0936,
            -0.1186,
            -0.1593,
            -0.0089,
            0.0719,
            0.161,
            0.0148,
            -0.104,
            -0.017,
            -0.0357,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0901,
            0.0657,
            -0.0145,
            -0.1374,
            -0.1411,
            -0.1253,
            -0.3479,
            -0.0937,
            -0.1223,
            -0.4446,
            -0.0483,
            -0.231,
            -0.013,
            0.0905,
            0.4081,
            -0.0491
          ],
          "after": [
            0.0277,
            0.1075,
            -0.0417,
            -0.0764,
            0.0322,
            -0.0106,
            0.0903,
            0.101,
            -0.0655,
            -0.0618,
            0.0139,
            -0.0163,
            0.1069,
            0.0333,
            -0.0522,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0998,
            0.0365,
            -0.1583,
            0.0286,
            -0.0351,
            -0.105,
            0.1804,
            0.02,
            0.0096,
            -0.0573,
            -0.1143,
            -0.027,
            0.01,
            0.016,
            -0.0245,
            -0.0429
          ],
          "after": [
            0.0442,
            -0.1468,
            -0.2944,
            0.052,
            0.0311,
            -0.0472,
            -0.1511,
            0.0712,
            0.111,
            0.0099,
            0.0133,
            0.084,
            0.0396,
            0.0236,
            -0.1304,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0007,
            0.001,
            -0.0018,
            -0.0004,
            -0.0027,
            -0.0024,
            -0.0017,
            -0.0008,
            -0.0032,
            -0.0003,
            -0.0024,
            -0.0005,
            0.001,
            0.0023,
            -0.0018
          ],
          "after": [
            -0.0845,
            0.0096,
            -0.1233,
            -0.0729,
            0.0662,
            0.1372,
            0.0866,
            -0.1612,
            0.0302,
            -0.0258,
            -0.148,
            -0.0663,
            0.0935,
            -0.0785,
            0.0412,
            0.0849
          ]
        }
      }
    },
    {
      "step": 922,
      "word": "emilia",
      "loss": 2.0146,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0868,
            -0.0117,
            -0.0202,
            -0.0815,
            -0.022,
            -0.0437,
            -0.0255,
            -0.016,
            -0.0452,
            0.0866,
            0.085,
            -0.0918,
            0.0462,
            0.037,
            -0.0168,
            -0.1143
          ],
          "after": [
            0.0082,
            -0.0036,
            0.1551,
            -0.0253,
            0.0295,
            -0.0935,
            -0.1185,
            -0.1592,
            -0.0089,
            0.072,
            0.161,
            0.0149,
            -0.104,
            -0.0171,
            -0.0358,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3343,
            0.4789,
            0.4785,
            0.0239,
            0.1199,
            -0.1145,
            0.4062,
            -0.0571,
            -0.0001,
            -0.2526,
            0.0024,
            -0.0002,
            -0.0698,
            -0.0081,
            -0.0966,
            -0.025
          ],
          "after": [
            0.0277,
            0.1074,
            -0.0417,
            -0.0763,
            0.0323,
            -0.0105,
            0.0902,
            0.101,
            -0.0655,
            -0.0617,
            0.0139,
            -0.0162,
            0.1068,
            0.0333,
            -0.0523,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0083,
            -0.1598,
            0.0681,
            -0.0519,
            0.0489,
            0.0817,
            -0.2392,
            -0.0967,
            0.1107,
            0.1577,
            0.0504,
            -0.0115,
            -0.1211,
            -0.0256,
            -0.0642,
            -0.2819
          ],
          "after": [
            0.0442,
            -0.1468,
            -0.2943,
            0.0519,
            0.0312,
            -0.0472,
            -0.1511,
            0.0712,
            0.111,
            0.0099,
            0.0134,
            0.0839,
            0.0396,
            0.0237,
            -0.1305,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0009,
            0.0017,
            0.0009,
            -0.0046,
            -0.0001,
            -0.0037,
            -0.0002,
            0.0009,
            0.0008,
            0.0023,
            0.0023,
            -0.0028,
            -0.0007,
            0.0033,
            -0.0011
          ],
          "after": [
            -0.0845,
            0.0096,
            -0.1235,
            -0.0728,
            0.0663,
            0.1373,
            0.0867,
            -0.1611,
            0.0303,
            -0.0258,
            -0.1481,
            -0.0662,
            0.0935,
            -0.0786,
            0.0412,
            0.085
          ]
        }
      }
    },
    {
      "step": 923,
      "word": "carissa",
      "loss": 2.1578,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1498,
            -0.0587,
            -0.0706,
            -0.1754,
            0.0883,
            -0.006,
            0.0684,
            -0.1793,
            0.0696,
            0.0262,
            0.1226,
            -0.0337,
            -0.0747,
            -0.0731,
            -0.0149,
            -0.014
          ],
          "after": [
            0.008,
            -0.0036,
            0.1552,
            -0.0252,
            0.0296,
            -0.0934,
            -0.1185,
            -0.1591,
            -0.009,
            0.072,
            0.1609,
            0.015,
            -0.104,
            -0.0173,
            -0.0359,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.022,
            0.0391,
            -0.1535,
            0.0055,
            0.0212,
            -0.1104,
            0.1723,
            0.0013,
            -0.1749,
            0.029,
            -0.0105,
            -0.0248,
            0.071,
            -0.0745,
            -0.1278,
            0.1162
          ],
          "after": [
            0.0277,
            0.1074,
            -0.0417,
            -0.0762,
            0.0323,
            -0.0104,
            0.0902,
            0.1011,
            -0.0654,
            -0.0615,
            0.014,
            -0.016,
            0.1068,
            0.0333,
            -0.0523,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0285,
            0.0261,
            -0.0344,
            0.071,
            -0.0161,
            -0.0301,
            0.11,
            0.0581,
            -0.0615,
            0.0585,
            0.02,
            0.0345,
            0.0294,
            -0.0458,
            -0.067,
            0.0428
          ],
          "after": [
            0.0443,
            -0.1468,
            -0.2942,
            0.0518,
            0.0312,
            -0.0473,
            -0.1512,
            0.0711,
            0.111,
            0.0099,
            0.0135,
            0.0839,
            0.0396,
            0.0238,
            -0.1305,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            0.0009,
            0.0031,
            0.002,
            -0.0018,
            0.0009,
            -0.0038,
            -0.0021,
            0.0006,
            0.0044,
            0.0002,
            -0.0057,
            0.0033,
            0.0041,
            -0.0002,
            -0.0031
          ],
          "after": [
            -0.0845,
            0.0095,
            -0.1237,
            -0.0728,
            0.0664,
            0.1374,
            0.0868,
            -0.161,
            0.0303,
            -0.0258,
            -0.1482,
            -0.066,
            0.0934,
            -0.0787,
            0.0412,
            0.0852
          ]
        }
      }
    },
    {
      "step": 924,
      "word": "chardonnay",
      "loss": 2.5535,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0995,
            -0.0835,
            0.0751,
            -0.0535,
            0.1657,
            0.2202,
            0.1403,
            0.0495,
            0.0545,
            0.026,
            -0.1269,
            0.3614,
            -0.0996,
            -0.1328,
            -0.0384,
            0.193
          ],
          "after": [
            0.0079,
            -0.0036,
            0.1554,
            -0.0251,
            0.0297,
            -0.0934,
            -0.1185,
            -0.159,
            -0.0091,
            0.0721,
            0.1609,
            0.0151,
            -0.104,
            -0.0173,
            -0.036,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0306,
            0.1025,
            -0.1469,
            0.0368,
            0.0344,
            -0.1635,
            0.2446,
            0.0284,
            -0.1963,
            -0.007,
            -0.0463,
            -0.0233,
            0.112,
            -0.0887,
            -0.1547,
            0.156
          ],
          "after": [
            0.0278,
            0.1073,
            -0.0417,
            -0.0761,
            0.0324,
            -0.0103,
            0.0902,
            0.1011,
            -0.0653,
            -0.0614,
            0.014,
            -0.0159,
            0.1067,
            0.0333,
            -0.0523,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0275,
            0.0232,
            -0.0286,
            0.0739,
            -0.0164,
            -0.0154,
            0.1045,
            0.0439,
            -0.0529,
            0.0385,
            0.0179,
            0.0347,
            0.0197,
            -0.0293,
            -0.0289,
            0.0481
          ],
          "after": [
            0.0444,
            -0.1468,
            -0.2942,
            0.0517,
            0.0313,
            -0.0473,
            -0.1513,
            0.0711,
            0.1111,
            0.0099,
            0.0136,
            0.0838,
            0.0396,
            0.0238,
            -0.1305,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            0.0012,
            -0.0015,
            0.0005,
            -0.0002,
            -0.0006,
            0.0018,
            0.0004,
            -0.0021,
            -0.0001,
            0.0004,
            -0.0011,
            0.0015,
            0.0,
            -0.0006,
            -0.0004
          ],
          "after": [
            -0.0845,
            0.0095,
            -0.1238,
            -0.0728,
            0.0664,
            0.1375,
            0.0869,
            -0.1609,
            0.0303,
            -0.0258,
            -0.1483,
            -0.0659,
            0.0933,
            -0.0788,
            0.0412,
            0.0854
          ]
        }
      }
    },
    {
      "step": 925,
      "word": "alem",
      "loss": 2.2106,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1199,
            0.1334,
            -0.2481,
            -0.1282,
            -0.146,
            -0.0682,
            -0.2324,
            0.1226,
            -0.0996,
            -0.117,
            -0.2511,
            0.3184,
            -0.2263,
            0.1376,
            0.2522,
            -0.5317
          ],
          "after": [
            0.0079,
            -0.0036,
            0.1555,
            -0.025,
            0.0298,
            -0.0933,
            -0.1184,
            -0.1589,
            -0.0091,
            0.0721,
            0.161,
            0.015,
            -0.1039,
            -0.0174,
            -0.0361,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0406,
            -0.3629,
            0.2712,
            0.0398,
            0.2687,
            0.393,
            -0.4779,
            0.1188,
            0.3335,
            0.2411,
            0.015,
            0.3191,
            -0.3627,
            0.04,
            0.1178,
            -0.3352
          ],
          "after": [
            0.0278,
            0.1073,
            -0.0417,
            -0.076,
            0.0323,
            -0.0103,
            0.0902,
            0.1011,
            -0.0653,
            -0.0614,
            0.014,
            -0.0159,
            0.1067,
            0.0333,
            -0.0523,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            0.1136,
            -0.0563,
            0.1362,
            -0.1801,
            0.0833,
            0.0119,
            -0.4155,
            -0.1437,
            0.1934,
            0.0608,
            0.3152,
            0.0495,
            -0.1901,
            0.0321,
            -0.1078,
            -0.3036
          ],
          "after": [
            0.0444,
            -0.1468,
            -0.2942,
            0.0517,
            0.0313,
            -0.0473,
            -0.1513,
            0.0711,
            0.111,
            0.0099,
            0.0135,
            0.0837,
            0.0396,
            0.0239,
            -0.1305,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            -0.0022,
            0.0023,
            -0.0001,
            -0.0007,
            0.0015,
            -0.0029,
            -0.0006,
            -0.0001,
            0.0011,
            0.002,
            -0.001,
            -0.0015,
            0.0009,
            -0.0009,
            -0.0022
          ],
          "after": [
            -0.0846,
            0.0095,
            -0.124,
            -0.0727,
            0.0665,
            0.1375,
            0.087,
            -0.1608,
            0.0304,
            -0.0259,
            -0.1485,
            -0.0657,
            0.0933,
            -0.079,
            0.0412,
            0.0855
          ]
        }
      }
    },
    {
      "step": 926,
      "word": "saier",
      "loss": 2.1532,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0501,
            -0.3877,
            0.0532,
            -0.1535,
            0.1194,
            0.1883,
            -0.3212,
            0.1302,
            0.0524,
            -0.0472,
            0.1864,
            -0.2328,
            -0.2465,
            -0.0246,
            0.0042,
            -0.1302
          ],
          "after": [
            0.0078,
            -0.0035,
            0.1556,
            -0.0249,
            0.0298,
            -0.0934,
            -0.1183,
            -0.1589,
            -0.0091,
            0.0722,
            0.161,
            0.015,
            -0.1037,
            -0.0175,
            -0.0362,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1848,
            0.0838,
            -0.0198,
            0.0806,
            -0.2359,
            -0.1642,
            -0.0391,
            0.0751,
            -0.0228,
            -0.3173,
            -0.0261,
            0.2606,
            -0.0991,
            -0.2079,
            0.4937,
            -0.0134
          ],
          "after": [
            0.0278,
            0.1073,
            -0.0417,
            -0.076,
            0.0324,
            -0.0103,
            0.0902,
            0.101,
            -0.0652,
            -0.0613,
            0.0141,
            -0.016,
            0.1068,
            0.0333,
            -0.0524,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0008,
            0.0856,
            -0.0215,
            0.0752,
            -0.1219,
            0.051,
            -0.0603,
            0.0292,
            -0.0463,
            0.0438,
            -0.1193,
            0.0623,
            0.0777,
            -0.0041,
            0.0672,
            -0.0217
          ],
          "after": [
            0.0444,
            -0.1468,
            -0.2942,
            0.0517,
            0.0313,
            -0.0474,
            -0.1512,
            0.0711,
            0.111,
            0.0099,
            0.0135,
            0.0836,
            0.0396,
            0.0239,
            -0.1305,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            -0.0015,
            -0.0,
            -0.003,
            0.0013,
            -0.0004,
            -0.002,
            -0.0017,
            0.0006,
            0.0031,
            0.0022,
            -0.004,
            0.0006,
            0.0034,
            -0.0017,
            -0.0017
          ],
          "after": [
            -0.0847,
            0.0096,
            -0.1241,
            -0.0727,
            0.0665,
            0.1376,
            0.0871,
            -0.1606,
            0.0304,
            -0.026,
            -0.1486,
            -0.0656,
            0.0932,
            -0.0791,
            0.0412,
            0.0857
          ]
        }
      }
    },
    {
      "step": 927,
      "word": "meeko",
      "loss": 2.5384,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0078,
            -0.0035,
            0.1557,
            -0.0247,
            0.0299,
            -0.0934,
            -0.1182,
            -0.1589,
            -0.0092,
            0.0722,
            0.161,
            0.015,
            -0.1036,
            -0.0175,
            -0.0363,
            -0.1861
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1393,
            -0.1929,
            0.3994,
            -0.0801,
            0.2573,
            0.6541,
            -0.442,
            -0.0501,
            0.7707,
            0.0082,
            0.1383,
            0.5727,
            -0.7853,
            -0.036,
            0.6067,
            -0.3195
          ],
          "after": [
            0.0279,
            0.1073,
            -0.0417,
            -0.076,
            0.0323,
            -0.0103,
            0.0903,
            0.101,
            -0.0653,
            -0.0612,
            0.014,
            -0.0161,
            0.1069,
            0.0334,
            -0.0525,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0061,
            0.025,
            -0.0399,
            -0.0266,
            0.1346,
            -0.0283,
            0.1797,
            0.0637,
            0.0641,
            0.0105,
            -0.0115,
            0.0049,
            -0.0889,
            -0.0112,
            -0.0115,
            0.1175
          ],
          "after": [
            0.0445,
            -0.1468,
            -0.2942,
            0.0517,
            0.0313,
            -0.0474,
            -0.1513,
            0.0711,
            0.111,
            0.0098,
            0.0135,
            0.0835,
            0.0396,
            0.024,
            -0.1305,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0008,
            -0.0009,
            0.0003,
            -0.0006,
            0.0016,
            -0.0004,
            -0.0006,
            0.0014,
            0.0019,
            0.0002,
            0.002,
            -0.0032,
            -0.0001,
            0.0017,
            0.0014
          ],
          "after": [
            -0.0848,
            0.0096,
            -0.1242,
            -0.0726,
            0.0666,
            0.1376,
            0.0872,
            -0.1605,
            0.0304,
            -0.0261,
            -0.1488,
            -0.0654,
            0.0932,
            -0.0793,
            0.0412,
            0.0858
          ]
        }
      }
    },
    {
      "step": 928,
      "word": "luisana",
      "loss": 2.3659,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0166,
            0.1424,
            -0.0542,
            -0.0179,
            -0.0245,
            -0.099,
            0.0013,
            -0.1242,
            -0.0074,
            -0.1272,
            -0.0098,
            0.0759,
            -0.072,
            -0.0048,
            -0.0161,
            0.0394
          ],
          "after": [
            0.0078,
            -0.0035,
            0.1558,
            -0.0246,
            0.0299,
            -0.0934,
            -0.1181,
            -0.1589,
            -0.0092,
            0.0723,
            0.1611,
            0.015,
            -0.1035,
            -0.0176,
            -0.0364,
            -0.1861
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0145,
            0.0656,
            -0.1244,
            0.0033,
            0.0213,
            -0.1453,
            0.1801,
            0.0,
            -0.1574,
            0.0239,
            0.0135,
            -0.0212,
            0.0707,
            -0.0753,
            -0.1287,
            0.1237
          ],
          "after": [
            0.0279,
            0.1073,
            -0.0418,
            -0.0759,
            0.0323,
            -0.0104,
            0.0903,
            0.1009,
            -0.0653,
            -0.0612,
            0.014,
            -0.0162,
            0.107,
            0.0334,
            -0.0527,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.077,
            0.089,
            -0.0941,
            -0.0825,
            0.0609,
            0.0722,
            -0.0372,
            0.0791,
            -0.0304,
            0.0049,
            -0.1358,
            0.1878,
            -0.0752,
            0.0036,
            0.0528,
            -0.0881
          ],
          "after": [
            0.0445,
            -0.1469,
            -0.2941,
            0.0517,
            0.0313,
            -0.0475,
            -0.1513,
            0.0711,
            0.111,
            0.0098,
            0.0136,
            0.0834,
            0.0397,
            0.024,
            -0.1305,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0022,
            -0.0003,
            0.0049,
            -0.0048,
            0.0036,
            0.0004,
            -0.0035,
            -0.0027,
            -0.0018,
            -0.0,
            -0.0052,
            0.0012,
            0.0025,
            0.0011,
            0.0017
          ],
          "after": [
            -0.0849,
            0.0095,
            -0.1243,
            -0.0727,
            0.0667,
            0.1375,
            0.0872,
            -0.1603,
            0.0304,
            -0.0261,
            -0.1489,
            -0.0653,
            0.0932,
            -0.0794,
            0.0412,
            0.0859
          ]
        }
      }
    },
    {
      "step": 929,
      "word": "daviana",
      "loss": 1.9287,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1541,
            0.1767,
            -0.0098,
            0.0851,
            0.2026,
            -0.0969,
            0.0451,
            -0.1707,
            -0.1466,
            -0.0203,
            0.0326,
            0.1387,
            -0.0235,
            -0.2019,
            -0.1774,
            0.1861
          ],
          "after": [
            0.0078,
            -0.0035,
            0.1559,
            -0.0246,
            0.0299,
            -0.0934,
            -0.118,
            -0.1588,
            -0.0092,
            0.0724,
            0.1611,
            0.015,
            -0.1034,
            -0.0175,
            -0.0364,
            -0.186
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0338,
            0.0933,
            -0.1077,
            0.0526,
            0.037,
            -0.1457,
            0.2329,
            0.0217,
            -0.2051,
            0.0293,
            -0.0336,
            -0.0202,
            0.1071,
            -0.0516,
            -0.1751,
            0.1063
          ],
          "after": [
            0.0279,
            0.1073,
            -0.0418,
            -0.0759,
            0.0323,
            -0.0104,
            0.0903,
            0.1009,
            -0.0654,
            -0.0611,
            0.014,
            -0.0163,
            0.1071,
            0.0335,
            -0.0527,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.0525,
            0.0547,
            -0.0372,
            -0.0773,
            0.1063,
            -0.0567,
            0.0521,
            -0.0648,
            -0.0218,
            -0.0004,
            -0.0461,
            0.0268,
            0.0257,
            0.0139,
            0.0096,
            0.1072
          ],
          "after": [
            0.0446,
            -0.1469,
            -0.294,
            0.0517,
            0.0312,
            -0.0475,
            -0.1513,
            0.071,
            0.111,
            0.0097,
            0.0136,
            0.0833,
            0.0397,
            0.024,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0045,
            0.0,
            -0.0016,
            0.0016,
            -0.007,
            -0.0014,
            -0.0085,
            -0.0021,
            -0.0012,
            -0.0039,
            -0.0001,
            -0.0048,
            -0.0018,
            0.005,
            0.0057,
            -0.0023
          ],
          "after": [
            -0.085,
            0.0095,
            -0.1243,
            -0.0727,
            0.0669,
            0.1375,
            0.0874,
            -0.1601,
            0.0305,
            -0.0261,
            -0.149,
            -0.0651,
            0.0933,
            -0.0796,
            0.0411,
            0.086
          ]
        }
      }
    },
    {
      "step": 930,
      "word": "isobella",
      "loss": 2.6651,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.023,
            -0.002,
            0.0064,
            -0.0299,
            -0.0142,
            -0.0065,
            -0.0243,
            -0.0299,
            0.0024,
            0.0268,
            0.0368,
            -0.0348,
            0.0093,
            0.0066,
            -0.0008,
            -0.0238
          ],
          "after": [
            0.0078,
            -0.0035,
            0.156,
            -0.0245,
            0.0299,
            -0.0933,
            -0.1179,
            -0.1587,
            -0.0092,
            0.0724,
            0.161,
            0.015,
            -0.1034,
            -0.0175,
            -0.0364,
            -0.186
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.075,
            -0.2244,
            -0.0484,
            -0.0579,
            0.1372,
            0.1883,
            -0.0616,
            -0.0594,
            0.167,
            -0.11,
            0.0102,
            -0.0122,
            -0.1979,
            -0.1268,
            0.0415,
            -0.0005
          ],
          "after": [
            0.0279,
            0.1073,
            -0.0418,
            -0.0759,
            0.0323,
            -0.0104,
            0.0903,
            0.1009,
            -0.0654,
            -0.0611,
            0.014,
            -0.0164,
            0.1072,
            0.0336,
            -0.0528,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.0506,
            -0.1633,
            0.0719,
            -0.1137,
            0.1028,
            0.1076,
            -0.1541,
            0.0174,
            0.0384,
            0.0271,
            0.0664,
            -0.0835,
            -0.1342,
            -0.0008,
            -0.0182,
            -0.1042
          ],
          "after": [
            0.0447,
            -0.1469,
            -0.294,
            0.0518,
            0.0311,
            -0.0475,
            -0.1513,
            0.071,
            0.111,
            0.0097,
            0.0137,
            0.0832,
            0.0397,
            0.024,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0009,
            0.001,
            0.0013,
            0.0011,
            0.0013,
            -0.0011,
            0.0011,
            -0.0004,
            -0.0,
            -0.0021,
            0.0009,
            0.0004,
            0.0002,
            -0.0023,
            -0.0016,
            0.0022
          ],
          "after": [
            -0.0851,
            0.0095,
            -0.1244,
            -0.0728,
            0.067,
            0.1375,
            0.0875,
            -0.1599,
            0.0306,
            -0.0261,
            -0.1491,
            -0.0649,
            0.0933,
            -0.0798,
            0.041,
            0.086
          ]
        }
      }
    },
    {
      "step": 931,
      "word": "shaquan",
      "loss": 3.0283,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.4825,
            0.1349,
            -0.2034,
            0.2496,
            0.2875,
            -0.4836,
            0.3394,
            -0.0116,
            -0.3926,
            -0.2857,
            -0.0446,
            -0.0905,
            0.1268,
            -0.1345,
            -0.0923,
            0.5908
          ],
          "after": [
            0.0079,
            -0.0036,
            0.1561,
            -0.0245,
            0.0299,
            -0.0932,
            -0.1179,
            -0.1587,
            -0.0091,
            0.0725,
            0.1611,
            0.015,
            -0.1033,
            -0.0175,
            -0.0364,
            -0.1861
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0079,
            0.0183,
            -0.1632,
            -0.026,
            0.0364,
            -0.161,
            0.1746,
            0.0323,
            -0.1603,
            0.0086,
            -0.0369,
            -0.0243,
            0.077,
            -0.0525,
            -0.1191,
            0.0989
          ],
          "after": [
            0.0279,
            0.1073,
            -0.0418,
            -0.0758,
            0.0322,
            -0.0104,
            0.0903,
            0.1009,
            -0.0654,
            -0.061,
            0.014,
            -0.0164,
            0.1073,
            0.0337,
            -0.0528,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.0085,
            0.1046,
            -0.0185,
            0.1246,
            -0.0698,
            0.0457,
            0.0335,
            0.0145,
            -0.093,
            -0.0029,
            -0.0715,
            0.1036,
            0.1019,
            0.0374,
            0.0773,
            0.0858
          ],
          "after": [
            0.0448,
            -0.147,
            -0.294,
            0.0518,
            0.0311,
            -0.0476,
            -0.1513,
            0.071,
            0.111,
            0.0097,
            0.0137,
            0.0831,
            0.0398,
            0.024,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0021,
            -0.0004,
            -0.0004,
            -0.0005,
            0.0013,
            0.0001,
            0.0017,
            0.0014,
            0.0005,
            -0.0011,
            -0.0011,
            0.0009,
            -0.0004,
            -0.0013,
            0.0006,
            0.0003
          ],
          "after": [
            -0.0852,
            0.0094,
            -0.1244,
            -0.0728,
            0.0671,
            0.1375,
            0.0876,
            -0.1598,
            0.0306,
            -0.026,
            -0.1492,
            -0.0648,
            0.0933,
            -0.0799,
            0.0409,
            0.086
          ]
        }
      }
    },
    {
      "step": 932,
      "word": "idaly",
      "loss": 2.3298,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0454,
            0.1495,
            -0.1361,
            -0.0485,
            0.0289,
            -0.0313,
            -0.1152,
            0.1163,
            -0.0235,
            0.0415,
            -0.0978,
            0.2918,
            -0.1149,
            0.0603,
            0.0103,
            -0.2734
          ],
          "after": [
            0.008,
            -0.0036,
            0.1562,
            -0.0245,
            0.0298,
            -0.0931,
            -0.1179,
            -0.1586,
            -0.009,
            0.0726,
            0.1611,
            0.015,
            -0.1033,
            -0.0175,
            -0.0364,
            -0.1861
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0238,
            0.133,
            -0.134,
            0.0464,
            0.0531,
            -0.1949,
            0.2746,
            0.0007,
            -0.2195,
            0.016,
            -0.0127,
            -0.0755,
            0.137,
            -0.0841,
            -0.2228,
            0.1502
          ],
          "after": [
            0.0279,
            0.1073,
            -0.0418,
            -0.0758,
            0.0322,
            -0.0104,
            0.0902,
            0.1009,
            -0.0654,
            -0.061,
            0.014,
            -0.0165,
            0.1073,
            0.0337,
            -0.0528,
            0.223
          ]
        },
        "position_0": {
          "grad": [
            -0.0684,
            -0.2508,
            0.115,
            -0.1773,
            0.1546,
            0.1537,
            -0.2285,
            0.0337,
            0.059,
            0.0219,
            0.0966,
            -0.1247,
            -0.1805,
            0.0058,
            -0.0289,
            -0.1477
          ],
          "after": [
            0.0449,
            -0.1469,
            -0.294,
            0.0518,
            0.031,
            -0.0477,
            -0.1512,
            0.071,
            0.111,
            0.0096,
            0.0137,
            0.0831,
            0.0398,
            0.024,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0013,
            0.0018,
            -0.0006,
            -0.0,
            -0.0018,
            -0.0009,
            -0.0008,
            -0.0011,
            0.0007,
            0.0001,
            0.0008,
            -0.0002,
            -0.0004,
            0.0014,
            0.0001,
            -0.0008
          ],
          "after": [
            -0.0853,
            0.0094,
            -0.1244,
            -0.0728,
            0.0672,
            0.1375,
            0.0877,
            -0.1597,
            0.0306,
            -0.026,
            -0.1492,
            -0.0647,
            0.0933,
            -0.08,
            0.0409,
            0.0861
          ]
        }
      }
    },
    {
      "step": 933,
      "word": "symir",
      "loss": 2.4355,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.008,
            -0.0037,
            0.1563,
            -0.0245,
            0.0298,
            -0.0929,
            -0.1179,
            -0.1586,
            -0.0089,
            0.0727,
            0.1611,
            0.0149,
            -0.1032,
            -0.0175,
            -0.0363,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0507,
            0.0777,
            -0.1988,
            0.0223,
            0.0451,
            -0.1627,
            0.3022,
            0.0242,
            -0.2374,
            0.0517,
            -0.0625,
            0.0002,
            0.119,
            -0.0976,
            -0.2104,
            0.1756
          ],
          "after": [
            0.028,
            0.1073,
            -0.0417,
            -0.0758,
            0.0321,
            -0.0103,
            0.0902,
            0.1009,
            -0.0653,
            -0.061,
            0.014,
            -0.0165,
            0.1073,
            0.0338,
            -0.0528,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.0075,
            0.0985,
            -0.0219,
            0.1371,
            -0.1297,
            0.0741,
            0.0206,
            0.0428,
            -0.0829,
            0.0084,
            -0.1124,
            0.1001,
            0.1107,
            0.0144,
            0.1133,
            0.0505
          ],
          "after": [
            0.045,
            -0.1469,
            -0.294,
            0.0518,
            0.031,
            -0.0478,
            -0.1512,
            0.0709,
            0.111,
            0.0096,
            0.0137,
            0.083,
            0.0398,
            0.024,
            -0.1307,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0015,
            0.0008,
            -0.0005,
            -0.0015,
            -0.0003,
            -0.0022,
            0.0003,
            0.0004,
            -0.0007,
            0.0002,
            -0.0003,
            -0.0008,
            0.0003,
            0.0021,
            -0.0006
          ],
          "after": [
            -0.0853,
            0.0094,
            -0.1245,
            -0.0728,
            0.0673,
            0.1376,
            0.0878,
            -0.1596,
            0.0306,
            -0.0259,
            -0.1493,
            -0.0646,
            0.0934,
            -0.0801,
            0.0408,
            0.0861
          ]
        }
      }
    },
    {
      "step": 934,
      "word": "danilo",
      "loss": 2.144,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0309,
            0.1775,
            -0.2067,
            -0.0332,
            -0.2575,
            -0.0642,
            -0.0809,
            -0.0888,
            0.0988,
            -0.1603,
            -0.1083,
            -0.0848,
            0.0537,
            0.1582,
            0.067,
            0.0998
          ],
          "after": [
            0.0081,
            -0.0038,
            0.1564,
            -0.0245,
            0.0298,
            -0.0928,
            -0.1179,
            -0.1586,
            -0.0088,
            0.0728,
            0.1612,
            0.0149,
            -0.1032,
            -0.0175,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0574,
            0.1415,
            -0.1109,
            0.0796,
            0.0781,
            -0.1708,
            0.2985,
            0.0212,
            -0.2214,
            0.0112,
            -0.0126,
            -0.0088,
            0.0931,
            -0.1022,
            -0.2231,
            0.1584
          ],
          "after": [
            0.028,
            0.1073,
            -0.0416,
            -0.0759,
            0.0321,
            -0.0103,
            0.0901,
            0.1008,
            -0.0653,
            -0.061,
            0.0141,
            -0.0165,
            0.1073,
            0.034,
            -0.0528,
            0.2229
          ]
        },
        "position_0": {
          "grad": [
            -0.064,
            0.0434,
            -0.0356,
            -0.0966,
            0.0931,
            -0.0588,
            0.0411,
            -0.0613,
            0.003,
            -0.0418,
            -0.0692,
            0.0251,
            0.0171,
            0.0346,
            0.0439,
            0.1148
          ],
          "after": [
            0.0451,
            -0.1469,
            -0.294,
            0.0519,
            0.0309,
            -0.0479,
            -0.1512,
            0.0709,
            0.111,
            0.0096,
            0.0138,
            0.0829,
            0.0398,
            0.024,
            -0.1307,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0011,
            0.0039,
            -0.0013,
            -0.001,
            -0.0015,
            -0.003,
            0.0001,
            -0.0004,
            -0.0023,
            0.0016,
            -0.0,
            0.0002,
            0.0003,
            -0.0006,
            -0.0031
          ],
          "after": [
            -0.0854,
            0.0094,
            -0.1246,
            -0.0728,
            0.0674,
            0.1376,
            0.0879,
            -0.1595,
            0.0307,
            -0.0259,
            -0.1494,
            -0.0645,
            0.0934,
            -0.0802,
            0.0408,
            0.0862
          ]
        }
      }
    },
    {
      "step": 935,
      "word": "bayan",
      "loss": 1.9507,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0907,
            -0.0681,
            -0.146,
            -0.0964,
            -0.1971,
            -0.087,
            -0.1247,
            -0.0136,
            0.0496,
            0.0776,
            0.032,
            0.1265,
            -0.1447,
            0.0733,
            0.1081,
            -0.0179
          ],
          "after": [
            0.0081,
            -0.0039,
            0.1565,
            -0.0245,
            0.0298,
            -0.0927,
            -0.1179,
            -0.1585,
            -0.0088,
            0.0728,
            0.1612,
            0.0149,
            -0.1031,
            -0.0175,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0122,
            0.0163,
            -0.0662,
            0.0377,
            -0.0171,
            -0.1345,
            0.1018,
            0.0197,
            -0.1757,
            0.0806,
            -0.0039,
            -0.0227,
            0.1117,
            -0.0107,
            -0.1194,
            0.0655
          ],
          "after": [
            0.028,
            0.1073,
            -0.0416,
            -0.0759,
            0.032,
            -0.0102,
            0.0901,
            0.1008,
            -0.0652,
            -0.061,
            0.0141,
            -0.0165,
            0.1073,
            0.034,
            -0.0527,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.0725,
            -0.0156,
            0.0968,
            -0.0323,
            -0.0404,
            0.0026,
            0.0577,
            0.0095,
            -0.0857,
            -0.0621,
            -0.0361,
            -0.0563,
            0.1434,
            -0.0994,
            0.0059,
            0.2727
          ],
          "after": [
            0.0451,
            -0.1469,
            -0.2941,
            0.0519,
            0.0309,
            -0.048,
            -0.1512,
            0.0709,
            0.111,
            0.0096,
            0.0138,
            0.0829,
            0.0398,
            0.024,
            -0.1308,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.003,
            -0.0002,
            -0.0009,
            -0.0011,
            0.0038,
            0.0013,
            0.001,
            0.0031,
            0.0026,
            -0.004,
            -0.0015,
            0.0043,
            -0.0034,
            -0.0039,
            0.001,
            0.0006
          ],
          "after": [
            -0.0854,
            0.0094,
            -0.1246,
            -0.0728,
            0.0674,
            0.1376,
            0.0879,
            -0.1595,
            0.0306,
            -0.0257,
            -0.1494,
            -0.0645,
            0.0934,
            -0.0802,
            0.0407,
            0.0863
          ]
        }
      }
    },
    {
      "step": 936,
      "word": "kayen",
      "loss": 1.8081,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1292,
            -0.2883,
            0.0578,
            -0.1314,
            -0.0003,
            0.1232,
            -0.2005,
            0.0922,
            -0.039,
            0.125,
            0.0673,
            0.0654,
            -0.0366,
            0.0966,
            0.021,
            -0.0711
          ],
          "after": [
            0.0081,
            -0.0038,
            0.1566,
            -0.0244,
            0.0299,
            -0.0926,
            -0.1178,
            -0.1585,
            -0.0088,
            0.0729,
            0.1612,
            0.0148,
            -0.1031,
            -0.0176,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1638,
            0.2613,
            -0.0783,
            -0.1424,
            0.0033,
            -0.1799,
            -0.0295,
            0.0516,
            0.0132,
            -0.4809,
            -0.0654,
            0.0317,
            -0.1036,
            -0.0135,
            0.328,
            -0.0495
          ],
          "after": [
            0.0281,
            0.1072,
            -0.0415,
            -0.0759,
            0.032,
            -0.0101,
            0.09,
            0.1007,
            -0.0651,
            -0.0609,
            0.0141,
            -0.0166,
            0.1073,
            0.0341,
            -0.0527,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0867,
            0.0443,
            -0.1328,
            -0.0069,
            0.0219,
            -0.0733,
            0.1585,
            -0.0097,
            -0.0177,
            -0.0025,
            -0.054,
            -0.0149,
            0.0383,
            -0.0108,
            -0.0872,
            -0.0269
          ],
          "after": [
            0.0452,
            -0.1469,
            -0.294,
            0.0519,
            0.0308,
            -0.048,
            -0.1512,
            0.0709,
            0.1111,
            0.0096,
            0.0139,
            0.0829,
            0.0398,
            0.024,
            -0.1308,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0026,
            0.0013,
            -0.0002,
            0.0,
            0.001,
            0.0005,
            0.0006,
            0.0026,
            0.0008,
            -0.0037,
            -0.001,
            0.0019,
            -0.0013,
            -0.0021,
            0.0015,
            0.0015
          ],
          "after": [
            -0.0853,
            0.0094,
            -0.1247,
            -0.0728,
            0.0674,
            0.1376,
            0.088,
            -0.1596,
            0.0306,
            -0.0256,
            -0.1494,
            -0.0645,
            0.0935,
            -0.0801,
            0.0406,
            0.0863
          ]
        }
      }
    },
    {
      "step": 937,
      "word": "adalena",
      "loss": 1.7796,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1382,
            0.2273,
            0.0056,
            -0.2598,
            0.294,
            -0.2046,
            -0.0182,
            -0.0813,
            -0.1101,
            0.1201,
            -0.0264,
            0.2142,
            -0.0015,
            0.0153,
            -0.1026,
            -0.2043
          ],
          "after": [
            0.0082,
            -0.0039,
            0.1567,
            -0.0243,
            0.0299,
            -0.0925,
            -0.1177,
            -0.1585,
            -0.0087,
            0.0729,
            0.1612,
            0.0148,
            -0.103,
            -0.0176,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1398,
            -0.3336,
            0.088,
            -0.0716,
            0.0614,
            0.1393,
            -0.3868,
            0.0668,
            0.0811,
            0.0593,
            -0.0674,
            -0.0267,
            -0.0396,
            0.1837,
            0.0313,
            -0.2538
          ],
          "after": [
            0.0281,
            0.1072,
            -0.0415,
            -0.0758,
            0.032,
            -0.0101,
            0.09,
            0.1007,
            -0.0651,
            -0.0608,
            0.0142,
            -0.0166,
            0.1073,
            0.0342,
            -0.0527,
            0.2227
          ]
        },
        "position_0": {
          "grad": [
            0.0733,
            -0.0161,
            0.0735,
            -0.1376,
            0.0996,
            0.009,
            -0.232,
            -0.0998,
            0.0965,
            0.0917,
            0.2125,
            0.0702,
            -0.1138,
            -0.0196,
            -0.1292,
            -0.1854
          ],
          "after": [
            0.0453,
            -0.1469,
            -0.294,
            0.052,
            0.0308,
            -0.048,
            -0.1511,
            0.0709,
            0.1111,
            0.0096,
            0.0139,
            0.0828,
            0.0398,
            0.0241,
            -0.1308,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0016,
            0.0005,
            0.0016,
            0.0009,
            0.0015,
            -0.0011,
            -0.0,
            -0.0006,
            -0.0006,
            0.0013,
            0.0015,
            -0.0005,
            0.0008,
            -0.0004,
            -0.0035,
            -0.0006
          ],
          "after": [
            -0.0853,
            0.0093,
            -0.1248,
            -0.0728,
            0.0674,
            0.1376,
            0.0881,
            -0.1596,
            0.0305,
            -0.0255,
            -0.1495,
            -0.0645,
            0.0936,
            -0.0801,
            0.0406,
            0.0863
          ]
        }
      }
    },
    {
      "step": 938,
      "word": "derian",
      "loss": 1.6838,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1357,
            0.1846,
            -0.0108,
            0.0922,
            -0.029,
            -0.0882,
            0.008,
            -0.1103,
            0.0689,
            -0.2415,
            -0.0887,
            0.1987,
            -0.1485,
            -0.066,
            0.0134,
            0.1407
          ],
          "after": [
            0.0082,
            -0.004,
            0.1567,
            -0.0242,
            0.0298,
            -0.0924,
            -0.1177,
            -0.1585,
            -0.0087,
            0.0729,
            0.1613,
            0.0147,
            -0.103,
            -0.0176,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0632,
            -0.3234,
            0.2713,
            -0.1795,
            0.176,
            0.1944,
            -0.4404,
            -0.0792,
            0.4922,
            0.2026,
            0.1336,
            0.1715,
            -0.2744,
            0.0587,
            0.0428,
            -0.172
          ],
          "after": [
            0.0281,
            0.1072,
            -0.0415,
            -0.0757,
            0.0319,
            -0.0101,
            0.09,
            0.1007,
            -0.0651,
            -0.0608,
            0.0142,
            -0.0166,
            0.1074,
            0.0342,
            -0.0527,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0555,
            0.0306,
            -0.0311,
            -0.0971,
            0.072,
            -0.0665,
            0.0215,
            -0.0639,
            0.0149,
            -0.035,
            -0.0689,
            -0.0127,
            0.0058,
            0.0376,
            0.049,
            0.0916
          ],
          "after": [
            0.0453,
            -0.1469,
            -0.294,
            0.0521,
            0.0307,
            -0.048,
            -0.1511,
            0.071,
            0.1111,
            0.0096,
            0.0139,
            0.0828,
            0.0398,
            0.0241,
            -0.1308,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0026,
            -0.0028,
            -0.0001,
            -0.0012,
            -0.003,
            0.0009,
            -0.0037,
            0.0021,
            0.0026,
            -0.0012,
            -0.0014,
            0.0028,
            -0.0018,
            0.0006,
            0.0044,
            0.0019
          ],
          "after": [
            -0.0852,
            0.0094,
            -0.1248,
            -0.0728,
            0.0675,
            0.1376,
            0.0881,
            -0.1597,
            0.0305,
            -0.0254,
            -0.1495,
            -0.0645,
            0.0937,
            -0.0801,
            0.0406,
            0.0863
          ]
        }
      }
    },
    {
      "step": 939,
      "word": "amiryah",
      "loss": 2.1481,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2883,
            0.0369,
            0.3818,
            0.1394,
            0.3967,
            0.0796,
            0.3737,
            0.2104,
            0.2121,
            0.1516,
            -0.1129,
            0.0988,
            -0.0082,
            -0.1943,
            -0.2674,
            0.2105
          ],
          "after": [
            0.0083,
            -0.004,
            0.1567,
            -0.0242,
            0.0298,
            -0.0923,
            -0.1177,
            -0.1585,
            -0.0087,
            0.0729,
            0.1613,
            0.0146,
            -0.1029,
            -0.0176,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0508,
            0.0638,
            -0.091,
            0.015,
            0.0541,
            -0.1397,
            0.1666,
            0.0017,
            -0.121,
            0.0367,
            -0.034,
            -0.0078,
            0.0863,
            -0.049,
            -0.1566,
            0.0977
          ],
          "after": [
            0.0281,
            0.1073,
            -0.0415,
            -0.0757,
            0.0318,
            -0.01,
            0.09,
            0.1006,
            -0.0651,
            -0.0608,
            0.0142,
            -0.0166,
            0.1074,
            0.0342,
            -0.0527,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.073,
            -0.0156,
            0.0715,
            -0.1484,
            0.0894,
            0.016,
            -0.2332,
            -0.1107,
            0.0926,
            0.1054,
            0.2142,
            0.0485,
            -0.1198,
            -0.0152,
            -0.1354,
            -0.1861
          ],
          "after": [
            0.0453,
            -0.1469,
            -0.2941,
            0.0522,
            0.0306,
            -0.048,
            -0.1511,
            0.0711,
            0.111,
            0.0095,
            0.0138,
            0.0827,
            0.0398,
            0.0241,
            -0.1307,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0007,
            0.0001,
            -0.0005,
            -0.0019,
            0.0007,
            -0.0013,
            -0.0005,
            0.0011,
            -0.0007,
            0.0012,
            0.0006,
            -0.0022,
            0.0003,
            0.0015,
            -0.0007
          ],
          "after": [
            -0.0852,
            0.0094,
            -0.1249,
            -0.0728,
            0.0675,
            0.1376,
            0.0882,
            -0.1597,
            0.0304,
            -0.0253,
            -0.1495,
            -0.0646,
            0.0938,
            -0.0801,
            0.0405,
            0.0863
          ]
        }
      }
    },
    {
      "step": 940,
      "word": "allen",
      "loss": 1.8297,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.126,
            0.133,
            -0.1708,
            -0.0725,
            -0.0679,
            -0.0193,
            -0.0951,
            0.1091,
            -0.1293,
            -0.076,
            -0.1924,
            0.3256,
            -0.1443,
            0.0575,
            0.1873,
            -0.431
          ],
          "after": [
            0.0084,
            -0.0041,
            0.1568,
            -0.0241,
            0.0297,
            -0.0923,
            -0.1177,
            -0.1585,
            -0.0087,
            0.0729,
            0.1614,
            0.0145,
            -0.1028,
            -0.0176,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0298,
            -0.3486,
            0.1336,
            -0.1646,
            0.1878,
            0.0845,
            -0.3758,
            0.0907,
            0.1762,
            -0.0052,
            -0.0217,
            0.0391,
            -0.1444,
            0.0105,
            0.1974,
            -0.2898
          ],
          "after": [
            0.0281,
            0.1073,
            -0.0415,
            -0.0756,
            0.0318,
            -0.01,
            0.09,
            0.1006,
            -0.0651,
            -0.0608,
            0.0142,
            -0.0167,
            0.1074,
            0.0342,
            -0.0527,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.0871,
            -0.0493,
            0.1091,
            -0.1455,
            0.0844,
            0.0292,
            -0.3061,
            -0.1109,
            0.1441,
            0.0543,
            0.2669,
            0.0854,
            -0.1515,
            0.0038,
            -0.0998,
            -0.2366
          ],
          "after": [
            0.0453,
            -0.1469,
            -0.2941,
            0.0523,
            0.0305,
            -0.048,
            -0.151,
            0.0712,
            0.111,
            0.0095,
            0.0138,
            0.0827,
            0.0399,
            0.0241,
            -0.1307,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            0.001,
            0.0004,
            0.0004,
            0.0015,
            -0.0028,
            0.0045,
            0.0003,
            -0.0006,
            -0.0004,
            0.0009,
            -0.0006,
            0.0027,
            -0.0025,
            -0.0018,
            0.0009
          ],
          "after": [
            -0.0851,
            0.0094,
            -0.1249,
            -0.0728,
            0.0675,
            0.1377,
            0.0882,
            -0.1598,
            0.0303,
            -0.0252,
            -0.1496,
            -0.0646,
            0.0938,
            -0.08,
            0.0405,
            0.0863
          ]
        }
      }
    },
    {
      "step": 941,
      "word": "tylan",
      "loss": 1.9022,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0191,
            0.2071,
            -0.231,
            0.045,
            -0.2073,
            -0.2148,
            0.0635,
            -0.1132,
            0.0904,
            -0.0596,
            -0.0296,
            0.0434,
            -0.0942,
            -0.0024,
            0.1076,
            0.0577
          ],
          "after": [
            0.0085,
            -0.0042,
            0.1568,
            -0.0241,
            0.0297,
            -0.0922,
            -0.1177,
            -0.1585,
            -0.0087,
            0.073,
            0.1615,
            0.0144,
            -0.1027,
            -0.0176,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0359,
            0.1206,
            -0.1653,
            0.0703,
            -0.0192,
            -0.1546,
            0.2796,
            -0.0072,
            -0.2094,
            0.0474,
            -0.0061,
            -0.0674,
            0.1635,
            -0.0793,
            -0.1809,
            0.151
          ],
          "after": [
            0.0281,
            0.1074,
            -0.0415,
            -0.0755,
            0.0317,
            -0.01,
            0.0901,
            0.1005,
            -0.0651,
            -0.0608,
            0.0142,
            -0.0167,
            0.1074,
            0.0342,
            -0.0527,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0237,
            -0.2042,
            0.0928,
            0.0603,
            -0.1501,
            0.067,
            0.0121,
            0.0074,
            -0.08,
            -0.1292,
            -0.083,
            -0.288,
            0.0979,
            0.0734,
            0.1642,
            0.0942
          ],
          "after": [
            0.0453,
            -0.1469,
            -0.2942,
            0.0524,
            0.0305,
            -0.0481,
            -0.1509,
            0.0713,
            0.111,
            0.0095,
            0.0137,
            0.0827,
            0.0399,
            0.0241,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0006,
            -0.0017,
            0.0006,
            -0.0009,
            0.0017,
            -0.0007,
            0.0009,
            0.0009,
            -0.0001,
            0.0013,
            -0.0002,
            0.0005,
            0.0005,
            -0.0011,
            -0.0007,
            -0.0003
          ],
          "after": [
            -0.0851,
            0.0094,
            -0.125,
            -0.0728,
            0.0675,
            0.1377,
            0.0882,
            -0.1598,
            0.0303,
            -0.0251,
            -0.1496,
            -0.0646,
            0.0938,
            -0.08,
            0.0404,
            0.0863
          ]
        }
      }
    },
    {
      "step": 942,
      "word": "carlos",
      "loss": 2.1081,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0818,
            -0.0693,
            -0.0796,
            -0.0831,
            0.0598,
            0.0011,
            0.0891,
            -0.1584,
            0.1567,
            -0.0793,
            0.0477,
            0.0232,
            -0.0886,
            -0.0929,
            0.0463,
            0.0779
          ],
          "after": [
            0.0085,
            -0.0043,
            0.1569,
            -0.0241,
            0.0297,
            -0.0921,
            -0.1177,
            -0.1585,
            -0.0088,
            0.073,
            0.1616,
            0.0143,
            -0.1027,
            -0.0176,
            -0.0364,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.019,
            0.1286,
            -0.1989,
            0.0427,
            -0.0577,
            -0.1573,
            0.264,
            -0.0102,
            -0.2311,
            0.0201,
            0.0022,
            -0.0573,
            0.1387,
            -0.105,
            -0.1412,
            0.1844
          ],
          "after": [
            0.0281,
            0.1074,
            -0.0415,
            -0.0754,
            0.0317,
            -0.0099,
            0.09,
            0.1005,
            -0.0651,
            -0.0608,
            0.0142,
            -0.0167,
            0.1074,
            0.0343,
            -0.0526,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0369,
            0.0252,
            -0.0349,
            0.1067,
            -0.0393,
            -0.0306,
            0.1404,
            0.0746,
            -0.0806,
            0.0351,
            0.0282,
            0.0316,
            0.0517,
            -0.0285,
            -0.0488,
            0.0665
          ],
          "after": [
            0.0453,
            -0.1468,
            -0.2943,
            0.0525,
            0.0304,
            -0.0481,
            -0.1509,
            0.0713,
            0.111,
            0.0095,
            0.0136,
            0.0827,
            0.0399,
            0.0241,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0002,
            0.0014,
            -0.0005,
            0.0006,
            -0.001,
            0.0003,
            -0.0003,
            0.0004,
            0.0009,
            0.0006,
            0.001,
            0.0001,
            -0.0002,
            -0.0023,
            -0.0013
          ],
          "after": [
            -0.085,
            0.0094,
            -0.1251,
            -0.0727,
            0.0675,
            0.1378,
            0.0883,
            -0.1599,
            0.0302,
            -0.0251,
            -0.1496,
            -0.0647,
            0.0938,
            -0.0799,
            0.0404,
            0.0864
          ]
        }
      }
    },
    {
      "step": 943,
      "word": "canaan",
      "loss": 1.8663,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0779,
            0.4064,
            0.0564,
            0.0712,
            -0.4154,
            -0.1449,
            -0.3222,
            -0.222,
            0.2723,
            -0.5243,
            -0.0852,
            0.0346,
            -0.261,
            0.2549,
            0.1122,
            0.0062
          ],
          "after": [
            0.0086,
            -0.0044,
            0.1569,
            -0.0241,
            0.0297,
            -0.092,
            -0.1177,
            -0.1584,
            -0.0089,
            0.0731,
            0.1617,
            0.0142,
            -0.1025,
            -0.0176,
            -0.0365,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0391,
            0.0021,
            -0.133,
            0.0154,
            -0.0036,
            -0.0851,
            0.1179,
            0.0127,
            -0.1594,
            0.0626,
            0.0029,
            -0.038,
            0.084,
            -0.0595,
            -0.097,
            0.0887
          ],
          "after": [
            0.0281,
            0.1074,
            -0.0414,
            -0.0754,
            0.0316,
            -0.0099,
            0.09,
            0.1005,
            -0.0651,
            -0.0608,
            0.0143,
            -0.0167,
            0.1074,
            0.0343,
            -0.0526,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0291,
            0.0105,
            -0.0339,
            0.0942,
            -0.0657,
            -0.0551,
            0.095,
            0.0814,
            -0.0451,
            0.026,
            -0.0065,
            0.0156,
            0.0134,
            -0.0234,
            -0.0298,
            0.0266
          ],
          "after": [
            0.0453,
            -0.1468,
            -0.2943,
            0.0525,
            0.0304,
            -0.0481,
            -0.1509,
            0.0713,
            0.111,
            0.0094,
            0.0136,
            0.0827,
            0.0399,
            0.0241,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0001,
            0.0013,
            -0.0004,
            0.0013,
            -0.001,
            0.0004,
            -0.0002,
            0.0005,
            0.0012,
            0.0005,
            0.0011,
            -0.0001,
            -0.0006,
            -0.0015,
            -0.001
          ],
          "after": [
            -0.085,
            0.0094,
            -0.1251,
            -0.0727,
            0.0675,
            0.1378,
            0.0883,
            -0.1599,
            0.0302,
            -0.0251,
            -0.1497,
            -0.0647,
            0.0939,
            -0.0799,
            0.0405,
            0.0864
          ]
        }
      }
    },
    {
      "step": 944,
      "word": "dugan",
      "loss": 2.6932,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0193,
            0.2053,
            -0.2326,
            0.0353,
            -0.2164,
            -0.2138,
            0.0608,
            -0.1098,
            0.092,
            -0.0535,
            -0.0245,
            0.0363,
            -0.0938,
            -0.0001,
            0.1055,
            0.0573
          ],
          "after": [
            0.0086,
            -0.0045,
            0.157,
            -0.0241,
            0.0298,
            -0.0918,
            -0.1177,
            -0.1583,
            -0.009,
            0.0733,
            0.1617,
            0.0141,
            -0.1024,
            -0.0177,
            -0.0365,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0227,
            0.1233,
            -0.19,
            0.0355,
            0.0059,
            -0.1625,
            0.2048,
            0.0359,
            -0.2458,
            0.074,
            -0.0031,
            -0.0436,
            0.1503,
            -0.0636,
            -0.1862,
            0.1354
          ],
          "after": [
            0.0281,
            0.1074,
            -0.0414,
            -0.0754,
            0.0316,
            -0.0098,
            0.09,
            0.1004,
            -0.065,
            -0.0608,
            0.0143,
            -0.0167,
            0.1074,
            0.0344,
            -0.0526,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0786,
            0.0494,
            -0.0249,
            -0.0618,
            0.0802,
            -0.0713,
            0.0451,
            -0.074,
            -0.0127,
            -0.1171,
            -0.0701,
            -0.0013,
            0.0477,
            0.1049,
            0.1115,
            0.1786
          ],
          "after": [
            0.0454,
            -0.1468,
            -0.2943,
            0.0525,
            0.0304,
            -0.0481,
            -0.1509,
            0.0714,
            0.111,
            0.0095,
            0.0136,
            0.0828,
            0.0399,
            0.024,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0001,
            0.0011,
            0.0012,
            0.0007,
            -0.0014,
            0.0012,
            0.0,
            -0.0007,
            -0.0012,
            0.0,
            -0.0012,
            0.0005,
            -0.0007,
            -0.0,
            -0.0015
          ],
          "after": [
            -0.085,
            0.0094,
            -0.1252,
            -0.0727,
            0.0675,
            0.1379,
            0.0882,
            -0.1599,
            0.0301,
            -0.0251,
            -0.1497,
            -0.0647,
            0.0939,
            -0.0798,
            0.0405,
            0.0864
          ]
        }
      }
    },
    {
      "step": 945,
      "word": "abra",
      "loss": 2.1525,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.6616,
            -0.1298,
            -0.0681,
            -0.088,
            -0.3006,
            -0.191,
            0.1356,
            0.0656,
            -0.4835,
            0.2716,
            0.1856,
            -0.6773,
            0.6863,
            0.0408,
            0.0612,
            0.263
          ],
          "after": [
            0.0085,
            -0.0046,
            0.157,
            -0.024,
            0.0299,
            -0.0917,
            -0.1177,
            -0.1582,
            -0.009,
            0.0733,
            0.1617,
            0.0141,
            -0.1024,
            -0.0177,
            -0.0366,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0391,
            0.0675,
            -0.0898,
            0.0297,
            -0.0705,
            -0.1635,
            0.1109,
            -0.0099,
            -0.1573,
            0.0789,
            -0.0079,
            -0.0551,
            0.1469,
            0.0205,
            -0.1559,
            0.0629
          ],
          "after": [
            0.0281,
            0.1074,
            -0.0413,
            -0.0754,
            0.0316,
            -0.0097,
            0.09,
            0.1004,
            -0.0649,
            -0.0609,
            0.0143,
            -0.0167,
            0.1073,
            0.0344,
            -0.0525,
            0.2227
          ]
        },
        "position_0": {
          "grad": [
            0.1271,
            -0.0509,
            0.1356,
            -0.194,
            0.0318,
            0.0123,
            -0.3924,
            -0.1369,
            0.1816,
            0.031,
            0.3019,
            0.0382,
            -0.1916,
            0.0607,
            -0.0722,
            -0.2741
          ],
          "after": [
            0.0453,
            -0.1467,
            -0.2944,
            0.0526,
            0.0304,
            -0.0481,
            -0.1508,
            0.0715,
            0.1109,
            0.0095,
            0.0135,
            0.0828,
            0.04,
            0.024,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            -0.0,
            -0.0004,
            -0.0008,
            0.0003,
            -0.0002,
            -0.0017,
            0.0026,
            0.0012,
            -0.0024,
            -0.0038,
            0.0005,
            0.0018,
            0.0029,
            0.0008,
            0.003
          ],
          "after": [
            -0.085,
            0.0094,
            -0.1253,
            -0.0727,
            0.0675,
            0.138,
            0.0883,
            -0.16,
            0.0301,
            -0.025,
            -0.1497,
            -0.0647,
            0.0939,
            -0.0798,
            0.0405,
            0.0864
          ]
        }
      }
    },
    {
      "step": 946,
      "word": "jermell",
      "loss": 2.386,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0085,
            -0.0047,
            0.1571,
            -0.024,
            0.03,
            -0.0916,
            -0.1177,
            -0.1582,
            -0.0089,
            0.0734,
            0.1617,
            0.0142,
            -0.1025,
            -0.0177,
            -0.0367,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.4413,
            -0.4688,
            0.5085,
            -0.0212,
            0.0479,
            0.2995,
            -0.8306,
            -0.2368,
            0.5873,
            0.1046,
            0.3269,
            0.0331,
            -0.4032,
            0.1307,
            0.0348,
            -0.3974
          ],
          "after": [
            0.028,
            0.1074,
            -0.0413,
            -0.0754,
            0.0316,
            -0.0097,
            0.09,
            0.1005,
            -0.0649,
            -0.0609,
            0.0142,
            -0.0167,
            0.1073,
            0.0344,
            -0.0525,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.002,
            0.0358,
            -0.0258,
            0.1296,
            0.0019,
            -0.0792,
            0.1844,
            0.066,
            -0.1827,
            -0.0368,
            0.0028,
            0.0181,
            0.2591,
            -0.0994,
            -0.0914,
            0.1258
          ],
          "after": [
            0.0453,
            -0.1467,
            -0.2944,
            0.0526,
            0.0304,
            -0.048,
            -0.1508,
            0.0715,
            0.111,
            0.0095,
            0.0134,
            0.0827,
            0.0399,
            0.0239,
            -0.1306,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            0.0007,
            0.002,
            0.0008,
            0.0019,
            -0.0014,
            -0.0003,
            0.0003,
            0.0002,
            0.0012,
            0.0012,
            -0.0006,
            0.0014,
            -0.0017,
            -0.0041,
            -0.0003
          ],
          "after": [
            -0.0849,
            0.0094,
            -0.1254,
            -0.0727,
            0.0674,
            0.138,
            0.0883,
            -0.1601,
            0.0301,
            -0.025,
            -0.1497,
            -0.0647,
            0.0938,
            -0.0798,
            0.0405,
            0.0864
          ]
        }
      }
    },
    {
      "step": 947,
      "word": "brennan",
      "loss": 2.0238,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1964,
            0.1649,
            -0.0489,
            0.1318,
            -0.0597,
            -0.0262,
            0.0281,
            -0.0655,
            0.1196,
            -0.2852,
            -0.1658,
            0.2027,
            -0.1434,
            -0.0438,
            0.0798,
            0.2449
          ],
          "after": [
            0.0084,
            -0.0048,
            0.1572,
            -0.024,
            0.0301,
            -0.0914,
            -0.1177,
            -0.1581,
            -0.009,
            0.0735,
            0.1618,
            0.0142,
            -0.1025,
            -0.0177,
            -0.0367,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0572,
            -0.2079,
            0.1958,
            0.0868,
            0.2157,
            0.211,
            -0.2701,
            0.0619,
            0.0937,
            0.1032,
            0.1052,
            0.2262,
            -0.2646,
            -0.0015,
            0.0729,
            -0.0507
          ],
          "after": [
            0.028,
            0.1075,
            -0.0413,
            -0.0754,
            0.0315,
            -0.0097,
            0.09,
            0.1005,
            -0.065,
            -0.061,
            0.0141,
            -0.0167,
            0.1074,
            0.0344,
            -0.0524,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.0484,
            -0.0192,
            0.0598,
            0.0491,
            -0.077,
            0.0199,
            0.1204,
            0.0494,
            -0.071,
            -0.0711,
            -0.0611,
            -0.0113,
            0.0931,
            -0.0851,
            0.0647,
            0.2284
          ],
          "after": [
            0.0453,
            -0.1467,
            -0.2945,
            0.0526,
            0.0304,
            -0.048,
            -0.1508,
            0.0715,
            0.111,
            0.0095,
            0.0134,
            0.0827,
            0.0399,
            0.024,
            -0.1305,
            0.0519
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0018,
            0.0005,
            -0.0018,
            0.0023,
            -0.0056,
            0.0015,
            -0.0021,
            0.0005,
            -0.0031,
            -0.0006,
            -0.0008,
            -0.0035,
            0.0026,
            0.0058,
            0.0037,
            0.0024
          ],
          "after": [
            -0.085,
            0.0094,
            -0.1254,
            -0.0727,
            0.0675,
            0.1381,
            0.0883,
            -0.1601,
            0.0301,
            -0.0249,
            -0.1496,
            -0.0647,
            0.0938,
            -0.0799,
            0.0405,
            0.0864
          ]
        }
      }
    },
    {
      "step": 948,
      "word": "manon",
      "loss": 1.7465,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0779,
            0.1639,
            -0.2847,
            -0.0122,
            -0.377,
            -0.1159,
            -0.1311,
            -0.0884,
            0.1348,
            -0.243,
            -0.1433,
            -0.1479,
            0.067,
            0.2513,
            0.1561,
            0.1265
          ],
          "after": [
            0.0084,
            -0.0049,
            0.1572,
            -0.024,
            0.0302,
            -0.0913,
            -0.1177,
            -0.158,
            -0.009,
            0.0736,
            0.1619,
            0.0142,
            -0.1025,
            -0.0178,
            -0.0368,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0631,
            0.0447,
            -0.1326,
            0.0471,
            0.0153,
            -0.1215,
            0.203,
            0.0449,
            -0.1754,
            0.0592,
            -0.0399,
            0.0013,
            0.109,
            -0.0616,
            -0.139,
            0.1066
          ],
          "after": [
            0.028,
            0.1075,
            -0.0413,
            -0.0754,
            0.0315,
            -0.0097,
            0.0901,
            0.1005,
            -0.065,
            -0.061,
            0.014,
            -0.0167,
            0.1074,
            0.0345,
            -0.0524,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            -0.0101,
            0.0198,
            -0.0506,
            0.0285,
            0.0993,
            -0.0318,
            0.2294,
            0.0961,
            0.0536,
            -0.0122,
            -0.043,
            0.0253,
            -0.0879,
            -0.0246,
            0.0426,
            0.1285
          ],
          "after": [
            0.0453,
            -0.1467,
            -0.2945,
            0.0526,
            0.0303,
            -0.048,
            -0.1508,
            0.0715,
            0.111,
            0.0096,
            0.0133,
            0.0827,
            0.0399,
            0.024,
            -0.1305,
            0.0518
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0003,
            0.0005,
            0.0001,
            0.0022,
            -0.0007,
            0.0018,
            0.0008,
            -0.0002,
            0.0,
            -0.0002,
            0.0013,
            0.0004,
            -0.0016,
            -0.001,
            -0.0001
          ],
          "after": [
            -0.085,
            0.0094,
            -0.1255,
            -0.0727,
            0.0675,
            0.1381,
            0.0883,
            -0.1602,
            0.0301,
            -0.0249,
            -0.1496,
            -0.0647,
            0.0937,
            -0.0799,
            0.0406,
            0.0864
          ]
        }
      }
    },
    {
      "step": 949,
      "word": "bayan",
      "loss": 1.9219,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0882,
            -0.0731,
            -0.1388,
            -0.0946,
            -0.1882,
            -0.0808,
            -0.1198,
            -0.0094,
            0.0452,
            0.0855,
            0.035,
            0.1273,
            -0.1418,
            0.069,
            0.1012,
            -0.0189
          ],
          "after": [
            0.0084,
            -0.005,
            0.1573,
            -0.024,
            0.0303,
            -0.0912,
            -0.1177,
            -0.158,
            -0.009,
            0.0736,
            0.1619,
            0.0142,
            -0.1024,
            -0.0178,
            -0.0369,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0122,
            0.0148,
            -0.0678,
            0.0389,
            -0.018,
            -0.1336,
            0.1028,
            0.0203,
            -0.1774,
            0.0819,
            -0.0053,
            -0.0244,
            0.1137,
            -0.011,
            -0.1196,
            0.0666
          ],
          "after": [
            0.0279,
            0.1076,
            -0.0413,
            -0.0754,
            0.0314,
            -0.0097,
            0.0901,
            0.1005,
            -0.0649,
            -0.0611,
            0.014,
            -0.0167,
            0.1074,
            0.0345,
            -0.0523,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.0714,
            -0.0139,
            0.0951,
            -0.0302,
            -0.0392,
            0.0023,
            0.0629,
            0.01,
            -0.0881,
            -0.0614,
            -0.0375,
            -0.0549,
            0.146,
            -0.101,
            0.0045,
            0.2761
          ],
          "after": [
            0.0452,
            -0.1467,
            -0.2946,
            0.0526,
            0.0303,
            -0.0479,
            -0.1508,
            0.0715,
            0.111,
            0.0096,
            0.0133,
            0.0827,
            0.0399,
            0.0241,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.003,
            -0.0002,
            -0.0009,
            -0.0012,
            0.0038,
            0.0013,
            0.0009,
            0.0031,
            0.0027,
            -0.004,
            -0.0015,
            0.0043,
            -0.0035,
            -0.0039,
            0.0011,
            0.0005
          ],
          "after": [
            -0.0849,
            0.0094,
            -0.1255,
            -0.0727,
            0.0674,
            0.1381,
            0.0883,
            -0.1603,
            0.0301,
            -0.0249,
            -0.1496,
            -0.0648,
            0.0937,
            -0.0799,
            0.0406,
            0.0863
          ]
        }
      }
    },
    {
      "step": 950,
      "word": "avyn",
      "loss": 2.3616,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1207,
            -0.1025,
            -0.2041,
            0.3343,
            0.1704,
            0.0202,
            -0.012,
            -0.0557,
            -0.2887,
            0.0634,
            0.1675,
            -0.1034,
            0.2307,
            -0.1387,
            -0.019,
            0.4314
          ],
          "after": [
            0.0084,
            -0.005,
            0.1574,
            -0.0241,
            0.0304,
            -0.0911,
            -0.1176,
            -0.1579,
            -0.009,
            0.0737,
            0.1619,
            0.0142,
            -0.1025,
            -0.0179,
            -0.037,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0139,
            0.0253,
            -0.1278,
            0.0243,
            -0.0006,
            -0.1601,
            0.2399,
            0.0364,
            -0.1869,
            0.0879,
            -0.0502,
            -0.0197,
            0.1532,
            -0.0601,
            -0.1482,
            0.1107
          ],
          "after": [
            0.0279,
            0.1076,
            -0.0413,
            -0.0755,
            0.0314,
            -0.0097,
            0.0901,
            0.1005,
            -0.0649,
            -0.0612,
            0.014,
            -0.0167,
            0.1074,
            0.0345,
            -0.0523,
            0.2228
          ]
        },
        "position_0": {
          "grad": [
            0.1187,
            -0.0624,
            0.1432,
            -0.1574,
            0.0044,
            0.0213,
            -0.3878,
            -0.1324,
            0.1821,
            -0.0133,
            0.3042,
            0.0291,
            -0.1968,
            0.1003,
            -0.0311,
            -0.2548
          ],
          "after": [
            0.0452,
            -0.1467,
            -0.2947,
            0.0527,
            0.0303,
            -0.0479,
            -0.1508,
            0.0715,
            0.111,
            0.0097,
            0.0132,
            0.0827,
            0.0399,
            0.0241,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0011,
            -0.0011,
            0.0,
            0.0016,
            0.002,
            0.0011,
            0.0025,
            0.0021,
            0.0027,
            -0.0012,
            0.0022,
            0.0002,
            -0.0007,
            -0.0016,
            0.0032
          ],
          "after": [
            -0.0849,
            0.0094,
            -0.1255,
            -0.0727,
            0.0674,
            0.1381,
            0.0883,
            -0.1604,
            0.03,
            -0.0248,
            -0.1495,
            -0.0648,
            0.0937,
            -0.0798,
            0.0406,
            0.0863
          ]
        }
      }
    },
    {
      "step": 951,
      "word": "luzmaria",
      "loss": 2.7234,
      "learning_rate": 0.0002,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1508,
            -0.041,
            0.0599,
            -0.0521,
            0.1274,
            0.1853,
            0.0977,
            -0.113,
            0.1489,
            -0.0543,
            -0.0571,
            0.2217,
            -0.1579,
            -0.1978,
            0.0003,
            0.2413
          ],
          "after": [
            0.0084,
            -0.0051,
            0.1575,
            -0.0241,
            0.0304,
            -0.0911,
            -0.1176,
            -0.1578,
            -0.009,
            0.0738,
            0.1619,
            0.0142,
            -0.1025,
            -0.0179,
            -0.0371,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0811,
            0.1596,
            -0.1565,
            0.0524,
            0.0171,
            -0.1926,
            0.2769,
            0.0052,
            -0.2083,
            0.0143,
            -0.029,
            -0.0081,
            0.1378,
            -0.0825,
            -0.1966,
            0.1891
          ],
          "after": [
            0.0279,
            0.1076,
            -0.0413,
            -0.0755,
            0.0314,
            -0.0096,
            0.09,
            0.1005,
            -0.0649,
            -0.0612,
            0.0139,
            -0.0167,
            0.1074,
            0.0345,
            -0.0522,
            0.2227
          ]
        },
        "position_0": {
          "grad": [
            -0.0694,
            0.0985,
            -0.0981,
            -0.0522,
            0.0573,
            0.0742,
            0.0315,
            0.0809,
            -0.0617,
            0.0197,
            -0.121,
            0.1977,
            -0.0432,
            -0.0042,
            0.0375,
            -0.046
          ],
          "after": [
            0.0451,
            -0.1467,
            -0.2947,
            0.0527,
            0.0303,
            -0.0479,
            -0.1508,
            0.0715,
            0.111,
            0.0097,
            0.0132,
            0.0827,
            0.0399,
            0.0241,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0037,
            0.0032,
            -0.0029,
            0.0041,
            -0.0041,
            -0.0015,
            0.0025,
            -0.0003,
            -0.0023,
            0.0005,
            0.0004,
            -0.0024,
            0.0041,
            0.0005,
            -0.0014,
            0.0014
          ],
          "after": [
            -0.0849,
            0.0094,
            -0.1255,
            -0.0728,
            0.0674,
            0.1381,
            0.0883,
            -0.1605,
            0.03,
            -0.0248,
            -0.1495,
            -0.0648,
            0.0936,
            -0.0798,
            0.0406,
            0.0862
          ]
        }
      }
    },
    {
      "step": 952,
      "word": "zanylah",
      "loss": 2.1717,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1566,
            0.1106,
            0.0482,
            0.1484,
            -0.0765,
            0.1119,
            0.0112,
            0.0898,
            0.2217,
            -0.0754,
            -0.2928,
            -0.0127,
            0.1801,
            0.0995,
            -0.0685,
            0.2922
          ],
          "after": [
            0.0084,
            -0.0051,
            0.1576,
            -0.0242,
            0.0305,
            -0.0911,
            -0.1176,
            -0.1578,
            -0.0091,
            0.0738,
            0.162,
            0.0142,
            -0.1025,
            -0.0179,
            -0.0371,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0756,
            0.0991,
            -0.1198,
            0.0649,
            0.0353,
            -0.1522,
            0.2612,
            0.0133,
            -0.1931,
            0.0141,
            -0.0199,
            0.0132,
            0.094,
            -0.104,
            -0.1686,
            0.1631
          ],
          "after": [
            0.0279,
            0.1076,
            -0.0413,
            -0.0756,
            0.0314,
            -0.0095,
            0.09,
            0.1005,
            -0.0648,
            -0.0613,
            0.0139,
            -0.0168,
            0.1073,
            0.0346,
            -0.0521,
            0.2227
          ]
        },
        "position_0": {
          "grad": [
            0.0192,
            0.0372,
            -0.0412,
            0.1153,
            0.0612,
            -0.0606,
            0.1704,
            0.0056,
            0.014,
            0.0371,
            0.0279,
            0.0436,
            0.0017,
            -0.0618,
            0.0116,
            0.0345
          ],
          "after": [
            0.0451,
            -0.1467,
            -0.2947,
            0.0527,
            0.0303,
            -0.0479,
            -0.1508,
            0.0715,
            0.111,
            0.0097,
            0.0132,
            0.0826,
            0.0399,
            0.0241,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0,
            -0.0005,
            0.0009,
            -0.0013,
            -0.0,
            -0.0005,
            -0.0021,
            -0.0006,
            0.0007,
            -0.0003,
            0.0001,
            0.0002,
            -0.001,
            0.0011,
            0.0003,
            -0.0016
          ],
          "after": [
            -0.0849,
            0.0094,
            -0.1254,
            -0.0728,
            0.0674,
            0.1381,
            0.0882,
            -0.1606,
            0.03,
            -0.0248,
            -0.1494,
            -0.0648,
            0.0936,
            -0.0798,
            0.0406,
            0.0862
          ]
        }
      }
    },
    {
      "step": 953,
      "word": "maeva",
      "loss": 2.1469,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.296,
            -0.2826,
            -0.0596,
            0.0227,
            -0.1447,
            0.1132,
            -0.325,
            0.0081,
            -0.0491,
            0.3161,
            0.222,
            -0.3674,
            -0.0199,
            0.143,
            0.0417,
            -0.5664
          ],
          "after": [
            0.0084,
            -0.0051,
            0.1576,
            -0.0242,
            0.0305,
            -0.0911,
            -0.1176,
            -0.1577,
            -0.0091,
            0.0738,
            0.162,
            0.0142,
            -0.1025,
            -0.0179,
            -0.0372,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.3022,
            0.202,
            -0.1923,
            0.1933,
            -0.0713,
            0.4179,
            0.3765,
            0.2018,
            -0.0052,
            -0.216,
            -0.2518,
            0.2791,
            0.0026,
            -0.1306,
            0.2957,
            0.3845
          ],
          "after": [
            0.028,
            0.1076,
            -0.0412,
            -0.0757,
            0.0313,
            -0.0095,
            0.0899,
            0.1004,
            -0.0648,
            -0.0613,
            0.014,
            -0.0168,
            0.1073,
            0.0347,
            -0.0521,
            0.2226
          ]
        },
        "position_0": {
          "grad": [
            -0.0017,
            0.0372,
            -0.051,
            -0.0264,
            0.1447,
            -0.0343,
            0.2082,
            0.0686,
            0.0418,
            0.0351,
            -0.0187,
            0.0123,
            -0.0716,
            -0.0358,
            -0.0246,
            0.1222
          ],
          "after": [
            0.045,
            -0.1467,
            -0.2947,
            0.0528,
            0.0302,
            -0.0479,
            -0.1508,
            0.0715,
            0.111,
            0.0097,
            0.0131,
            0.0826,
            0.0399,
            0.0242,
            -0.1305,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0012,
            -0.0008,
            -0.0032,
            0.0017,
            -0.0037,
            0.0053,
            -0.0,
            0.0032,
            0.0015,
            -0.0004,
            -0.0025,
            -0.0001,
            0.0003,
            0.0014,
            0.0023,
            0.0052
          ],
          "after": [
            -0.0849,
            0.0094,
            -0.1254,
            -0.0729,
            0.0674,
            0.1381,
            0.0882,
            -0.1607,
            0.0299,
            -0.0248,
            -0.1494,
            -0.0649,
            0.0936,
            -0.0798,
            0.0406,
            0.0861
          ]
        }
      }
    },
    {
      "step": 954,
      "word": "niala",
      "loss": 2.131,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2872,
            0.073,
            -0.2637,
            -0.1507,
            -0.0055,
            -0.1065,
            -0.1552,
            0.1648,
            -0.1477,
            0.2379,
            -0.0021,
            -0.0273,
            0.0388,
            0.1979,
            -0.0056,
            -0.5529
          ],
          "after": [
            0.0083,
            -0.0051,
            0.1577,
            -0.0242,
            0.0306,
            -0.0911,
            -0.1175,
            -0.1577,
            -0.0091,
            0.0738,
            0.162,
            0.0142,
            -0.1025,
            -0.0179,
            -0.0372,
            -0.1868
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0538,
            0.0859,
            -0.1159,
            0.0213,
            0.0238,
            -0.1319,
            0.253,
            0.0058,
            -0.1478,
            0.0435,
            0.0053,
            -0.027,
            0.0874,
            -0.0984,
            -0.1401,
            0.1333
          ],
          "after": [
            0.028,
            0.1075,
            -0.0412,
            -0.0757,
            0.0313,
            -0.0095,
            0.0899,
            0.1003,
            -0.0647,
            -0.0613,
            0.014,
            -0.0168,
            0.1073,
            0.0347,
            -0.0521,
            0.2225
          ]
        },
        "position_0": {
          "grad": [
            -0.0258,
            0.1454,
            -0.0907,
            0.0039,
            -0.3416,
            0.009,
            -0.0209,
            -0.0285,
            0.0989,
            -0.1897,
            -0.2655,
            -0.0556,
            -0.0483,
            0.1054,
            0.1767,
            0.1079
          ],
          "after": [
            0.045,
            -0.1467,
            -0.2947,
            0.0528,
            0.0302,
            -0.0479,
            -0.1508,
            0.0715,
            0.111,
            0.0098,
            0.0132,
            0.0826,
            0.0399,
            0.0242,
            -0.1306,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0001,
            -0.0005,
            0.0002,
            -0.0012,
            0.0004,
            -0.001,
            0.0001,
            0.0005,
            0.0002,
            -0.0003,
            -0.0001,
            -0.0002,
            0.0009,
            0.0006,
            0.0005
          ],
          "after": [
            -0.0849,
            0.0094,
            -0.1253,
            -0.0729,
            0.0674,
            0.138,
            0.0882,
            -0.1608,
            0.0299,
            -0.0248,
            -0.1493,
            -0.0649,
            0.0936,
            -0.0798,
            0.0406,
            0.086
          ]
        }
      }
    },
    {
      "step": 955,
      "word": "elioenai",
      "loss": 2.4256,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1456,
            -0.14,
            0.0947,
            0.0417,
            0.0799,
            0.1635,
            -0.062,
            0.164,
            0.1176,
            -0.0563,
            -0.1078,
            -0.0015,
            -0.1199,
            -0.0811,
            0.0083,
            0.0589
          ],
          "after": [
            0.0083,
            -0.0051,
            0.1578,
            -0.0243,
            0.0306,
            -0.0911,
            -0.1175,
            -0.1578,
            -0.0091,
            0.0738,
            0.1621,
            0.0142,
            -0.1025,
            -0.018,
            -0.0372,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.174,
            0.4815,
            0.284,
            0.0581,
            -0.1205,
            0.1326,
            0.1692,
            -0.1589,
            0.188,
            -0.4469,
            -0.1565,
            -0.0549,
            -0.0727,
            0.1116,
            0.3553,
            0.0796
          ],
          "after": [
            0.0281,
            0.1075,
            -0.0412,
            -0.0758,
            0.0313,
            -0.0095,
            0.0898,
            0.1003,
            -0.0647,
            -0.0612,
            0.0141,
            -0.0169,
            0.1073,
            0.0348,
            -0.0521,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            -0.0116,
            -0.1416,
            0.0532,
            -0.045,
            0.0333,
            0.0564,
            -0.219,
            -0.0789,
            0.1017,
            0.1325,
            0.0379,
            -0.0245,
            -0.132,
            -0.0359,
            -0.046,
            -0.253
          ],
          "after": [
            0.045,
            -0.1467,
            -0.2947,
            0.0528,
            0.0302,
            -0.0479,
            -0.1508,
            0.0715,
            0.1109,
            0.0098,
            0.0132,
            0.0825,
            0.04,
            0.0242,
            -0.1306,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0032,
            0.0012,
            -0.0039,
            0.0013,
            -0.0028,
            0.0028,
            -0.004,
            -0.0017,
            -0.0001,
            -0.0025,
            -0.0045,
            -0.006,
            -0.0003,
            0.0047,
            0.0039,
            0.0015
          ],
          "after": [
            -0.0849,
            0.0094,
            -0.1252,
            -0.0729,
            0.0675,
            0.138,
            0.0883,
            -0.1608,
            0.0299,
            -0.0247,
            -0.1492,
            -0.0648,
            0.0935,
            -0.0799,
            0.0406,
            0.0859
          ]
        }
      }
    },
    {
      "step": 956,
      "word": "iosefa",
      "loss": 2.8843,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0929,
            -0.0141,
            -0.0158,
            -0.0801,
            -0.0237,
            -0.0391,
            -0.0278,
            -0.0159,
            -0.0467,
            0.0889,
            0.0869,
            -0.0935,
            0.0506,
            0.0376,
            -0.0158,
            -0.1184
          ],
          "after": [
            0.0083,
            -0.0051,
            0.1578,
            -0.0243,
            0.0306,
            -0.0911,
            -0.1174,
            -0.1578,
            -0.0091,
            0.0737,
            0.1621,
            0.0143,
            -0.1025,
            -0.018,
            -0.0372,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1911,
            0.0087,
            0.2696,
            0.0123,
            0.0435,
            0.1222,
            -0.2159,
            -0.0003,
            0.0561,
            -0.0071,
            -0.0529,
            -0.2623,
            0.203,
            0.0759,
            0.0998,
            -0.1287
          ],
          "after": [
            0.0281,
            0.1074,
            -0.0412,
            -0.0759,
            0.0313,
            -0.0095,
            0.0898,
            0.1003,
            -0.0647,
            -0.0612,
            0.0141,
            -0.0168,
            0.1072,
            0.0348,
            -0.0521,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            -0.0614,
            -0.2076,
            0.102,
            -0.1732,
            0.1495,
            0.1285,
            -0.2524,
            0.0022,
            0.0629,
            0.0398,
            0.108,
            -0.1285,
            -0.1652,
            0.0026,
            -0.0705,
            -0.1455
          ],
          "after": [
            0.045,
            -0.1467,
            -0.2947,
            0.0528,
            0.0302,
            -0.0479,
            -0.1508,
            0.0715,
            0.1109,
            0.0098,
            0.0132,
            0.0826,
            0.04,
            0.0242,
            -0.1306,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0008,
            0.0006,
            -0.0034,
            0.0021,
            -0.0002,
            0.0013,
            0.0029,
            0.001,
            -0.0022,
            -0.0015,
            -0.0031,
            0.0026,
            -0.0011,
            -0.0,
            0.0002,
            0.0017
          ],
          "after": [
            -0.085,
            0.0093,
            -0.1251,
            -0.073,
            0.0676,
            0.1379,
            0.0883,
            -0.1609,
            0.0299,
            -0.0247,
            -0.149,
            -0.0648,
            0.0935,
            -0.08,
            0.0405,
            0.0858
          ]
        }
      }
    },
    {
      "step": 957,
      "word": "iyanah",
      "loss": 2.0975,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0835,
            0.0952,
            0.0634,
            0.1358,
            -0.1556,
            0.092,
            -0.0315,
            0.0455,
            0.1867,
            0.106,
            -0.22,
            -0.057,
            0.1987,
            0.1422,
            -0.1246,
            0.2112
          ],
          "after": [
            0.0083,
            -0.0051,
            0.1579,
            -0.0243,
            0.0307,
            -0.0911,
            -0.1174,
            -0.1578,
            -0.0091,
            0.0737,
            0.1621,
            0.0143,
            -0.1025,
            -0.0181,
            -0.0372,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0179,
            -0.0452,
            -0.0405,
            -0.0052,
            0.075,
            -0.0596,
            0.0878,
            0.0089,
            -0.0739,
            0.0766,
            -0.0049,
            -0.0127,
            0.0378,
            -0.0515,
            -0.1171,
            0.0449
          ],
          "after": [
            0.0282,
            0.1074,
            -0.0412,
            -0.0759,
            0.0313,
            -0.0095,
            0.0898,
            0.1003,
            -0.0646,
            -0.0612,
            0.0142,
            -0.0168,
            0.1072,
            0.0348,
            -0.0521,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0524,
            -0.2329,
            0.1031,
            -0.2026,
            0.1261,
            0.1119,
            -0.2882,
            0.0082,
            0.0995,
            0.0459,
            0.0789,
            -0.1581,
            -0.2025,
            -0.0073,
            -0.0542,
            -0.1941
          ],
          "after": [
            0.045,
            -0.1466,
            -0.2948,
            0.0529,
            0.0302,
            -0.048,
            -0.1507,
            0.0715,
            0.1108,
            0.0098,
            0.0131,
            0.0826,
            0.0401,
            0.0242,
            -0.1306,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0003,
            -0.0005,
            -0.0003,
            0.0003,
            0.0001,
            0.0012,
            0.0,
            -0.0002,
            0.0011,
            0.0004,
            -0.0007,
            0.0007,
            -0.0001,
            -0.0003,
            -0.0003
          ],
          "after": [
            -0.085,
            0.0093,
            -0.125,
            -0.073,
            0.0676,
            0.1378,
            0.0883,
            -0.161,
            0.0299,
            -0.0247,
            -0.1489,
            -0.0648,
            0.0935,
            -0.08,
            0.0405,
            0.0858
          ]
        }
      }
    },
    {
      "step": 958,
      "word": "vinisha",
      "loss": 2.4141,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.071,
            -0.026,
            -0.0269,
            -0.0525,
            -0.0061,
            -0.0275,
            -0.0162,
            -0.0275,
            -0.0459,
            0.0424,
            0.0689,
            -0.056,
            0.018,
            0.0398,
            -0.0038,
            -0.0615
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1579,
            -0.0243,
            0.0307,
            -0.0911,
            -0.1173,
            -0.1578,
            -0.0092,
            0.0736,
            0.1622,
            0.0143,
            -0.1026,
            -0.0181,
            -0.0372,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0575,
            0.0384,
            -0.1045,
            0.0085,
            0.0862,
            -0.1201,
            0.2327,
            0.0329,
            -0.1619,
            0.0462,
            -0.042,
            0.0112,
            0.0719,
            -0.0696,
            -0.1722,
            0.1179
          ],
          "after": [
            0.0283,
            0.1073,
            -0.0412,
            -0.076,
            0.0313,
            -0.0095,
            0.0897,
            0.1003,
            -0.0646,
            -0.0612,
            0.0142,
            -0.0168,
            0.1071,
            0.0348,
            -0.0521,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0733,
            0.049,
            -0.0242,
            0.067,
            0.0637,
            0.0741,
            0.051,
            0.0056,
            -0.0711,
            -0.0196,
            -0.0113,
            -0.0407,
            0.0156,
            -0.0704,
            -0.0116,
            0.1524
          ],
          "after": [
            0.0451,
            -0.1466,
            -0.2948,
            0.053,
            0.0301,
            -0.0481,
            -0.1507,
            0.0715,
            0.1108,
            0.0098,
            0.0131,
            0.0827,
            0.0402,
            0.0242,
            -0.1306,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0028,
            -0.0006,
            -0.0018,
            -0.0085,
            0.0053,
            -0.0018,
            -0.0027,
            -0.0039,
            0.0002,
            0.001,
            0.0029,
            -0.0099,
            0.0004,
            0.0037,
            -0.0003,
            -0.007
          ],
          "after": [
            -0.0851,
            0.0093,
            -0.1249,
            -0.073,
            0.0676,
            0.1378,
            0.0883,
            -0.1609,
            0.0299,
            -0.0246,
            -0.1489,
            -0.0647,
            0.0935,
            -0.0801,
            0.0405,
            0.0858
          ]
        }
      }
    },
    {
      "step": 959,
      "word": "azelea",
      "loss": 1.9996,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0892,
            0.0383,
            0.0018,
            0.1143,
            0.2224,
            -0.2321,
            0.2256,
            -0.0178,
            0.0367,
            0.2236,
            0.221,
            0.0249,
            0.1085,
            -0.0976,
            -0.1517,
            -0.0971
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1579,
            -0.0243,
            0.0307,
            -0.0911,
            -0.1173,
            -0.1579,
            -0.0092,
            0.0736,
            0.1622,
            0.0143,
            -0.1026,
            -0.0181,
            -0.0372,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1881,
            -0.5722,
            0.5127,
            -0.0201,
            0.1395,
            0.5781,
            -0.8445,
            0.0296,
            0.5052,
            0.1722,
            0.1194,
            0.2134,
            -0.518,
            0.1907,
            0.3628,
            -0.4752
          ],
          "after": [
            0.0283,
            0.1073,
            -0.0412,
            -0.076,
            0.0313,
            -0.0096,
            0.0897,
            0.1003,
            -0.0646,
            -0.0612,
            0.0143,
            -0.0168,
            0.1072,
            0.0348,
            -0.0521,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.092,
            -0.028,
            0.0919,
            -0.174,
            0.0796,
            0.0028,
            -0.3079,
            -0.1203,
            0.1355,
            0.0763,
            0.2268,
            0.0369,
            -0.1358,
            0.0196,
            -0.1271,
            -0.2222
          ],
          "after": [
            0.0451,
            -0.1465,
            -0.2949,
            0.0531,
            0.0301,
            -0.0481,
            -0.1506,
            0.0715,
            0.1107,
            0.0097,
            0.0131,
            0.0827,
            0.0402,
            0.0243,
            -0.1306,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0011,
            -0.0002,
            0.0007,
            0.0011,
            0.0007,
            0.0002,
            -0.0,
            0.0007,
            -0.0001,
            0.0009,
            0.0008,
            -0.0008,
            0.0007,
            -0.0004,
            -0.0022,
            0.0009
          ],
          "after": [
            -0.0852,
            0.0093,
            -0.1248,
            -0.073,
            0.0676,
            0.1377,
            0.0883,
            -0.1609,
            0.0299,
            -0.0246,
            -0.1488,
            -0.0646,
            0.0935,
            -0.0801,
            0.0405,
            0.0858
          ]
        }
      }
    },
    {
      "step": 960,
      "word": "keygan",
      "loss": 2.1825,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.106,
            0.1734,
            -0.031,
            0.0799,
            -0.0241,
            -0.0939,
            0.0098,
            -0.1045,
            0.0543,
            -0.2268,
            -0.0831,
            0.1704,
            -0.134,
            -0.0547,
            0.008,
            0.1179
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1579,
            -0.0244,
            0.0307,
            -0.0911,
            -0.1173,
            -0.1578,
            -0.0092,
            0.0736,
            0.1622,
            0.0143,
            -0.1026,
            -0.0181,
            -0.0372,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1973,
            -0.2951,
            0.2287,
            -0.0348,
            0.2116,
            0.2597,
            -0.5642,
            -0.1285,
            0.4441,
            0.2066,
            0.202,
            0.2138,
            -0.2238,
            0.038,
            -0.1228,
            -0.34
          ],
          "after": [
            0.0283,
            0.1074,
            -0.0413,
            -0.076,
            0.0312,
            -0.0096,
            0.0898,
            0.1003,
            -0.0647,
            -0.0612,
            0.0142,
            -0.0169,
            0.1072,
            0.0348,
            -0.0521,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.083,
            0.0468,
            -0.1146,
            0.0185,
            0.0268,
            -0.0514,
            0.1563,
            -0.0115,
            -0.033,
            -0.0049,
            -0.0364,
            -0.0073,
            0.0439,
            -0.0018,
            -0.0734,
            0.0033
          ],
          "after": [
            0.0451,
            -0.1465,
            -0.2949,
            0.0531,
            0.03,
            -0.0481,
            -0.1506,
            0.0716,
            0.1107,
            0.0097,
            0.0131,
            0.0827,
            0.0403,
            0.0243,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            -0.0011,
            0.0007,
            0.0011,
            0.0015,
            0.0025,
            -0.0009,
            -0.001,
            0.0002,
            0.0015,
            0.0005,
            0.0039,
            -0.0028,
            -0.001,
            -0.0005,
            -0.0016
          ],
          "after": [
            -0.0852,
            0.0093,
            -0.1247,
            -0.073,
            0.0675,
            0.1377,
            0.0883,
            -0.1609,
            0.0299,
            -0.0247,
            -0.1488,
            -0.0646,
            0.0935,
            -0.0802,
            0.0405,
            0.0858
          ]
        }
      }
    },
    {
      "step": 961,
      "word": "miia",
      "loss": 2.3066,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.5349,
            -0.0883,
            -0.467,
            -0.1912,
            -0.2508,
            -0.1124,
            -0.0279,
            0.0422,
            -0.2097,
            0.295,
            0.1608,
            -0.5925,
            0.3107,
            0.2852,
            0.1312,
            -0.2695
          ],
          "after": [
            0.0081,
            -0.0051,
            0.158,
            -0.0244,
            0.0308,
            -0.091,
            -0.1173,
            -0.1579,
            -0.0092,
            0.0735,
            0.1621,
            0.0144,
            -0.1027,
            -0.0182,
            -0.0372,
            -0.1867
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0389,
            0.0472,
            -0.1786,
            -0.001,
            0.0221,
            -0.1215,
            0.1795,
            0.0112,
            -0.2089,
            0.055,
            -0.0532,
            -0.1012,
            0.1507,
            -0.0098,
            -0.1818,
            0.1145
          ],
          "after": [
            0.0283,
            0.1074,
            -0.0413,
            -0.0761,
            0.0312,
            -0.0097,
            0.0898,
            0.1003,
            -0.0647,
            -0.0612,
            0.0142,
            -0.0169,
            0.1072,
            0.0348,
            -0.0521,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0009,
            0.0139,
            -0.0489,
            -0.0276,
            0.1265,
            -0.0426,
            0.1889,
            0.0834,
            0.1001,
            0.0213,
            -0.0337,
            -0.0247,
            -0.1253,
            -0.0289,
            -0.0003,
            0.0973
          ],
          "after": [
            0.0452,
            -0.1465,
            -0.2949,
            0.0532,
            0.03,
            -0.0482,
            -0.1506,
            0.0716,
            0.1107,
            0.0097,
            0.013,
            0.0827,
            0.0403,
            0.0243,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0046,
            -0.0003,
            0.0044,
            0.0066,
            -0.0114,
            -0.0054,
            -0.0104,
            0.0041,
            0.0006,
            -0.0012,
            0.0053,
            0.0143,
            -0.0105,
            -0.01,
            0.0136,
            -0.0021
          ],
          "after": [
            -0.0852,
            0.0093,
            -0.1247,
            -0.073,
            0.0676,
            0.1377,
            0.0884,
            -0.161,
            0.0299,
            -0.0247,
            -0.1489,
            -0.0646,
            0.0936,
            -0.0801,
            0.0404,
            0.0858
          ]
        }
      }
    },
    {
      "step": 962,
      "word": "ritter",
      "loss": 2.5586,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0081,
            -0.0051,
            0.1581,
            -0.0244,
            0.0308,
            -0.091,
            -0.1173,
            -0.1579,
            -0.0092,
            0.0735,
            0.1621,
            0.0144,
            -0.1027,
            -0.0182,
            -0.0372,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1741,
            -0.3193,
            0.0562,
            -0.104,
            -0.114,
            0.0605,
            -0.2796,
            0.0034,
            -0.0343,
            0.039,
            -0.062,
            -0.0045,
            -0.036,
            0.1059,
            0.0366,
            0.0012
          ],
          "after": [
            0.0283,
            0.1075,
            -0.0413,
            -0.0761,
            0.0312,
            -0.0097,
            0.0898,
            0.1003,
            -0.0647,
            -0.0613,
            0.0142,
            -0.0169,
            0.1072,
            0.0348,
            -0.0521,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            0.0093,
            -0.0409,
            0.0051,
            -0.0282,
            -0.052,
            0.1466,
            0.0739,
            -0.0492,
            0.1292,
            -0.0768,
            -0.1099,
            0.0196,
            -0.1713,
            -0.0805,
            0.1344,
            0.0943
          ],
          "after": [
            0.0452,
            -0.1464,
            -0.2949,
            0.0532,
            0.0299,
            -0.0482,
            -0.1506,
            0.0716,
            0.1106,
            0.0097,
            0.013,
            0.0828,
            0.0404,
            0.0243,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0023,
            0.0009,
            -0.0004,
            0.0,
            0.0029,
            0.0005,
            0.0007,
            -0.0011,
            0.0005,
            0.0008,
            0.0002,
            -0.003,
            0.0022,
            0.0027,
            -0.0037,
            -0.0009
          ],
          "after": [
            -0.0852,
            0.0093,
            -0.1247,
            -0.073,
            0.0676,
            0.1377,
            0.0885,
            -0.161,
            0.0299,
            -0.0247,
            -0.1489,
            -0.0647,
            0.0937,
            -0.0801,
            0.0403,
            0.0859
          ]
        }
      }
    },
    {
      "step": 963,
      "word": "chisom",
      "loss": 2.7888,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0081,
            -0.0052,
            0.1581,
            -0.0244,
            0.0308,
            -0.091,
            -0.1173,
            -0.1579,
            -0.0092,
            0.0734,
            0.1621,
            0.0145,
            -0.1027,
            -0.0183,
            -0.0372,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0014,
            0.0659,
            -0.2589,
            -0.014,
            0.0165,
            -0.1862,
            0.264,
            0.0396,
            -0.2576,
            0.007,
            -0.0778,
            -0.0756,
            0.1669,
            -0.0748,
            -0.1747,
            0.1986
          ],
          "after": [
            0.0283,
            0.1075,
            -0.0413,
            -0.0761,
            0.0311,
            -0.0097,
            0.0898,
            0.1003,
            -0.0647,
            -0.0613,
            0.0143,
            -0.0169,
            0.1072,
            0.0348,
            -0.0521,
            0.2224
          ]
        },
        "position_0": {
          "grad": [
            -0.0423,
            0.0312,
            -0.0259,
            0.132,
            -0.0504,
            -0.0256,
            0.1349,
            0.0631,
            -0.0757,
            0.0039,
            0.0334,
            0.0124,
            0.0562,
            0.0093,
            -0.0158,
            0.0973
          ],
          "after": [
            0.0452,
            -0.1464,
            -0.2949,
            0.0532,
            0.0299,
            -0.0482,
            -0.1506,
            0.0716,
            0.1106,
            0.0097,
            0.013,
            0.0828,
            0.0404,
            0.0244,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            0.0011,
            0.0005,
            0.0014,
            0.001,
            0.0031,
            0.0002,
            -0.0009,
            -0.0013,
            0.0006,
            0.0004,
            -0.0009,
            0.0013,
            0.0007,
            -0.0016,
            0.0011
          ],
          "after": [
            -0.0852,
            0.0093,
            -0.1247,
            -0.0731,
            0.0676,
            0.1377,
            0.0885,
            -0.161,
            0.0299,
            -0.0247,
            -0.149,
            -0.0647,
            0.0937,
            -0.0801,
            0.0403,
            0.0859
          ]
        }
      }
    },
    {
      "step": 964,
      "word": "royalty",
      "loss": 2.6839,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1943,
            0.1073,
            -0.0182,
            -0.0622,
            0.1653,
            0.0687,
            -0.0496,
            0.1455,
            0.0003,
            -0.0778,
            -0.1636,
            0.331,
            -0.0995,
            -0.0657,
            0.0273,
            -0.1472
          ],
          "after": [
            0.008,
            -0.0052,
            0.1582,
            -0.0244,
            0.0308,
            -0.0909,
            -0.1173,
            -0.1579,
            -0.0092,
            0.0734,
            0.1621,
            0.0145,
            -0.1027,
            -0.0183,
            -0.0372,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0859,
            0.1227,
            -0.0833,
            0.0807,
            0.0314,
            -0.1419,
            0.2645,
            -0.0067,
            -0.1749,
            -0.0059,
            -0.0059,
            0.015,
            0.0815,
            -0.1102,
            -0.1582,
            0.1504
          ],
          "after": [
            0.0283,
            0.1075,
            -0.0413,
            -0.0761,
            0.0311,
            -0.0097,
            0.0898,
            0.1003,
            -0.0647,
            -0.0613,
            0.0143,
            -0.0169,
            0.1072,
            0.0348,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0016,
            -0.0283,
            0.002,
            -0.0612,
            0.0017,
            0.1319,
            0.0632,
            -0.0666,
            0.0979,
            -0.0326,
            -0.0571,
            0.0369,
            -0.144,
            -0.0927,
            0.0655,
            0.0817
          ],
          "after": [
            0.0452,
            -0.1464,
            -0.2949,
            0.0533,
            0.0299,
            -0.0483,
            -0.1506,
            0.0716,
            0.1105,
            0.0097,
            0.013,
            0.0828,
            0.0405,
            0.0244,
            -0.1305,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.001,
            0.0033,
            -0.0003,
            0.0017,
            0.0012,
            -0.001,
            0.0014,
            -0.0009,
            -0.0005,
            -0.0035,
            -0.0002,
            0.0015,
            -0.0011,
            -0.0019,
            0.0009,
            0.0027
          ],
          "after": [
            -0.0852,
            0.0093,
            -0.1247,
            -0.0731,
            0.0677,
            0.1377,
            0.0885,
            -0.161,
            0.0299,
            -0.0246,
            -0.149,
            -0.0647,
            0.0938,
            -0.0801,
            0.0403,
            0.0859
          ]
        }
      }
    },
    {
      "step": 965,
      "word": "nur",
      "loss": 3.1363,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.008,
            -0.0052,
            0.1582,
            -0.0244,
            0.0308,
            -0.0909,
            -0.1173,
            -0.1579,
            -0.0092,
            0.0734,
            0.1621,
            0.0145,
            -0.1028,
            -0.0183,
            -0.0372,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0193,
            0.0574,
            -0.179,
            0.0291,
            -0.0652,
            -0.1339,
            0.2602,
            0.0427,
            -0.1692,
            0.105,
            -0.0298,
            -0.052,
            0.1736,
            -0.0606,
            -0.1608,
            0.1262
          ],
          "after": [
            0.0283,
            0.1075,
            -0.0413,
            -0.0761,
            0.0311,
            -0.0097,
            0.0898,
            0.1003,
            -0.0647,
            -0.0614,
            0.0143,
            -0.0169,
            0.1072,
            0.0348,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0491,
            0.1974,
            -0.1175,
            0.1384,
            -0.636,
            0.0355,
            0.0374,
            0.0048,
            0.137,
            -0.4445,
            -0.4289,
            -0.1051,
            -0.0721,
            0.2593,
            0.4459,
            0.2601
          ],
          "after": [
            0.0453,
            -0.1464,
            -0.2949,
            0.0533,
            0.0299,
            -0.0484,
            -0.1506,
            0.0716,
            0.1105,
            0.0098,
            0.0131,
            0.0828,
            0.0406,
            0.0244,
            -0.1306,
            0.0517
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0018,
            -0.0023,
            0.0009,
            -0.003,
            0.0024,
            -0.0011,
            -0.004,
            0.0008,
            0.0001,
            -0.0024,
            0.0001,
            -0.0006,
            -0.0019,
            0.0004,
            0.0029,
            -0.0015
          ],
          "after": [
            -0.0852,
            0.0093,
            -0.1247,
            -0.0731,
            0.0676,
            0.1377,
            0.0886,
            -0.161,
            0.0299,
            -0.0246,
            -0.149,
            -0.0647,
            0.0938,
            -0.08,
            0.0402,
            0.0859
          ]
        }
      }
    },
    {
      "step": 966,
      "word": "makson",
      "loss": 2.3412,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2296,
            0.1213,
            0.0771,
            0.0595,
            0.0728,
            -0.0528,
            0.2962,
            0.0358,
            -0.0764,
            -0.0222,
            -0.0377,
            0.0537,
            0.0115,
            -0.0617,
            -0.1513,
            -0.0773
          ],
          "after": [
            0.008,
            -0.0052,
            0.1582,
            -0.0244,
            0.0308,
            -0.0909,
            -0.1173,
            -0.1579,
            -0.0092,
            0.0734,
            0.1621,
            0.0145,
            -0.1028,
            -0.0183,
            -0.0372,
            -0.1866
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0793,
            0.133,
            -0.1676,
            0.0368,
            0.0107,
            -0.2022,
            0.3023,
            0.0599,
            -0.2235,
            0.031,
            -0.0603,
            0.0071,
            0.1011,
            -0.0722,
            -0.1492,
            0.1915
          ],
          "after": [
            0.0283,
            0.1075,
            -0.0413,
            -0.0761,
            0.0311,
            -0.0097,
            0.0898,
            0.1003,
            -0.0646,
            -0.0614,
            0.0143,
            -0.0169,
            0.1072,
            0.0348,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0142,
            0.0464,
            -0.0533,
            0.0439,
            0.1473,
            -0.0113,
            0.2596,
            0.0747,
            -0.0085,
            0.0302,
            -0.0087,
            0.0712,
            -0.0271,
            -0.0471,
            -0.0175,
            0.1535
          ],
          "after": [
            0.0453,
            -0.1464,
            -0.2948,
            0.0533,
            0.03,
            -0.0484,
            -0.1506,
            0.0716,
            0.1104,
            0.0099,
            0.0131,
            0.0828,
            0.0406,
            0.0244,
            -0.1306,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0024,
            0.0039,
            0.0015,
            0.0012,
            0.0016,
            -0.0051,
            0.0072,
            0.0014,
            -0.0011,
            0.0043,
            -0.001,
            -0.0007,
            0.0029,
            0.0013,
            -0.0048,
            0.0047
          ],
          "after": [
            -0.0852,
            0.0092,
            -0.1247,
            -0.0732,
            0.0676,
            0.1377,
            0.0886,
            -0.161,
            0.0299,
            -0.0246,
            -0.149,
            -0.0648,
            0.0938,
            -0.08,
            0.0402,
            0.0858
          ]
        }
      }
    },
    {
      "step": 967,
      "word": "beckett",
      "loss": 2.926,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.008,
            -0.0052,
            0.1582,
            -0.0244,
            0.0308,
            -0.0909,
            -0.1173,
            -0.1579,
            -0.0092,
            0.0734,
            0.1621,
            0.0145,
            -0.1028,
            -0.0183,
            -0.0372,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2436,
            -0.4394,
            0.1944,
            -0.0265,
            0.2581,
            0.5299,
            -0.6006,
            -0.174,
            0.5848,
            -0.028,
            0.1488,
            0.1335,
            -0.3852,
            0.0179,
            0.0489,
            -0.3389
          ],
          "after": [
            0.0283,
            0.1076,
            -0.0413,
            -0.0761,
            0.0311,
            -0.0097,
            0.0898,
            0.1003,
            -0.0646,
            -0.0614,
            0.0143,
            -0.0169,
            0.1072,
            0.0349,
            -0.0519,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0456,
            0.0082,
            0.0708,
            0.018,
            -0.0135,
            0.0237,
            0.0981,
            0.0141,
            -0.0917,
            -0.0645,
            -0.0254,
            -0.009,
            0.1426,
            -0.0575,
            0.0077,
            0.2672
          ],
          "after": [
            0.0453,
            -0.1464,
            -0.2948,
            0.0532,
            0.03,
            -0.0484,
            -0.1507,
            0.0716,
            0.1104,
            0.0099,
            0.0132,
            0.0828,
            0.0406,
            0.0244,
            -0.1307,
            0.0516
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0056,
            0.0007,
            -0.0033,
            0.0007,
            0.0016,
            0.0011,
            0.0009,
            -0.0011,
            0.0012,
            0.0006,
            0.0022,
            -0.0022,
            0.0009,
            -0.0002,
            -0.0039,
            0.0023
          ],
          "after": [
            -0.0853,
            0.0092,
            -0.1247,
            -0.0732,
            0.0676,
            0.1378,
            0.0886,
            -0.161,
            0.0299,
            -0.0246,
            -0.1491,
            -0.0648,
            0.0938,
            -0.08,
            0.0402,
            0.0858
          ]
        }
      }
    },
    {
      "step": 968,
      "word": "jonna",
      "loss": 1.9447,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.3388,
            -0.0833,
            -0.1545,
            -0.0897,
            -0.0338,
            -0.0666,
            -0.0301,
            0.0412,
            -0.1408,
            0.1894,
            0.1183,
            -0.311,
            0.1546,
            0.1403,
            -0.0027,
            -0.2512
          ],
          "after": [
            0.008,
            -0.0052,
            0.1583,
            -0.0244,
            0.0308,
            -0.0909,
            -0.1173,
            -0.158,
            -0.0092,
            0.0733,
            0.1621,
            0.0145,
            -0.1028,
            -0.0183,
            -0.0371,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0733,
            0.054,
            -0.1474,
            0.0081,
            0.0171,
            -0.1121,
            0.2764,
            0.0823,
            -0.1949,
            0.0546,
            -0.055,
            0.0039,
            0.1173,
            -0.0747,
            -0.1162,
            0.1323
          ],
          "after": [
            0.0282,
            0.1076,
            -0.0413,
            -0.0761,
            0.031,
            -0.0097,
            0.0898,
            0.1003,
            -0.0646,
            -0.0614,
            0.0143,
            -0.0169,
            0.1072,
            0.0349,
            -0.0519,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0095,
            0.0133,
            -0.0055,
            0.1681,
            -0.0736,
            -0.1375,
            0.1693,
            0.0993,
            -0.1926,
            -0.152,
            -0.029,
            -0.0272,
            0.3071,
            -0.0558,
            -0.0248,
            0.1551
          ],
          "after": [
            0.0453,
            -0.1465,
            -0.2948,
            0.0532,
            0.0301,
            -0.0484,
            -0.1507,
            0.0716,
            0.1104,
            0.01,
            0.0132,
            0.0828,
            0.0406,
            0.0244,
            -0.1307,
            0.0515
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0015,
            -0.0007,
            0.0001,
            0.0002,
            -0.0006,
            0.0004,
            -0.0003,
            0.0001,
            0.0008,
            0.0014,
            0.0005,
            -0.0002,
            0.0006,
            0.001,
            -0.0016,
            -0.0002
          ],
          "after": [
            -0.0853,
            0.0091,
            -0.1247,
            -0.0732,
            0.0676,
            0.1378,
            0.0886,
            -0.161,
            0.0299,
            -0.0246,
            -0.1491,
            -0.0648,
            0.0939,
            -0.08,
            0.0403,
            0.0858
          ]
        }
      }
    },
    {
      "step": 969,
      "word": "jaaziel",
      "loss": 2.343,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1201,
            0.1033,
            0.2713,
            0.0508,
            0.1811,
            0.1135,
            -0.0133,
            0.028,
            0.1145,
            -0.0301,
            0.3626,
            0.16,
            -0.1911,
            -0.1865,
            -0.0457,
            -0.0981
          ],
          "after": [
            0.008,
            -0.0053,
            0.1583,
            -0.0243,
            0.0308,
            -0.0909,
            -0.1173,
            -0.158,
            -0.0091,
            0.0733,
            0.1621,
            0.0145,
            -0.1028,
            -0.0183,
            -0.0371,
            -0.1865
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0368,
            0.1495,
            -0.1789,
            0.052,
            -0.5451,
            -0.0663,
            0.0178,
            0.0058,
            -0.0169,
            -0.0842,
            0.002,
            -0.0286,
            0.1722,
            0.0142,
            0.349,
            0.0518
          ],
          "after": [
            0.0282,
            0.1076,
            -0.0412,
            -0.0761,
            0.0311,
            -0.0097,
            0.0898,
            0.1003,
            -0.0646,
            -0.0614,
            0.0143,
            -0.0169,
            0.1072,
            0.0349,
            -0.0519,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0109,
            0.0466,
            -0.0265,
            0.0864,
            0.0334,
            -0.1065,
            0.154,
            0.0455,
            -0.1885,
            0.0028,
            0.0086,
            0.0115,
            0.2612,
            -0.1106,
            -0.1435,
            0.1031
          ],
          "after": [
            0.0453,
            -0.1465,
            -0.2948,
            0.0532,
            0.0301,
            -0.0484,
            -0.1508,
            0.0715,
            0.1104,
            0.0101,
            0.0132,
            0.0828,
            0.0406,
            0.0245,
            -0.1307,
            0.0514
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0005,
            -0.0007,
            0.0002,
            -0.001,
            -0.0009,
            0.0007,
            -0.0008,
            -0.0004,
            -0.0006,
            0.0001,
            -0.0,
            0.0,
            0.0004,
            0.0013,
            0.0001
          ],
          "after": [
            -0.0854,
            0.0091,
            -0.1247,
            -0.0732,
            0.0676,
            0.1378,
            0.0886,
            -0.161,
            0.0299,
            -0.0246,
            -0.1491,
            -0.0648,
            0.0939,
            -0.08,
            0.0403,
            0.0857
          ]
        }
      }
    },
    {
      "step": 970,
      "word": "alaylah",
      "loss": 2.06,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3061,
            -0.0677,
            0.2405,
            -0.1052,
            0.314,
            0.2886,
            -0.0716,
            0.3217,
            0.021,
            0.1701,
            -0.2861,
            0.4482,
            -0.0222,
            -0.0891,
            -0.1295,
            -0.1536
          ],
          "after": [
            0.008,
            -0.0053,
            0.1582,
            -0.0243,
            0.0308,
            -0.0909,
            -0.1173,
            -0.158,
            -0.0091,
            0.0733,
            0.1621,
            0.0145,
            -0.1028,
            -0.0183,
            -0.0371,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0435,
            0.1172,
            -0.082,
            0.0511,
            0.0315,
            -0.1792,
            0.228,
            -0.0292,
            -0.1665,
            -0.0112,
            0.0127,
            -0.0093,
            0.0775,
            -0.0966,
            -0.1529,
            0.1673
          ],
          "after": [
            0.0282,
            0.1076,
            -0.0412,
            -0.0762,
            0.0311,
            -0.0097,
            0.0898,
            0.1003,
            -0.0646,
            -0.0614,
            0.0143,
            -0.017,
            0.1072,
            0.0349,
            -0.0519,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0694,
            -0.0179,
            0.0697,
            -0.1432,
            0.0977,
            0.0132,
            -0.2353,
            -0.1059,
            0.0968,
            0.0996,
            0.2149,
            0.0709,
            -0.1166,
            -0.0236,
            -0.1395,
            -0.1906
          ],
          "after": [
            0.0453,
            -0.1465,
            -0.2948,
            0.0532,
            0.0301,
            -0.0484,
            -0.1508,
            0.0715,
            0.1104,
            0.0101,
            0.0132,
            0.0828,
            0.0406,
            0.0245,
            -0.1307,
            0.0514
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0012,
            0.0009,
            -0.0012,
            0.0,
            0.0004,
            -0.002,
            -0.0,
            0.0005,
            -0.0006,
            -0.0001,
            0.0007,
            -0.0013,
            0.0005,
            0.001,
            -0.0014
          ],
          "after": [
            -0.0854,
            0.0091,
            -0.1247,
            -0.0732,
            0.0676,
            0.1378,
            0.0886,
            -0.161,
            0.0299,
            -0.0247,
            -0.1491,
            -0.0648,
            0.0939,
            -0.08,
            0.0403,
            0.0857
          ]
        }
      }
    },
    {
      "step": 971,
      "word": "gianni",
      "loss": 2.4575,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.1192,
            0.1541,
            -0.2052,
            0.0883,
            -0.2888,
            -0.0571,
            -0.0509,
            -0.1067,
            0.041,
            -0.0308,
            -0.1208,
            -0.0627,
            0.0277,
            0.1528,
            0.0218,
            0.0873
          ],
          "after": [
            0.008,
            -0.0053,
            0.1582,
            -0.0243,
            0.0307,
            -0.0909,
            -0.1173,
            -0.1581,
            -0.0091,
            0.0732,
            0.1621,
            0.0145,
            -0.1028,
            -0.0183,
            -0.0371,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0412,
            0.0318,
            -0.1379,
            -0.0347,
            0.0696,
            -0.0879,
            0.1588,
            0.022,
            -0.1632,
            0.0808,
            -0.0061,
            -0.0081,
            0.0785,
            -0.0864,
            -0.1336,
            0.1098
          ],
          "after": [
            0.0282,
            0.1076,
            -0.0412,
            -0.0762,
            0.0311,
            -0.0097,
            0.0898,
            0.1003,
            -0.0646,
            -0.0615,
            0.0143,
            -0.017,
            0.1071,
            0.035,
            -0.0519,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0165,
            0.0283,
            0.002,
            0.1083,
            -0.1068,
            -0.1357,
            -0.0221,
            0.0057,
            -0.0838,
            0.0413,
            0.0696,
            -0.1379,
            0.0277,
            0.0299,
            -0.0587,
            0.0185
          ],
          "after": [
            0.0453,
            -0.1465,
            -0.2948,
            0.0531,
            0.0301,
            -0.0484,
            -0.1508,
            0.0715,
            0.1104,
            0.0102,
            0.0132,
            0.0829,
            0.0406,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            -0.0002,
            0.0006,
            -0.0003,
            0.002,
            0.0003,
            0.0011,
            0.0002,
            -0.0002,
            -0.0002,
            -0.0002,
            0.0004,
            0.0003,
            -0.0005,
            -0.0015,
            0.0
          ],
          "after": [
            -0.0854,
            0.0091,
            -0.1247,
            -0.0732,
            0.0676,
            0.1378,
            0.0886,
            -0.161,
            0.0299,
            -0.0247,
            -0.1491,
            -0.0648,
            0.0939,
            -0.08,
            0.0403,
            0.0857
          ]
        }
      }
    },
    {
      "step": 972,
      "word": "zidon",
      "loss": 2.2658,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.008,
            -0.0053,
            0.1582,
            -0.0243,
            0.0307,
            -0.0909,
            -0.1173,
            -0.1581,
            -0.0091,
            0.0732,
            0.1621,
            0.0145,
            -0.1028,
            -0.0183,
            -0.037,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0245,
            0.0855,
            -0.1879,
            0.0435,
            0.0159,
            -0.1535,
            0.2597,
            0.0478,
            -0.2488,
            0.0328,
            -0.0582,
            -0.0701,
            0.1762,
            -0.0459,
            -0.2284,
            0.1241
          ],
          "after": [
            0.0282,
            0.1076,
            -0.0411,
            -0.0762,
            0.0311,
            -0.0097,
            0.0897,
            0.1003,
            -0.0646,
            -0.0615,
            0.0143,
            -0.017,
            0.1071,
            0.035,
            -0.0518,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0234,
            0.0341,
            -0.033,
            0.1536,
            0.0585,
            -0.0687,
            0.2053,
            0.0027,
            0.0227,
            -0.0074,
            0.0571,
            0.0253,
            0.0192,
            -0.0353,
            0.05,
            0.0675
          ],
          "after": [
            0.0453,
            -0.1465,
            -0.2948,
            0.0531,
            0.0301,
            -0.0484,
            -0.1508,
            0.0715,
            0.1105,
            0.0102,
            0.0132,
            0.0829,
            0.0405,
            0.0246,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0007,
            0.0011,
            -0.0021,
            0.005,
            -0.0004,
            0.0023,
            0.0003,
            0.0012,
            0.0012,
            -0.0002,
            0.0001,
            0.0003,
            -0.0006,
            -0.0028,
            -0.0004
          ],
          "after": [
            -0.0855,
            0.0091,
            -0.1247,
            -0.0732,
            0.0675,
            0.1378,
            0.0886,
            -0.161,
            0.0299,
            -0.0247,
            -0.1492,
            -0.0648,
            0.0939,
            -0.08,
            0.0403,
            0.0857
          ]
        }
      }
    },
    {
      "step": 973,
      "word": "nouri",
      "loss": 2.5517,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.008,
            -0.0053,
            0.1582,
            -0.0243,
            0.0307,
            -0.0909,
            -0.1173,
            -0.1581,
            -0.0091,
            0.0732,
            0.1621,
            0.0144,
            -0.1028,
            -0.0183,
            -0.037,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.032,
            0.0438,
            -0.0972,
            0.0101,
            0.0497,
            -0.1112,
            0.2234,
            0.0387,
            -0.1251,
            0.0788,
            -0.0049,
            -0.0073,
            0.0868,
            -0.0878,
            -0.1583,
            0.0894
          ],
          "after": [
            0.0282,
            0.1076,
            -0.0411,
            -0.0762,
            0.0311,
            -0.0096,
            0.0897,
            0.1003,
            -0.0645,
            -0.0615,
            0.0144,
            -0.0169,
            0.1071,
            0.035,
            -0.0518,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0362,
            0.1387,
            -0.0801,
            0.0471,
            -0.3889,
            0.0202,
            -0.0221,
            -0.0281,
            0.098,
            -0.2528,
            -0.252,
            -0.0877,
            -0.0567,
            0.1546,
            0.2439,
            0.142
          ],
          "after": [
            0.0453,
            -0.1466,
            -0.2948,
            0.0531,
            0.0302,
            -0.0483,
            -0.1509,
            0.0715,
            0.1105,
            0.0102,
            0.0132,
            0.0829,
            0.0405,
            0.0246,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0001,
            0.0011,
            -0.0002,
            0.0002,
            0.0012,
            0.0003,
            -0.0002,
            -0.0008,
            0.0001,
            -0.0002,
            -0.0004,
            0.0004,
            -0.0008,
            -0.0002,
            -0.0002,
            0.0007
          ],
          "after": [
            -0.0855,
            0.0091,
            -0.1247,
            -0.0732,
            0.0675,
            0.1378,
            0.0886,
            -0.161,
            0.0299,
            -0.0247,
            -0.1492,
            -0.0648,
            0.0939,
            -0.08,
            0.0403,
            0.0857
          ]
        }
      }
    },
    {
      "step": 974,
      "word": "aven",
      "loss": 2.1104,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1204,
            -0.0886,
            -0.1842,
            0.2903,
            0.2049,
            0.0054,
            -0.0416,
            -0.0709,
            -0.2727,
            0.0821,
            0.1597,
            -0.1096,
            0.225,
            -0.1436,
            -0.0691,
            0.4008
          ],
          "after": [
            0.008,
            -0.0053,
            0.1583,
            -0.0244,
            0.0307,
            -0.091,
            -0.1173,
            -0.1581,
            -0.0091,
            0.0732,
            0.1621,
            0.0144,
            -0.1028,
            -0.0183,
            -0.037,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0651,
            -0.301,
            0.2984,
            -0.0171,
            0.2046,
            0.4964,
            -0.6636,
            -0.0673,
            0.5363,
            0.154,
            0.3091,
            0.1693,
            -0.534,
            -0.0456,
            0.1892,
            -0.2556
          ],
          "after": [
            0.0282,
            0.1076,
            -0.0411,
            -0.0762,
            0.0311,
            -0.0097,
            0.0897,
            0.1003,
            -0.0645,
            -0.0615,
            0.0143,
            -0.017,
            0.1071,
            0.0351,
            -0.0518,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            0.1228,
            -0.0605,
            0.1429,
            -0.1734,
            0.0221,
            0.0104,
            -0.4144,
            -0.1399,
            0.1911,
            -0.004,
            0.2987,
            0.0259,
            -0.1991,
            0.0972,
            -0.0511,
            -0.2714
          ],
          "after": [
            0.0452,
            -0.1466,
            -0.2948,
            0.0531,
            0.0302,
            -0.0483,
            -0.1508,
            0.0716,
            0.1104,
            0.0103,
            0.0132,
            0.0829,
            0.0405,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            -0.0003,
            -0.0001,
            -0.0005,
            0.0012,
            0.0022,
            -0.0028,
            0.0017,
            0.0029,
            0.0007,
            -0.0001,
            0.0013,
            -0.0017,
            0.0004,
            -0.0016,
            0.0021
          ],
          "after": [
            -0.0855,
            0.0091,
            -0.1247,
            -0.0732,
            0.0675,
            0.1378,
            0.0886,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0648,
            0.0939,
            -0.08,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 975,
      "word": "arieanna",
      "loss": 2.0352,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0682,
            0.0812,
            -0.0236,
            -0.0571,
            -0.0064,
            0.1023,
            0.0935,
            -0.2585,
            0.1884,
            -0.1616,
            -0.0215,
            0.1511,
            -0.3002,
            -0.2792,
            0.0128,
            0.1353
          ],
          "after": [
            0.0081,
            -0.0053,
            0.1583,
            -0.0244,
            0.0307,
            -0.091,
            -0.1173,
            -0.1581,
            -0.0091,
            0.0732,
            0.1621,
            0.0144,
            -0.1028,
            -0.0183,
            -0.037,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.103,
            0.0385,
            0.0073,
            0.0441,
            -0.1791,
            -0.1196,
            -0.0575,
            0.0338,
            0.0209,
            -0.1742,
            -0.0238,
            0.1306,
            -0.0193,
            -0.1082,
            0.3105,
            -0.037
          ],
          "after": [
            0.0282,
            0.1076,
            -0.0411,
            -0.0762,
            0.0311,
            -0.0097,
            0.0897,
            0.1003,
            -0.0645,
            -0.0615,
            0.0143,
            -0.017,
            0.1071,
            0.0351,
            -0.0518,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            0.0687,
            -0.0384,
            0.0618,
            -0.1461,
            0.0431,
            0.0005,
            -0.2535,
            -0.0869,
            0.1202,
            0.0919,
            0.1606,
            0.0204,
            -0.1506,
            -0.0224,
            -0.101,
            -0.2208
          ],
          "after": [
            0.0452,
            -0.1466,
            -0.2948,
            0.0531,
            0.0302,
            -0.0483,
            -0.1508,
            0.0716,
            0.1104,
            0.0103,
            0.0132,
            0.0829,
            0.0406,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0019,
            0.0004,
            -0.0,
            0.0007,
            -0.0024,
            -0.0004,
            -0.0033,
            -0.0018,
            -0.0018,
            -0.0005,
            0.001,
            -0.0028,
            -0.0011,
            0.0012,
            0.0022,
            -0.0029
          ],
          "after": [
            -0.0855,
            0.0091,
            -0.1247,
            -0.0732,
            0.0674,
            0.1378,
            0.0886,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0648,
            0.0939,
            -0.08,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 976,
      "word": "jayesh",
      "loss": 2.0761,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0836,
            -0.2019,
            0.0866,
            -0.1116,
            0.0704,
            0.1251,
            -0.1201,
            0.0659,
            -0.0711,
            0.1567,
            0.0699,
            0.0898,
            -0.0156,
            0.0385,
            -0.0417,
            -0.0483
          ],
          "after": [
            0.0081,
            -0.0053,
            0.1583,
            -0.0244,
            0.0307,
            -0.091,
            -0.1173,
            -0.1581,
            -0.0091,
            0.0732,
            0.1621,
            0.0144,
            -0.1028,
            -0.0183,
            -0.037,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1532,
            0.2113,
            -0.0779,
            -0.1304,
            0.0316,
            -0.1313,
            -0.0365,
            0.0434,
            0.017,
            -0.4382,
            -0.0649,
            0.0776,
            -0.1063,
            -0.0084,
            0.2712,
            -0.0541
          ],
          "after": [
            0.0283,
            0.1076,
            -0.0411,
            -0.0762,
            0.0311,
            -0.0097,
            0.0897,
            0.1003,
            -0.0645,
            -0.0615,
            0.0143,
            -0.017,
            0.1071,
            0.0351,
            -0.0518,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            0.0059,
            0.0431,
            -0.0194,
            0.1114,
            0.0211,
            -0.1111,
            0.1608,
            0.0492,
            -0.2022,
            -0.0352,
            0.0157,
            0.0024,
            0.2875,
            -0.1009,
            -0.126,
            0.1306
          ],
          "after": [
            0.0452,
            -0.1466,
            -0.2948,
            0.0531,
            0.0303,
            -0.0483,
            -0.1508,
            0.0716,
            0.1104,
            0.0103,
            0.0132,
            0.0829,
            0.0406,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0005,
            -0.0,
            -0.0003,
            0.0003,
            0.0,
            0.0007,
            0.0004,
            0.0009,
            -0.0002,
            -0.0002,
            -0.0001,
            0.0003,
            -0.0,
            -0.0005,
            0.0,
            0.0008
          ],
          "after": [
            -0.0855,
            0.009,
            -0.1247,
            -0.0732,
            0.0674,
            0.1378,
            0.0886,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0648,
            0.0939,
            -0.08,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 977,
      "word": "syrai",
      "loss": 2.377,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0987,
            -0.3583,
            0.1994,
            -0.1129,
            0.2092,
            0.1468,
            -0.2435,
            0.0942,
            0.0146,
            -0.0888,
            0.1536,
            -0.1055,
            -0.2184,
            0.0194,
            -0.0294,
            -0.1426
          ],
          "after": [
            0.0081,
            -0.0053,
            0.1583,
            -0.0244,
            0.0307,
            -0.091,
            -0.1173,
            -0.1581,
            -0.0091,
            0.0732,
            0.162,
            0.0144,
            -0.1028,
            -0.0183,
            -0.037,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0111,
            0.0004,
            -0.1542,
            -0.0085,
            0.0042,
            -0.1117,
            0.1989,
            0.0041,
            -0.181,
            0.059,
            -0.023,
            -0.0296,
            0.0829,
            -0.0697,
            -0.1384,
            0.1031
          ],
          "after": [
            0.0283,
            0.1076,
            -0.0411,
            -0.0762,
            0.0311,
            -0.0096,
            0.0897,
            0.1003,
            -0.0645,
            -0.0615,
            0.0143,
            -0.017,
            0.1071,
            0.0352,
            -0.0518,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0017,
            0.0862,
            -0.0232,
            0.12,
            -0.1705,
            0.0509,
            -0.024,
            0.0397,
            -0.0513,
            0.011,
            -0.1285,
            0.065,
            0.0669,
            0.0125,
            0.1286,
            0.0035
          ],
          "after": [
            0.0451,
            -0.1466,
            -0.2948,
            0.053,
            0.0303,
            -0.0482,
            -0.1508,
            0.0716,
            0.1104,
            0.0103,
            0.0132,
            0.0829,
            0.0406,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            -0.0004,
            0.0,
            -0.0001,
            0.0006,
            0.0003,
            0.0008,
            0.0001,
            -0.0001,
            0.0003,
            -0.0006,
            -0.0008,
            0.001,
            0.0003,
            -0.0009,
            0.0005
          ],
          "after": [
            -0.0856,
            0.009,
            -0.1247,
            -0.0732,
            0.0674,
            0.1378,
            0.0886,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0648,
            0.0939,
            -0.08,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 978,
      "word": "alesa",
      "loss": 1.9225,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.2575,
            0.0378,
            -0.319,
            -0.2386,
            -0.1159,
            -0.1278,
            -0.2553,
            0.1298,
            -0.2171,
            0.1603,
            -0.0715,
            -0.0867,
            -0.0034,
            0.238,
            0.1447,
            -0.7462
          ],
          "after": [
            0.0081,
            -0.0053,
            0.1583,
            -0.0244,
            0.0306,
            -0.0911,
            -0.1172,
            -0.1581,
            -0.0091,
            0.0731,
            0.162,
            0.0144,
            -0.1028,
            -0.0183,
            -0.037,
            -0.1864
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0191,
            -0.3678,
            0.2513,
            -0.001,
            0.2323,
            0.34,
            -0.4585,
            0.0879,
            0.3056,
            0.1918,
            0.0496,
            0.2797,
            -0.3708,
            0.0277,
            0.1236,
            -0.325
          ],
          "after": [
            0.0283,
            0.1076,
            -0.0411,
            -0.0762,
            0.0311,
            -0.0096,
            0.0897,
            0.1002,
            -0.0645,
            -0.0615,
            0.0143,
            -0.017,
            0.1072,
            0.0352,
            -0.0518,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            0.0982,
            -0.0492,
            0.1132,
            -0.1862,
            0.0769,
            0.0056,
            -0.3685,
            -0.1251,
            0.1739,
            0.0603,
            0.262,
            0.0461,
            -0.1681,
            0.0183,
            -0.115,
            -0.2653
          ],
          "after": [
            0.0451,
            -0.1467,
            -0.2948,
            0.0531,
            0.0303,
            -0.0482,
            -0.1508,
            0.0716,
            0.1104,
            0.0104,
            0.0131,
            0.0829,
            0.0406,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0006,
            -0.0007,
            0.0012,
            0.0009,
            -0.0009,
            0.0016,
            -0.0019,
            -0.0,
            -0.0,
            0.0014,
            0.0002,
            -0.0013,
            0.0005,
            0.0013,
            -0.0006,
            -0.0007
          ],
          "after": [
            -0.0856,
            0.009,
            -0.1247,
            -0.0732,
            0.0674,
            0.1378,
            0.0886,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0648,
            0.094,
            -0.0801,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 979,
      "word": "caidyn",
      "loss": 2.1033,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0721,
            -0.2918,
            0.0762,
            -0.1018,
            0.1527,
            0.1615,
            -0.1835,
            0.1027,
            -0.0093,
            -0.0174,
            0.1942,
            -0.1723,
            -0.166,
            -0.0419,
            -0.0244,
            -0.094
          ],
          "after": [
            0.0081,
            -0.0052,
            0.1583,
            -0.0243,
            0.0306,
            -0.0911,
            -0.1172,
            -0.1582,
            -0.0091,
            0.0731,
            0.162,
            0.0144,
            -0.1028,
            -0.0183,
            -0.037,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0162,
            0.0651,
            -0.1565,
            0.041,
            0.0409,
            -0.1184,
            0.2075,
            0.0274,
            -0.2178,
            0.0448,
            -0.0177,
            -0.0581,
            0.1181,
            -0.0842,
            -0.1585,
            0.1335
          ],
          "after": [
            0.0283,
            0.1076,
            -0.0411,
            -0.0762,
            0.031,
            -0.0097,
            0.0897,
            0.1002,
            -0.0645,
            -0.0615,
            0.0143,
            -0.0171,
            0.1072,
            0.0352,
            -0.0518,
            0.2221
          ]
        },
        "position_0": {
          "grad": [
            -0.0355,
            0.0285,
            -0.0325,
            0.0835,
            -0.0226,
            -0.0313,
            0.1205,
            0.0571,
            -0.0802,
            0.0498,
            0.0367,
            0.0249,
            0.049,
            -0.0307,
            -0.0736,
            0.0598
          ],
          "after": [
            0.0451,
            -0.1467,
            -0.2948,
            0.053,
            0.0303,
            -0.0482,
            -0.1508,
            0.0717,
            0.1104,
            0.0104,
            0.0131,
            0.0829,
            0.0406,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0011,
            0.0014,
            0.0001,
            0.0015,
            0.0,
            0.0004,
            0.0006,
            0.0001,
            0.001,
            -0.0001,
            0.0014,
            -0.0001,
            -0.0013,
            -0.001,
            0.0
          ],
          "after": [
            -0.0856,
            0.0091,
            -0.1247,
            -0.0732,
            0.0674,
            0.1378,
            0.0886,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0647,
            0.094,
            -0.0801,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 980,
      "word": "zelaya",
      "loss": 1.9957,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.085,
            -0.2453,
            0.1719,
            -0.178,
            0.129,
            0.1411,
            -0.1226,
            0.077,
            -0.1011,
            0.181,
            0.0972,
            0.1174,
            -0.0066,
            0.0455,
            -0.0824,
            -0.1189
          ],
          "after": [
            0.0081,
            -0.0052,
            0.1583,
            -0.0243,
            0.0306,
            -0.0911,
            -0.1172,
            -0.1582,
            -0.009,
            0.0731,
            0.162,
            0.0144,
            -0.1027,
            -0.0183,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.1658,
            -0.2057,
            0.4265,
            -0.0269,
            0.2602,
            0.1493,
            -0.4081,
            -0.0742,
            0.4549,
            0.1873,
            0.1993,
            0.1844,
            -0.3673,
            0.0265,
            0.0242,
            -0.1745
          ],
          "after": [
            0.0283,
            0.1076,
            -0.0411,
            -0.0762,
            0.031,
            -0.0097,
            0.0897,
            0.1002,
            -0.0646,
            -0.0615,
            0.0142,
            -0.0171,
            0.1072,
            0.0352,
            -0.0518,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0225,
            0.039,
            -0.0359,
            0.1,
            0.092,
            -0.0639,
            0.1584,
            -0.0119,
            0.0229,
            0.0483,
            0.0545,
            0.0339,
            0.0111,
            -0.0662,
            -0.0203,
            0.0313
          ],
          "after": [
            0.045,
            -0.1467,
            -0.2948,
            0.053,
            0.0303,
            -0.0482,
            -0.1508,
            0.0717,
            0.1104,
            0.0104,
            0.0131,
            0.0829,
            0.0406,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0006,
            0.0008,
            -0.0014,
            -0.0006,
            -0.0013,
            -0.0022,
            -0.0005,
            0.0018,
            -0.0004,
            0.0006,
            -0.0007,
            -0.0007,
            0.0014,
            0.0002,
            -0.0008
          ],
          "after": [
            -0.0856,
            0.0091,
            -0.1248,
            -0.0732,
            0.0674,
            0.1378,
            0.0887,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0647,
            0.094,
            -0.0801,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 981,
      "word": "azreal",
      "loss": 2.4774,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.3545,
            0.195,
            0.031,
            0.1988,
            0.3123,
            -0.1128,
            0.2858,
            0.1739,
            0.1301,
            -0.0492,
            -0.1247,
            0.6244,
            -0.1571,
            -0.2376,
            0.0334,
            -0.0297
          ],
          "after": [
            0.0081,
            -0.0052,
            0.1583,
            -0.0243,
            0.0306,
            -0.0911,
            -0.1171,
            -0.1582,
            -0.009,
            0.0731,
            0.1619,
            0.0144,
            -0.1027,
            -0.0182,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1014,
            -0.2153,
            0.0837,
            -0.1263,
            0.2242,
            -0.0116,
            -0.3192,
            0.0424,
            0.0555,
            -0.148,
            0.0488,
            0.0147,
            -0.1314,
            -0.014,
            0.1929,
            -0.0859
          ],
          "after": [
            0.0283,
            0.1076,
            -0.0411,
            -0.0762,
            0.031,
            -0.0097,
            0.0897,
            0.1002,
            -0.0646,
            -0.0615,
            0.0142,
            -0.0171,
            0.1072,
            0.0353,
            -0.0518,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0929,
            -0.0267,
            0.0834,
            -0.1467,
            0.0643,
            0.0123,
            -0.2624,
            -0.1005,
            0.1156,
            0.0798,
            0.212,
            0.0529,
            -0.126,
            0.003,
            -0.1166,
            -0.21
          ],
          "after": [
            0.045,
            -0.1467,
            -0.2948,
            0.053,
            0.0303,
            -0.0482,
            -0.1507,
            0.0717,
            0.1104,
            0.0103,
            0.0131,
            0.0829,
            0.0406,
            0.0245,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            0.0002,
            0.0,
            -0.0006,
            0.0018,
            -0.0016,
            -0.0002,
            -0.0005,
            -0.0008,
            -0.0013,
            -0.0008,
            0.0002,
            0.0006,
            0.0003,
            -0.0009,
            -0.0014
          ],
          "after": [
            -0.0856,
            0.0091,
            -0.1248,
            -0.0732,
            0.0674,
            0.1378,
            0.0887,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0647,
            0.094,
            -0.0801,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 982,
      "word": "heavenleigh",
      "loss": 2.9164,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1354,
            0.0053,
            0.0418,
            0.0148,
            0.211,
            0.0365,
            0.0809,
            -0.0096,
            -0.1093,
            0.0433,
            0.0426,
            0.045,
            0.0649,
            -0.15,
            -0.089,
            0.1493
          ],
          "after": [
            0.0081,
            -0.0051,
            0.1583,
            -0.0243,
            0.0305,
            -0.0912,
            -0.1171,
            -0.1582,
            -0.009,
            0.0731,
            0.1619,
            0.0144,
            -0.1027,
            -0.0182,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.3733,
            -0.5059,
            0.3275,
            -0.2563,
            0.0682,
            0.4622,
            -1.0359,
            -0.0236,
            0.6289,
            0.2162,
            0.1561,
            -0.0831,
            -0.2452,
            0.3006,
            0.2281,
            -0.6196
          ],
          "after": [
            0.0283,
            0.1077,
            -0.0411,
            -0.0762,
            0.031,
            -0.0097,
            0.0898,
            0.1002,
            -0.0646,
            -0.0615,
            0.0142,
            -0.0171,
            0.1073,
            0.0353,
            -0.0518,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0053,
            -0.047,
            0.0646,
            -0.0024,
            0.0043,
            0.0663,
            -0.0583,
            0.0387,
            0.0874,
            0.1017,
            -0.047,
            -0.082,
            0.0843,
            0.0579,
            -0.0179,
            0.0104
          ],
          "after": [
            0.045,
            -0.1467,
            -0.2949,
            0.053,
            0.0303,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.013,
            0.0829,
            0.0406,
            0.0245,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0002,
            -0.0002,
            0.0003,
            -0.0004,
            0.0009,
            -0.0001,
            -0.0007,
            0.0006,
            0.0002,
            0.0004,
            0.0002,
            0.0005,
            -0.0001,
            -0.0001,
            -0.0007,
            -0.0008
          ],
          "after": [
            -0.0856,
            0.0091,
            -0.1248,
            -0.0732,
            0.0674,
            0.1378,
            0.0887,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0647,
            0.094,
            -0.0801,
            0.0404,
            0.0857
          ]
        }
      }
    },
    {
      "step": 983,
      "word": "jazzleen",
      "loss": 2.5046,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0514,
            0.0646,
            0.0654,
            0.1679,
            0.1662,
            -0.0773,
            0.2759,
            0.0012,
            -0.0345,
            0.097,
            0.0954,
            0.13,
            0.0379,
            -0.1352,
            -0.0694,
            0.0041
          ],
          "after": [
            0.0081,
            -0.0051,
            0.1583,
            -0.0243,
            0.0305,
            -0.0912,
            -0.1171,
            -0.1583,
            -0.009,
            0.0731,
            0.1619,
            0.0144,
            -0.1027,
            -0.0182,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2867,
            -0.2039,
            -0.3978,
            -0.211,
            -0.4926,
            0.399,
            -0.2439,
            0.0503,
            0.1589,
            0.0659,
            -0.2071,
            -0.2423,
            0.1537,
            0.3574,
            0.5779,
            -0.3298
          ],
          "after": [
            0.0282,
            0.1077,
            -0.0411,
            -0.0761,
            0.031,
            -0.0097,
            0.0898,
            0.1002,
            -0.0646,
            -0.0615,
            0.0142,
            -0.0171,
            0.1073,
            0.0352,
            -0.0519,
            0.2222
          ]
        },
        "position_0": {
          "grad": [
            0.0034,
            0.0503,
            -0.0305,
            0.1116,
            0.0334,
            -0.0741,
            0.1964,
            0.0535,
            -0.1961,
            -0.0076,
            0.0058,
            0.0442,
            0.2527,
            -0.0998,
            -0.1092,
            0.1283
          ],
          "after": [
            0.045,
            -0.1467,
            -0.2949,
            0.053,
            0.0303,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.013,
            0.0829,
            0.0406,
            0.0246,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0003,
            0.0011,
            0.0001,
            0.0011,
            0.0008,
            -0.0009,
            0.0019,
            -0.0001,
            -0.0009,
            -0.0003,
            0.0008,
            0.0003,
            0.0022,
            -0.0006,
            -0.0034,
            0.0018
          ],
          "after": [
            -0.0856,
            0.0091,
            -0.1248,
            -0.0732,
            0.0673,
            0.1378,
            0.0887,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0647,
            0.094,
            -0.0801,
            0.0405,
            0.0857
          ]
        }
      }
    },
    {
      "step": 984,
      "word": "rielle",
      "loss": 2.0931,
      "learning_rate": 0.0001,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0081,
            -0.0051,
            0.1582,
            -0.0243,
            0.0305,
            -0.0912,
            -0.1171,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0143,
            -0.1027,
            -0.0182,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.076,
            -0.2265,
            0.0627,
            0.0646,
            -0.4355,
            0.3359,
            -0.4681,
            0.0543,
            0.2843,
            0.0659,
            0.0423,
            0.184,
            -0.1557,
            0.0718,
            0.5862,
            -0.2785
          ],
          "after": [
            0.0282,
            0.1077,
            -0.0411,
            -0.0761,
            0.031,
            -0.0098,
            0.0899,
            0.1002,
            -0.0647,
            -0.0616,
            0.0142,
            -0.0172,
            0.1073,
            0.0352,
            -0.0519,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0066,
            -0.0405,
            0.0047,
            -0.0612,
            -0.0062,
            0.1432,
            0.052,
            -0.0659,
            0.1381,
            -0.0512,
            -0.0917,
            0.0365,
            -0.171,
            -0.1007,
            0.0897,
            0.0737
          ],
          "after": [
            0.0449,
            -0.1467,
            -0.2949,
            0.053,
            0.0303,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.013,
            0.0828,
            0.0406,
            0.0246,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0007,
            -0.0002,
            0.0002,
            0.0025,
            -0.0025,
            0.0004,
            -0.003,
            0.0013,
            0.0,
            0.0006,
            0.0009,
            0.0042,
            -0.0031,
            -0.0015,
            0.0029,
            0.0006
          ],
          "after": [
            -0.0856,
            0.0091,
            -0.1248,
            -0.0732,
            0.0673,
            0.1378,
            0.0887,
            -0.161,
            0.0298,
            -0.0247,
            -0.1492,
            -0.0648,
            0.094,
            -0.0801,
            0.0405,
            0.0857
          ]
        }
      }
    },
    {
      "step": 985,
      "word": "lindsay",
      "loss": 2.768,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0915,
            -0.2337,
            0.0939,
            0.078,
            0.14,
            0.2346,
            0.0001,
            0.1541,
            0.0383,
            -0.0164,
            -0.1662,
            0.3227,
            -0.1261,
            -0.0356,
            0.0238,
            0.2209
          ],
          "after": [
            0.0081,
            -0.0051,
            0.1582,
            -0.0243,
            0.0304,
            -0.0912,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0143,
            -0.1027,
            -0.0182,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0571,
            0.1342,
            -0.1444,
            0.0492,
            0.0479,
            -0.1672,
            0.305,
            0.0245,
            -0.2111,
            0.0117,
            -0.0175,
            -0.0083,
            0.0977,
            -0.1089,
            -0.1827,
            0.1706
          ],
          "after": [
            0.0282,
            0.1078,
            -0.0411,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1002,
            -0.0647,
            -0.0616,
            0.0142,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0836,
            0.0871,
            -0.1065,
            -0.0327,
            0.0222,
            0.0767,
            0.0105,
            0.1106,
            -0.0362,
            -0.008,
            -0.1613,
            0.204,
            -0.0805,
            -0.0006,
            0.0889,
            -0.0761
          ],
          "after": [
            0.0449,
            -0.1467,
            -0.2949,
            0.053,
            0.0303,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.013,
            0.0828,
            0.0406,
            0.0246,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0014,
            0.0023,
            -0.0007,
            0.0027,
            0.0014,
            0.0011,
            0.0043,
            0.0017,
            0.0002,
            0.0047,
            -0.0009,
            -0.0018,
            0.0044,
            0.0004,
            -0.0032,
            0.0006
          ],
          "after": [
            -0.0856,
            0.009,
            -0.1248,
            -0.0732,
            0.0673,
            0.1378,
            0.0887,
            -0.161,
            0.0298,
            -0.0248,
            -0.1492,
            -0.0648,
            0.094,
            -0.0801,
            0.0405,
            0.0857
          ]
        }
      }
    },
    {
      "step": 986,
      "word": "willson",
      "loss": 2.8106,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0081,
            -0.0051,
            0.1582,
            -0.0243,
            0.0304,
            -0.0912,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0143,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0737,
            0.1888,
            -0.1745,
            0.0602,
            0.0111,
            -0.2084,
            0.3942,
            0.0112,
            -0.2684,
            -0.0105,
            -0.0098,
            -0.0241,
            0.1162,
            -0.1274,
            -0.1939,
            0.2456
          ],
          "after": [
            0.0282,
            0.1078,
            -0.0411,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0142,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0827,
            0.115,
            -0.0776,
            0.1321,
            -0.0433,
            -0.0582,
            0.2145,
            0.0142,
            -0.0943,
            -0.0356,
            -0.0003,
            -0.1088,
            0.1221,
            0.012,
            -0.0496,
            0.0698
          ],
          "after": [
            0.0449,
            -0.1467,
            -0.2949,
            0.053,
            0.0303,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.013,
            0.0828,
            0.0405,
            0.0246,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0004,
            0.0053,
            -0.0015,
            0.0027,
            -0.0012,
            -0.001,
            0.005,
            -0.003,
            0.0013,
            0.0022,
            0.0019,
            -0.0003,
            0.0031,
            -0.0012,
            -0.0015,
            0.005
          ],
          "after": [
            -0.0856,
            0.009,
            -0.1248,
            -0.0733,
            0.0673,
            0.1378,
            0.0887,
            -0.161,
            0.0297,
            -0.0248,
            -0.1492,
            -0.0648,
            0.0939,
            -0.0801,
            0.0405,
            0.0857
          ]
        }
      }
    },
    {
      "step": 987,
      "word": "shannon",
      "loss": 1.974,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0954,
            0.1543,
            -0.1822,
            0.0714,
            -0.2402,
            -0.0318,
            -0.0239,
            -0.0925,
            0.0244,
            -0.0252,
            -0.1047,
            -0.0442,
            0.0444,
            0.1334,
            0.0114,
            0.0826
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1582,
            -0.0243,
            0.0304,
            -0.0912,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0143,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0376,
            0.0194,
            -0.16,
            -0.0153,
            0.0424,
            -0.152,
            0.231,
            0.0451,
            -0.1645,
            0.0154,
            -0.05,
            0.0098,
            0.0768,
            -0.0889,
            -0.1152,
            0.1273
          ],
          "after": [
            0.0282,
            0.1078,
            -0.0411,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0142,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0082,
            0.0755,
            -0.0228,
            0.1349,
            -0.1134,
            0.0417,
            0.0282,
            0.043,
            -0.0551,
            -0.006,
            -0.1091,
            0.0891,
            0.0648,
            0.0192,
            0.1168,
            0.0391
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2949,
            0.053,
            0.0303,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.013,
            0.0828,
            0.0405,
            0.0246,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0,
            0.0001,
            -0.0004,
            0.0003,
            0.0002,
            -0.0003,
            0.0008,
            -0.0003,
            -0.0,
            0.0006,
            0.0002,
            0.0005,
            -0.0001,
            -0.0002,
            -0.0,
            0.0004
          ],
          "after": [
            -0.0856,
            0.009,
            -0.1248,
            -0.0733,
            0.0673,
            0.1378,
            0.0887,
            -0.161,
            0.0297,
            -0.0248,
            -0.1492,
            -0.0648,
            0.0939,
            -0.0801,
            0.0405,
            0.0856
          ]
        }
      }
    },
    {
      "step": 988,
      "word": "loany",
      "loss": 2.4647,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.152,
            0.1634,
            -0.2623,
            0.1036,
            -0.347,
            -0.059,
            -0.0867,
            -0.135,
            0.0581,
            -0.0413,
            -0.1216,
            -0.0881,
            0.0328,
            0.1978,
            0.0362,
            0.1083
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1582,
            -0.0243,
            0.0304,
            -0.0912,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0143,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.039,
            0.0659,
            -0.1243,
            0.0374,
            -0.0121,
            -0.1338,
            0.2275,
            0.0093,
            -0.1887,
            0.0404,
            0.0053,
            -0.0304,
            0.1059,
            -0.0982,
            -0.1237,
            0.1376
          ],
          "after": [
            0.0282,
            0.1078,
            -0.0411,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0937,
            0.0957,
            -0.1158,
            -0.1213,
            0.029,
            0.0692,
            -0.0887,
            0.1179,
            -0.0135,
            -0.0363,
            -0.2018,
            0.2078,
            -0.1123,
            0.0373,
            0.1109,
            -0.1393
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.013,
            0.0828,
            0.0405,
            0.0246,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0017,
            0.0011,
            -0.0009,
            0.0036,
            0.0008,
            0.0024,
            0.0003,
            -0.0012,
            -0.0014,
            -0.0004,
            -0.0016,
            0.0018,
            -0.0023,
            -0.002,
            0.0025,
            0.0039
          ],
          "after": [
            -0.0856,
            0.009,
            -0.1248,
            -0.0733,
            0.0673,
            0.1378,
            0.0887,
            -0.161,
            0.0297,
            -0.0248,
            -0.1492,
            -0.0648,
            0.0939,
            -0.0801,
            0.0405,
            0.0856
          ]
        }
      }
    },
    {
      "step": 989,
      "word": "ryell",
      "loss": 2.5305,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1582,
            -0.0243,
            0.0304,
            -0.0912,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0142,
            -0.1026,
            -0.0182,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.1084,
            0.5997,
            -0.0916,
            0.2418,
            -0.0556,
            0.0275,
            0.1869,
            -0.0679,
            -0.0069,
            -0.3755,
            0.0925,
            0.3372,
            -0.2914,
            -0.1324,
            0.0786,
            0.1734
          ],
          "after": [
            0.0282,
            0.1078,
            -0.0411,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0013,
            -0.062,
            0.0132,
            -0.044,
            -0.0385,
            0.1727,
            0.0687,
            -0.0597,
            0.1658,
            -0.0991,
            -0.1139,
            0.0256,
            -0.1927,
            -0.101,
            0.1568,
            0.1009
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.013,
            0.0827,
            0.0406,
            0.0246,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0004,
            0.0011,
            -0.0005,
            -0.0002,
            -0.0019,
            0.0018,
            -0.0031,
            0.0002,
            0.0014,
            0.0004,
            0.0008,
            0.0014,
            -0.0028,
            0.0013,
            0.0007,
            -0.0019
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1378,
            0.0887,
            -0.161,
            0.0297,
            -0.0248,
            -0.1492,
            -0.0648,
            0.0939,
            -0.08,
            0.0405,
            0.0856
          ]
        }
      }
    },
    {
      "step": 990,
      "word": "brighton",
      "loss": 2.6706,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0912,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0142,
            -0.1026,
            -0.0182,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0982,
            0.1718,
            -0.1411,
            0.028,
            0.0373,
            -0.1892,
            0.2807,
            0.0279,
            -0.2157,
            0.0251,
            -0.0316,
            -0.0012,
            0.146,
            -0.0795,
            -0.1933,
            0.1716
          ],
          "after": [
            0.0282,
            0.1078,
            -0.0411,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0356,
            0.0013,
            0.0531,
            0.0337,
            -0.0322,
            0.0316,
            0.1271,
            0.0203,
            -0.097,
            -0.0494,
            -0.0231,
            -0.0018,
            0.1097,
            -0.0674,
            0.0282,
            0.2405
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0482,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.0131,
            0.0827,
            0.0406,
            0.0246,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.001,
            0.0017,
            -0.0013,
            -0.0006,
            0.0028,
            0.0022,
            0.0039,
            0.0003,
            -0.0044,
            0.0043,
            0.0009,
            -0.0001,
            0.0029,
            0.0014,
            -0.0043,
            0.0035
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1378,
            0.0887,
            -0.161,
            0.0297,
            -0.0249,
            -0.1492,
            -0.0648,
            0.0939,
            -0.08,
            0.0405,
            0.0856
          ]
        }
      }
    },
    {
      "step": 991,
      "word": "sharod",
      "loss": 2.1933,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0785,
            -0.0646,
            -0.0566,
            0.0011,
            0.068,
            0.1455,
            0.1021,
            -0.139,
            0.129,
            -0.0157,
            0.0628,
            0.1166,
            -0.1578,
            -0.1274,
            -0.0144,
            0.1618
          ],
          "after": [
            0.0081,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0142,
            -0.1026,
            -0.0182,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0498,
            0.1281,
            -0.197,
            0.016,
            0.0116,
            -0.2144,
            0.3225,
            0.0308,
            -0.2285,
            -0.0277,
            -0.0689,
            -0.0108,
            0.1223,
            -0.1071,
            -0.1659,
            0.1767
          ],
          "after": [
            0.0282,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0062,
            0.0937,
            -0.0223,
            0.1305,
            -0.1016,
            0.0508,
            0.0249,
            0.0323,
            -0.0782,
            0.0154,
            -0.0981,
            0.0892,
            0.0969,
            0.0138,
            0.0981,
            0.0474
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1507,
            0.0717,
            0.1103,
            0.0103,
            0.0131,
            0.0827,
            0.0406,
            0.0247,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0005,
            -0.0019,
            0.0001,
            -0.0013,
            -0.0004,
            -0.0001,
            -0.0007,
            0.0001,
            0.0,
            0.0012,
            0.0012,
            -0.0005,
            -0.0002,
            0.0003,
            -0.0006,
            -0.0014
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1378,
            0.0886,
            -0.161,
            0.0298,
            -0.0249,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0405,
            0.0856
          ]
        }
      }
    },
    {
      "step": 992,
      "word": "kayleah",
      "loss": 1.9997,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1096,
            -0.2345,
            0.2941,
            0.0512,
            0.2089,
            0.2895,
            -0.0464,
            0.2284,
            0.0892,
            0.1931,
            -0.1341,
            0.156,
            0.1129,
            -0.0083,
            -0.1662,
            0.1495
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0867,
            -0.3233,
            0.0738,
            -0.0712,
            0.0324,
            0.1274,
            -0.3228,
            0.0818,
            0.0425,
            0.0624,
            -0.0798,
            -0.0326,
            -0.0467,
            0.1623,
            0.0778,
            -0.1835
          ],
          "after": [
            0.0282,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0654,
            0.0475,
            -0.1078,
            -0.0101,
            0.0479,
            -0.0539,
            0.1385,
            -0.0142,
            -0.0305,
            0.0378,
            -0.0352,
            0.0056,
            0.0345,
            -0.0298,
            -0.1034,
            -0.0146
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1103,
            0.0103,
            0.0131,
            0.0827,
            0.0405,
            0.0247,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0011,
            -0.0007,
            0.0003,
            0.0008,
            0.0,
            0.0021,
            -0.0002,
            0.001,
            -0.0,
            0.0001,
            -0.0006,
            0.0014,
            -0.0009,
            -0.0004,
            0.0003,
            0.0012
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1378,
            0.0886,
            -0.161,
            0.0298,
            -0.0249,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    },
    {
      "step": 993,
      "word": "olamide",
      "loss": 2.4828,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.1427,
            0.0728,
            0.1627,
            -0.0334,
            0.265,
            -0.0224,
            0.2775,
            0.077,
            0.0365,
            0.0079,
            0.0677,
            0.085,
            -0.0807,
            -0.1411,
            -0.0918,
            0.0596
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1582,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1862
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.2537,
            -0.261,
            -0.1506,
            -0.2129,
            -0.3164,
            0.1398,
            -0.2879,
            -0.0904,
            0.1177,
            0.1582,
            0.0875,
            -0.292,
            0.0998,
            0.1669,
            0.218,
            -0.1728
          ],
          "after": [
            0.0282,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0792,
            -0.0628,
            0.0774,
            -0.109,
            0.1201,
            -0.0024,
            -0.1804,
            -0.0677,
            -0.0126,
            0.1801,
            0.0971,
            0.0712,
            -0.0663,
            -0.0434,
            -0.1195,
            -0.1721
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1103,
            0.0103,
            0.0131,
            0.0827,
            0.0405,
            0.0247,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0009,
            -0.0031,
            0.0004,
            -0.002,
            0.0004,
            0.0014,
            -0.0025,
            -0.0009,
            0.0001,
            0.003,
            0.0017,
            -0.0007,
            -0.0018,
            0.0012,
            -0.0008,
            -0.0027
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1377,
            0.0886,
            -0.161,
            0.0298,
            -0.0249,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    },
    {
      "step": 994,
      "word": "rolan",
      "loss": 1.9347,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.028,
            0.2043,
            -0.2264,
            0.036,
            -0.2063,
            -0.2095,
            0.0695,
            -0.1125,
            0.0955,
            -0.0577,
            -0.0255,
            0.046,
            -0.1004,
            -0.0066,
            0.1048,
            0.0609
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0238,
            0.1306,
            -0.1557,
            0.0636,
            -0.0627,
            -0.1508,
            0.2927,
            -0.0071,
            -0.212,
            0.0298,
            -0.007,
            -0.0376,
            0.142,
            -0.0898,
            -0.1625,
            0.1575
          ],
          "after": [
            0.0281,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0011,
            -0.0427,
            0.0065,
            -0.0394,
            -0.0116,
            0.1687,
            0.1001,
            -0.0657,
            0.1333,
            -0.0906,
            -0.0972,
            0.0601,
            -0.17,
            -0.1063,
            0.1333,
            0.1233
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1103,
            0.0103,
            0.0131,
            0.0827,
            0.0406,
            0.0247,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0014,
            -0.0019,
            0.0008,
            -0.0012,
            0.0031,
            0.0001,
            -0.0004,
            0.0018,
            0.0005,
            0.0003,
            -0.0006,
            0.0021,
            -0.001,
            -0.0013,
            -0.0002,
            -0.0011
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1377,
            0.0886,
            -0.161,
            0.0298,
            -0.0249,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    },
    {
      "step": 995,
      "word": "jhordan",
      "loss": 2.4728,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2008,
            0.1618,
            -0.0534,
            0.1242,
            -0.0654,
            -0.0148,
            0.0312,
            -0.0744,
            0.125,
            -0.2831,
            -0.1778,
            0.2122,
            -0.156,
            -0.0544,
            0.0743,
            0.2525
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.103,
            0.189,
            -0.1986,
            0.0369,
            0.027,
            -0.2246,
            0.3966,
            0.0706,
            -0.2745,
            -0.0429,
            -0.1066,
            -0.0259,
            0.1719,
            -0.0895,
            -0.1872,
            0.209
          ],
          "after": [
            0.0281,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0004,
            0.0291,
            -0.0203,
            0.1455,
            -0.0368,
            -0.0927,
            0.1704,
            0.0664,
            -0.1713,
            -0.069,
            -0.0055,
            -0.0037,
            0.2341,
            -0.0655,
            -0.0409,
            0.1278
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1104,
            0.0103,
            0.0131,
            0.0827,
            0.0406,
            0.0247,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0028,
            0.0024,
            0.0018,
            0.0,
            0.0002,
            -0.0007,
            0.003,
            -0.0011,
            -0.0004,
            0.0014,
            0.002,
            0.0026,
            -0.0002,
            -0.0035,
            -0.0006,
            -0.0005
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1377,
            0.0886,
            -0.161,
            0.0298,
            -0.0249,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    },
    {
      "step": 996,
      "word": "jenee",
      "loss": 2.1657,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.1619,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.4096,
            -0.3743,
            0.357,
            -0.1368,
            -0.2012,
            0.6725,
            -1.3982,
            -0.3776,
            0.9294,
            -0.4844,
            0.0224,
            0.0533,
            -0.5791,
            0.446,
            1.0068,
            -0.6934
          ],
          "after": [
            0.0281,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.019,
            0.0083,
            -0.0054,
            0.1195,
            -0.0534,
            -0.1512,
            0.1186,
            0.0782,
            -0.1747,
            -0.1229,
            -0.033,
            -0.0503,
            0.282,
            -0.0655,
            -0.0618,
            0.1156
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1104,
            0.0103,
            0.0131,
            0.0827,
            0.0405,
            0.0247,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0003,
            0.0001,
            -0.0021,
            -0.001,
            -0.0022,
            -0.0001,
            -0.0001,
            -0.0009,
            0.0016,
            0.0016,
            -0.0001,
            -0.0,
            -0.0005,
            0.0012,
            0.0021,
            -0.0004
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1377,
            0.0886,
            -0.161,
            0.0298,
            -0.0249,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    },
    {
      "step": 997,
      "word": "alon",
      "loss": 1.9084,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.0898,
            0.057,
            -0.3073,
            -0.1159,
            -0.2034,
            -0.0485,
            -0.2838,
            0.1268,
            -0.0928,
            -0.1807,
            -0.2179,
            0.2776,
            -0.2144,
            0.1931,
            0.3309,
            -0.5132
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.162,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.0031,
            0.0771,
            -0.1397,
            0.0049,
            -0.0277,
            -0.1734,
            0.216,
            -0.0071,
            -0.1622,
            0.0305,
            0.0102,
            -0.0664,
            0.137,
            -0.0725,
            -0.138,
            0.1318
          ],
          "after": [
            0.0281,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.1092,
            -0.0822,
            0.1421,
            -0.1796,
            0.026,
            0.0141,
            -0.4351,
            -0.1316,
            0.2155,
            0.0077,
            0.3012,
            0.0382,
            -0.2183,
            0.0631,
            -0.0573,
            -0.3103
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1104,
            0.0103,
            0.0131,
            0.0827,
            0.0405,
            0.0247,
            -0.1307,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0031,
            -0.0003,
            -0.0013,
            -0.0003,
            0.0023,
            -0.0008,
            0.007,
            0.0025,
            0.0005,
            0.0043,
            0.0015,
            0.0029,
            0.001,
            -0.0051,
            -0.0007,
            0.0004
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1377,
            0.0886,
            -0.161,
            0.0298,
            -0.025,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    },
    {
      "step": 998,
      "word": "oceania",
      "loss": 2.5212,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0577,
            0.1312,
            -0.1636,
            -0.0578,
            -0.1125,
            -0.1377,
            0.0227,
            -0.1064,
            0.0074,
            0.0186,
            0.0115,
            -0.0395,
            -0.0157,
            0.0258,
            0.0131,
            -0.019
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.162,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.2362,
            -0.0726,
            0.3572,
            0.0963,
            0.1482,
            0.1655,
            -0.0887,
            0.0096,
            0.266,
            0.0881,
            0.063,
            0.2861,
            -0.2912,
            -0.0365,
            0.014,
            -0.169
          ],
          "after": [
            0.0281,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.0898,
            -0.0746,
            0.0872,
            -0.1232,
            0.0826,
            -0.0243,
            -0.2444,
            -0.0682,
            0.0269,
            0.152,
            0.0745,
            0.027,
            -0.0945,
            -0.0148,
            -0.0878,
            -0.2024
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1104,
            0.0103,
            0.0131,
            0.0826,
            0.0405,
            0.0247,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0002,
            0.0,
            0.0005,
            0.0017,
            -0.0018,
            -0.0012,
            -0.0021,
            -0.0007,
            -0.0001,
            0.0005,
            0.0007,
            0.0021,
            -0.002,
            -0.0003,
            0.0018,
            -0.0007
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1377,
            0.0886,
            -0.161,
            0.0298,
            -0.025,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    },
    {
      "step": 999,
      "word": "insiya",
      "loss": 2.4642,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            0.0863,
            -0.0111,
            -0.0163,
            -0.0748,
            -0.019,
            -0.0361,
            -0.0248,
            -0.0181,
            -0.0431,
            0.0825,
            0.0791,
            -0.0872,
            0.0471,
            0.0355,
            -0.0201,
            -0.1037
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.162,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            -0.0103,
            -0.037,
            -0.0907,
            -0.0378,
            0.1089,
            -0.0837,
            0.1615,
            0.0237,
            -0.0904,
            0.0764,
            -0.0112,
            -0.003,
            0.0371,
            -0.0624,
            -0.1652,
            0.0675
          ],
          "after": [
            0.0281,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            -0.0591,
            -0.2312,
            0.0952,
            -0.1349,
            0.088,
            0.1336,
            -0.2058,
            0.0509,
            0.0799,
            -0.0046,
            0.041,
            -0.1159,
            -0.1885,
            0.0059,
            0.0186,
            -0.1397
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1104,
            0.0103,
            0.0131,
            0.0826,
            0.0405,
            0.0247,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            0.0001,
            -0.0001,
            -0.0,
            0.0001,
            -0.0004,
            -0.0006,
            -0.0001,
            0.0005,
            0.0004,
            0.0,
            -0.0003,
            0.0004,
            0.0003,
            -0.0001,
            0.0004,
            0.0001
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1377,
            0.0886,
            -0.161,
            0.0298,
            -0.025,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    },
    {
      "step": 1000,
      "word": "akio",
      "loss": 2.6272,
      "learning_rate": 0.0,
      "params": {
        "token_letter_a": {
          "grad": [
            -0.2896,
            0.1663,
            0.0413,
            -0.081,
            0.1151,
            -0.2448,
            0.2345,
            0.0066,
            0.0637,
            -0.0283,
            -0.1182,
            -0.0069,
            0.0557,
            0.0555,
            -0.3086,
            -0.192
          ],
          "after": [
            0.0082,
            -0.0051,
            0.1583,
            -0.0243,
            0.0304,
            -0.0913,
            -0.1172,
            -0.1583,
            -0.009,
            0.073,
            0.162,
            0.0142,
            -0.1026,
            -0.0181,
            -0.0369,
            -0.1863
          ]
        },
        "lm_head_letter_e": {
          "grad": [
            0.019,
            0.0255,
            -0.1066,
            -0.049,
            0.0294,
            -0.1722,
            0.1487,
            0.0331,
            -0.1486,
            0.0494,
            -0.0328,
            -0.1085,
            0.1185,
            0.0012,
            -0.1424,
            0.1045
          ],
          "after": [
            0.0281,
            0.1078,
            -0.041,
            -0.076,
            0.031,
            -0.0098,
            0.0899,
            0.1001,
            -0.0647,
            -0.0616,
            0.0141,
            -0.0172,
            0.1073,
            0.0352,
            -0.052,
            0.2223
          ]
        },
        "position_0": {
          "grad": [
            0.113,
            -0.0512,
            0.1395,
            -0.2113,
            0.0635,
            0.0105,
            -0.4473,
            -0.1607,
            0.2007,
            0.036,
            0.3266,
            0.0419,
            -0.203,
            0.0675,
            -0.1099,
            -0.2949
          ],
          "after": [
            0.0449,
            -0.1468,
            -0.2948,
            0.053,
            0.0304,
            -0.0483,
            -0.1508,
            0.0717,
            0.1104,
            0.0103,
            0.0131,
            0.0826,
            0.0405,
            0.0247,
            -0.1306,
            0.0513
          ]
        },
        "attn_wq_row_0": {
          "grad": [
            -0.0018,
            -0.0022,
            0.0015,
            -0.0002,
            -0.0016,
            0.0006,
            -0.0022,
            0.0025,
            0.0007,
            0.0008,
            0.003,
            0.0035,
            -0.001,
            -0.0031,
            0.0001,
            -0.0026
          ],
          "after": [
            -0.0856,
            0.0089,
            -0.1248,
            -0.0733,
            0.0673,
            0.1377,
            0.0886,
            -0.161,
            0.0298,
            -0.025,
            -0.1493,
            -0.0648,
            0.0939,
            -0.08,
            0.0406,
            0.0856
          ]
        }
      }
    }
  ]
}